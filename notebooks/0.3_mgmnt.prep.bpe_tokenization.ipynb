{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp mgmnt.prep.bpe_tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Optional\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bpe_tokenization\n",
    "> Module to load tokenizers\n",
    "\n",
    "> @Alvaro May 24th 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module provides a wrapper class to work with tokenizers from [SentencePiece](https://github.com/google/sentencepiece) and [HugginFace (transformers)](https://huggingface.co/transformers/main_classes/tokenizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "# export\n",
    "def _check_file_existence(path) -> bool:\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        logging.error('Provided file cannot be found.')\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class CustomTokenizer(ABC):\n",
    "    \"\"\"\n",
    "    Custom wrapper class to handle tokenizers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: Optional[str]=None):\n",
    "        if model_path is None:\n",
    "            self.train_tokenizer()\n",
    "        else:\n",
    "            self.load_tokenizer_model(model_path)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def tokenize_txt(self, txt: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Encode a text fragment according to the available\n",
    "        tokens from the tokenizer model.\n",
    "        \n",
    "        :param txt: Text to be tokenized\n",
    "        :return: List of str containing extracted tokens.\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    @abstractmethod\n",
    "    def encode_txt(self, txt: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Encode a text fragment according to the tokens ids.\n",
    "        \n",
    "        :param txt: Text to be encoded\n",
    "        :return: List of int containing tokens ids.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def tokenize_df(self, df, txt_column) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform tokenization\n",
    "        :param df: DataFrame containing \n",
    "        :param txt_column: Name of the column where to perform tokenization\n",
    "        \n",
    "        :return: DataFrame containing tokenized data\n",
    "        \"\"\"\n",
    "        tokenized_df = df[txt_column].apply(lambda txt: self.tokenize_txt(txt))\n",
    "        return tokenized_df\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load_tokenizer_model(self, model_path: str):\n",
    "        \"\"\"\n",
    "        Load a pre-trained tokenizer model.\n",
    "        :param model_path: Location of the pretrained model to be loaded  \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def train_tokenizer(self):\n",
    "        \"\"\"\n",
    "        Train a tokenizer from scratch\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        \n",
    "        msg = \"Not implemented yet\"\n",
    "        logging.warning(msg)\n",
    "        raise Exception(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class HFTokenizer(CustomTokenizer):\n",
    "    \"\"\"\n",
    "    Custom wrapper of a HuggingFace tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: Optional[str]=None):\n",
    "        super().__init__(model_path)\n",
    "        \n",
    "    def tokenize_txt(self, txt: str) -> List[str]:\n",
    "        return self.tokenizer.encode(txt).tokens\n",
    "    \n",
    "    def encode_txt(self, txt: str) -> List[int]:\n",
    "        return self.tokenizer.encoed(txt).ids\n",
    "        \n",
    "    def load_tokenizer_model(self, model_path: str):\n",
    "        if not _check_file_existence(model_path):\n",
    "            msg = \"HF Tokenizer model could not be loaded.\"\n",
    "            logging.error(msg)\n",
    "            raise Exception(msg)\n",
    "            \n",
    "        self.tokenizer = Tokenizer.from_file(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class SPTokenizer(CustomTokenizer):\n",
    "    \"\"\"\n",
    "    Custom wrapper of SentencePiece tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: Optional[str]):\n",
    "        super().__init__(model_path)\n",
    "        \n",
    "    def tokenize_txt(self, txt: str) -> List[str]:\n",
    "        return self.tokenizer.encode_as_pieces(txt)\n",
    "    \n",
    "    def encode_txt(self, txt: str) -> List[int]:\n",
    "        return self.tokenizer.encode_as_ids(txt)\n",
    "        \n",
    "    def load_tokenizer_model(self, model_path: str):\n",
    "        if not _check_file_existence(model_path):\n",
    "            msg = \"SP model could not be loaded.\"\n",
    "            logging.error(msg)\n",
    "            raise Exception(msg)\n",
    "            \n",
    "        sp_processor = spm.SentencePieceProcessor()\n",
    "        sp_processor.load(model_path)\n",
    "        self.tokenizer = sp_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 0.0_mgmnt.prep.i.ipynb.\n",
      "Converted 0.1_mgmnt.prep.conv.ipynb.\n",
      "Converted 0.3_mgmnt.prep.bpe.ipynb.\n",
      "Converted 0.6_mgmnt.prep.nltk.ipynb.\n",
      "Converted 0.7_mgmnt.prep.files_mgmnt.ipynb.\n",
      "Converted 0.8_mgmnt.prep.bpe_tokenization.ipynb.\n",
      "Converted 1.0_exp.i.ipynb.\n",
      "Converted 1.1_exp.info-[inspect].ipynb.\n",
      "Converted 1.1_exp.info.ipynb.\n",
      "Converted 1.2_exp.csnc.ipynb.\n",
      "Converted 1.2_exp.gen.code.ipynb.\n",
      "Converted 1.3_exp.csnc_python.ipynb.\n",
      "Converted 10.0_utils.clusterization.ipynb.\n",
      "Converted 10.1_utils.visualization.ipynb.\n",
      "Converted 2.0_repr.codebert.ipynb.\n",
      "Converted 2.0_repr.i.ipynb.\n",
      "Converted 2.1_repr.codeberta.ipynb.\n",
      "Converted 2.1_repr.roberta.train.ipynb.\n",
      "Converted 2.2_repr.roberta.eval.ipynb.\n",
      "Converted 2.3_repr.word2vec.train.ipynb.\n",
      "Converted 2.6_repr.word2vec.eval.ipynb.\n",
      "Converted 2.7_repr.distmetrics.ipynb.\n",
      "Converted 2.8_repr.sentence_transformers.ipynb.\n",
      "Converted 3.1_mining.unsupervised.traceability.eda.ipynb.\n",
      "Converted 3.2_mining.unsupervised.eda.traceability.d2v.ipynb.\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "h\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "Converted 3.2_mining.unsupervised.mutual_information.traceability.approach.sacp-w2v.ipynb.\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "h\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "E\n",
      "Converted 3.2_mining.unsupervised.mutual_information.traceability.approach.sacp.w2v.ipynb.\n",
      "Converted 3.2_mutual_information_theory.eval.ipynb.\n",
      "Converted 3.4_facade.ipynb.\n",
      "Converted 4.0_mining.ir.ipynb.\n",
      "Converted 5.0_experiment.mining.ir.unsupervised.d2v.ipynb.\n",
      "Converted 5.0_experiment.mining.ir.unsupervised.w2v-exp4.ipynb.\n",
      "Converted 5.0_experiment.mining.ir.unsupervised.w2v-exp5.ipynb.\n",
      "Converted 5.0_experiment.mining.ir.unsupervised.w2v-exp6.ipynb.\n",
      "Converted 5.0_experiment.mining.ir.unsupervised.w2v.ipynb.\n",
      "Converted 6.0_desc.stats.ipynb.\n",
      "Converted 6.0_eval.mining.ir.unsupervised.x2v.ipynb.\n",
      "Converted 6.1_desc.metrics.java.ipynb.\n",
      "Converted 6.1_desc.metrics.main.ipynb.\n",
      "Converted 6.1_desc.metrics.se.ipynb.\n",
      "Converted 6.2_desc.metrics.java.ipynb.\n",
      "Converted 6.2_desc.metrics.main.ipynb.\n",
      "Converted 7.0_inf.i.ipynb.\n",
      "Converted 7.1_inf.bayesian.ipynb.\n",
      "Converted 7.2_inf.causal.ipynb.\n",
      "Converted 7.3_statistical_analysis.ipynb.\n",
      "Converted 8.0_interpretability.i.ipynb.\n",
      "Converted 8.1_interpretability.error_checker.ipynb.\n",
      "Converted 8.2_interpretability.metrics_python.ipynb.\n",
      "Converted 8.3_interpretability.metrics_java.ipynb.\n",
      "Converted 8.4_interpretability.metrics_example.ipynb.\n",
      "Converted 8.5_interpretability.d2v_vectorization.ipynb.\n",
      "Converted 8.6_interpretability.prototypes_criticisms.ipynb.\n",
      "Converted 8.7_interpretability.info_theory_processing.ipynb.\n",
      "Converted 9.0_ds.causality.eval.traceability.ipynb.\n",
      "Converted 9.0_ds.description.eval.traceability.ipynb.\n",
      "Converted 9.0_ds.prediction.eval.traceability.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
