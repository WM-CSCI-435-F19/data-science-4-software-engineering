# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/0.5_mgmnt.prep.token_mgmnt.ipynb (unless otherwise specified).

__all__ = ['extra_tokens', 'java_reserved_tokens', 'java_operator_tokens', 'java_structural_tokens',
           'java_extra_tokens', 'java_special_tokens', '__replace_tokenizer_toks', 'replace_tokenizer_toks',
           'replace_spec_toks_to_original']

# Cell

import pandas as pd
import re

from typing import Dict, Optional

# Cell
# dicts of special tokens we are adding to the tokenizers so they do not get split

extra_tokens = {"<n>": "\n"}

# from https://docs.oracle.com/javase/tutorial/java/nutsandbolts/_keywords.html
java_reserved_tokens = {
    "<abstract>": "abstract",
    "<assert>": "assert",
    "<boolean>": "boolean",
    "<break>": "break",
    "<byte>": "byte",
    "<case>": "case",
    "<catch>": "catch",
    "<char>": "char",
    "<class>": "class",
    "<const>": "const",
    "<continue>": "continue",
    "<default>": "default",
    "<do>": "do",
    "<double>": "double",
    "<else>": "else",
    "<enum>": "enum",
    "<extends>": "extends",
    "<final>": "final",
    "<finally>": "finally",
    "<float>": "float",
    "<for>": "for",
    "<goto>": "goto",
    "<if>": "if",
    "<implements>": "implements",
    "<import>": "import",
    "<instanceof>": "instanceof",
    "<int>": "int",
    "<interface>": "interface",
    "<long>": "long",
    "<native>": "native",
    "<new>": "new",
    "<package>": "package",
    "<private>": "private",
    "<protected>": "protected",
    "<public>": "public",
    "<return>": "return",
    "<short>": "short",
    "<static>": "static",
    "<strictfp>": "strictfp",
    "<super>": "super",
    "<switch>": "switch",
    "<synchronized>": "synchronized",
    "<this>": "this",
    "<throw>": "throw",
    "<throws>": "throws",
    "<transient>": "transient",
    "<try>": "try",
    "<void>": "void",
    "<volatile>": "volatile",
    "<while>": "while",
}

# from https://docs.oracle.com/javase/tutorial/java/nutsandbolts/opsummary.html
java_operator_tokens = {
    "<=>": "=",
    "<+>": "+",
    "<->": "-",
    "<*>": "*",
    "</>": "/",
    "<%>": "%",
    "<++>": "++",
    "<-->": "--",
    "<!>": "!",
    "<==>": "==",
    "<!=>": "!=",
    "<greater>": ">",
    "<greater_equal>": ">=",
    "<lesser>": "<",
    "<lesser_equal>": "<=",
    "<&&>": "&&",
    "<||>": "||",
    "<?>": "?",
    "<:>": ":",
    "<~>": "~",
    "<double_lesser>": "<<",
    "<double_greater>": ">>",
    "<triple_greater>": ">>>",
    "<&>": "&",
    "<^>": "^",
    "<|>": "|",
}

java_structural_tokens = {
    "<{>": "{",
    "<}>": "}",
    "<[>": "[",
    "<]>": "]",
    "<lesser>": "<",
    "<greater>": ">",
    "<(>": "(",
    "<)>": ")",
    "<;>": ";",
}

java_extra_tokens = {
    "<@>": "@",
    "<...>": "...",
    "<null>": "null",
    "<true>": "true",
    "<false>": "false",
}

# combination of all dictionaries
java_special_tokens = {
    **java_reserved_tokens,
    **java_operator_tokens,
    **java_structural_tokens,
    **java_extra_tokens,
    **extra_tokens,
}

# Cell

def __replace_tokenizer_toks(code_snippet: str) -> str:
    """
    Function to replace special tokens introduced by the tokenizer/model (bos, eos, pad)
    :param code_snippet: String representing the code snippet
    :return: String containing the clean string
    """
    pattern = re.compile("|".join(["<pad>", "<sos>", "<eos>"]))

    clean_snippet = pattern.sub("", code_snippet)
    return clean_snippet

def replace_tokenizer_toks(df: pd.DataFrame, n: Optional[int]=None) -> pd.DataFrame:
    """
    Function to replreace
    :param df: Pandas DataFrame containing the collection of code snippets
    :return: Clean DataFrame
    """

    if n is None:
        n = len(df)
    df = df.iloc[:n].copy()
    df.code = df.code.apply(lambda snippet: __replace_tokenizer_toks(snippet))
    return df

# Cell

def _replace_spec_toks(mthd: str, spec_toks: Dict[str, str]) -> str:
    """
    Performs the replacement of special tokens by the original ones
    """
    spec_toks = spec_toks.copy()
    # Add special tokenizer tokens -> deleted for code analysis
    spec_toks['<bos>'] = ""
    spec_toks['<eos>'] = ""
    spec_toks['<pad>'] = ""

    spec_toks = dict(
        (re.escape(k), v)
        for k, v in sorted(
            spec_toks.items(), key=lambda x: len(x[1]), reverse=True
        )
    )
    # construct regex pattern for finding all special tokens in a method
    pattern = re.compile("|".join(spec_toks.keys()))
    # replace all special tokens in a method
    mthd = pattern.sub(lambda m: spec_toks[re.escape(m.group(0))], mthd)

    mthd = __replace_tokenizer_toks(mthd)
    return mthd

# Cell

def replace_spec_toks_to_original(df: pd.DataFrame, spec_toks: Dict[str, str],
                        n: Optional[int] = None) -> pd.DataFrame:
    if n is None:
        n = len(df)
    df = df.iloc[:n].copy()
    df.code = df.code.apply(lambda m: _replace_spec_toks(m, spec_toks))
    return df