# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/5.2_utils.distances.ipynb (unless otherwise specified).

__all__ = ['logger', 'fhandler', 'formatter', 'CustomDistance', 'EuclideanDistance', 'JaccardDistance', 'kl_divergence',
           'js_divergence', 'statistical_distance']

# Cell

import numpy as np
from abc import ABC, abstractmethod
from typing import Tuple, List, Optional, Any
from .statistics import estimate_bins_number

# Cell

#Logging configuration

import logging
logger = logging.getLogger()
fhandler = logging.FileHandler(filename='mylog.log', mode='a')
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
fhandler.setFormatter(formatter)
logger.addHandler(fhandler)
logger.setLevel(logging.INFO)

# Cell


class CustomDistance(ABC):

    @abstractmethod
    def compute_distance(self, x: np.ndarray, y: np.ndarray) -> float:
        """
        Computes the distance (dissimilarity) metric among 2 real vectors
        """
        pass

# Cell

class EuclideanDistance(CustomDistance):

    def compute_distance(self, x: np.ndarray, y: np.ndarray) -> float:
        return np.linalg.norm(x - y)

# Cell

class JaccardDistance(CustomDistance):
    def compute_distance(self, x: np.ndarray, y: np.ndarray) -> float:
        x_set = set(x)
        y_set = set(y)

        jacc_idx= len(x_set & y_set) / len(x_set | y_set)

        return jacc_idx

# Cell

def kl_divergence(p, q):
    '''
    Kullback-Leibler divergence calculates a score that measures the divergence of one probability distribution from another.
    '''
    return sum(p[i] * np.log2(p[i]/q[i]) for i in range(len(p)))

# Cell

def js_divergence(p, q):
    #https://machinelearningmastery.com/divergence-between-probability-distributions/
    '''Jensen-Shannon Divergence that provides a normalized and symmetrical version of the KL divergence,
    Jensen-Shannon divergence extends KL divergence to calculate a symmetrical score and distance measure of one probability distribution from another.'''
    m = 0.5 * (p + q)
    jensen_divergence = 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)
    jensen_distance = np.sqrt(jensen_divergence)
    return jensen_divergence, jensen_distance

# Cell

# TODO: Check bins estimation --> estimate for vector 1: hmn vector and
# use the same estimation for vector 2

def statistical_distance(dataset1: np.ndarray, dataset2: np.ndarray,
                         bins: Optional[Any]=None) -> Tuple[float, float]:
    """Bining Bootstrapped Arrays

    :param dataset1: first vector to compare (shoould be human-data)
    :param dataset2: second vector to compare (should be generated-data)
    :param bins: Int or array indicating bins distribution for binning
                - If int: quantity of bins to used
                - If array: bin edges for bins
    Returns divergence and distance of given distributions
    """
    total_data = np.concatenate((dataset1, dataset2), axis=None)

    MIN = total_data.min()
    MAX = total_data.max()

    if bins is None:
        # bins = estimate_bins_number(total_data)
        _, bin_edges = np.histogram(total_data)

    # Compute frequency and bins
    control_frequency, control_bins = np.histogram(dataset1, bin_edges, range=[MIN, MAX])
    treat_frequency, treat_bins = np.histogram(dataset2, bin_edges, range=[MIN, MAX])

    #Solving empty bins
    treat_frequency =  np.array([ i / np.sum(treat_frequency) if i != 0 else 1.e-20 for i in treat_frequency])
    control_frequency =  np.array([ i / np.sum(control_frequency) if i != 0 else 1.e-20 for i in control_frequency])

    # Jensen Shannon (T || C) [Divergence & Distance]
    divergence, distance = js_divergence(p=control_frequency, q= treat_frequency)

    logging.info(f"Divergence: {round(divergence,2)}")
    logging.info(f"Distance: {round(distance,2)}")

    return divergence, distance