# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/8.1_codexplainer.error_checker.ipynb (unless otherwise specified).

__all__ = ['logger', 'fhandler', 'formatter', 'fixed_errors', 'regex_errors', 'fixed_errors', 'regex_errors',
           'named_errors', 'warnings', 'name_coincidence_errors', 'jarWrapper', 'process_chars_for_bpes',
           'JavaErrorChecker', 'selected_errors', 'get_error_columns', 'group_error_df', 'JavaErrorAnalyzer',
           'compare_sample_sets_errors', 'gather_errors_data_model', 'get_distances_4_errors_dfs',
           'compute_err_divergence_dimension_wise', 'perform_error_distance_analysis']

# Cell

import pandas as pd
import numpy as np

from pathlib import Path
import os, shutil

from subprocess import *
from tqdm import tqdm

from typing import Optional, List, Tuple, Dict, Callable

from ..utils.distances import EuclideanDistance, JaccardDistance, CustomDistance, statistical_distance
from ..utils.statistics import bootstrapping
from ..mgmnt.prep.files_mgmnt import *
from ..mgmnt.prep.token_mgmnt import *

# Cell

#Logging configuration

import logging
logger = logging.getLogger()
fhandler = logging.FileHandler(filename='mylog.log', mode='a')
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
fhandler.setFormatter(formatter)
logger.addHandler(fhandler)
logger.setLevel(logging.INFO)

# Cell

# Most generic errors --> the ones to be replaced first without loosing most specific errors

fixed_errors = {
    "cannot_find_symbol": "cannot find symbol",
    "non_existing_package": "package [a-zA-Z0-9_\.\$]+ does not exist",
}


# Errors to be found based on regex
regex_errors = {
    "not_visible_package": "package [a-zA-Z0-9_\.]+ is not visible",
    "exception": "^[A-Za-z]+Exception$",
    "special_char_expected": "'?(\(|\)|;|\[|\]|\<|\>)'? expected",
    "non_applicable_method_diff_len": "method .* cannot be applied .* reason: actual and formal argument lists differ in length",
    "non_applicable_method_varargs_mismatch": "method .* cannot be applied .* reason: varargs mismatch",
    "ambiguous_reference": "reference to [A-Za-z]+ is ambiguous",
    "no_suitable_method": "^no suitable method found for",
#     "no_suitable_method_diff_length": "no suitable method found for .* actual and formal argument lists differ in length",
#     "no_suitable_method_varargs_mismatch": "no suitable method found for .* varargs mismatch",
    "non_static_var_from_static_context": "non-static variable .* cannot be referenced from a static context",
    "cannot_be_dereferenced": "<?[A-Za-z]+>? cannot be dereferenced",
    "method_cannot_be_applied_to_given_types": "method .* in class .* cannot be applied to given types",
    "no_suitable_constructor": "^no suitable constructor found for",
    "unexpected_type": "unexpected type(\s)*required:(\s)*([a-zA-Z0-9_]+)(\s)*found:",
    "incomparable_types": "^incomparable types:",
    "deprecated_usage": "has been deprecated and marked for removal$",
    "non_static_method_referenced_from_static_context": "non-static method [a-zA-Z0-9_\.]+\s*\(\s*\)\s*cannot be referenced from a static context",
    "name_clash": "^name clash:",
    "cannot_inherit_from": "^cannot inherit",
    "unclosed_literal": "unclosed [a-zA-Z]+ literal",
    "try_without_catch_finally_declarations": "'try' without 'catch', 'finally' or resource declarations",
}

# Cell

# Most generic errors --> the ones to be replaced first without loosing most specific errors

fixed_errors = {
    "cannot_find_symbol": "cannot find symbol",
    "non_existing_package": "package [a-zA-Z0-9_\.\$]+ does not exist",
}


# Errors to be found based on regex
regex_errors = {
    "not_visible_package": "package [a-zA-Z0-9_\.]+ is not visible",
    "exception": "^[A-Za-z]+Exception$",
    "special_char_expected": "'?(\(|\)|;|\[|\]|\<|\>)'? expected",
    "non_applicable_method_diff_len": "method .* cannot be applied .* reason: actual and formal argument lists differ in length",
    "non_applicable_method_varargs_mismatch": "method .* cannot be applied .* reason: varargs mismatch",
    "ambiguous_reference": "reference to [A-Za-z]+ is ambiguous",
    "no_suitable_method": "^no suitable method found for",
#     "no_suitable_method_diff_length": "no suitable method found for .* actual and formal argument lists differ in length",
#     "no_suitable_method_varargs_mismatch": "no suitable method found for .* varargs mismatch",
    "non_static_var_from_static_context": "non-static variable .* cannot be referenced from a static context",
    "cannot_be_dereferenced": "<?[A-Za-z]+>? cannot be dereferenced",
    "method_cannot_be_applied_to_given_types": "method .* in class .* cannot be applied to given types",
    "no_suitable_constructor": "^no suitable constructor found for",
    "unexpected_type": "unexpected type(\s)*required:(\s)*([a-zA-Z0-9_]+)(\s)*found:",
    "incomparable_types": "^incomparable types:",
    "deprecated_usage": "has been deprecated and marked for removal$",
    "non_static_method_referenced_from_static_context": "non-static method [a-zA-Z0-9_\.]+\s*\(\s*\)\s*cannot be referenced from a static context",
    "name_clash": "^name clash:",
    "cannot_inherit_from": "^cannot inherit",
    "unclosed_literal": "unclosed [a-zA-Z]+ literal",
    "try_without_catch_finally_declarations": "'try' without 'catch', 'finally' or resource declarations",
    "constructor_cannot_be_applied": "^constructor [a-zA-Z0-9\.\_\$]+ .* cannot be applied to given types",
    "cannot_infer_type_for_local_var": "^cannot infer type for local variable",
    "abstract_class_cannot_be_instantiated": "^[a-zA-Z0-9_\.\$\<\>]+ is abstract; cannot be instantiated",
    "type_not_allowed": "'[a-zA-Z0-9_\.\<\>]+' type not allowed here",
    "class_doesnot_override_abst_mthd": "[a-zA-z0-9\.\$\<\>]+ is not abstract and does not override abstract method",
    "element_has_private_access": "^[a-zA-z0-9\.\$\<\>\(\)\s]+ has private access in",
    "element_has_protected_access": "[a-zA-z0-9\.\$\<\>\(\)\s]+ has protected access in",
    "element_cannot_be_accessed_from_outside_pkg": "cannot be accessed from outside package$",
    "illegal_static_declaration": "^Illegal static declaration in inner class",
    "pkg_deprecated_marked_for_removal": "[a-zA-Z0-9_\.\$\<\>]+ in [a-zA-Z0-9_\.\$\<\>]+ has been deprecated and marked for removal",
    "cannot_inherit_from": "^cannot inherit from final java.lang.Module",


}

# Cell

# Errors to be found based on specific names
# Note just a part of the error description is included but it is searched based on str coincidence

named_errors = {
    "method_doesnot_override_implement_super": "method does not override or implement a method",
    "unexpected_lambda": "lambda expression not expected",
    "expected_structure": "class, interface, or enum expected",
    "illegal_parenthesized_expression": "illegal parenthesized expression",
    "incompatible_types": "incompatible types",
    "for_each_not_applicable": "for-each not applicable to expression type",
    "return_required": "return required",
    #"cannot_infer_type_for_local variable": "cannot infer type for local variable",
    "element_cannot_be_accessed_from_outside_the_pkg": "cannot be accessed from outside package",
    "private_access": "has private access in",
    "unclosed_character_literal": "unclosed character literal",
    "not_a_statement": "not a statement"
}


# Warning (errors) to be found based on names (str coincidence)

warnings = {
    "warn_deprecated_API": "uses or overrides a deprecated API",
    "warn_unsafe_operations": "uses unchecked or unsafe operations",
    "xdiags_warning": "recompile with -Xdiags:verbose to get full output",
    "xlint_warning": "Recompile with -Xlint",
    "non_varargs-warning": "non-varargs call of varargs method with inexact argument type for last parameter"
}

# Cell

name_coincidence_errors = {
    **named_errors,
    **warnings
}

# Cell

def jarWrapper(*args):
    """
    Function for executing jar files from python
    :param args: Arguments to be passed for the executed program
    :returns: Output produced by the executed jar
    """
    process = Popen(['java', '-jar']+list(args), stdout=PIPE, stderr=PIPE)
    ret = []
    while process.poll() is None:
        line = process.stdout.readline()
        if line != '' and line.endswith(b'\n'):
            ret.append(line[:-1])
    stdout, stderr = process.communicate()

    ret += stdout.split(b'\n')
    if stderr != '':
        ret += stderr.split(b'\n')

    if '' in ret:
        ret.remove('')
    return ret

# Cell

def process_chars_for_bpes(code_snippet):
    return code_snippet.replace('\n', '<n>').replace('\t', '<t>').replace('@', '<@>')

# Cell

class JavaErrorChecker():
    def __init__(self, jar_path):
        self.jar_path = jar_path

    def perform_analysis(self, code_df, idx_column, error_column):
        """
        Performs the error analysis and gets the report error report
        :param  code_df: Pandas Dataframe with the required structure containing the code to be analyzed
        :param idx_column: Column name to reference the snippets in the generated report
        :param error_column: Column name to reference the error details in the generated report
        """

        self.__create_aux_dirs()
        code_df = self.__process_df(code_df)
        out_csv_name = "error_analysis_report.csv"
        code_csv_name = "code.csv"
        code_df.to_csv(str(self.resources_path/code_csv_name), index=False)
        args = [self.jar_path, code_csv_name, out_csv_name]

        try:
            jar_result = jarWrapper(*args)
            logging.info(f'jar program produced this output: {jar_result}')
        except Exception as e:
            logging.error(f'An error ocurred when trying to execute the jar program')
            #print(e.message, e.args)

        processed_df =  self.__process_results(out_csv_name, idx_column, error_column)
        self.__remove_aux_files()

        return processed_df


    def __process_results(self, out_name, idx_column, error_column):
        errors_report_df = pd.read_csv(str(self.resources_path/out_name), delimiter = '\t')
        clean_df = self.__get_generic_errors(errors_report_df, fixed_errors, regex_errors, name_coincidence_errors, error_column)
        dimension_based_df = self.__get_error_dims_records(clean_df, idx_column, error_column)
        return dimension_based_df

    def __get_generic_errors(self, df, fixed_errors, regex_errors, named_errors, error_column):
        """
        Performs processing for error names
        :param df: Pandas dataframe containing errors info.
        :param fixed_errors: Dictionary containing the names-regex for more generic error names
        :param regex_errors: Dictionary containing regex defining generic error names
        :param named_errors: Dictionary containing generic error names
        :param error_column: Column name to access to access errors in df
        :returns: Processed dataframe
        """
        logging.info('Processing external report.')
        df_copy = df.copy()

        # Replace most generic errors first
        for k, v in fixed_errors.items():
            df_copy.loc[df_copy[error_column].str.contains(v, case=False, regex=True), error_column] = k

        for k, v in regex_errors.items():
            df_copy.loc[df_copy[error_column].str.contains(v, case=False, regex=True), error_column] = k

        for k, v in named_errors.items():
            df_copy.loc[df_copy[error_column].str.contains(v, case=False, regex=False), error_column] = k

        return df_copy


    def __get_error_dims_records(self, df, idx_column, error_column):
        logging.info('Getting error dimensions.')
        # Group/count errors for each class
        grouped_errors = df.groupby([idx_column, error_column])[error_column].count().reset_index(name='count')

        # Turn errors rows into columns (dimensions for vectors)
        pivot_df = grouped_errors.pivot(index=idx_column, columns=error_column, values='count').reset_index()
        pivot_df = pivot_df.fillna(0)

        return pivot_df

    def __remove_aux_files(self):
        """
        Removes auxiliar - tmp directories/files created for the functioning of the external package
        """
        logging.info('Removing auxiliar directories/files.')
        shutil.rmtree(str(self.base_path))

    def __create_aux_dirs(self):
        logging.info('Creating auxiliar directories.')
        self.out_path = Path(".")
        # Configure directories for jar tool
        self.base_path = self.out_path / "java"
        self.resources_path = self.base_path / "resources"
        self.compiled_path = self.resources_path / "compiled"
        self.base_path.mkdir(exist_ok=True)
        self.resources_path.mkdir(exist_ok=True)
        self.compiled_path.mkdir(exist_ok=True)

    def __process_df(self, df):
        """
        Function to process the df according to the format required by the
        java analyzer
        """
        df = df.copy()

        # Add special tokens expected by the analyzer
        df['code'] = df['code'].apply(lambda method: process_chars_for_bpes(method))

        # Format the dataframe appropriately as the analyzer program expects
        columns = df.columns

        if "idx" not in columns:
            idx_column = [i for i in range(df.shape[0])]
            df['idx'] = idx_column
        if "stop_column" not in columns:
            stop_column = ['*stop*' for _ in range(df.shape[0])]
            df['stop_column'] = stop_column

        df = df[['idx', 'code', 'stop_column']]

        return df

# Cell

selected_errors = ['cannot_find_symbol',
    'illegal_parenthesized_expression',
    'incompatible_types',
    'method_doesnot_override_implement_super',
    'non_applicable_method_diff_len',
    'non_existing_package',
    'warn_deprecated_API',
    'warn_unsafe_operations',
    'xdiags_warning',
    'xlint_warning',
    'method_cannot_be_applied_to_given_types',
    'expected_structure',
    'cannot_be_dereferenced',
    'for_each_not_applicable',
    'deprecated_usage',
    'non_static_var_from_static_context',
    'unexpected_lambda',
    'ambiguous_reference',
    'non_applicable_method_varargs_mismatch',
    'no_suitable_constructor',
    'not_visible_package',
    'no_suitable_method',
    'unexpected_type',
    'incomparable_types',
    'invalid method declaration; return type required',
    'private_access',
    'method reference not expected here',
    'non_varargs-warning'
]

# Cell

def get_error_columns(df_columns: List[str]) -> List[str]:
    """
    Get the appropriate columns according to the selected errors
    and present errors in the provided dataset

    :param df_columns: List containing the columns of a given dataset

    :return: List containing appropriate columns.
    """

    return [e for e in df_columns if e != 'ID Class']

# Cell

def group_error_df(error_df: pd.DataFrame, selected_errors) -> pd.DataFrame:
    """
    :param error_df: Pandas Dataframe containing the errors for the code snippet
                     records
    :return: Pandas DataFrame containing the grouped errors
    """
    error_df = error_df.copy()

    actual_columns = [e for e in error_df.columns if e != 'ID Class']
    actual_err_set = set(actual_columns)
    sel_error_set = set(selected_errors)

    present_errors = list(actual_err_set & sel_error_set)
    diff = list(actual_err_set - sel_error_set)

    # present_errors = [e for e in selected_errors if e in actual_columns]

    n_diffs = len(diff)

    other_errors = error_df.loc[:, diff]
    error_df = error_df.loc[:, ['ID Class', *present_errors]]

    # Group errors with less frequency -> other (and count them)
    other_errors['other'] = other_errors.sum(axis=1)
    other_errors['n_others'] = (other_errors[diff] > 0).sum(1)

    error_df['other'] = other_errors['other']
    error_df['n_grouped_errors'] = other_errors['n_others']

    return error_df

# Cell

class JavaErrorAnalyzer:
    def __init__(self, jar_path: str):
        """
        :param jar_path: Location of the jar file containing the tool
        """

        self.java_error_checker = JavaErrorChecker(jar_path)

    def get_errors_java_data(self, df: pd.DataFrame,
                             id_column: Optional[str]='ID Class',
                             error_column: Optional[str]=' error message',
                             base_errors:Optional[list]=selected_errors) -> pd.DataFrame:
        """
        Function to perform the process of getting error-related data
        :param df: Pandas DataFrame containing code snippets
        :param jar_path: Path with the corresponding location of the jar pkg (error detection program)
        :param id_column: Idx column name in the generated report
        :param error_column: Column name of the error in the generated report

        :return: Pandas DataFrame containing the grouped errors
        """


        error_report_df = self.java_error_checker.perform_analysis(df, id_column, error_column)

        grouped_df = group_error_df(error_report_df, base_errors)

        return grouped_df

# Cell

def _get_appropriate_columns(columns_s1: List[str], columns_s2: List[str]):
    s1 = set(columns_s1)
    s2 = set(columns_s2)

    inters = list(s1 & s2)
    s1_only = list(s1 - s2)
    s2_only = list(s2 - s1)

    return inters, s1_only, s2_only

# Cell

def _concat_vectors(intersect_vect, own_vect, outer_vect, is_set1: bool):
    """
    Concatenates the vectors for posterior comparison
    """
    outer_dims = np.zeros(len(outer_vect))

    if not is_set1:
        outer_dims, own_vect = own_vect, outer_dims

    full_vector = np.concatenate([intersect_vect, own_vect, outer_dims],  axis=0)

    return full_vector

# Cell

def compare_sample_sets_errors(sample_set1: pd.DataFrame, sample_set2: pd.DataFrame,
                               distance: CustomDistance,
                               stats: Optional[List]=['mean']) -> List[float]:
    """
    Performs comparison for 2 set of error data (coming from 2 diff. distributions)

    :param sample_set1: pd.DF containing the grouped error info. for 1st sample set
    :param sample_set1: pd.DF containing the grouped error info. for 2nd sample set
    :param distance: CustomDistance object implementing the desired similarity
    :param stats: List indicating the stats of interest for performing comparison

    :return: float indicating the similarity (or distance value)
    """
    comparisons = []

    columns_s1 = get_error_columns(list(sample_set1.columns))
    columns_s2 = get_error_columns(list(sample_set2.columns))

    shared_cols, s1_cols, s2_cols = _get_appropriate_columns(columns_s1, columns_s2)

    for stat in stats:
        error_vect_s1 = sample_set1.describe()[shared_cols].loc[stat].values
        error_vect_s2 = sample_set2.describe()[shared_cols].loc[stat].values

        err_vect_s1_only = sample_set1.describe()[s1_cols].loc[stat].values
        err_vect_s2_only = sample_set2.describe()[s2_cols].loc[stat].values

        full_vector_s1 = _concat_vectors(error_vect_s1, err_vect_s1_only, err_vect_s2_only, is_set1=True)
        full_vector_s2 = _concat_vectors(error_vect_s2, err_vect_s2_only, err_vect_s1_only, is_set1=False)

        distance_value = distance.compute_distance(full_vector_s1, full_vector_s2)
        comparisons.append(distance_value)

    return comparisons

# Cell

def _perform_error_counting(freqs: Dict[str, int], errors_df: pd.DataFrame):
    """
    Accumulate the frequency of errors occurrence
    """

    errors = get_error_columns(list(errors_df.columns))

    for err in errors:
        freqs[err] = 1 if err not in freqs else freqs[err] + 1

# Cell

def _gather_description_stats(errors_list: List[str], exp_stats_dict: Dict, stats: List[str],
             decript_df: pd.DataFrame):
    """
    Gather data related to a set of errors and a set of statistics behavior (e.g., mean, median)

    """
    for err in errors_list:
        if err not in exp_stats_dict:
            exp_stats_dict[err] = {}
            for stat in stats:
                exp_stats_dict[err][stat] = {}

        for stat in stats:
            exp_stats_dict[err][stat] = decript_df[err].loc[stat]

def _gather_stats_for_experiment(gen_errors: pd.DataFrame, hmn_errors: pd.DataFrame,
                                stats: List[str]) -> Tuple[Dict, Dict]:
    """
    Gather the data for 2 sets of errors, regarding the behavior of several statistics (e.g, mean)
    for a single experiment

    :param gen_errors: pd.DF containing errors data for generator model
    :param hmn_errors: pd.DF containing errors data for human data
    :param stats: List containing the stats of interest

    :return: Tuple[Dict, Dict] containing the data for both error samples (model, human)
    """
    gen_err_cols = get_error_columns(list(gen_errors.columns))
    hmn_err_cols = get_error_columns(list(hmn_errors.columns))

    gen_err_descript = gen_errors.describe()
    hmn_err_descript = hmn_errors.describe()

    errors = gen_err_cols + hmn_err_cols
    gen_exp_stats = {}
    hmn_exp_stats = {}

    _gather_description_stats(gen_err_cols, gen_exp_stats, stats, gen_err_descript)
    _gather_description_stats(hmn_err_cols, hmn_exp_stats, stats, hmn_err_descript)

    return gen_exp_stats, hmn_exp_stats

# Cell

def gather_errors_data_model(gen_samples_path: str, hmn_samples_path: str,
                             metrics_tool_jar: str, n_samples: Optional[int]=None,
                             samples_extension: Optional[str]="jsonl",
                             compar_stats: Optional[List]=['mean'],
                             compar_dists: Optional[List]=None,
                             dist_names: Optional[List]=None) -> Tuple:
    """

    """
    check_file_existence(metrics_tool_jar)

    # Utils objects for performing analysis
    java_error_analyzer = JavaErrorAnalyzer(jar_path=metrics_tool_jar)

    # Custom distance objects

    # Data for multiple distances, for multiple stats
    distances_accumulation = {}

    if compar_dists is None:
        compar_dists = []
        dist_names = ["Euclidean", "Jaccard"]

        euclid_dist = EuclideanDistance()
        jacc_dist = JaccardDistance()

        compar_dists.append(euclid_dist)
        compar_dists.append(jacc_dist)

    for dist in dist_names:
        distances_accumulation[dist] = {}
        for stat in compar_stats:
            distances_accumulation[dist][stat] = []

    if len(compar_dists) != len(dist_names):
        logging.error("Comparison distances and names don't match.")
        return

    gen_sample_set = get_files_list(gen_samples_path, samples_extension)
    hmn_sample_set = get_files_list(hmn_samples_path, samples_extension)

    # To gather info. about frequency of errors throughout the series of experiments
    err_freqs = {
        "Model": {},
        "Human": {}
    }

    gen_errors_stats = []
    hmn_errors_stats = []

    if n_samples is None:
        n_samples = len(gen_sample_set)

    # For each experiment
    for i in range(n_samples):
        gen_sample_path = gen_sample_set[i]
        hmn_sample_path = hmn_sample_set[i]

        gen_samples = jsonl_to_dataframe(str(gen_sample_path))
        hmn_samples = jsonl_to_dataframe(str(hmn_sample_path))

        gen_errors = java_error_analyzer.get_errors_java_data(gen_samples)
        hmn_errors = java_error_analyzer.get_errors_java_data(hmn_samples)

        # Perform comparison for the given distances
        for i in range(len(compar_dists)):
            dist = compar_dists[i]
            dist_name = dist_names[i]

            custom_dist = compare_sample_sets_errors(gen_errors, hmn_errors, dist, compar_stats)

            # Gather data for each stat of interest
            for i in range(len(compar_stats)):
                stat = compar_stats[i]
                distances_accumulation[dist_name][stat].append(custom_dist[i])

        gen_exp_stats, hmn_exp_stats = _gather_stats_for_experiment(gen_errors, hmn_errors, compar_stats)

        gen_errors_stats.append(gen_exp_stats)
        hmn_errors_stats.append(hmn_exp_stats)

        _perform_error_counting(err_freqs["Model"], gen_errors)
        _perform_error_counting(err_freqs["Human"], hmn_errors)

    return distances_accumulation, err_freqs, gen_errors_stats, hmn_errors_stats

# Cell

def get_distances_4_errors_dfs(gen_err_df: pd.DataFrame, hmn_err_df: pd.DataFrame,
                               estimation_function: Callable,
                               bootstrap_samples_size: int):
    """
    Compares 2 error sets
    """
    gen_err_dims = get_error_columns(list(gen_err_df.columns))
    hmn_err_dims = get_error_columns(list(hmn_err_df.columns))

    js_dimension_compar = compute_err_divergence_dimension_wise(
        gen_err_df, hmn_err_df,
        gen_err_dims, hmn_err_dims,
        estimation_function, bootstrap_samples_size
    )

    return js_dimension_compar


def compute_err_divergence_dimension_wise(gen_err_df: pd.DataFrame, hmn_err_df: pd.DataFrame,
                           dims_gen_err: List[str], dims_hmn_err: List[str],
                           est_func: Callable, bs_size: int):
    """
    Compute the err divergence for 2 given error sets

    :param gen_err_df: pd.DataFrame containing grouped errors for model set
    :param hmn_err_df: pd.DataFrame containing grouped errors for human set
    :param dims_gen_err: List[str] indicating dimensions present in gen. model for comparison
    :param dims_hmn_err: List[str] indicating dimensions present in human set for comparison
    :param est_function: Callable(function) indicating the estimation function

    :return: Dictionary containing dimension-wise computations of JS divergence and distance
    """
    div_dist_data = {}
    dims = set(dims_gen_err + dims_hmn_err)

    for dimension in dims:
        # Get the data for the specific dimension to be compared, in case
        # any of the 2 sets does not contain the specified dimension, zeros are used as data

        if dimension in dims_gen_err:
            gen_dim_data = gen_err_df[dimension].values
            gen_artificial_dim = False
        else:
            gen_dim_data = np.zeros(len(gen_err_df))
            gen_artificial_dim = True

        if dimension in dims_hmn_err:
            hmn_dim_data = hmn_err_df[dimension].values
            hmn_artificial_dim = False
        else:
            hmn_dim_data = np.zeros(len(hmn_err_df))
            hmn_artificial_dim = True
#
#         logging.info("Starting bootstrapping.")
#         bs_gen_dim_data = bootstrapping(gen_dim_data, est_func, sample_size=bs_size)
#         bs_hmn_dim_data = bootstrapping(hmn_dim_data, est_func, sample_size=bs_size)

#         bs_gen_dim = bs_gen_dim_data['bootstrap_repl'].values
#         bs_hmn_dim = bs_hmn_dim_data['bootstrap_repl'].values

        logging.info("Starting JS computations.")
        # Bins setting is estimated via Freedman–Diaconis rule
        js_div, js_dist = statistical_distance(hmn_dim_data, gen_dim_data)
        div_dist_data[dimension] = {
            "JS-Divergence": js_div,
            "JS-Distance": js_dist
        }

    return div_dist_data

# Cell

def perform_error_distance_analysis(gen_samples_path: str, hmn_samples_path: str,
                                    metrics_tool_jar: str, n_experiments: Optional[int]=None,
                                    samples_extension: Optional[str]="jsonl",
                                    compar_stats: Optional[List]=['mean'],
                                    dist_names: Optional[List]=None,
                                    np_est_function: Optional[Callable]=np.mean,
                                    bootstrap_samples_size: Optional[int]=500
                                    ) -> Tuple:
    """
    Perform error-based analysis for Generated data vs Real data

    :return: Tuple[Array, Dict, Array, Array]: [experiments_distances, error_freqs, gen_err_stats, hmn_err_stats]
             Containing
             - experiment distances: (accumulation over collection of experiments)
             - error freqs: Frequency behavior of errors over all the experiments (mere counting)
             - gen error stats: Description of errors behavior throughout the experiments (based on certain stats [e.g. mean])
                                for generator samples
             - hmn error stats: Description of errors behavior throughout the experiments (based on certain stats [e.g. mean])
                                for human samples
    """

    check_file_existence(metrics_tool_jar)

    # Utils objects for performing analysis
    # Build the Error analyzer tool

    logging.info("Building error analyzer tool.")
    java_error_analyzer = JavaErrorAnalyzer(jar_path=metrics_tool_jar)

    # Get sample file names
    gen_sample_set = get_files_list(gen_samples_path, samples_extension)
    hmn_sample_set = get_files_list(hmn_samples_path, samples_extension)

    # Collection of dimension-based comparison (JS divergence & distance)
    # for each experiment
    distance_data = []

    # To gather info. about frequency of errors throughout the series of experiments
    err_freqs = {
        "Model": {},
        "Human": {}
    }

    gen_errors_stats = []
    hmn_errors_stats = []

    # For each experiment (i.e, couple of compared sample sets)
    for i in tqdm(range(n_experiments)):
        gen_sample_path = gen_sample_set[i]
        hmn_sample_path = hmn_sample_set[i]

        gen_samples = jsonl_to_dataframe(str(gen_sample_path))
        hmn_samples = jsonl_to_dataframe(str(hmn_sample_path))

        gen_samples = replace_spec_toks_to_original(gen_samples, java_special_tokens)
        hmn_samples = replace_spec_toks_to_original(hmn_samples, java_special_tokens)

        logging.info('Gen samples: ')
        logging.info(gen_samples.head())

        logging.info('Human samples: ')
        logging.info(hmn_samples.head())

        logging.info("Gathering errors for samples.")
        gen_errors = java_error_analyzer.get_errors_java_data(gen_samples)
        hmn_errors = java_error_analyzer.get_errors_java_data(hmn_samples)

        logging.info("Starting comparison computation.")

        dimension_comparison = get_distances_4_errors_dfs(
            gen_errors, hmn_errors,
            np_est_function,
            bootstrap_samples_size
        )

        distance_data.append(dimension_comparison)

        # Accumulate errors frequency over all experiments
        # Grouped by model and human data

        logging.info("Counting error freqs.")

        _perform_error_counting(err_freqs["Model"], gen_errors)
        _perform_error_counting(err_freqs["Human"], hmn_errors)

        logging.info("Gathering individual error freqs.")
        gen_exp_stats, hmn_exp_stats = _gather_stats_for_experiment(gen_errors, hmn_errors, compar_stats)

        gen_errors_stats.append(gen_exp_stats)
        hmn_errors_stats.append(hmn_exp_stats)

    return distance_data, err_freqs, gen_errors_stats, hmn_errors_stats