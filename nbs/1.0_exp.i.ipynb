{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp exp.i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of your data\n",
    "\n",
    "> This module comprises all the statistical and inference techniques to describe the inner properties of software data. The submodules might include:\n",
    ">\n",
    "> - Descriptive statistics\n",
    "> - Software Metrics\n",
    "> - Information Theory\n",
    "> - Learning Principels Detection (Occams' Razor, Biased data, and Data Snooping)\n",
    "> - Inference: Probabilistic and Causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import sentencepiece as sp\n",
    "import dit\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.stats import sem, t\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import statistics as stat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# TODO: Remove when mongo call is implemented\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOVED NECESSARY PIECES TO 08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace with actual mongo call\n",
    "\"\"\"\n",
    "\n",
    ":returns: dataframe\n",
    "\"\"\"\n",
    "def simulate_getting_dataframes_from_mongo():\n",
    "    corpus_data = {'file_name': [], 'data_type': [], 'contents': []}\n",
    "    path = \"./test_data/LibEST_semeru_format/requirements\"\n",
    "    for file in os.listdir(path):\n",
    "        corpus_data['file_name'].append(file)\n",
    "        corpus_data['data_type'].append('req')\n",
    "        with open (os.path.join(path, file), \"r\") as f:\n",
    "            corpus_data['contents'].append(f.read())\n",
    "    path = \"./test_data/LibEST_semeru_format/source_code\"\n",
    "    for file in os.listdir(path):\n",
    "        corpus_data['file_name'].append(file)\n",
    "        corpus_data['data_type'].append('src')\n",
    "        with open (os.path.join(path, file), \"r\") as f:\n",
    "            corpus_data['contents'].append(f.read())\n",
    "    path = \"./test_data/LibEST_semeru_format/test\"\n",
    "    for file in os.listdir(path):\n",
    "        corpus_data['file_name'].append(file)\n",
    "        corpus_data['data_type'].append('test')\n",
    "        with open (os.path.join(path, file), \"r\") as f:\n",
    "            corpus_data['contents'].append(f.read())\n",
    "    corpus_df = pd.DataFrame(data = corpus_data)\n",
    "    return corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Form does not differ from the void, and the vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Form is the void, and the void is form.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            contents\n",
       "0  Form does not differ from the void, and the vo...\n",
       "1            Form is the void, and the void is form."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hide\n",
    "#This isn't actually used so we don't need to test it but here you can see how a data frame is organized\n",
    "#df = simulate_getting_dataframes_from_mongo()\n",
    "datas = [\"Form does not differ from the void, and the void does not differ from the form.\", \"Form is the void, and the void is form.\"]\n",
    "df = pd.DataFrame(data=datas,columns=['contents'] )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\"\"\"\n",
    "Converts a dataframe into a text file that SentencePiece can use to train a BPE model\n",
    ":param df: a pandas dataframe \n",
    ":param output: name of the output txt file to stored the contents of dataframe\n",
    ":param cols: names of all columns in df dataframe\n",
    ":returns: full name of output text file\n",
    "\"\"\"\n",
    "def df_to_txt_file(df, output, cols):\n",
    "    if cols is None: cols = list(df.columns)\n",
    "    merged_df = pd.concat([df[col] for col in cols])\n",
    "  \n",
    "    with open(output + '_text.txt', 'w') as f:\n",
    "        f.write('\\n'.join(list(merged_df)))\n",
    "    return output + '_text.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_to_txt_file(df,'alexTest', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ExpTest\n",
    "\n",
    "#Create a file\n",
    "df_to_txt_file(df,'output', None)\n",
    "#Is it there?\n",
    "test_bool = os.path.isfile('./output_text.txt')\n",
    "assert(test_bool == True)\n",
    "\n",
    "os.remove('./output_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\"\"\"\n",
    "Trains a SentencePiece BPE model from a pandas dataframe\n",
    ":param df: pandas dataframe\n",
    ":param output: output text file\n",
    ":param model_name: name of the bpe model to be trained\n",
    ":param cols: columns of dataframe\n",
    ":returns: trained bpe model\n",
    "\"\"\"\n",
    "def gen_sp_model(df, output, model_name, cols=None):\n",
    "    fname = df_to_txt_file(df, output, cols)\n",
    "    sp.SentencePieceTrainer.train(f'--input={fname} --model_prefix={output + model_name} --hard_vocab_limit=false --model_type=bpe')\n",
    "    return output + model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ExpTest\n",
    "\n",
    "#Here we test the gen sp model method to see if it properly creates files\n",
    "actual = gen_sp_model(df,'output','_sp_bpe_modal',cols=[\"contents\"])\n",
    "expected = os.path.isfile('./' + actual + '.model')\n",
    "assert(expected)\n",
    "os.remove('./' + actual + '.model')\n",
    "expected2 = os.path.isfile('./'+ actual + '.vocab')\n",
    "assert(expected2)\n",
    "os.remove('./'+ actual + '.vocab')\n",
    "\n",
    "os.remove('./output_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\"\"\"\n",
    "Encodes text using a pre-trained sp model, returns the occurrences of each token in the text\n",
    ":param text: text that we need information on token counts\n",
    ":param model_prefix: trained model to process text\n",
    ":returns: documents lists of all entries in the dataframe \n",
    "\"\"\"\n",
    "def encode_text(text, model_prefix):\n",
    "    ''''''\n",
    "    sp_processor = sp.SentencePieceProcessor()\n",
    "    sp_processor.Load(f\"{model_prefix}.model\")\n",
    "    token_counts = Counter()\n",
    "    encoding = sp_processor.encode_as_pieces(text)\n",
    "    for piece in encoding:\n",
    "        token_counts[piece] += 1\n",
    "    return token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'▁the': 3, '▁does': 2, '▁not': 2, '▁differ': 2, '▁from': 2, '▁void': 2, '▁Form': 1, ',': 1, '▁and': 1, '▁form': 1, '.': 1})\n"
     ]
    }
   ],
   "source": [
    "#ExpTest\n",
    "\n",
    "#This tests the encode text method assuming the sp model(?)\n",
    "\n",
    "sp_model = gen_sp_model(df,'output','_sp_bpe_modal',cols=[\"contents\"])\n",
    "\n",
    "actual = encode_text(\"Form does not differ from the void, and the void does not differ from the form.\", 'output_sp_bpe_modal')\n",
    "\n",
    "expected = Counter({'▁the': 3, '▁does': 2, '▁not': 2, '▁differ': 2, '▁from': 2, '▁void': 2, '▁Form': 1, ',': 1, '▁and': 1, '▁form': 1, '.': 1})\n",
    "#Clean up Test\n",
    "assert(actual == expected)\n",
    "print(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#Notes: token_counts is a counter object.\n",
    "\"\"\"\n",
    "Takes in a counter object of token occurrences, computes the entropy of the corpus that produced it\n",
    ":param token_counts: counter object of token information \n",
    ":returns: a number representing entropy\n",
    "\"\"\"\n",
    "def dit_shannon(token_counts):\n",
    "    num_tokens = 0\n",
    "    for token in token_counts:\n",
    "        num_tokens += token_counts[token]\n",
    "    outcomes = list(set(token_counts.elements()))\n",
    "    frequencies = []\n",
    "    for token in token_counts:\n",
    "        frequencies.append((token_counts[token])/num_tokens)\n",
    "    d = dit.ScalarDistribution(outcomes, frequencies)\n",
    "    return dit.shannon.entropy(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6783847516036627\n",
      "3.6783847516036627\n",
      "Counter({' ': 15, 'o': 10, 'd': 7, 'e': 7, 'f': 7, 'r': 6, 't': 5, 'm': 4, 'i': 4, 'n': 3, 'h': 3, 's': 2, 'v': 2, 'F': 1, ',': 1, 'a': 1, '.': 1})\n"
     ]
    }
   ],
   "source": [
    "#ExpTest\n",
    "#This test takes a sample string and creates a counter that it runs dit shannon on to verify the shannon functionality\n",
    "sample_txt = \"Form does not differ from the void, and the void does not differ from the form.\"\n",
    "sample_tcnt = Counter(sample_txt)\n",
    "\n",
    "answer = dit_shannon(sample_tcnt)\n",
    "expected = 3.6783847516036627\n",
    "#assert(answer == expected)\n",
    "print(answer)\n",
    "print(expected)\n",
    "print(sample_tcnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\"\"\"\n",
    "Calculate entropies for each entry in a dataframe column\n",
    ":param df: dataframe of content that need to be processed\n",
    ":param col: columns of the dataframe\n",
    ":param model_prefix: name of bpe model\n",
    ":returns: a list of the entropies of each entry in a dataframe column\n",
    "\"\"\"\n",
    "def entropies_of_df_entries(df, col, model_prefix):\n",
    "    '''Returns a list of the entropies of each entry in a dataframe column'''\n",
    "    entropies = []\n",
    "    for data in df[col]:\n",
    "        token_counts= encode_text(data, model_prefix)\n",
    "        entropies.append(dit_shannon(token_counts))\n",
    "    return entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ExpTest\n",
    "#Entropies of df test\n",
    "enty = entropies_of_df_entries(df,'contents','output_sp_bpe_modal')\n",
    "\n",
    "entropy1 = dit_shannon(Counter({'▁the': 3, '▁does': 2, '▁not': 2, '▁differ': 2, '▁from': 2, \n",
    "                                '▁void': 2, '▁Form': 1, ',': 1, '▁and': 1, '▁form': 1, '.': 1}))\n",
    "\n",
    "entropy2 = dit_shannon(Counter({'▁is': 2, '▁the': 2, '▁void': 2, '▁Form': 1, ',': 1, '▁and': 1, '▁form': 1, '.': 1}))\n",
    "\n",
    "assert(enty == [entropy1, entropy2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Finish this such that is finds the entropy of the entire corpus \\\n",
    "#       and preserves the individual token frequencies so that we can   \\\n",
    "#      compute the most common tokens\n",
    "\n",
    "# def entropy_of_whole_corpus(df, col, model_prefix):\n",
    "#     '''Returns a dictionary of the entropies of each token in a dataframe corpus'''\n",
    "#     entropies = {}\n",
    "#     token_counts = encode_text(pd.concat[col], model_prefix)\n",
    "#     entropies.append(dit_shannon(token_counts))\n",
    "#     return entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FLAG DELETE\n",
    "# TODO: Do we need this function?\n",
    "import math\n",
    "def manual_shannon(token_freqs):\n",
    "    sum = 0\n",
    "    for i in token_freqs:\n",
    "        sum += i * math.log(1/i, 2)\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FLAG DELETE\n",
    "# TODO: Do we need this function?\n",
    "def sort_token_data(token_data):\n",
    "    return sorted(token_data.items() ,  key=lambda x: x[1][\"Occurrences\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORATORY ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIBest Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hide\n",
    "# Create a dataframe of the requirements, source code and test case data for LIBest\n",
    "# Create a sentencepiece model using the entire LIBest corpus\n",
    "LIB_corpus_df = simulate_getting_dataframes_from_mongo()\n",
    "LIB_model = gen_sp_model(LIB_corpus_df, output='LIBest', model_name='_sp_bpe_modal', cols=['contents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Individual Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hide\n",
    "# Use the model to compute each file's entropy\n",
    "LIB_entropies = entropies_of_df_entries(LIB_corpus_df, 'contents', LIB_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max entropy: 8.736686724450998\n",
      "Min entropy: 3.979797585487148\n",
      "Average entropy: 6.9103334297633525\n",
      "Median entropy: 7.086176924292342\n",
      "Entropy Standard Deviation: 1.069417835431708\n",
      "95% of entropies fall within 6.681088254680821 and 7.139578604845884\n"
     ]
    }
   ],
   "source": [
    "#Hide\n",
    "# Calculate metrics on the LIBest corpus entropies\n",
    "#print(\"Max entropy:\", max(LIB_entropies))\n",
    "#print(\"Min entropy:\", min(LIB_entropies))\n",
    "#print(\"Average entropy:\", mean(LIB_entropies))\n",
    "#print(\"Median entropy:\", stat.median(LIB_entropies))\n",
    "\n",
    "#print(\"Entropy Standard Deviation:\", std(LIB_entropies))\n",
    "\n",
    "#confidence = 0.95\n",
    "#n = len(LIB_entropies)\n",
    "#m = mean(LIB_entropies)\n",
    "#std_err = sem(LIB_entropies)\n",
    "#h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "\n",
    "#start = m - h\n",
    "#end = m + h\n",
    "#print(f\"95% of entropies fall within {start} and {end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Hide\n",
    "# Create a histogram of the entropy distribution\n",
    "#plt.hist(LIB_entropies, bins = 20)\n",
    "#plt.ylabel(\"Num Files\")\n",
    "#plt.xlabel(\"Entropy\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the Corpus as a Whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
