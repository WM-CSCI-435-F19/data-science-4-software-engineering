{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "addressed-greek",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils.statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "human-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "from typing import Callable, Optional, List\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "spectacular-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "#Logging configuration\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='mylog.log', mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-strategy",
   "metadata": {},
   "source": [
    "## statistics\n",
    "\n",
    "> Generic interface for defining distance metrics for multiple purposes (clustering, sample sets comparison, etc).\n",
    "\n",
    "> Functions for computing divergence metrics (e.g., KL div, Jensen Shannon divergence)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-support",
   "metadata": {},
   "source": [
    "### Utils - Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-decrease",
   "metadata": {},
   "source": [
    "Estimation for number of bins using <i>Freedman–Diaconis rule</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "systematic-lounge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def estimate_bins_number(data: np.ndarray) -> int:\n",
    "    \"\"\"Using the Freedman–Diaconis rule\"\"\"\n",
    "    # logging.info(f\"Estimating bins number for {data}\")\n",
    "    iqr = get_iqr(data)\n",
    "    h = (2 * iqr) / (len(data) ** (1/3))\n",
    "    max_ = data.max()\n",
    "    min_ = data.min()\n",
    "    \n",
    "    n_bins = (max_ - min_) / h\n",
    "    \n",
    "    if np.isposinf(n_bins) or np.isneginf(n_bins) or np.isnan(n_bins):\n",
    "        return 1\n",
    "    \n",
    "    return int(np.ceil(n_bins))\n",
    "    \n",
    "def get_iqr(data: np.ndarray) -> float:\n",
    "    q75, q25 = np.percentile(data, [75 ,25])\n",
    "    iqr = q75 - q25\n",
    "    return iqr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-underwear",
   "metadata": {},
   "source": [
    "Bootstrapping-based estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "soviet-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def bootstrapping( np_data: np.ndarray, np_func: Callable,\n",
    "                  n_resamples: Optional[int]=500, sample_size: Optional[int]=None,\n",
    "                  columns: Optional[List[str]]=['bootstrap_repl'],\n",
    "                  full_zeros: Optional[bool]=False) -> pd.DataFrame:\n",
    "    \"\"\"Estimates a based on bootstrapped samples given data and a function\n",
    "    For instance, a bootstrap sample of means, or medians. \n",
    "    The bootstrap samples are as long as the original set size,\n",
    "    additionally, bootstrapping is performed based on resampling with replacement (np.random.choice)\n",
    "    \"\"\"\n",
    "    \n",
    "    if sample_size is None:\n",
    "        #Creating the boostrapped estimates based on replicates as big as the orignal data size\n",
    "        sample_size = len(np_data)\n",
    "        \n",
    "    #Cleaning NaNs\n",
    "    np_data_clean = np_data[ np.logical_not( np.isnan(np_data) ) ] \n",
    "    \n",
    "    if full_zeros:\n",
    "        bootstrap_repl = [ 0.0  for _ in range(n_resamples) ]\n",
    "    else:\n",
    "        bootstrap_repl = [ np_func( np.random.choice( np_data_clean, size=sample_size ) ) for _ in range(n_resamples) ]\n",
    "    \n",
    "    # Provide info. about empirical and bootstrapped mean\n",
    "    logging.info(\"Empirical Mean: \" + str(np.mean(np_data_clean)))\n",
    "    logging.info( \"Bootstrapped Mean: \" + str( np.mean(bootstrap_repl) ) )\n",
    "    \n",
    "    return pd.DataFrame(bootstrap_repl, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def perform_dim_wise_bootstrapping(data_df: pd.DataFrame,\n",
    "                                   sample_size: Optional[int]=500,\n",
    "                                   est_funct: Optional[Callable]=np.mean\n",
    "                                   ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Bootstrap dataframe for each dimension in the given df\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    data_df: DataFrame\n",
    "        dataset containing the data to be bootstrapped\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    bs_data_df: DataFrame\n",
    "        Dataset containing boostrapped data\n",
    "    \"\"\"\n",
    "    dims = list(data_df.columns)\n",
    "    data_dict = {}\n",
    "    \n",
    "    for dim in dims:\n",
    "        dim_data = data_df[dim].values\n",
    "        zeros_data = verify_if_zeros(dim_data)\n",
    "\n",
    "        bs_dim_data = bootstrapping(dim_data, np.mean, sample_size=500, full_zeros=zeros_data)\n",
    "        bs_data = bs_dim_data['bootstrap_repl'].values\n",
    "        data_dict[dim] = bs_data\n",
    "        \n",
    "    return pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def confidence_intervals(np_data, conf=0.95):\n",
    "    \"\"\"Confidence Intervals with Gaussian Distribution (n>=30)\"\"\"\n",
    "    CI = st.norm.interval(alpha=conf, loc = np.mean( np_data ), scale = st.sem(np_data))\n",
    "    logging.info( CI )\n",
    "    return CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "chicken-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def get_z_confidence_interval(data: np.ndarray, crit_value=2.575):\n",
    "    \"\"\"\n",
    "    Get the z-interval for a given sample set, using the provided critical value\n",
    "    \n",
    "    :param data: np.ndarray containing data for estimating the confidence interval\n",
    "    :param crit_value: float indicating the critical value for the desired confidence\n",
    "    :return: Tuple[float, float]\n",
    "             - Containing min and max values of the interval\n",
    "    \"\"\"\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    \n",
    "    int_min = mean - (crit_value * (std/np.sqrt(len(data))))\n",
    "    int_max = mean + (crit_value * (std/np.sqrt(len(data))))\n",
    "    \n",
    "    return int_min, int_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "quantitative-northeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "def get_t_confidence_interval(data: np.ndarray, crit_value):\n",
    "    \"\"\"\n",
    "    Get the t-interval for a given sample set, using the provided critical value\n",
    "    \n",
    "    :param data: np.ndarray containing data for estimating the confidence interval\n",
    "    :param crit_value: float indicating the critical value for the desired confidence\n",
    "    :return: Tuple[float, float]\n",
    "             - Containing min and max values of the interval\n",
    "    \"\"\"\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    \n",
    "    int_min = mean - (crit_value * (std/np.sqrt(len(data))))\n",
    "    int_max = mean + (crit_value * (std/np.sqrt(len(data))))\n",
    "    \n",
    "    return int_min, int_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-landing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def get_all_t_confidence_intervals(data_df: pd.DataFrame,\n",
    "                                   crit_value: Optional[float]=2.364\n",
    "                                   ) -> Dict:\n",
    "    \"\"\"\n",
    "    Get the dimension-wise confidence intervals using t-values\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_df: pd.DataFrame\n",
    "        Data for interval estimation (dimension-wise)\n",
    "    crit_value: Optional[float]\n",
    "        default: 2.364\n",
    "        Crit. value to compute confidence intervals\n",
    "        \n",
    "    Returns\n",
    "    result: Dict\n",
    "       Dictionary containing tuples with the edge values\n",
    "       of the intervals\n",
    "    -------\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for dim in list(data_df.columns):\n",
    "        min_int, max_int = get_t_confidence_interval(data_df[dim].values, crit_value)\n",
    "        result[dim] = (min_int, max_int)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "constitutional-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def min_max_normalize_df(df: pd.DataFrame):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        \n",
    "        if max_value == min_value:\n",
    "            # No need to normalize, all values would be equal\n",
    "            continue\n",
    "            \n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "refined-white",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "def min_max_scale_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform min-max standarization of the provided dataset\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    df: DataFrame\n",
    "        Dataset to be standarized\n",
    "    Returns\n",
    "    -----------\n",
    "    Standarized dataset\n",
    "    \"\"\"\n",
    "    # create a scaler object\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(df.values)\n",
    "    # fit and transform the data\n",
    "    df_norm = pd.DataFrame(scaler.transform(df.values), columns=df.columns)\n",
    "    return df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def min_max_dimwise_scale_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform dimensionwise min-max standarization\n",
    "    for the provided dataset\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    df: Dataframe\n",
    "        Dataset to be standarized\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    Standarized data frame\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    dimensions = list(df.columns)\n",
    "    \n",
    "    scaled_data = {}\n",
    "    for dim in dimensions:\n",
    "        scaler.fit(df[dim].values)\n",
    "        scaled_data[dim] = scaler.transform(df[dim].values)\n",
    "        \n",
    "    return pd.DataFrame(scaled_data.items(), columns=dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def get_t_confidence_interval(data: np.ndarray, crit_value):\n",
    "    \"\"\"\n",
    "    Get the t-interval for a given sample set, using the provided critical value\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: np.ndarray\n",
    "        Array containing data for estimating the confidence interval\n",
    "    crit_value: float\n",
    "        value indicating the critical value for the desired confidence\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Tuple(float, float)\n",
    "        Edge values for the confidence interval\n",
    "    \"\"\"\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    \n",
    "    int_min = mean - (crit_value * (std/np.sqrt(len(data))))\n",
    "    int_max = mean + (crit_value * (std/np.sqrt(len(data))))\n",
    "    \n",
    "    return int_min, int_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "suffering-popularity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 0.1_mgmnt.prep.ipynb.\n",
      "Converted 0.2_mgmnt.prep.files_mgmnt.ipynb.\n",
      "Converted 0.3_mgmnt.prep.bpe_tokenization.ipynb.\n",
      "Converted 0.4_mgmnt.prep.tokenization_counting.ipynb.\n",
      "Converted 0.5_mgmnt.prep.token_mgmnt.ipynb.\n",
      "Converted 1.1_exp.info.ipynb.\n",
      "Converted 1.2_exp.desc.metrics.java.ipynb.\n",
      "Converted 1.4_exp.metrics_python.ipynb.\n",
      "Converted 1.5_exp.metrics_java.ipynb.\n",
      "Converted 2.0_repr.codebert.ipynb.\n",
      "Converted 2.0_repr.i.ipynb.\n",
      "Converted 2.1_repr.codeberta.ipynb.\n",
      "Converted 2.1_repr.roberta.train.ipynb.\n",
      "Converted 2.2_repr.roberta.eval.ipynb.\n",
      "Converted 2.3_repr.word2vec.train.ipynb.\n",
      "Converted 2.6_repr.word2vec.eval.ipynb.\n",
      "Converted 2.7_repr.distmetrics.ipynb.\n",
      "Converted 2.8_repr.sentence_transformers.ipynb.\n",
      "Converted 3.1_traceability.unsupervised.eda.ipynb.\n",
      "Converted 3.2_traceability.unsupervised.approach.d2v.ipynb.\n",
      "Converted 3.2_traceability.unsupervised.approach.w2v.ipynb.\n",
      "Converted 4.0_infoxplainer.ir.ipynb.\n",
      "Converted 4.1_infoxplainer.ir.unsupervised.d2v.ipynb.\n",
      "Converted 4.2_infoxplainer.ir.unsupervised.w2v.ipynb.\n",
      "Converted 4.3_infoxplainer.ir.eval.x2v.ipynb.\n",
      "Converted 4.4_infoxplainer.causality.eval.traceability.ipynb.\n",
      "Converted 4.5_infoxplainer.description.eval.traceability.ipynb.\n",
      "Converted 4.6_infoxplainer.prediction.eval.traceability.ipynb.\n",
      "Converted 5.0_utils.clusterization.ipynb.\n",
      "Converted 5.1_utils.visualization.ipynb.\n",
      "Converted 5.2_utils.distances.ipynb.\n",
      "Converted 5.3_utils.plotting.ipynb.\n",
      "Converted 5.3_utils.statistics.ipynb.\n",
      "Converted 8.1_codexplainer.error_checker.ipynb.\n",
      "Converted 8.2_codexplainer.metrics.ipynb.\n",
      "Converted 8.4_codexplainer.metrics_example.ipynb.\n",
      "Converted 8.5_codexplainer.d2v_vectorization.ipynb.\n",
      "Converted 8.6_codexplainer.prototypes_criticisms.ipynb.\n",
      "Converted 8.7_codexplainer.mutual_info.ipynb.\n",
      "Converted 8.8_codexplainer.utils.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
