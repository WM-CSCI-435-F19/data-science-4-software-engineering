{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp ds.description.eval.traceability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical Methods for Software Traceability: Description\n",
    "\n",
    "> This module is dedicated to evaluate word2vec/doc2vec or any neural unsupervised approaches on traceability datasets by using the theory of information transmission.\n",
    "> Implementing descriptive analysis\n",
    "> Author: @danaderp Feb 2021\n",
    "\n",
    "Software Traceability is analyzed in three classes of tasks ways: description, prediction, and causal inference. \n",
    "\n",
    "## Description:\n",
    "### Descriptive Analysis of Traceability Distance\n",
    "\n",
    "### Descriptive Analysis of Information Transmission\n",
    "\n",
    "\n",
    " \n",
    "We rely on information theory and manifold analysis to evaluate software retrieval unsupervised:\n",
    "- Information Theory for Software Information Retrieval (Shared Information and Mutual Information)\n",
    "- Prototypes and Criticisms for Paragraph Vectors \n",
    "\n",
    "## Case Studies of Info Transmission Traceability\n",
    "- Case 0: Overlapping Information\n",
    "- Case 1: Edge Cases of Self-Information\n",
    "- Case 2: Minimum and maximum loss\n",
    "- Case 3: Minimum and maximum noise\n",
    "- Case 4: Orphan informative links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import ds4se as ds\n",
    "from ds4se.mining.ir import VectorizationType\n",
    "from ds4se.mining.ir import SimilarityMetric\n",
    "from ds4se.mining.ir import EntropyMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from itertools import product \n",
    "from random import sample \n",
    "import functools \n",
    "import os\n",
    "from enum import Enum, unique, auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "from pandas.plotting import lag_plot\n",
    "import math as m\n",
    "import random as r\n",
    "import collections\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html\n",
    "#export\n",
    "import scipy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment 1.1.2/3\n",
    "experiment = 'experiments1.1.x/'\n",
    "path_data = '../dvc-ds4se/metrics/traceability/' + experiment \n",
    "def sacp_params():\n",
    "    return{\n",
    "        \"system\": 'sacp',\n",
    "        \"experiment_path_w2v\": path_data + '[sacp-python-common-VectorizationType.word2vec-LinkType.issue2src-True-1614046787.460911].csv',\n",
    "        \"experiment_path_d2v\": path_data + '[sacp-python-common-VectorizationType.doc2vec-LinkType.issue2src-True-1614251614.931571].csv',\n",
    "        'saving_path': '../../'+'data/cisco/sacp_data/',\n",
    "        'system_long': 'sacp-python-common',\n",
    "        'timestamp': 1595953540.866044,\n",
    "        'language': 'all-corpus'\n",
    "    }     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../dvc-ds4se/metrics/traceability/experiments1.1.x/[sacp-python-common-VectorizationType.word2vec-LinkType.issue2src-True-1614046787.460911].csv'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = sacp_params()\n",
    "\n",
    "params['experiment_path_w2v']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VectorEvaluation():\n",
    "    '''Approaches Common Evaluations and Interpretations (statistical analysis)\n",
    "       Loading unsupervised results\n",
    "       w2v includes entropy evaluation\n",
    "    '''\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.df_w2v = pd.read_csv(params['experiment_path_w2v'], header=0, index_col=0, sep=' ')\n",
    "        self.df_d2v = pd.read_csv(params['experiment_path_d2v'], header=0, index_col=0, sep=' ')\n",
    "        \n",
    "        self.sim_list_w2v = [SimilarityMetric.SCM_sim, SimilarityMetric.WMD_sim]\n",
    "        self.sim_list_d2v = [SimilarityMetric.COS_sim , SimilarityMetric.EUC_sim]\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Entropy\n",
    "        manifoldEntropy =  self.sim_list_w2v + [EntropyMetric.Entropy_src, EntropyMetric.Entropy_tgt, \n",
    "                           EntropyMetric.JI, EntropyMetric.MI, EntropyMetric.Loss, \n",
    "                           EntropyMetric.Noise]\n",
    "        manifoldEntropy = [str(i) for i in manifoldEntropy] + ['Linked?']\n",
    "        self.manifoldEntropy = self.df_w2v[manifoldEntropy].copy()\n",
    "\n",
    "        ##Concat D2V\n",
    "        for i in self.sim_list_d2v:\n",
    "            self.manifoldEntropy[ str(i) ] = self.df_d2v[str(i)].values\n",
    "        \n",
    "        self.manifoldEntropy = self.manifoldEntropy[~self.manifoldEntropy.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "        \n",
    "        #SharedInfo\n",
    "        sharedInfo = self.sim_list_w2v +[EntropyMetric.MSI_I,EntropyMetric.MSI_X]\n",
    "        sharedInfo = [str(i) for i in sharedInfo] + ['Linked?']\n",
    "        self.sharedInfo = self.df_w2v[sharedInfo].copy()\n",
    "        \n",
    "        for i in self.sim_list_d2v:\n",
    "            self.sharedInfo[ str(i) ] = self.df_d2v[ str(i) ].values #No filtered by nans\n",
    "            \n",
    "        self.df_sys = self.LoadCorpus(params['timestamp'], language=params['language'], sep='~')\n",
    "            \n",
    "    def LoadCorpus(self, timestamp, language='js', sep=',', mode='a'):\n",
    "        path_to_link = self.params['saving_path'] + '['+ self.params['system_long']  + '-' + language + '-{}].csv'.format(timestamp)\n",
    "        return pd.read_csv(path_to_link, header=0, index_col=0, sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ../dvc-ds4se/metrics/traceability/experiments1.1.x/[sacp-python-common-VectorizationType.doc2vec-LinkType.issue2src-True-1614251614.931571].csv does not exist: '../dvc-ds4se/metrics/traceability/experiments1.1.x/[sacp-python-common-VectorizationType.doc2vec-LinkType.issue2src-True-1614251614.931571].csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-605182c8d878>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#tst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_vector_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorEvaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-d26a2747d48c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'experiment_path_w2v'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_d2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'experiment_path_d2v'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim_list_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mSimilarityMetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSCM_sim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSimilarityMetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWMD_sim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ../dvc-ds4se/metrics/traceability/experiments1.1.x/[sacp-python-common-VectorizationType.doc2vec-LinkType.issue2src-True-1614251614.931571].csv does not exist: '../dvc-ds4se/metrics/traceability/experiments1.1.x/[sacp-python-common-VectorizationType.doc2vec-LinkType.issue2src-True-1614251614.931571].csv'"
     ]
    }
   ],
   "source": [
    "#tst\n",
    "test_vector_eval = VectorEvaluation(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "test_vector_eval.df_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst \n",
    "test_vector_eval.df_d2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert test_vector_eval.df_w2v.Source[0] == test_vector_eval.df_d2v.Source[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert test_vector_eval.df_w2v.Target[0] == test_vector_eval.df_d2v.Target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert test_vector_eval.df_w2v.Source[1091] == test_vector_eval.df_d2v.Source[1091]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert test_vector_eval.df_w2v.Target[1091] == test_vector_eval.df_d2v.Target[1091]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "test_vector_eval.manifoldEntropy.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "test_vector_eval.sharedInfo.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sys\n",
    "test_vector_eval.df_sys.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector_eval.df_sys.groupby(['type']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data Science Description Task\n",
    "Classification/evaluation metrics for highly imbalanced data [(see Forum)](https://stats.stackexchange.com/questions/222558/classification-evaluation-metrics-for-highly-imbalanced-data).\n",
    "Description is using data to provide a quantitative summary of certain features of the world. Descriptive tasks include, for example, computing the proportion of individuals with diabetes in a large healthcare database and representing social networks in a community. The analytics employed for description range from elementary calculations (e.g., a mean or a proportion) to sophisticated techniques such as unsupervised learning algorithms (e.g., cluster analysis) and clever data visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ExploratoryDataSoftwareAnalysis(VectorEvaluation):\n",
    "    '''\n",
    "    This class process \"Description\" Data Science Techniques\n",
    "    '''\n",
    "    def __init__(self, params):\n",
    "        super().__init__(params)\n",
    "        \n",
    "        descriptors = {\n",
    "            \"entropy_set\":['Linked?','EntropyMetric.Entropy_src','EntropyMetric.Entropy_tgt','EntropyMetric.JI','EntropyMetric.MI', 'EntropyMetric.Loss', 'EntropyMetric.Noise'],\n",
    "            \"similarity_set\":['Linked?','SimilarityMetric.SCM_sim', 'SimilarityMetric.WMD_sim', 'SimilarityMetric.COS_sim', 'SimilarityMetric.EUC_sim' ],\n",
    "            \"shared_set\":['Linked?','EntropyMetric.MSI_I', 'EntropyMetric.MSI_X']\n",
    "        }\n",
    "        \n",
    "        self.entropy_set = self.manifoldEntropy[descriptors['entropy_set']].copy()\n",
    "        self.similarity_set = self.manifoldEntropy[descriptors['similarity_set']].copy()\n",
    "        self.shared_set = self.sharedInfo[descriptors['shared_set']].copy()\n",
    "        \n",
    "    def mean_confidence_interval(self, data, confidence=0.95):\n",
    "        a = 1.0 * np.array(data)\n",
    "        n = len(a)\n",
    "        m, se = np.mean(a), sc.stats.sem(a)\n",
    "        h = se * sc.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "        return round(m,2), round(m-h,2), round(m+h,2), round(h,2)\n",
    "    \n",
    "    def bootstrapping_mean(self, vector, samples=10000, size = 500 ):\n",
    "        '''Confidence Intervals are  Statistical test performed visully'''\n",
    "        np.random.seed(42)\n",
    "        mean = []\n",
    "        for i in range(samples):\n",
    "            boot = np.random.choice(vector, size=size)\n",
    "            mean += [np.mean(boot)]\n",
    "            pass\n",
    "        \n",
    "        return mean\n",
    "\n",
    "    def bootstrapping_plot(self,df,size=20, samples=500, color='b', params = ['Linked?']):\n",
    "        pd.plotting.bootstrap_plot(\n",
    "            df[params],\n",
    "            size=size,\n",
    "            samples=samples,\n",
    "            color=color\n",
    "        )\n",
    "        pass    \n",
    "    \n",
    "    def confidence_interval_plot(self, \n",
    "                                 list_category, \n",
    "                                 list_low, \n",
    "                                 list_up,\n",
    "                                 color,\n",
    "                                 byLink = False\n",
    "                                ):\n",
    "        data_dict = {}\n",
    "        data_dict['category'] = list_category\n",
    "        data_dict['lower'] = list_low\n",
    "        data_dict['upper'] = list_up\n",
    "        dataset = pd.DataFrame(data_dict)\n",
    "        for lower,upper,y in zip(dataset['lower'],dataset['upper'],range(len(dataset))):\n",
    "            #Todo please modify this orange\n",
    "            if byLink:\n",
    "                #logging.info(str(y))\n",
    "                plt.plot((lower,upper),(y,y),'o',color=color[y%2])\n",
    "            else:\n",
    "                plt.plot((lower,upper),(y,y),'o',color=color[0])\n",
    "        plt.yticks(range(len(dataset)),list(dataset['category']))\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        pass\n",
    "    \n",
    "    def __bootstrapping_iter(\n",
    "                                self,\n",
    "                                df,\n",
    "                                samples=10000, \n",
    "                                conf=0.95\n",
    "                                ):\n",
    "        list_category = []\n",
    "        list_low = []\n",
    "        list_up =[]\n",
    "        for column_name, item in df.iteritems():\n",
    "            m = self.bootstrapping_mean(\n",
    "                item.values,\n",
    "                samples=10000, \n",
    "                size=len(item.values) #Size is by default the same size of the original sample\n",
    "                )\n",
    "            ci = self.mean_confidence_interval(m, conf)\n",
    "            list_category += [column_name]\n",
    "            list_low += [ci[1]]\n",
    "            list_up += [ci[2]]\n",
    "            logging.info(column_name + ' : ' + str(ci))\n",
    "        return list_category, list_low, list_up\n",
    "    \n",
    "    def ci_bootstrapping(self,\n",
    "                         df, \n",
    "                         samples=10000, \n",
    "                         conf=0.95, \n",
    "                         color=['blue','red'],\n",
    "                         byLink = False\n",
    "                        ):\n",
    "        '''Main method for generating bootstrapping CIs'''\n",
    "        list_category = []\n",
    "        list_low = []\n",
    "        list_up = []\n",
    "        \n",
    "        if byLink:\n",
    "            list_category_1,list_low_1,list_up_1 = self.__bootstrapping_iter(\n",
    "                                df[df['Linked?']== 1], samples, conf)\n",
    "            list_category_0,list_low_0,list_up_0 = self.__bootstrapping_iter(\n",
    "                                df[df['Linked?']== 0], samples, conf)                       \n",
    "            color_ = []\n",
    "            for index in range(len(list_category_0)):\n",
    "                list_category += [list_category_1[index]+'_1',list_category_0[index]+'_0']\n",
    "                list_low += [list_low_1[index], list_low_0[index]]\n",
    "                list_up += [list_up_1[index], list_up_0[index]]\n",
    "                \n",
    "                if index%2 == 0:\n",
    "                    color_.insert(index,color[0])\n",
    "                else:\n",
    "                    color_.insert(index,color[1])\n",
    "                pass\n",
    "                                  \n",
    "            self.confidence_interval_plot(list_category,list_low,list_up,color_,byLink)\n",
    "            \n",
    "        else:\n",
    "            list_category,list_low,list_up = self.__bootstrapping_iter(\n",
    "                                df, samples, conf)\n",
    "            self.confidence_interval_plot(list_category,list_low,list_up,color)\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA = ExploratoryDataSoftwareAnalysis( params = params ) #<---- Parameter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance and Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA.similarity_set.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA.similarity_set.groupby(['Linked?']).agg(['count','mean','std','median']).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Transmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA.entropy_set.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EDA.entropy_set.groupby(['Linked?']).agg(['count','mean','std','median']).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA.shared_set.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA.shared_set.groupby(['Linked?']).agg(['count','mean','std','median']).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2] Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA.similarity_set.hist(color='k', bins=50, figsize=[10,8], alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA.entropy_set.hist(color='k', bins=50, figsize=[15,10], alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA.shared_set.hist(color='k', bins=50, figsize=[10,5], alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sections are focused on the three common error bars: standard deviations, confidence intervals, and stardard errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3] Confidence Intervals \n",
    "Confidence Intervals. This proposes a range of plausible values for an unknown parameter (for example, the mean). The interval has an associated confidence level that the true parameter is in the proposed range. The confidence level is chosen by the investigator. For a given estimation in a given sample, using a higher confidence level generates a wider (i.e., less precise) confidence interval. In general terms, a confidence interval for an unknown parameter is based on sampling the distribution of a corresponding estimator\n",
    "\n",
    "We use bootstrapping! Bootstrapping is a robust technique. We use it for calculating the mean and standard error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance & Similarity Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance & Similarity Measures\n",
    "EDA.ci_bootstrapping(df = EDA.similarity_set, samples=10000, conf=0.95 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA.ci_bootstrapping(df = EDA.similarity_set, \n",
    "                     samples=10000, \n",
    "                     conf=0.95,\n",
    "                     byLink = True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Information Measures\n",
    "EDA.ci_bootstrapping(df = EDA.entropy_set, samples=10000, conf=0.95 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA.ci_bootstrapping(\n",
    "    df = EDA.entropy_set, samples=10000, conf=0.95, byLink = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shared Information\n",
    "EDA.ci_bootstrapping(df = EDA.shared_set, samples=10000, conf=0.95 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA.ci_bootstrapping(\n",
    "    df = EDA.shared_set, samples=10000, conf=0.95, byLink = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4] Standard Deviations\n",
    "These tell you how the data is distributed around the mean . Big standard deviations tell you that some data points were pretty far from the mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA.manifoldEntropy.boxplot(figsize=[30, 10], showfliers=False) #Without Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deviations grouped by Ground Truth (GT)\n",
    "EDA.manifoldEntropy.boxplot(by='Linked?',\n",
    "                            figsize=[10, 10],\n",
    "                            bootstrap = 10000,\n",
    "                            meanline = True,\n",
    "                            showfliers=False #Without Outliers\n",
    "                           ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA.sharedInfo.boxplot(figsize=[19, 5], showfliers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deviations grouped by Ground Truth (GT)\n",
    "EDA.sharedInfo.boxplot(by='Linked?',figsize=[10, 10], showfliers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5] Scatter Matrix\n",
    "Scatter matrix help us to identify correlations. However, the proper analysis is perform in the \"Predictive notebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(EDA.manifoldEntropy, alpha=0.2, figsize=(35, 35), diagonal='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(EDA.sharedInfo, \n",
    "               alpha=0.2, \n",
    "               figsize=(20, 20), \n",
    "               diagonal='kde',\n",
    "               #grid = True\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [6] Auto Correlations\n",
    "Lag plots are used to check if a data set or time series is random. Random data should not exhibit any structure in the lag plot. Non-random structure implies that the underlying data are not random. The lag argument may be passed, and when lag=1 the plot is essentially data[:-1] vs. data[1:].\n",
    "\n",
    "The shape of the lag plot can provide clues about the underlying structure of your data. For example:\n",
    "\n",
    "A linear shape to the plot suggests that an autoregressive model is probably a better choice.\n",
    "An elliptical plot suggests that the data comes from a single-cycle sinusoidal model.\n",
    "\n",
    "Randomness\n",
    "Creating a lag plot enables you to check for randomness. Random data will spread fairly evenly both horizontally and vertically. If you cannot see a pattern in the graph, your data is most probably random. On the other hand a shape or trend to the graph (like a linear pattern) indicates the data is not random\n",
    "\n",
    "If your data shows a linear pattern, it suggests autocorrelation is present. A positive linear trend (i.e. going upwards from left to right) is suggestive of positive autocorrelation; a negative linear trend (going downwards from left to right) is suggestive of negative autocorrelation. The tighter the data is clustered around the diagonal, the more autocorrelation is present; perfectly autocorrelated data will cluster in a single diagonal line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot( EDA.similarity_set[['SimilarityMetric.COS_sim']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.autocorrelation_plot(EDA.similarity_set[['SimilarityMetric.COS_sim']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot( EDA.similarity_set[['SimilarityMetric.WMD_sim']] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info. Transmission Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot( EDA.entropy_set[['EntropyMetric.MI']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.autocorrelation_plot(EDA.entropy_set[['EntropyMetric.MI']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot( EDA.entropy_set[['EntropyMetric.Loss']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot( EDA.entropy_set[['EntropyMetric.Noise']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot( EDA.entropy_set[['EntropyMetric.Entropy_src']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot( EDA.entropy_set[['EntropyMetric.Entropy_tgt']] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot( EDA.shared_set[['EntropyMetric.MSI_I']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot( EDA.shared_set[['EntropyMetric.MSI_X']] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Studies of Info Transmission Traceability\n",
    "The goal of this section is to detect specific examples of odd traceability cases for a given dataset (or corpus). For instance, edge cases (min max), imbalanced data, or orphan links. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 0: Overlapping Information\n",
    "This study shows potential overlaps on artifacts by measures the Minimum Shared Information Entropy (MSI_I). Ideally, lower values of MSI means that the link should not exist. Conversely, higher values of MSI means that there are enough conditions for the link to exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAX MSI_I\n",
    "EDA.df_w2v[EDA.df_w2v['EntropyMetric.MSI_I']==EDA.df_w2v['EntropyMetric.MSI_I'].max()][['Source','Target','EntropyMetric.MSI_I','Linked?']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MIN MSI_I\n",
    "EDA.df_w2v[EDA.df_w2v['EntropyMetric.MSI_I']==EDA.df_w2v['EntropyMetric.MSI_I'].min()][['Source','Target','EntropyMetric.MSI_I','Linked?']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nan Cases of MSI\n",
    "These cases are particularly relevant because show potential links that do not share any information at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nan_msi = EDA.df_w2v[np.isnan(EDA.df_w2v['EntropyMetric.MSI_I'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nan_msi.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nan_msi.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Positive Links [Warning]!!\n",
    "df_nan_msi[df_nan_msi['Linked?']==1][['Source','Target','SimilarityMetric.WMD_sim']].to_csv('msi_i_sacp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Edge Cases of Self-Information\n",
    "This study highlights the imbalance of information between the source and target artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max Self Information\n",
    "max_tgt =EDA.df_w2v[EDA.df_w2v['EntropyMetric.Entropy_tgt']==EDA.df_w2v['EntropyMetric.Entropy_tgt'].max()][['Target']].values\n",
    "np.unique(max_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Min Self Information\n",
    "min_tgt = EDA.df_w2v[EDA.df_w2v['EntropyMetric.Entropy_tgt']==EDA.df_w2v['EntropyMetric.Entropy_tgt'].min()][['Target']].values\n",
    "np.unique(min_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.99 quantile for the Target Artifacts\n",
    "#We can make variation of the quantile to retrieve more links/cases\n",
    "np.unique(EDA.df_w2v[EDA.df_w2v['EntropyMetric.Entropy_tgt']>=EDA.df_w2v['EntropyMetric.Entropy_tgt'].quantile(.99)][['Target']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max Self Information\n",
    "max_src = EDA.df_w2v[EDA.df_w2v['EntropyMetric.Entropy_src']==EDA.df_w2v['EntropyMetric.Entropy_src'].max()][['Source']].values\n",
    "np.unique(max_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Min Self Information\n",
    "min_src = EDA.df_w2v[EDA.df_w2v['EntropyMetric.Entropy_src']==EDA.df_w2v['EntropyMetric.Entropy_src'].min()][['Source']].values\n",
    "np.unique(min_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.99 quantile for the Source Artifacts\n",
    "np.unique(EDA.df_w2v[EDA.df_w2v['EntropyMetric.Entropy_src']>=EDA.df_w2v['EntropyMetric.Entropy_src'].quantile(.99)][['Source']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Minimum and maximum loss\n",
    "This study presents edge cases for the entropy loss. This is useful to detect poorly documented target artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Max Loss\n",
    "EDA.df_w2v[EDA.df_w2v['EntropyMetric.Loss']==EDA.df_w2v['EntropyMetric.Loss'].max()][['Source','Target','EntropyMetric.Loss','Linked?']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Min Loss\n",
    "EDA.df_w2v[EDA.df_w2v['EntropyMetric.Loss']==EDA.df_w2v['EntropyMetric.Loss'].min()][['Source','Target','EntropyMetric.Loss','Linked?']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quantile .99\n",
    "EDA.df_w2v['EntropyMetric.Loss'].quantile(.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quantile .99 Links\n",
    "EDA.df_w2v[EDA.df_w2v['EntropyMetric.Loss']>=EDA.df_w2v['EntropyMetric.Loss'].quantile(.99)][['Source','Target','EntropyMetric.Loss','Linked?']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Minimum and maximum noise\n",
    " This study presents edge cases for the entropy noise. This is useful to detect poorly documented source artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Max Noise\n",
    "EDA.df_w2v[EDA.df_w2v['EntropyMetric.Noise']==EDA.df_w2v['EntropyMetric.Noise'].max()][['Source','Target','EntropyMetric.Noise','Linked?']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Min Noise\n",
    "EDA.df_w2v[EDA.df_w2v['EntropyMetric.Noise']==EDA.df_w2v['EntropyMetric.Noise'].min()][['Source','Target','EntropyMetric.Noise','Linked?']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quantile .99\n",
    "EDA.df_w2v['EntropyMetric.Noise'].quantile(.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quantile .99 Links\n",
    "EDA.df_w2v[EDA.df_w2v['EntropyMetric.Noise']>=EDA.df_w2v['EntropyMetric.Noise'].quantile(.99)][['Source','Target','EntropyMetric.Noise','Linked?']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: Orphan informative links\n",
    "This study points out a set of informative links that are not found in the ground truth. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Max Mutual Information\n",
    "EDA.df_w2v[EDA.df_w2v['EntropyMetric.MI']==EDA.df_w2v['EntropyMetric.MI'].min()][['Source','Target','EntropyMetric.MI','Linked?']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Min Mutual Information\n",
    "EDA.df_w2v[EDA.df_w2v['EntropyMetric.MI']==EDA.df_w2v['EntropyMetric.MI'].min()][['Source','Target','EntropyMetric.MI','Linked?']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quantile .99\n",
    "EDA.df_w2v['EntropyMetric.Noise'].quantile(.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quantile .99 Links\n",
    "EDA.df_w2v[EDA.df_w2v['EntropyMetric.MI']>=EDA.df_w2v['EntropyMetric.MI'].quantile(.99)][['Source','Target','EntropyMetric.MI','Linked?']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Inspections on the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA.df_w2v.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA.df_sys.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA.df_w2v[EDA.df_w2v['Target']=='sacp-python-common/setup.py'][['Source','Target','EntropyMetric.Entropy_tgt','Linked?']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA.df_w2v[EDA.df_w2v['Source']==193][['Source','Target','EntropyMetric.Entropy_src','EntropyMetric.Entropy_tgt','Linked?']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_build_docs #<-------- [Activate when stable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -e ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
