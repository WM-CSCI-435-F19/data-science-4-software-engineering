{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "empirical-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp codexplainer.metrics_java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "turkish-diving",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ds4se.exp.metrics_java import *\n",
    "from ds4se.mgmnt.prep.files_mgmnt import *\n",
    "from ds4se.mgmnt.prep.token_mgmnt import *\n",
    "from ds4se.codexplainer.utils import *\n",
    "\n",
    "from ds4se.utils.distances import statistical_distance\n",
    "from ds4se.utils.statistics import bootstrapping\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import Optional, List, Tuple, Callable, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "weird-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "#Logging configuration\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='mylog.log', mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-primary",
   "metadata": {},
   "source": [
    "### metrics explainer\n",
    "\n",
    "> This module provides methods for performing comparisons between code samples leveraging metrics module (java) \n",
    "\n",
    "> @Alvaro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-webster",
   "metadata": {},
   "source": [
    "## Parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sorted-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def _get_metrics_cols(columns: List[str]):\n",
    "    cols = [col for col in columns if (col != \"file\" and col != \"class\")]\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def _gather_description_stats(metrics: List[str], exp_stats_dict: Dict, stats: List[str],\n",
    "             decript_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Gather data related to a set of errors and a set of statistics behavior (e.g., mean, median)\n",
    "    \n",
    "    \"\"\"\n",
    "    for met in metrics:\n",
    "        if met not in exp_stats_dict:\n",
    "            exp_stats_dict[met] = {}\n",
    "            for stat in stats:\n",
    "                exp_stats_dict[met][stat] = {}\n",
    "                \n",
    "        for stat in stats:\n",
    "            exp_stats_dict[met][stat] = decript_df[met].loc[stat]\n",
    "\n",
    "def _gather_stats_for_experiment(gen_metrics: pd.DataFrame, hmn_metrics: pd.DataFrame,\n",
    "                                stats: List[str]) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Gather the data for 2 sets of errors, regarding the behavior of several statistics (e.g, mean)\n",
    "    for a single experiment\n",
    "    \n",
    "    :param gen_errors: pd.DF containing errors data for generator model\n",
    "    :param hmn_errors: pd.DF containing errors data for human data\n",
    "    :param stats: List containing the stats of interest\n",
    "    \n",
    "    :return: Tuple[Dict, Dict] containing the data for both error samples (model, human)\n",
    "    \"\"\"\n",
    "    gen_metrics_cols = _get_metrics_cols(list(gen_metrics.columns))\n",
    "    \n",
    "    gen_metrics_descript = gen_metrics.describe()\n",
    "    hmn_metrics_descript = hmn_metrics.describe()\n",
    "    \n",
    "    gen_exp_stats = {}\n",
    "    hmn_exp_stats = {}\n",
    "    \n",
    "    _gather_description_stats(gen_metrics_cols, gen_exp_stats, stats, gen_metrics_descript)\n",
    "    _gather_description_stats(gen_metrics_cols, hmn_exp_stats, stats, hmn_metrics_descript)\n",
    "    \n",
    "    return gen_exp_stats, hmn_exp_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def verify_if_zeros(array: np.ndarray):\n",
    "    return np.all((np.array(array) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def compare_metrics(gen_metrics: pd.DataFrame, hmn_metrics: pd.DataFrame,\n",
    "                    est_func: Callable, bs_size: int, bins_method: Optional[str]='auto'):\n",
    "    \"\"\"\n",
    "    Perform dimension-wise comparison of given metrics sets\n",
    "    \"\"\"\n",
    "    div_dist_data = {}\n",
    "    \n",
    "    gen_dims = _get_metrics_cols(list(gen_metrics.columns))\n",
    "    \n",
    "    for dimension in gen_dims:\n",
    "        gen_dim_data = gen_metrics[dimension].values\n",
    "        hmn_dim_data = hmn_metrics[dimension].values\n",
    "        \n",
    "        gen_dim_data_zeros = verify_if_zeros(gen_dim_data)\n",
    "        hmn_dim_data_zeros = verify_if_zeros(hmn_dim_data)\n",
    "        \n",
    "        # Bootstrapping should not be performed here\n",
    "        # but when gathering data for estimating experiments' behavior instead\n",
    "        \n",
    "#         logging.info(\"Starting bootstrapping.\")\n",
    "#         bs_gen_dim_data = bootstrapping(gen_dim_data, est_func, sample_size=bs_size, full_zeros=gen_dim_data_zeros)\n",
    "#         bs_hmn_dim_data = bootstrapping(hmn_dim_data, est_func, sample_size=bs_size, full_zeros=hmn_dim_data_zeros)\n",
    "        \n",
    "#         bs_gen_dim = bs_gen_dim_data['bootstrap_repl'].values\n",
    "#         bs_hmn_dim = bs_hmn_dim_data['bootstrap_repl'].values\n",
    "            \n",
    "        \n",
    "        logging.info(\"Starting JS computations.\")\n",
    "        # Bins setting is estimated via Freedmanâ€“Diaconis rule\n",
    "        js_div, js_dist = statistical_distance(hmn_dim_data, gen_dim_data)\n",
    "        \n",
    "        div_dist_data[dimension] = {\n",
    "            \"JS-Divergence\": js_div,\n",
    "            \"JS-Distance\": js_dist\n",
    "        }\n",
    "        \n",
    "    return div_dist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def _group_js_exp_data_by_metrics_categories(experiment_list: List,\n",
    "                                             categories: Optional[Dict]=metrics_categories\n",
    "                                             )-> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Group data according to established metrics categories\n",
    "    :param experiment_list: List with dictionaries containing data for categories\n",
    "    :param categories:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Init structure for gathering data\n",
    "    # -------------\n",
    "    grouped_data_dist = {}\n",
    "    grouped_data_div = {}\n",
    "\n",
    "    for cat in categories:\n",
    "        grouped_data_dist[cat] = []\n",
    "        grouped_data_div[cat] = []\n",
    "    # -------------\n",
    "    \n",
    "    # Gather data by categories\n",
    "    for experiment in experiment_list:\n",
    "        for cat in categories:\n",
    "            cat_dims = categories[cat]\n",
    "            logging.info(f\"Metrics dimensions for category: {cat}\")\n",
    "            \n",
    "            exp_div_data = {}\n",
    "            exp_dist_data = {}\n",
    "            \n",
    "            for dim, measures in experiment.items():\n",
    "                if dim not in cat_dims:\n",
    "                    continue\n",
    "\n",
    "                exp_div_data[dim] = measures['JS-Divergence']\n",
    "                exp_dist_data[dim] = measures['JS-Distance']\n",
    "        \n",
    "            grouped_data_dist[cat].append(exp_dist_data)\n",
    "            grouped_data_div[cat].append(exp_div_data)\n",
    "            \n",
    "    return grouped_data_dist, grouped_data_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "def perform_metrics_distance_analysis(gen_samples_path: str, hmn_samples_path: str,\n",
    "                                      ck_tool_jar: str, n_experiments: int,\n",
    "                                      physical_files_path: str,\n",
    "                                      out_path: str,\n",
    "                                      rm_java_files: Optional[bool]=True,\n",
    "                                      samples_code_col: Optional[str]=\"code\",\n",
    "                                      np_est_function: Optional[Callable]=np.mean,\n",
    "                                      bootstrap_samples_size: Optional[int]=500,\n",
    "                                      samples_extension: Optional[str]=\"jsonl\",\n",
    "                                      compar_stats: Optional[List]=['mean'],\n",
    "                                      group_by_metrics_categories: Optional[bool]=True,\n",
    "                                      process_tokens: Optional[bool]=True,\n",
    "                                      bins_method: Optional[str]='auto'\n",
    "                                     ):\n",
    "    \"\"\"\n",
    "    Perform the analysis for getting comparison info. about\n",
    "    2 sets of samples (generator vs human)\n",
    "    \n",
    "    \n",
    "    :param gen_samples_path: Str indicating path containing generator samples\n",
    "    :param hmn_samples_path: Str indicating path containing human samples\n",
    "    :param ck_tool_jar: Str indicating path containing aux. CK tool (for computing metrics)\n",
    "    :param n_experiments: int indicating number of experiments to consider\n",
    "    :param physical_files_path: Str indicating location for (tmp) java files\n",
    "    :param rm_java_files: bool indicating whether to remove generated java files or not\n",
    "    :param samples_code_col: Str indicating column name containing code in sample sets\n",
    "    :param np_est_function: np function indicating indicating estimator for boostraping\n",
    "                            - Default: np.mean    \n",
    "    :param bootstrap_samples_size: int indicating the size for resample sets used\n",
    "                                   in bootstrapping\n",
    "    :param samples_extension: Str indicating file extension for sample files\n",
    "                              - Default: jsonl\n",
    "    :param compar_stats: Statistics of interest for gathering data about\n",
    "                         - Default: ['mean']\n",
    "    \n",
    "    :return: Tuple[DataFrame, DataFrame, Dict, Dict, Dict, Dict]\n",
    "             Containing:\n",
    "             - Distance data for all experiments\n",
    "             - Divergence data for all experiments\n",
    "             - Generator samples metrics stats (according to specified input) accross all experiments\n",
    "             - Human samples metrics stats (according to specified input) accross all experiments\n",
    "             - Metric-based grouping for distance data\n",
    "             - Metric-based grouping for divergence data             \n",
    "    \"\"\"\n",
    "    \n",
    "    # Data structures for gathering data across experiments\n",
    "    distance_data = []\n",
    "    gen_metrics_stats = []\n",
    "    hmn_metrics_stats = []\n",
    "    categorized_js_data = []\n",
    "    \n",
    "    logging.info(\"Building metrics analyzer.\")\n",
    "    \n",
    "    java_analyzer = JavaAnalyzer(ck_tool_jar)\n",
    "    \n",
    "    # Get sample file names\n",
    "    gen_sample_set = get_files_list(gen_samples_path, samples_extension)\n",
    "    hmn_sample_set = get_files_list(hmn_samples_path, samples_extension)\n",
    "    \n",
    "    for i in tqdm(range(n_experiments)):\n",
    "        gen_sample_path = gen_sample_set[i]\n",
    "        hmn_sample_path = hmn_sample_set[i]\n",
    "        \n",
    "        gen_samples = jsonl_to_dataframe(str(gen_sample_path))\n",
    "        hmn_samples = jsonl_to_dataframe(str(hmn_sample_path))\n",
    "        \n",
    "        if process_tokens:\n",
    "            gen_samples = replace_spec_toks_to_original(gen_samples, java_special_tokens)\n",
    "            hmn_samples = replace_spec_toks_to_original(hmn_samples, java_special_tokens)\n",
    "        \n",
    "        logging.info('Gen samples: ')\n",
    "        logging.info(gen_samples.head())\n",
    "        \n",
    "        logging.info('Human samples: ')\n",
    "        logging.info(hmn_samples.head())\n",
    "    \n",
    "        logging.info(\"Gathering metrics for samples.\")\n",
    "        gen_metrics = java_analyzer.compute_metrics(gen_samples[samples_code_col], physical_files_path, remove_java_files=rm_java_files)\n",
    "        hmn_metrics = java_analyzer.compute_metrics(hmn_samples[samples_code_col], physical_files_path, remove_java_files=rm_java_files)\n",
    "        \n",
    "        # ------------------------------------------------------\n",
    "        # Save metrics\n",
    "        gen_metrics.to_csv(f\"{out_path}/{gen_sample_path.stem}-gen-metrics.csv\")\n",
    "        hmn_metrics.to_csv(f\"{out_path}/{hmn_sample_path.stem}-hmn-metrics.csv\")\n",
    "        # ------------------------------------------------------\n",
    "        \n",
    "        logging.info(\"Performing comparison.\")\n",
    "        experiment_comp = compare_metrics(gen_metrics, hmn_metrics, np_est_function, bootstrap_samples_size, bins_method)\n",
    "        distance_data.append(experiment_comp)\n",
    "        \n",
    "        logging.info(f\"Gathering statistics for: {compar_stats}...\")\n",
    "        gen_exp_stats, hmn_exp_stats = _gather_stats_for_experiment(gen_metrics, hmn_metrics, compar_stats)\n",
    "        gen_metrics_stats.append(gen_exp_stats)\n",
    "        hmn_metrics_stats.append(hmn_exp_stats)\n",
    "        \n",
    "    grouped_distances, grouped_divs = {}, {}\n",
    "    if group_by_metrics_categories:\n",
    "        grouped_distances, grouped_divs= _group_js_exp_data_by_metrics_categories(distance_data)\n",
    "        \n",
    "        \n",
    "    distance_df, divergence_df, gen_mtcs_stats_dict, hmn_mtcs_stats_dict = get_tabular_data(\n",
    "        distance_data, gen_metrics_stats, hmn_metrics_stats, compar_stats\n",
    "    )\n",
    "    \n",
    "    return distance_df, divergence_df, gen_mtcs_stats_dict, hmn_mtcs_stats_dict, grouped_distances, grouped_divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def get_tabular_data(distance_data, gen_metrics_stats, hmn_metrics_stats, compar_stats):\n",
    "    distance_df, divergence_df = get_tabulardata_distances_data(distance_data)\n",
    "    gen_mtcs_stats_dict = get_data_stats_dataframe(gen_metrics_stats, compar_stats)\n",
    "    hmn_mtcs_stats_dict = get_data_stats_dataframe(hmn_metrics_stats, compar_stats)\n",
    "    \n",
    "    return distance_df, divergence_df, gen_mtcs_stats_dict, hmn_mtcs_stats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "short-constant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 0.1_mgmnt.prep.ipynb.\n",
      "Converted 0.2_mgmnt.prep.files_mgmnt.ipynb.\n",
      "Converted 0.3_mgmnt.prep.bpe_tokenization.ipynb.\n",
      "Converted 0.4_mgmnt.prep.tokenization_counting.ipynb.\n",
      "Converted 0.5_mgmnt.prep.token_mgmnt.ipynb.\n",
      "Converted 1.1_exp.info.ipynb.\n",
      "Converted 1.2_exp.desc.metrics.java.ipynb.\n",
      "Converted 1.4_exp.metrics_python.ipynb.\n",
      "Converted 1.5_exp.metrics_java.ipynb.\n",
      "Converted 2.0_repr.codebert.ipynb.\n",
      "Converted 2.0_repr.i.ipynb.\n",
      "Converted 2.1_repr.codeberta.ipynb.\n",
      "Converted 2.1_repr.roberta.train.ipynb.\n",
      "Converted 2.2_repr.roberta.eval.ipynb.\n",
      "Converted 2.3_repr.word2vec.train.ipynb.\n",
      "Converted 2.6_repr.word2vec.eval.ipynb.\n",
      "Converted 2.7_repr.distmetrics.ipynb.\n",
      "Converted 2.8_repr.sentence_transformers.ipynb.\n",
      "Converted 3.1_traceability.unsupervised.eda.ipynb.\n",
      "Converted 3.2_traceability.unsupervised.approach.d2v.ipynb.\n",
      "Converted 3.2_traceability.unsupervised.approach.w2v.ipynb.\n",
      "Converted 4.0_infoxplainer.ir.ipynb.\n",
      "Converted 4.1_infoxplainer.ir.unsupervised.d2v.ipynb.\n",
      "Converted 4.2_infoxplainer.ir.unsupervised.w2v.ipynb.\n",
      "Converted 4.3_infoxplainer.ir.eval.x2v.ipynb.\n",
      "Converted 4.4_infoxplainer.causality.eval.traceability.ipynb.\n",
      "Converted 4.5_infoxplainer.description.eval.traceability.ipynb.\n",
      "Converted 4.6_infoxplainer.prediction.eval.traceability.ipynb.\n",
      "Converted 5.0_utils.clusterization.ipynb.\n",
      "Converted 5.1_utils.visualization.ipynb.\n",
      "Converted 5.2_utils.distances.ipynb.\n",
      "Converted 5.3_utils.plotting.ipynb.\n",
      "Converted 5.3_utils.statistics.ipynb.\n",
      "Converted 8.1_codexplainer.error_checker.ipynb.\n",
      "Converted 8.2_codexplainer.metrics_java.ipynb.\n",
      "Converted 8.4_codexplainer.metrics_example.ipynb.\n",
      "Converted 8.5_codexplainer.d2v_vectorization.ipynb.\n",
      "Converted 8.6_codexplainer.prototypes_criticisms.ipynb.\n",
      "Converted 8.7_codexplainer.mutual_info.ipynb.\n",
      "Converted 8.8_codexplainer.utils.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
