{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiment.mining.ir.unsupervised.w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Unsupervised Approaches for SE Traceability [approach d2v]\n",
    "\n",
    "> Just Paper. Full Experimentation. This module is dedicated to evaluate word2vec/doc2vec or any neural unsupervised approaches on traceability datasets. Consider to Copy the entire notebook for a new and separeted empirical evaluation. \n",
    "> Implementing mutual information analysis\n",
    "> Author: @danaderp April 2020\n",
    "> Author: @danielrc Nov 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This copy is for Cisco purposes. It was adapted to process private github data from cisco. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds4se.mining.ir import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prg import prg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ds4se as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#??\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from itertools import product \n",
    "from random import sample \n",
    "import functools \n",
    "import os\n",
    "from enum import Enum, unique, auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "from pandas.plotting import lag_plot\n",
    "import math as m\n",
    "import random as r\n",
    "import collections\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html\n",
    "#export\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Similarity with BasicSequenceVectorization\n",
    "\n",
    "We test diferent similarities based on [blog](https://www.kdnuggets.com/2017/08/comparing-distance-measurements-python-scipy.html) and [blog2](https://www.kdnuggets.com/2019/01/comparison-text-distance-metrics.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experients Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../dvc-ds4se/' #dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_trained_model = path_data+'models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model'\n",
    "#path_to_trained_model = path_data/'models/wv/bpe128k/[word2vec-Java-Py-Wiki-SK-500-20E-128k[15]-1595189771.501188].model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_prefix = path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_8k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 1 with Libest Conv preprocessing\n",
    "def libest_params():\n",
    "        return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.req2src,\n",
    "        \"system\": 'libest',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_type\": SoftwareArtifacts.REQ.value,\n",
    "        \"target_type\": SoftwareArtifacts.TC.value,\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": path_data + 'se-benchmarking/traceability/cisco/libest_data/[libest-all-corpus-1596063103.098236].csv',\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','conv'],\n",
    "            \"prep\": Preprocessing.conv\n",
    "        },\n",
    "        \"saving_path\": path_data + 'se-benchmarking/traceability',\n",
    "        \"names\": ['Source','Target','Linked?'],\n",
    "        \"model_prefix\":path_model_prefix, #For BPE Analysis\n",
    "        \"path_mappings\": '/tf/main/benchmarking/traceability/testbeds/groundtruth/english/[libest-ground-req-to-tc].txt',    \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 2 with Libest BPE preprocessing\n",
    "def libest_params_bpe():\n",
    "        return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.req2src,\n",
    "        \"system\": 'libest',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_type\": 'req', #TODO Standardize the artifacts \n",
    "        \"target_type\": 'tc',\n",
    "        #\"path_mappings\": 'cisco/libest_data/sacp-pr-mappings.csv',\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": path_data + 'se-benchmarking/traceability/cisco/libest_data/[libest-all-corpus-1596063103.098236].csv',\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','bpe128k'],\n",
    "            \"prep\": Preprocessing.bpe\n",
    "        },\n",
    "        \"saving_path\": path_data + 'se-benchmarking/traceability',\n",
    "        \"names\": ['Source','Target','Linked?'],\n",
    "        \"model_prefix\":path_data + 'models/bpe/sentencepiece/wiki_py_java_bpe_8k' #For BPE Analysis    \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CISCO GitHub Parameters\n",
    "def sacp_params():\n",
    "    return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.issue2src,\n",
    "        \"system\": 'sacp-python-common',\n",
    "        \"path_to_trained_model\": path_data + 'models/wv/conv/[word2vec-Py-Java-Wiki-SK-500-20E[0]-1592979270.711115].model',\n",
    "        \"source_type\": 'pr', #TODO Standardize the artifacts \n",
    "        \"target_type\": 'py',\n",
    "        \"path_mappings\": '/tf/data/cisco/sacp_data/sacp-pr-mappings.csv',\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": '/tf/data/cisco/sacp_data/[sacp-python-common-all-corpus-1596383717.992744].csv', #MUST have bpe8k <----\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','conv'],\n",
    "            \"prep\": Preprocessing.conv\n",
    "        },\n",
    "        \"saving_path\":  path_data/'se-benchmarking/traceability/cisco/sacp',\n",
    "        \"names\": ['Source','Target','Linked?']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_trained_model = path_data + 'models/wv/bpe8k/[word2vec-Java-Py-Wiki-SK-500-20E-8k[12]-1594546477.788739].model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sacp_params_bpe():\n",
    "    return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.issue2src,\n",
    "        \"system\": 'sacp-python-common',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_type\": 'pr', #TODO Standardize the artifacts \n",
    "        \"target_type\": 'py',\n",
    "        \"path_mappings\": '/tf/data/cisco/sacp_data/sacp-pr-mappings.csv',\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": '/tf/data/cisco/sacp_data/[sacp-python-common-all-corpus-1596383717.992744].csv',\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','bpe8k'],\n",
    "            \"prep\": Preprocessing.bpe\n",
    "        },\n",
    "        \"saving_path\": path_data + 'se-benchmarking/traceability/cisco/sacp',\n",
    "        \"names\": ['Source','Target','Linked?'],\n",
    "        \"model_prefix\":path_data + 'models/bpe/sentencepiece/wiki_py_java_bpe_8k' #For BPE Analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectorizationType': <VectorizationType.word2vec: 1>,\n",
       " 'linkType': <LinkType.req2src: 2>,\n",
       " 'system': 'libest',\n",
       " 'path_to_trained_model': '../dvc-ds4se/models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model',\n",
       " 'source_type': 'req',\n",
       " 'target_type': 'tc',\n",
       " 'system_path_config': {'system_path': '../dvc-ds4se/se-benchmarking/traceability/cisco/libest_data/[libest-all-corpus-1596063103.098236].csv',\n",
       "  'sep': '~',\n",
       "  'names': ['ids', 'conv'],\n",
       "  'prep': <Preprocessing.conv: 1>},\n",
       " 'saving_path': '../dvc-ds4se/se-benchmarking/traceability',\n",
       " 'names': ['Source', 'Target', 'Linked?'],\n",
       " 'model_prefix': '../dvc-ds4se/models/bpe/sentencepiece/wiki_py_java_bpe_8k',\n",
       " 'path_mappings': '/tf/main/benchmarking/traceability/testbeds/groundtruth/english/[libest-ground-req-to-tc].txt'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parameters = default_params()\n",
    "parameters = libest_params()\n",
    "#parameters = _params()\n",
    "#parameters = sacp_params_bpe()\n",
    "#parameters = libest_params_bpe()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Similarity with Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 02:24:17,829 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:24:17,886 : INFO : built Dictionary(6957 unique tokens: ['\");', '\"../../', '(\"\\\\', '();', ')))']...) from 87 documents (total 88944 corpus positions)\n",
      "2020-12-16 02:24:17,888 : INFO : conventional preprocessing documents, dictionary, and vocab for the test corpus\n",
      "2020-12-16 02:24:17,889 : INFO : loading Word2Vec object from ../dvc-ds4se/models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model\n",
      "2020-12-16 02:24:17,958 : INFO : loading wv recursively from ../dvc-ds4se/models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.wv.* with mmap=None\n",
      "2020-12-16 02:24:17,960 : INFO : loading vectors from ../dvc-ds4se/models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.wv.vectors.npy with mmap=None\n",
      "2020-12-16 02:24:17,986 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-12-16 02:24:17,988 : INFO : loading vocabulary recursively from ../dvc-ds4se/models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.vocabulary.* with mmap=None\n",
      "2020-12-16 02:24:17,989 : INFO : loading trainables recursively from ../dvc-ds4se/models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.trainables.* with mmap=None\n",
      "2020-12-16 02:24:17,990 : INFO : loading syn1neg from ../dvc-ds4se/models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.trainables.syn1neg.npy with mmap=None\n",
      "2020-12-16 02:24:18,015 : INFO : setting ignored attribute cum_table to None\n",
      "2020-12-16 02:24:18,016 : INFO : loaded ../dvc-ds4se/models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model\n",
      "2020-12-16 02:24:18,088 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-12-16 02:24:18,130 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7f47fd4d2588>\n",
      "2020-12-16 02:24:18,132 : INFO : iterating over columns in dictionary order\n",
      "2020-12-16 02:24:18,137 : INFO : PROGRESS: at 0.01% columns (1 / 6957, 0.014374% density, 0.014374% projected density)\n",
      "2020-12-16 02:24:18,823 : INFO : PROGRESS: at 14.39% columns (1001 / 6957, 0.024382% density, 0.083932% projected density)\n",
      "2020-12-16 02:24:19,364 : INFO : PROGRESS: at 28.76% columns (2001 / 6957, 0.030523% density, 0.070520% projected density)\n",
      "2020-12-16 02:24:19,682 : INFO : PROGRESS: at 43.14% columns (3001 / 6957, 0.033585% density, 0.058909% projected density)\n",
      "2020-12-16 02:24:20,178 : INFO : PROGRESS: at 57.51% columns (4001 / 6957, 0.037911% density, 0.055301% projected density)\n",
      "2020-12-16 02:24:20,900 : INFO : PROGRESS: at 71.88% columns (5001 / 6957, 0.042316% density, 0.053245% projected density)\n",
      "2020-12-16 02:24:21,383 : INFO : PROGRESS: at 86.26% columns (6001 / 6957, 0.044597% density, 0.049412% projected density)\n",
      "2020-12-16 02:24:21,686 : INFO : constructed a sparse term similarity matrix with 0.045688% density\n"
     ]
    }
   ],
   "source": [
    "#[step 1]Creating the Vectorization Class\n",
    "doc2vec = ds.mining.ir.DOC2VecSeqVect( params = parameters )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 02:25:45,242 : INFO : Removed 60 and 1468 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:25:45,243 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:25:45,245 : INFO : built Dictionary(192 unique tokens: ['add', 'address', 'arc', 'attribut', 'author']...) from 2 documents (total 1459 corpus positions)\n",
      "2020-12-16 02:25:45,471 : INFO : token count processed\n",
      "2020-12-16 02:25:45,477 : INFO : frequencies processed\n",
      "2020-12-16 02:25:46,095 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:25:46,096 : INFO : entropies processed\n",
      "2020-12-16 02:25:46,097 : INFO : extropies processed\n",
      "2020-12-16 02:25:46,100 : INFO : token count processed\n",
      "2020-12-16 02:25:46,101 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:25:46,102 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:25:46,103 : INFO : vocab #6957\n",
      "2020-12-16 02:25:46,105 : INFO : diff #set()\n",
      "2020-12-16 02:25:47,354 : INFO : alphabet #6957\n",
      "2020-12-16 02:25:47,974 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ50.txt', 'test_data/LibEST_semeru_format/test/us894.c')[[1.055691843781778, 0.4864542334128923], [0.8034098446369171, 0.19659016], [4.685971707296554, 1.4103942397745337], [7.920854438810503, 6.158280003987111]]\n",
      "2020-12-16 02:25:47,978 : INFO : Removed 13 and 1391 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:25:47,979 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:25:47,980 : INFO : built Dictionary(160 unique tokens: ['accord', 'author', 'check', 'client', 'est']...) from 2 documents (total 1287 corpus positions)\n",
      "2020-12-16 02:25:48,044 : INFO : token count processed\n",
      "2020-12-16 02:25:48,050 : INFO : frequencies processed\n",
      "2020-12-16 02:25:48,669 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:25:48,670 : INFO : entropies processed\n",
      "2020-12-16 02:25:48,671 : INFO : extropies processed\n",
      "2020-12-16 02:25:48,674 : INFO : token count processed\n",
      "2020-12-16 02:25:48,675 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:25:48,676 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:25:48,677 : INFO : vocab #6957\n",
      "2020-12-16 02:25:48,679 : INFO : diff #set()\n",
      "2020-12-16 02:25:49,926 : INFO : alphabet #6957\n",
      "2020-12-16 02:25:50,653 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ28.txt', 'test_data/LibEST_semeru_format/test/us898.c')[[1.1156213994492137, 0.4726743642602323], [0.79994697868824, 0.20005302], [3.6537567082870006, 1.3764967152699399], [7.226828917238281, 4.598404531478929]]\n",
      "2020-12-16 02:25:50,657 : INFO : Removed 13 and 2062 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:25:50,658 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:25:50,659 : INFO : built Dictionary(121 unique tokens: ['accord', 'author', 'check', 'client', 'est']...) from 2 documents (total 639 corpus positions)\n",
      "2020-12-16 02:25:50,710 : INFO : token count processed\n",
      "2020-12-16 02:25:50,716 : INFO : frequencies processed\n",
      "2020-12-16 02:25:51,333 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:25:51,334 : INFO : entropies processed\n",
      "2020-12-16 02:25:51,335 : INFO : extropies processed\n",
      "2020-12-16 02:25:51,337 : INFO : token count processed\n",
      "2020-12-16 02:25:51,339 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:25:51,340 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:25:51,341 : INFO : vocab #6957\n",
      "2020-12-16 02:25:51,343 : INFO : diff #set()\n",
      "2020-12-16 02:25:52,594 : INFO : alphabet #6957\n",
      "2020-12-16 02:25:53,217 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ28.txt', 'test_data/LibEST_semeru_format/test/us900.c')[[1.1820037286862948, 0.45829435892030473], [0.9533130936324596, 0.046686906], [2.873140679513133, 1.3293299963322658], [7.190003047440713, 4.580061849738532]]\n",
      "2020-12-16 02:25:53,221 : INFO : Removed 123 and 2062 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:25:53,222 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:25:53,224 : INFO : built Dictionary(161 unique tokens: ['agre', 'associ', 'attribut', 'base', 'case']...) from 2 documents (total 908 corpus positions)\n",
      "2020-12-16 02:25:53,421 : INFO : token count processed\n",
      "2020-12-16 02:25:53,434 : INFO : frequencies processed\n",
      "2020-12-16 02:25:54,053 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:25:54,054 : INFO : entropies processed\n",
      "2020-12-16 02:25:54,055 : INFO : extropies processed\n",
      "2020-12-16 02:25:54,061 : INFO : token count processed\n",
      "2020-12-16 02:25:54,063 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:25:54,065 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:25:54,066 : INFO : vocab #6957\n",
      "2020-12-16 02:25:54,069 : INFO : diff #set()\n",
      "2020-12-16 02:25:55,317 : INFO : alphabet #6957\n",
      "2020-12-16 02:25:55,962 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us900.c')[[1.0798269951630166, 0.4808092222697687], [0.9475328847765923, 0.052467115], [4.367132914624721, 1.395703744955988], [7.5024307645666735, 5.77613251550434]]\n",
      "2020-12-16 02:25:55,966 : INFO : Removed 47 and 957 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:25:55,967 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:25:55,968 : INFO : built Dictionary(151 unique tokens: ['altern', 'author', 'base', 'cbc', 'check']...) from 2 documents (total 870 corpus positions)\n",
      "2020-12-16 02:25:56,088 : INFO : token count processed\n",
      "2020-12-16 02:25:56,101 : INFO : frequencies processed\n",
      "2020-12-16 02:25:56,731 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:25:56,732 : INFO : entropies processed\n",
      "2020-12-16 02:25:56,733 : INFO : extropies processed\n",
      "2020-12-16 02:25:56,735 : INFO : token count processed\n",
      "2020-12-16 02:25:56,738 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:25:56,739 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:25:56,740 : INFO : vocab #6957\n",
      "2020-12-16 02:25:56,742 : INFO : diff #set()\n",
      "2020-12-16 02:25:58,007 : INFO : alphabet #6957\n",
      "2020-12-16 02:25:58,637 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ22.txt', 'test_data/LibEST_semeru_format/test/us3496.c')[[1.0502271567101682, 0.4877508312808705], [0.7914147824048996, 0.20858522], [3.520786538256951, 1.3578292915020274], [7.457978916025753, 5.3268140343989705]]\n",
      "2020-12-16 02:25:58,640 : INFO : Removed 65 and 730 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:25:58,641 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:25:58,643 : INFO : built Dictionary(137 unique tokens: ['access', 'alt', 'attribut', 'base', 'cannot']...) from 2 documents (total 849 corpus positions)\n",
      "2020-12-16 02:25:58,759 : INFO : token count processed\n",
      "2020-12-16 02:25:58,765 : INFO : frequencies processed\n",
      "2020-12-16 02:25:59,406 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:25:59,407 : INFO : entropies processed\n",
      "2020-12-16 02:25:59,407 : INFO : extropies processed\n",
      "2020-12-16 02:25:59,410 : INFO : token count processed\n",
      "2020-12-16 02:25:59,411 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:25:59,412 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:25:59,413 : INFO : vocab #6957\n",
      "2020-12-16 02:25:59,415 : INFO : diff #set()\n",
      "2020-12-16 02:26:00,685 : INFO : alphabet #6957\n",
      "2020-12-16 02:26:01,311 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ36.txt', 'test_data/LibEST_semeru_format/test/us1005.c')[[0.9929361752706867, 0.5017722154921378], [0.6378212571144104, 0.36217874], [4.084036145431757, 1.3798649765997049], [7.478362150909152, 5.616124690829417]]\n",
      "2020-12-16 02:26:01,314 : INFO : Removed 15 and 939 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:26:01,315 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:26:01,316 : INFO : built Dictionary(141 unique tokens: ['author', 'client', 'est', 'exist', 'held']...) from 2 documents (total 683 corpus positions)\n",
      "2020-12-16 02:26:01,369 : INFO : token count processed\n",
      "2020-12-16 02:26:01,374 : INFO : frequencies processed\n",
      "2020-12-16 02:26:01,996 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:26:01,997 : INFO : entropies processed\n",
      "2020-12-16 02:26:01,998 : INFO : extropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 02:26:02,000 : INFO : token count processed\n",
      "2020-12-16 02:26:02,002 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:26:02,003 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:26:02,004 : INFO : vocab #6957\n",
      "2020-12-16 02:26:02,006 : INFO : diff #set()\n",
      "2020-12-16 02:26:03,257 : INFO : alphabet #6957\n",
      "2020-12-16 02:26:03,882 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ6.txt', 'test_data/LibEST_semeru_format/test/us1060.c')[[1.108496871070142, 0.4742715124317269], [0.8058314025402069, 0.1941686], [2.939884429263502, 1.3213548427323423], [7.625029353358799, 4.1011854841616815]]\n",
      "2020-12-16 02:26:03,885 : INFO : Removed 19 and 664 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:26:03,886 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:26:03,888 : INFO : built Dictionary(146 unique tokens: ['base', 'connect', 'correspond', 'cover', 'data']...) from 2 documents (total 416 corpus positions)\n",
      "2020-12-16 02:26:03,975 : INFO : token count processed\n",
      "2020-12-16 02:26:03,981 : INFO : frequencies processed\n",
      "2020-12-16 02:26:04,626 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:26:04,627 : INFO : entropies processed\n",
      "2020-12-16 02:26:04,628 : INFO : extropies processed\n",
      "2020-12-16 02:26:04,630 : INFO : token count processed\n",
      "2020-12-16 02:26:04,631 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:26:04,633 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:26:04,633 : INFO : vocab #6957\n",
      "2020-12-16 02:26:04,635 : INFO : diff #set()\n",
      "2020-12-16 02:26:05,885 : INFO : alphabet #6957\n",
      "2020-12-16 02:26:06,508 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ24.txt', 'test_data/LibEST_semeru_format/test/us1864.c')[[1.1136803558855108, 0.47310843250991813], [0.8102816790342331, 0.18971832], [3.202819531114783, 1.354100834893818], [8.68839952352524, 5.116649322510982]]\n",
      "2020-12-16 02:26:06,512 : INFO : Removed 27 and 2029 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:26:06,513 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:26:06,514 : INFO : built Dictionary(177 unique tokens: ['accord', 'advis', 'allow', 'attribut', 'author']...) from 2 documents (total 1538 corpus positions)\n",
      "2020-12-16 02:26:06,597 : INFO : token count processed\n",
      "2020-12-16 02:26:06,603 : INFO : frequencies processed\n",
      "2020-12-16 02:26:07,225 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:26:07,226 : INFO : entropies processed\n",
      "2020-12-16 02:26:07,227 : INFO : extropies processed\n",
      "2020-12-16 02:26:07,230 : INFO : token count processed\n",
      "2020-12-16 02:26:07,231 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:26:07,232 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:26:07,233 : INFO : vocab #6957\n",
      "2020-12-16 02:26:07,235 : INFO : diff #set()\n",
      "2020-12-16 02:26:08,491 : INFO : alphabet #6957\n",
      "2020-12-16 02:26:09,111 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ47.txt', 'test_data/LibEST_semeru_format/test/us899.c')[[1.1124942100913837, 0.4733740784817305], [0.7188483774662018, 0.28115162], [3.7236726367683355, 1.3713755289752376], [8.197758189426917, 5.029755174784235]]\n",
      "2020-12-16 02:26:09,115 : INFO : Removed 32 and 939 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:26:09,116 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:26:09,118 : INFO : built Dictionary(157 unique tokens: ['accept', 'access', 'also', 'altern', 'author']...) from 2 documents (total 731 corpus positions)\n",
      "2020-12-16 02:26:09,261 : INFO : token count processed\n",
      "2020-12-16 02:26:09,268 : INFO : frequencies processed\n",
      "2020-12-16 02:26:09,900 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:26:09,901 : INFO : entropies processed\n",
      "2020-12-16 02:26:09,901 : INFO : extropies processed\n",
      "2020-12-16 02:26:09,906 : INFO : token count processed\n",
      "2020-12-16 02:26:09,908 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:26:09,910 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:26:09,911 : INFO : vocab #6957\n",
      "2020-12-16 02:26:09,914 : INFO : diff #set()\n",
      "2020-12-16 02:26:11,286 : INFO : alphabet #6957\n",
      "2020-12-16 02:26:11,907 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ20.txt', 'test_data/LibEST_semeru_format/test/us1060.c')[[1.029252243441073, 0.4927923589746859], [0.7968145459890366, 0.20318545], [4.005274700862298, 1.3814298028890244], [7.708808689886886, 5.3294288967428125]]\n",
      "2020-12-16 02:26:11,910 : INFO : Removed 82 and 1181 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:26:11,911 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:26:11,912 : INFO : built Dictionary(186 unique tokens: ['abl', 'accept', 'also', 'avail', 'band']...) from 2 documents (total 866 corpus positions)\n",
      "2020-12-16 02:26:12,118 : INFO : token count processed\n",
      "2020-12-16 02:26:12,128 : INFO : frequencies processed\n",
      "2020-12-16 02:26:12,757 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:26:12,758 : INFO : entropies processed\n",
      "2020-12-16 02:26:12,759 : INFO : extropies processed\n",
      "2020-12-16 02:26:12,764 : INFO : token count processed\n",
      "2020-12-16 02:26:12,766 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:26:12,768 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:26:12,769 : INFO : vocab #6957\n",
      "2020-12-16 02:26:12,771 : INFO : diff #set()\n",
      "2020-12-16 02:26:14,028 : INFO : alphabet #6957\n",
      "2020-12-16 02:26:14,653 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ34.txt', 'test_data/LibEST_semeru_format/test/us748.c')[[0.9417134036208602, 0.5150090626841346], [0.6687410175800323, 0.33125898], [4.3031740313999, 1.3786351494276183], [8.73272717515758, 5.827249364289527]]\n",
      "2020-12-16 02:26:14,657 : INFO : Removed 66 and 2029 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:26:14,658 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:26:14,661 : INFO : built Dictionary(194 unique tokens: ['attribut', 'author', 'build', 'channel', 'client']...) from 2 documents (total 1613 corpus positions)\n",
      "2020-12-16 02:26:14,876 : INFO : token count processed\n",
      "2020-12-16 02:26:14,885 : INFO : frequencies processed\n",
      "2020-12-16 02:26:15,523 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:26:15,524 : INFO : entropies processed\n",
      "2020-12-16 02:26:15,525 : INFO : extropies processed\n",
      "2020-12-16 02:26:15,531 : INFO : token count processed\n",
      "2020-12-16 02:26:15,533 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:26:15,536 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:26:15,537 : INFO : vocab #6957\n",
      "2020-12-16 02:26:15,538 : INFO : diff #set()\n",
      "2020-12-16 02:26:16,802 : INFO : alphabet #6957\n",
      "2020-12-16 02:26:17,428 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ1.txt', 'test_data/LibEST_semeru_format/test/us899.c')[[0.9835890929623241, 0.5041366700129328], [0.674843043088913, 0.32515696], [4.58708047508761, 1.400369045302398], [8.268208252466158, 5.981354346744601]]\n",
      "2020-12-16 02:26:17,432 : INFO : Removed 62 and 2029 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:26:17,433 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:26:17,434 : INFO : built Dictionary(196 unique tokens: ['anon', 'aris', 'associ', 'author', 'avail']...) from 2 documents (total 1643 corpus positions)\n",
      "2020-12-16 02:26:17,629 : INFO : token count processed\n",
      "2020-12-16 02:26:17,639 : INFO : frequencies processed\n",
      "2020-12-16 02:26:18,272 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:26:18,273 : INFO : entropies processed\n",
      "2020-12-16 02:26:18,274 : INFO : extropies processed\n",
      "2020-12-16 02:26:18,280 : INFO : token count processed\n",
      "2020-12-16 02:26:18,282 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:26:18,284 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:26:18,285 : INFO : vocab #6957\n",
      "2020-12-16 02:26:18,288 : INFO : diff #set()\n",
      "2020-12-16 02:26:19,558 : INFO : alphabet #6957\n",
      "2020-12-16 02:26:20,183 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ18.txt', 'test_data/LibEST_semeru_format/test/us899.c')[[0.9834862802333125, 0.5041628016112985], [0.6587059199810028, 0.34129408], [4.280309687627117, 1.3820678447734642], [8.235031022259143, 5.545344865366236]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 02:26:20,186 : INFO : Removed 13 and 1349 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:26:20,187 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:26:20,190 : INFO : built Dictionary(138 unique tokens: ['access', 'afford', 'defin', 'est', 'full']...) from 2 documents (total 1282 corpus positions)\n",
      "2020-12-16 02:26:20,239 : INFO : token count processed\n",
      "2020-12-16 02:26:20,245 : INFO : frequencies processed\n",
      "2020-12-16 02:26:20,876 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:26:20,877 : INFO : entropies processed\n",
      "2020-12-16 02:26:20,880 : INFO : extropies processed\n",
      "2020-12-16 02:26:20,883 : INFO : token count processed\n",
      "2020-12-16 02:26:20,885 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:26:20,886 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:26:20,887 : INFO : vocab #6957\n",
      "2020-12-16 02:26:20,889 : INFO : diff #set()\n",
      "2020-12-16 02:26:22,172 : INFO : alphabet #6957\n",
      "2020-12-16 02:26:22,808 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ11.txt', 'test_data/LibEST_semeru_format/test/us897.c')[[1.134613747086053, 0.46846882784536237], [0.8869036436080933, 0.11309636], [2.610577243331642, 1.2991811195817504], [6.844565523482998, 4.117608054754624]]\n",
      "2020-12-16 02:26:22,812 : INFO : Removed 61 and 1630 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:26:22,813 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:26:22,815 : INFO : built Dictionary(199 unique tokens: ['accept', 'access', 'allow', 'altern', 'anchor']...) from 2 documents (total 924 corpus positions)\n",
      "2020-12-16 02:26:23,016 : INFO : token count processed\n",
      "2020-12-16 02:26:23,026 : INFO : frequencies processed\n",
      "2020-12-16 02:26:23,646 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:26:23,647 : INFO : entropies processed\n",
      "2020-12-16 02:26:23,648 : INFO : extropies processed\n",
      "2020-12-16 02:26:23,653 : INFO : token count processed\n",
      "2020-12-16 02:26:23,656 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:26:23,658 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:26:23,659 : INFO : vocab #6957\n",
      "2020-12-16 02:26:23,661 : INFO : diff #set()\n",
      "2020-12-16 02:26:24,923 : INFO : alphabet #6957\n",
      "2020-12-16 02:26:25,549 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ32.txt', 'test_data/LibEST_semeru_format/test/us2174.c')[[1.0407154860347976, 0.490024212999454], [0.6965071260929108, 0.30349287], [4.351738068377556, 1.3946177637332438], [9.30062650258688, 5.954154482154259]]\n",
      "2020-12-16 02:26:25,552 : INFO : Removed 19 and 939 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:26:25,553 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:26:25,554 : INFO : built Dictionary(150 unique tokens: ['base', 'connect', 'correspond', 'cover', 'data']...) from 2 documents (total 693 corpus positions)\n",
      "2020-12-16 02:26:25,649 : INFO : token count processed\n",
      "2020-12-16 02:26:25,655 : INFO : frequencies processed\n",
      "2020-12-16 02:26:26,300 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:26:26,301 : INFO : entropies processed\n",
      "2020-12-16 02:26:26,302 : INFO : extropies processed\n",
      "2020-12-16 02:26:26,304 : INFO : token count processed\n",
      "2020-12-16 02:26:26,305 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:26:26,307 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:26:26,307 : INFO : vocab #6957\n",
      "2020-12-16 02:26:26,309 : INFO : diff #set()\n",
      "2020-12-16 02:26:27,563 : INFO : alphabet #6957\n",
      "2020-12-16 02:26:28,215 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ24.txt', 'test_data/LibEST_semeru_format/test/us1060.c')[[1.0967621265483944, 0.47692582164585345], [0.8545064479112625, 0.14549355], [3.8820451081368623, 1.3879142369542152], [7.680453367725382, 5.10137808149812]]\n",
      "2020-12-16 02:26:28,218 : INFO : Removed 5 and 1111 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:26:28,219 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:26:28,221 : INFO : built Dictionary(144 unique tokens: ['client', 'est', 'full', 'function', 'https']...) from 2 documents (total 1054 corpus positions)\n",
      "2020-12-16 02:26:28,276 : INFO : token count processed\n",
      "2020-12-16 02:26:28,281 : INFO : frequencies processed\n",
      "2020-12-16 02:26:28,901 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:26:28,902 : INFO : entropies processed\n",
      "2020-12-16 02:26:28,902 : INFO : extropies processed\n",
      "2020-12-16 02:26:28,905 : INFO : token count processed\n",
      "2020-12-16 02:26:28,906 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:26:28,907 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:26:28,908 : INFO : vocab #6957\n",
      "2020-12-16 02:26:28,910 : INFO : diff #set()\n",
      "2020-12-16 02:26:30,161 : INFO : alphabet #6957\n",
      "2020-12-16 02:26:30,783 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ39.txt', 'test_data/LibEST_semeru_format/test/us1159.c')[[1.097056249009889, 0.47685893045174316], [0.7450655698776245, 0.25493443], [3.238901256602631, 1.35795027283845], [7.352417566952207, 4.074263785471826]]\n",
      "2020-12-16 02:26:30,786 : INFO : Removed 14 and 730 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:26:30,787 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:26:30,789 : INFO : built Dictionary(117 unique tokens: ['also', 'approach', 'base', 'basic', 'cannot']...) from 2 documents (total 765 corpus positions)\n",
      "2020-12-16 02:26:30,852 : INFO : token count processed\n",
      "2020-12-16 02:26:30,865 : INFO : frequencies processed\n",
      "2020-12-16 02:26:31,597 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:26:31,598 : INFO : entropies processed\n",
      "2020-12-16 02:26:31,599 : INFO : extropies processed\n",
      "2020-12-16 02:26:31,603 : INFO : token count processed\n",
      "2020-12-16 02:26:31,606 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:26:31,608 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:26:31,609 : INFO : vocab #6957\n",
      "2020-12-16 02:26:31,611 : INFO : diff #set()\n",
      "2020-12-16 02:26:32,879 : INFO : alphabet #6957\n",
      "2020-12-16 02:26:33,507 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ8.txt', 'test_data/LibEST_semeru_format/test/us1005.c')[[1.0308231192535504, 0.4924111758032182], [0.6991336643695831, 0.30086634], [3.541743520601859, 1.3672728671130518], [7.3390973996212185, 4.604115441654914]]\n",
      "2020-12-16 02:26:33,511 : INFO : Removed 21 and 1349 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:26:33,511 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:26:33,513 : INFO : built Dictionary(142 unique tokens: ['author', 'base', 'check', 'client', 'date']...) from 2 documents (total 1306 corpus positions)\n",
      "2020-12-16 02:26:33,588 : INFO : token count processed\n",
      "2020-12-16 02:26:33,594 : INFO : frequencies processed\n",
      "2020-12-16 02:26:34,241 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:26:34,242 : INFO : entropies processed\n",
      "2020-12-16 02:26:34,242 : INFO : extropies processed\n",
      "2020-12-16 02:26:34,245 : INFO : token count processed\n",
      "2020-12-16 02:26:34,246 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:26:34,248 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:26:34,249 : INFO : vocab #6957\n",
      "2020-12-16 02:26:34,250 : INFO : diff #set()\n",
      "2020-12-16 02:26:35,500 : INFO : alphabet #6957\n",
      "2020-12-16 02:26:36,119 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ33.txt', 'test_data/LibEST_semeru_format/test/us897.c')[[1.0622673412515562, 0.48490318398443744], [0.7237752377986908, 0.27622476], [3.801445732171807, 1.37711375860245], [6.861696777833968, 4.870959732483806]]\n",
      "2020-12-16 02:26:36,123 : INFO : Removed 51 and 1194 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:26:36,124 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:26:36,126 : INFO : built Dictionary(186 unique tokens: ['abil', 'abl', 'along', 'also', 'attribut']...) from 2 documents (total 843 corpus positions)\n",
      "2020-12-16 02:26:36,325 : INFO : token count processed\n",
      "2020-12-16 02:26:36,331 : INFO : frequencies processed\n",
      "2020-12-16 02:26:36,957 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:26:36,958 : INFO : entropies processed\n",
      "2020-12-16 02:26:36,958 : INFO : extropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 02:26:36,961 : INFO : token count processed\n",
      "2020-12-16 02:26:36,962 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:26:36,963 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:26:36,964 : INFO : vocab #6957\n",
      "2020-12-16 02:26:36,966 : INFO : diff #set()\n",
      "2020-12-16 02:26:38,223 : INFO : alphabet #6957\n",
      "2020-12-16 02:26:38,844 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ14.txt', 'test_data/LibEST_semeru_format/test/us903.c')[[1.1013643084569777, 0.4758813100496104], [0.7730646878480911, 0.22693531], [4.005836087367967, 1.3705076892564731], [8.75622759695926, 5.937295746858414]]\n",
      "2020-12-16 02:26:38,847 : INFO : Removed 27 and 664 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:26:38,848 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:26:38,850 : INFO : built Dictionary(137 unique tokens: ['accord', 'advis', 'allow', 'attribut', 'author']...) from 2 documents (total 411 corpus positions)\n",
      "2020-12-16 02:26:38,922 : INFO : token count processed\n",
      "2020-12-16 02:26:38,930 : INFO : frequencies processed\n",
      "2020-12-16 02:26:39,558 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:26:39,559 : INFO : entropies processed\n",
      "2020-12-16 02:26:39,562 : INFO : extropies processed\n",
      "2020-12-16 02:26:39,564 : INFO : token count processed\n",
      "2020-12-16 02:26:39,565 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:26:39,567 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:26:39,568 : INFO : vocab #6957\n",
      "2020-12-16 02:26:39,570 : INFO : diff #set()\n",
      "2020-12-16 02:26:40,820 : INFO : alphabet #6957\n",
      "2020-12-16 02:26:41,442 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ47.txt', 'test_data/LibEST_semeru_format/test/us1864.c')[[1.1586242293648505, 0.4632580262912356], [0.8195871561765671, 0.18041284], [3.75, 1.3846096858033596], [8.68281965979615, 4.997339924164789]]\n",
      "2020-12-16 02:26:41,445 : INFO : Removed 26 and 541 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:26:41,446 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:26:41,447 : INFO : built Dictionary(131 unique tokens: ['administr', 'advis', 'burden', 'check', 'client']...) from 2 documents (total 389 corpus positions)\n",
      "2020-12-16 02:26:41,551 : INFO : token count processed\n",
      "2020-12-16 02:26:41,557 : INFO : frequencies processed\n",
      "2020-12-16 02:26:42,195 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:26:42,196 : INFO : entropies processed\n",
      "2020-12-16 02:26:42,197 : INFO : extropies processed\n",
      "2020-12-16 02:26:42,199 : INFO : token count processed\n",
      "2020-12-16 02:26:42,200 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:26:42,201 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:26:42,202 : INFO : vocab #6957\n",
      "2020-12-16 02:26:42,204 : INFO : diff #set()\n",
      "2020-12-16 02:26:43,465 : INFO : alphabet #6957\n",
      "2020-12-16 02:26:44,094 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ16.txt', 'test_data/LibEST_semeru_format/test/us896.c')[[1.0188328481387243, 0.49533570890822204], [0.8113676905632019, 0.18863231], [3.9857808213564594, 1.3824139750316857], [7.642361397902208, 5.687483324989305]]\n",
      "2020-12-16 02:26:44,097 : INFO : Removed 4 and 2062 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:26:44,101 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:26:44,102 : INFO : built Dictionary(113 unique tokens: ['client', 'est', 'key', 'pair', 'request']...) from 2 documents (total 620 corpus positions)\n",
      "2020-12-16 02:26:44,132 : INFO : token count processed\n",
      "2020-12-16 02:26:44,152 : INFO : frequencies processed\n",
      "2020-12-16 02:26:44,791 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:26:44,792 : INFO : entropies processed\n",
      "2020-12-16 02:26:44,793 : INFO : extropies processed\n",
      "2020-12-16 02:26:44,796 : INFO : token count processed\n",
      "2020-12-16 02:26:44,797 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:26:44,798 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:26:44,799 : INFO : vocab #6957\n",
      "2020-12-16 02:26:44,801 : INFO : diff #set()\n",
      "2020-12-16 02:26:46,052 : INFO : alphabet #6957\n",
      "2020-12-16 02:26:46,684 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ10.txt', 'test_data/LibEST_semeru_format/test/us900.c')[[1.194122298370071, 0.45576310889455046], [0.9494613371789455, 0.050538663], [2.725480556997868, 1.3192201298976014], [7.145345711204595, 3.361417994880327]]\n",
      "2020-12-16 02:26:46,687 : INFO : Removed 242 and 1161 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:26:46,688 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:26:46,691 : INFO : built Dictionary(204 unique tokens: [')\",', '106', '2005', '2006', '2008']...) from 2 documents (total 1197 corpus positions)\n",
      "2020-12-16 02:26:47,013 : INFO : token count processed\n",
      "2020-12-16 02:26:47,018 : INFO : frequencies processed\n",
      "2020-12-16 02:26:47,643 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:26:47,644 : INFO : entropies processed\n",
      "2020-12-16 02:26:47,645 : INFO : extropies processed\n",
      "2020-12-16 02:26:47,648 : INFO : token count processed\n",
      "2020-12-16 02:26:47,649 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:26:47,651 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:26:47,651 : INFO : vocab #6957\n",
      "2020-12-16 02:26:47,653 : INFO : diff #set()\n",
      "2020-12-16 02:26:48,903 : INFO : alphabet #6957\n",
      "2020-12-16 02:26:49,525 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ52.txt', 'test_data/LibEST_semeru_format/test/us4020.c')[[1.1731327608555406, 0.460165167086391], [0.9112118482589722, 0.08878815], [4.2527152789797045, 1.3930676526991816], [7.714725187420844, 6.6367624150616065]]\n",
      "2020-12-16 02:26:49,528 : INFO : Removed 176 and 1921 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:26:49,529 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:26:49,530 : INFO : built Dictionary(220 unique tokens: ['abl', 'accept', 'access', 'act', 'activ']...) from 2 documents (total 851 corpus positions)\n",
      "2020-12-16 02:26:49,868 : INFO : token count processed\n",
      "2020-12-16 02:26:49,874 : INFO : frequencies processed\n",
      "2020-12-16 02:26:50,495 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:26:50,495 : INFO : entropies processed\n",
      "2020-12-16 02:26:50,496 : INFO : extropies processed\n",
      "2020-12-16 02:26:50,502 : INFO : token count processed\n",
      "2020-12-16 02:26:50,504 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:26:50,507 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:26:50,508 : INFO : vocab #6957\n",
      "2020-12-16 02:26:50,510 : INFO : diff #set()\n",
      "2020-12-16 02:26:51,787 : INFO : alphabet #6957\n",
      "2020-12-16 02:26:52,514 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ51.txt', 'test_data/LibEST_semeru_format/test/us895.c')[[1.093198835043804, 0.4777377013871084], [0.9432711526751518, 0.056728847], [4.421519929208282, 1.395607867823301], [7.743650201034459, 6.703101849231112]]\n",
      "2020-12-16 02:26:52,517 : INFO : Removed 56 and 1444 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:26:52,518 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:26:52,521 : INFO : built Dictionary(192 unique tokens: [')\",', '180', '2012', '800', '://']...) from 2 documents (total 1112 corpus positions)\n",
      "2020-12-16 02:26:52,711 : INFO : token count processed\n",
      "2020-12-16 02:26:52,720 : INFO : frequencies processed\n",
      "2020-12-16 02:26:53,344 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:26:53,345 : INFO : entropies processed\n",
      "2020-12-16 02:26:53,346 : INFO : extropies processed\n",
      "2020-12-16 02:26:53,352 : INFO : token count processed\n",
      "2020-12-16 02:26:53,354 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:26:53,356 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:26:53,357 : INFO : vocab #6957\n",
      "2020-12-16 02:26:53,359 : INFO : diff #set()\n",
      "2020-12-16 02:26:54,623 : INFO : alphabet #6957\n",
      "2020-12-16 02:26:55,245 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ53.txt', 'test_data/LibEST_semeru_format/test/us893.c')[[1.1641422743508227, 0.462076829167791], [0.9022139683365822, 0.09778603], [3.823067982273661, 1.3806412677662332], [8.091376365716016, 5.967046954593918]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 02:26:55,248 : INFO : Removed 30 and 664 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:26:55,249 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:26:55,250 : INFO : built Dictionary(136 unique tokens: ['author', 'band', 'base', 'client', 'est']...) from 2 documents (total 409 corpus positions)\n",
      "2020-12-16 02:26:55,307 : INFO : token count processed\n",
      "2020-12-16 02:26:55,320 : INFO : frequencies processed\n",
      "2020-12-16 02:26:55,950 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:26:55,951 : INFO : entropies processed\n",
      "2020-12-16 02:26:55,952 : INFO : extropies processed\n",
      "2020-12-16 02:26:55,954 : INFO : token count processed\n",
      "2020-12-16 02:26:55,955 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:26:55,957 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:26:55,957 : INFO : vocab #6957\n",
      "2020-12-16 02:26:55,959 : INFO : diff #set()\n",
      "2020-12-16 02:26:57,238 : INFO : alphabet #6957\n",
      "2020-12-16 02:26:57,859 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ5.txt', 'test_data/LibEST_semeru_format/test/us1864.c')[[1.072961297636761, 0.48240167394346944], [0.7262915670871735, 0.27370843], [3.4225779953216033, 1.3541446630335485], [8.66071405220342, 4.971405183598105]]\n",
      "2020-12-16 02:26:57,863 : INFO : Removed 47 and 1391 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:26:57,864 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:26:57,865 : INFO : built Dictionary(184 unique tokens: ['allow', 'anchor', 'author', 'avail', 'client']...) from 2 documents (total 1356 corpus positions)\n",
      "2020-12-16 02:26:58,030 : INFO : token count processed\n",
      "2020-12-16 02:26:58,035 : INFO : frequencies processed\n",
      "2020-12-16 02:26:58,680 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:26:58,681 : INFO : entropies processed\n",
      "2020-12-16 02:26:58,682 : INFO : extropies processed\n",
      "2020-12-16 02:26:58,685 : INFO : token count processed\n",
      "2020-12-16 02:26:58,686 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:26:58,688 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:26:58,689 : INFO : vocab #6957\n",
      "2020-12-16 02:26:58,691 : INFO : diff #set()\n",
      "2020-12-16 02:26:59,946 : INFO : alphabet #6957\n",
      "2020-12-16 02:27:00,565 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ4.txt', 'test_data/LibEST_semeru_format/test/us898.c')[[1.089344810950601, 0.47861894061661575], [0.7835303694009781, 0.21646963], [3.802568777243488, 1.3596667985057875], [7.312245534554323, 5.363232101650167]]\n",
      "2020-12-16 02:27:00,568 : INFO : Removed 74 and 939 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:27:00,569 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:27:00,571 : INFO : built Dictionary(172 unique tokens: ['accept', 'administr', 'advis', 'also', 'answer']...) from 2 documents (total 817 corpus positions)\n",
      "2020-12-16 02:27:00,796 : INFO : token count processed\n",
      "2020-12-16 02:27:00,807 : INFO : frequencies processed\n",
      "2020-12-16 02:27:01,430 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:27:01,431 : INFO : entropies processed\n",
      "2020-12-16 02:27:01,431 : INFO : extropies processed\n",
      "2020-12-16 02:27:01,434 : INFO : token count processed\n",
      "2020-12-16 02:27:01,435 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:27:01,438 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:27:01,439 : INFO : vocab #6957\n",
      "2020-12-16 02:27:01,442 : INFO : diff #set()\n",
      "2020-12-16 02:27:02,711 : INFO : alphabet #6957\n",
      "2020-12-16 02:27:03,334 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ38.txt', 'test_data/LibEST_semeru_format/test/us1060.c')[[0.9512271402746568, 0.5124979964450673], [0.7900047153234482, 0.20999528], [4.997754941557433, 1.4117682323976446], [7.8683309794568475, 6.222425187212736]]\n",
      "2020-12-16 02:27:03,337 : INFO : Removed 62 and 1111 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:27:03,338 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:27:03,339 : INFO : built Dictionary(176 unique tokens: ['anon', 'aris', 'associ', 'author', 'avail']...) from 2 documents (total 1180 corpus positions)\n",
      "2020-12-16 02:27:03,522 : INFO : token count processed\n",
      "2020-12-16 02:27:03,528 : INFO : frequencies processed\n",
      "2020-12-16 02:27:04,149 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:27:04,150 : INFO : entropies processed\n",
      "2020-12-16 02:27:04,151 : INFO : extropies processed\n",
      "2020-12-16 02:27:04,156 : INFO : token count processed\n",
      "2020-12-16 02:27:04,158 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:27:04,160 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:27:04,161 : INFO : vocab #6957\n",
      "2020-12-16 02:27:04,165 : INFO : diff #set()\n",
      "2020-12-16 02:27:05,431 : INFO : alphabet #6957\n",
      "2020-12-16 02:27:06,063 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ18.txt', 'test_data/LibEST_semeru_format/test/us1159.c')[[1.0514132340833446, 0.48746882558103455], [0.7352365255355835, 0.26476347], [3.956364605965054, 1.36803018186868], [7.501689078207446, 5.451376018703349]]\n",
      "2020-12-16 02:27:06,066 : INFO : Removed 123 and 939 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:27:06,067 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:27:06,069 : INFO : built Dictionary(180 unique tokens: ['agre', 'associ', 'attribut', 'base', 'case']...) from 2 documents (total 948 corpus positions)\n",
      "2020-12-16 02:27:06,311 : INFO : token count processed\n",
      "2020-12-16 02:27:06,321 : INFO : frequencies processed\n",
      "2020-12-16 02:27:06,941 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:27:06,942 : INFO : entropies processed\n",
      "2020-12-16 02:27:06,943 : INFO : extropies processed\n",
      "2020-12-16 02:27:06,948 : INFO : token count processed\n",
      "2020-12-16 02:27:06,951 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:27:06,953 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:27:06,954 : INFO : vocab #6957\n",
      "2020-12-16 02:27:06,956 : INFO : diff #set()\n",
      "2020-12-16 02:27:08,211 : INFO : alphabet #6957\n",
      "2020-12-16 02:27:08,832 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us1060.c')[[1.0446191854461877, 0.48908863181863116], [0.8561900854110718, 0.14380991], [4.572538267537758, 1.3996511561223428], [7.8992318904585925, 5.838976888088428]]\n",
      "2020-12-16 02:27:08,836 : INFO : Removed 22 and 2029 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:27:08,837 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:27:08,840 : INFO : built Dictionary(186 unique tokens: ['also', 'associ', 'avail', 'client', 'connect']...) from 2 documents (total 1565 corpus positions)\n",
      "2020-12-16 02:27:08,978 : INFO : token count processed\n",
      "2020-12-16 02:27:08,988 : INFO : frequencies processed\n",
      "2020-12-16 02:27:09,619 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:27:09,620 : INFO : entropies processed\n",
      "2020-12-16 02:27:09,621 : INFO : extropies processed\n",
      "2020-12-16 02:27:09,627 : INFO : token count processed\n",
      "2020-12-16 02:27:09,629 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:27:09,631 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:27:09,632 : INFO : vocab #6957\n",
      "2020-12-16 02:27:09,634 : INFO : diff #set()\n",
      "2020-12-16 02:27:10,910 : INFO : alphabet #6957\n",
      "2020-12-16 02:27:11,534 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ15.txt', 'test_data/LibEST_semeru_format/test/us899.c')[[1.050412948239541, 0.48770663531879643], [0.6975863873958588, 0.3024136], [3.6789347039237965, 1.3729955572394423], [8.21150462245447, 5.164976710451661]]\n",
      "2020-12-16 02:27:11,537 : INFO : Removed 61 and 664 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:27:11,538 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:27:11,539 : INFO : built Dictionary(161 unique tokens: ['accept', 'access', 'allow', 'altern', 'anchor']...) from 2 documents (total 471 corpus positions)\n",
      "2020-12-16 02:27:11,678 : INFO : token count processed\n",
      "2020-12-16 02:27:11,684 : INFO : frequencies processed\n",
      "2020-12-16 02:27:12,324 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:27:12,324 : INFO : entropies processed\n",
      "2020-12-16 02:27:12,325 : INFO : extropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 02:27:12,327 : INFO : token count processed\n",
      "2020-12-16 02:27:12,329 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:27:12,330 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:27:12,331 : INFO : vocab #6957\n",
      "2020-12-16 02:27:12,332 : INFO : diff #set()\n",
      "2020-12-16 02:27:13,590 : INFO : alphabet #6957\n",
      "2020-12-16 02:27:14,318 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ32.txt', 'test_data/LibEST_semeru_format/test/us1864.c')[[1.0570176580637678, 0.4861406979565169], [0.723444014787674, 0.276556], [4.111698692943856, 1.390201917474914], [8.739508040451842, 5.897158532459386]]\n",
      "2020-12-16 02:27:14,321 : INFO : Removed 59 and 577 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:27:14,322 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:27:14,323 : INFO : built Dictionary(152 unique tokens: ['anchor', 'author', 'avail', 'behind', 'browser']...) from 2 documents (total 650 corpus positions)\n",
      "2020-12-16 02:27:14,453 : INFO : token count processed\n",
      "2020-12-16 02:27:14,459 : INFO : frequencies processed\n",
      "2020-12-16 02:27:15,092 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:27:15,093 : INFO : entropies processed\n",
      "2020-12-16 02:27:15,094 : INFO : extropies processed\n",
      "2020-12-16 02:27:15,096 : INFO : token count processed\n",
      "2020-12-16 02:27:15,097 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:27:15,099 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:27:15,099 : INFO : vocab #6957\n",
      "2020-12-16 02:27:15,101 : INFO : diff #set()\n",
      "2020-12-16 02:27:16,351 : INFO : alphabet #6957\n",
      "2020-12-16 02:27:16,972 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ2.txt', 'test_data/LibEST_semeru_format/test/us3612.c')[[1.0637775731401105, 0.48454834135951225], [0.7526376843452454, 0.24736232], [3.586350159156343, 1.36189963862188], [7.655011999214624, 5.786838174987436]]\n",
      "2020-12-16 02:27:16,976 : INFO : Removed 65 and 1630 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:27:16,977 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:27:16,978 : INFO : built Dictionary(195 unique tokens: ['access', 'alt', 'attribut', 'base', 'cannot']...) from 2 documents (total 949 corpus positions)\n",
      "2020-12-16 02:27:17,164 : INFO : token count processed\n",
      "2020-12-16 02:27:17,170 : INFO : frequencies processed\n",
      "2020-12-16 02:27:17,793 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:27:17,794 : INFO : entropies processed\n",
      "2020-12-16 02:27:17,794 : INFO : extropies processed\n",
      "2020-12-16 02:27:17,797 : INFO : token count processed\n",
      "2020-12-16 02:27:17,799 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:27:17,800 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:27:17,801 : INFO : vocab #6957\n",
      "2020-12-16 02:27:17,802 : INFO : diff #set()\n",
      "2020-12-16 02:27:19,056 : INFO : alphabet #6957\n",
      "2020-12-16 02:27:19,678 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ36.txt', 'test_data/LibEST_semeru_format/test/us2174.c')[[1.0102914305472408, 0.49744031377966935], [0.6443778872489929, 0.3556221], [4.396461357786372, 1.3904209813319337], [9.263477512973235, 5.781192801833932]]\n",
      "2020-12-16 02:27:19,681 : INFO : Removed 13 and 957 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:27:19,682 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:27:19,683 : INFO : built Dictionary(126 unique tokens: ['accord', 'author', 'check', 'client', 'est']...) from 2 documents (total 785 corpus positions)\n",
      "2020-12-16 02:27:19,733 : INFO : token count processed\n",
      "2020-12-16 02:27:19,739 : INFO : frequencies processed\n",
      "2020-12-16 02:27:20,357 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:27:20,358 : INFO : entropies processed\n",
      "2020-12-16 02:27:20,359 : INFO : extropies processed\n",
      "2020-12-16 02:27:20,361 : INFO : token count processed\n",
      "2020-12-16 02:27:20,362 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:27:20,364 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:27:20,365 : INFO : vocab #6957\n",
      "2020-12-16 02:27:20,366 : INFO : diff #set()\n",
      "2020-12-16 02:27:21,622 : INFO : alphabet #6957\n",
      "2020-12-16 02:27:22,243 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ28.txt', 'test_data/LibEST_semeru_format/test/us3496.c')[[1.1091580153494505, 0.4741228455727237], [0.8231562376022339, 0.17684376], [3.155221528859512, 1.349079551388468], [7.349094326077514, 4.580880797428595]]\n",
      "2020-12-16 02:27:22,246 : INFO : Removed 47 and 664 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:27:22,247 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:27:22,250 : INFO : built Dictionary(157 unique tokens: ['allow', 'anchor', 'author', 'avail', 'client']...) from 2 documents (total 471 corpus positions)\n",
      "2020-12-16 02:27:22,392 : INFO : token count processed\n",
      "2020-12-16 02:27:22,403 : INFO : frequencies processed\n",
      "2020-12-16 02:27:23,048 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:27:23,049 : INFO : entropies processed\n",
      "2020-12-16 02:27:23,050 : INFO : extropies processed\n",
      "2020-12-16 02:27:23,054 : INFO : token count processed\n",
      "2020-12-16 02:27:23,056 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:27:23,059 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:27:23,060 : INFO : vocab #6957\n",
      "2020-12-16 02:27:23,063 : INFO : diff #set()\n",
      "2020-12-16 02:27:24,321 : INFO : alphabet #6957\n",
      "2020-12-16 02:27:24,943 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ4.txt', 'test_data/LibEST_semeru_format/test/us1864.c')[[1.0424725717656633, 0.4896026579860147], [0.6540057957172394, 0.3459942], [3.3474341548153594, 1.3078740931363841], [8.658846583397173, 5.44432539655808]]\n",
      "2020-12-16 02:27:24,946 : INFO : Removed 47 and 1225 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:27:24,947 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:27:24,950 : INFO : built Dictionary(205 unique tokens: ['altern', 'author', 'base', 'cbc', 'check']...) from 2 documents (total 1271 corpus positions)\n",
      "2020-12-16 02:27:25,164 : INFO : token count processed\n",
      "2020-12-16 02:27:25,174 : INFO : frequencies processed\n",
      "2020-12-16 02:27:25,816 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:27:25,817 : INFO : entropies processed\n",
      "2020-12-16 02:27:25,818 : INFO : extropies processed\n",
      "2020-12-16 02:27:25,823 : INFO : token count processed\n",
      "2020-12-16 02:27:25,826 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:27:25,828 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:27:25,829 : INFO : vocab #6957\n",
      "2020-12-16 02:27:25,831 : INFO : diff #set()\n",
      "2020-12-16 02:27:27,088 : INFO : alphabet #6957\n",
      "2020-12-16 02:27:27,715 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ22.txt', 'test_data/LibEST_semeru_format/test/us3512.c')[[1.027783545684644, 0.49314928219440124], [0.7117276191711426, 0.28827238], [3.931758922018929, 1.3774553103031586], [8.171141187118925, 5.407607721551495]]\n",
      "2020-12-16 02:27:27,718 : INFO : Removed 5 and 1921 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:27:27,719 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:27:27,720 : INFO : built Dictionary(108 unique tokens: ['client', 'est', 'full', 'function', 'https']...) from 2 documents (total 535 corpus positions)\n",
      "2020-12-16 02:27:27,757 : INFO : token count processed\n",
      "2020-12-16 02:27:27,768 : INFO : frequencies processed\n",
      "2020-12-16 02:27:28,388 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:27:28,389 : INFO : entropies processed\n",
      "2020-12-16 02:27:28,389 : INFO : extropies processed\n",
      "2020-12-16 02:27:28,392 : INFO : token count processed\n",
      "2020-12-16 02:27:28,393 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:27:28,395 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:27:28,395 : INFO : vocab #6957\n",
      "2020-12-16 02:27:28,397 : INFO : diff #set()\n",
      "2020-12-16 02:27:29,653 : INFO : alphabet #6957\n",
      "2020-12-16 02:27:30,273 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ39.txt', 'test_data/LibEST_semeru_format/test/us895.c')[[1.169749100252545, 0.4608827812780778], [0.9440939202904701, 0.05590608], [2.9219280948873623, 1.3359016564230495], [7.136116076330507, 4.06249751281558]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 02:27:30,277 : INFO : Removed 138 and 1468 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:27:30,278 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:27:30,281 : INFO : built Dictionary(196 unique tokens: ['132', 'access', 'address', 'attr', 'attribut']...) from 2 documents (total 1574 corpus positions)\n",
      "2020-12-16 02:27:30,546 : INFO : token count processed\n",
      "2020-12-16 02:27:30,556 : INFO : frequencies processed\n",
      "2020-12-16 02:27:31,172 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:27:31,173 : INFO : entropies processed\n",
      "2020-12-16 02:27:31,173 : INFO : extropies processed\n",
      "2020-12-16 02:27:31,176 : INFO : token count processed\n",
      "2020-12-16 02:27:31,178 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:27:31,179 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:27:31,180 : INFO : vocab #6957\n",
      "2020-12-16 02:27:31,181 : INFO : diff #set()\n",
      "2020-12-16 02:27:32,440 : INFO : alphabet #6957\n",
      "2020-12-16 02:27:33,061 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ49.txt', 'test_data/LibEST_semeru_format/test/us894.c')[[1.02128548406699, 0.49473466656868237], [0.7805512547492981, 0.21944875], [4.8790343826422005, 1.4111376350060019], [8.005841692672565, 6.224471618633816]]\n",
      "2020-12-16 02:27:33,065 : INFO : Removed 45 and 1349 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:27:33,065 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:27:33,067 : INFO : built Dictionary(152 unique tokens: ['anchor', 'attack', 'author', 'base', 'client']...) from 2 documents (total 1315 corpus positions)\n",
      "2020-12-16 02:27:33,160 : INFO : token count processed\n",
      "2020-12-16 02:27:33,166 : INFO : frequencies processed\n",
      "2020-12-16 02:27:33,891 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:27:33,892 : INFO : entropies processed\n",
      "2020-12-16 02:27:33,893 : INFO : extropies processed\n",
      "2020-12-16 02:27:33,895 : INFO : token count processed\n",
      "2020-12-16 02:27:33,897 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:27:33,898 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:27:33,899 : INFO : vocab #6957\n",
      "2020-12-16 02:27:33,901 : INFO : diff #set()\n",
      "2020-12-16 02:27:35,164 : INFO : alphabet #6957\n",
      "2020-12-16 02:27:35,786 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ23.txt', 'test_data/LibEST_semeru_format/test/us897.c')[[1.1221767673824763, 0.4712142811898815], [0.8598650246858597, 0.14013498], [3.2487781244591325, 1.3283280683424405], [6.927395151008854, 5.198354399329533]]\n",
      "2020-12-16 02:27:35,790 : INFO : Removed 6 and 1682 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:27:35,791 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:27:35,793 : INFO : built Dictionary(206 unique tokens: ['client', 'est', 'less', 'mutual', 'section']...) from 2 documents (total 1269 corpus positions)\n",
      "2020-12-16 02:27:35,857 : INFO : token count processed\n",
      "2020-12-16 02:27:35,863 : INFO : frequencies processed\n",
      "2020-12-16 02:27:36,485 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:27:36,486 : INFO : entropies processed\n",
      "2020-12-16 02:27:36,487 : INFO : extropies processed\n",
      "2020-12-16 02:27:36,489 : INFO : token count processed\n",
      "2020-12-16 02:27:36,491 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:27:36,492 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:27:36,493 : INFO : vocab #6957\n",
      "2020-12-16 02:27:36,496 : INFO : diff #set()\n",
      "2020-12-16 02:27:37,766 : INFO : alphabet #6957\n",
      "2020-12-16 02:27:38,388 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ7.txt', 'test_data/LibEST_semeru_format/test/us1883.c')[[1.1099564827484756, 0.4739434240356361], [0.7441072165966034, 0.25589278], [3.238901256602631, 1.3579502728384498], [9.009761848186042, 3.7183751770766165]]\n",
      "2020-12-16 02:27:38,391 : INFO : Removed 61 and 2062 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:27:38,392 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:27:38,393 : INFO : built Dictionary(151 unique tokens: ['accept', 'access', 'allow', 'altern', 'anchor']...) from 2 documents (total 708 corpus positions)\n",
      "2020-12-16 02:27:38,528 : INFO : token count processed\n",
      "2020-12-16 02:27:38,535 : INFO : frequencies processed\n",
      "2020-12-16 02:27:39,153 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:27:39,154 : INFO : entropies processed\n",
      "2020-12-16 02:27:39,155 : INFO : extropies processed\n",
      "2020-12-16 02:27:39,158 : INFO : token count processed\n",
      "2020-12-16 02:27:39,159 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:27:39,160 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:27:39,161 : INFO : vocab #6957\n",
      "2020-12-16 02:27:39,163 : INFO : diff #set()\n",
      "2020-12-16 02:27:40,412 : INFO : alphabet #6957\n",
      "2020-12-16 02:27:41,029 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ32.txt', 'test_data/LibEST_semeru_format/test/us900.c')[[1.1505832668257545, 0.46499013333996264], [0.9474450051784515, 0.052554995], [3.5144093452479406, 1.3636298092327386], [7.346550070211094, 5.807253607411404]]\n",
      "2020-12-16 02:27:41,032 : INFO : Removed 14 and 939 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:27:41,033 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:27:41,034 : INFO : built Dictionary(143 unique tokens: ['also', 'approach', 'base', 'basic', 'cannot']...) from 2 documents (total 689 corpus positions)\n",
      "2020-12-16 02:27:41,105 : INFO : token count processed\n",
      "2020-12-16 02:27:41,111 : INFO : frequencies processed\n",
      "2020-12-16 02:27:41,729 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:27:41,730 : INFO : entropies processed\n",
      "2020-12-16 02:27:41,731 : INFO : extropies processed\n",
      "2020-12-16 02:27:41,735 : INFO : token count processed\n",
      "2020-12-16 02:27:41,738 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:27:41,740 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:27:41,741 : INFO : vocab #6957\n",
      "2020-12-16 02:27:41,743 : INFO : diff #set()\n",
      "2020-12-16 02:27:43,055 : INFO : alphabet #6957\n",
      "2020-12-16 02:27:43,678 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ8.txt', 'test_data/LibEST_semeru_format/test/us1060.c')[[1.0546091443782015, 0.4867105759439399], [0.8106949329376221, 0.18930507], [3.4676694465277844, 1.3652172389854602], [7.644342889087002, 4.607721616615096]]\n",
      "2020-12-16 02:27:43,682 : INFO : Removed 22 and 2062 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:27:43,683 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:27:43,684 : INFO : built Dictionary(136 unique tokens: ['also', 'associ', 'avail', 'client', 'connect']...) from 2 documents (total 675 corpus positions)\n",
      "2020-12-16 02:27:43,775 : INFO : token count processed\n",
      "2020-12-16 02:27:43,781 : INFO : frequencies processed\n",
      "2020-12-16 02:27:44,416 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:27:44,416 : INFO : entropies processed\n",
      "2020-12-16 02:27:44,417 : INFO : extropies processed\n",
      "2020-12-16 02:27:44,420 : INFO : token count processed\n",
      "2020-12-16 02:27:44,421 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:27:44,422 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:27:44,423 : INFO : vocab #6957\n",
      "2020-12-16 02:27:44,424 : INFO : diff #set()\n",
      "2020-12-16 02:27:45,679 : INFO : alphabet #6957\n",
      "2020-12-16 02:27:46,308 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ15.txt', 'test_data/LibEST_semeru_format/test/us900.c')[[1.1481451811778596, 0.46551788434135777], [0.9468868859112263, 0.053113114], [3.0037016960573486, 1.3267221642905656], [7.2478866653044935, 5.098377992062698]]\n",
      "2020-12-16 02:27:46,312 : INFO : Removed 45 and 957 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:27:46,312 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:27:46,314 : INFO : built Dictionary(142 unique tokens: ['anchor', 'attack', 'author', 'base', 'client']...) from 2 documents (total 815 corpus positions)\n",
      "2020-12-16 02:27:46,397 : INFO : token count processed\n",
      "2020-12-16 02:27:46,403 : INFO : frequencies processed\n",
      "2020-12-16 02:27:47,038 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:27:47,039 : INFO : entropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 02:27:47,040 : INFO : extropies processed\n",
      "2020-12-16 02:27:47,042 : INFO : token count processed\n",
      "2020-12-16 02:27:47,044 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:27:47,045 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:27:47,046 : INFO : vocab #6957\n",
      "2020-12-16 02:27:47,047 : INFO : diff #set()\n",
      "2020-12-16 02:27:48,302 : INFO : alphabet #6957\n",
      "2020-12-16 02:27:48,922 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ23.txt', 'test_data/LibEST_semeru_format/test/us3496.c')[[1.150531849841959, 0.4650012507712868], [0.8678732961416245, 0.1321267], [2.972253928364927, 1.311764078962958], [7.444103259461646, 5.177130113905775]]\n",
      "2020-12-16 02:27:48,925 : INFO : Removed 47 and 2029 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:27:48,926 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:27:48,929 : INFO : built Dictionary(190 unique tokens: ['altern', 'author', 'base', 'cbc', 'check']...) from 2 documents (total 1614 corpus positions)\n",
      "2020-12-16 02:27:49,111 : INFO : token count processed\n",
      "2020-12-16 02:27:49,120 : INFO : frequencies processed\n",
      "2020-12-16 02:27:49,738 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:27:49,739 : INFO : entropies processed\n",
      "2020-12-16 02:27:49,740 : INFO : extropies processed\n",
      "2020-12-16 02:27:49,743 : INFO : token count processed\n",
      "2020-12-16 02:27:49,745 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:27:49,746 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:27:49,747 : INFO : vocab #6957\n",
      "2020-12-16 02:27:49,748 : INFO : diff #set()\n",
      "2020-12-16 02:27:50,995 : INFO : alphabet #6957\n",
      "2020-12-16 02:27:51,618 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ22.txt', 'test_data/LibEST_semeru_format/test/us899.c')[[0.9567952199398826, 0.5110396784548168], [0.6332234740257263, 0.36677653], [4.256791149131849, 1.3857048941704397], [8.216640739426587, 5.4384586602102125]]\n",
      "2020-12-16 02:27:51,621 : INFO : Removed 47 and 1161 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:27:51,622 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:27:51,625 : INFO : built Dictionary(149 unique tokens: ['altern', 'author', 'base', 'cbc', 'check']...) from 2 documents (total 1052 corpus positions)\n",
      "2020-12-16 02:27:51,760 : INFO : token count processed\n",
      "2020-12-16 02:27:51,768 : INFO : frequencies processed\n",
      "2020-12-16 02:27:52,389 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:27:52,390 : INFO : entropies processed\n",
      "2020-12-16 02:27:52,391 : INFO : extropies processed\n",
      "2020-12-16 02:27:52,393 : INFO : token count processed\n",
      "2020-12-16 02:27:52,395 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:27:52,396 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:27:52,397 : INFO : vocab #6957\n",
      "2020-12-16 02:27:52,399 : INFO : diff #set()\n",
      "2020-12-16 02:27:53,755 : INFO : alphabet #6957\n",
      "2020-12-16 02:27:54,375 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ22.txt', 'test_data/LibEST_semeru_format/test/us4020.c')[[1.066364849237549, 0.4839416429141165], [0.7882104963064194, 0.2117895], [3.643552536176544, 1.3588926826227992], [7.164122595713905, 5.318174704507041]]\n",
      "2020-12-16 02:27:54,378 : INFO : Removed 47 and 939 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:27:54,379 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:27:54,380 : INFO : built Dictionary(164 unique tokens: ['allow', 'anchor', 'author', 'avail', 'client']...) from 2 documents (total 748 corpus positions)\n",
      "2020-12-16 02:27:54,522 : INFO : token count processed\n",
      "2020-12-16 02:27:54,528 : INFO : frequencies processed\n",
      "2020-12-16 02:27:55,169 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:27:55,170 : INFO : entropies processed\n",
      "2020-12-16 02:27:55,170 : INFO : extropies processed\n",
      "2020-12-16 02:27:55,173 : INFO : token count processed\n",
      "2020-12-16 02:27:55,174 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:27:55,175 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:27:55,176 : INFO : vocab #6957\n",
      "2020-12-16 02:27:55,178 : INFO : diff #set()\n",
      "2020-12-16 02:27:56,425 : INFO : alphabet #6957\n",
      "2020-12-16 02:27:57,074 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ4.txt', 'test_data/LibEST_semeru_format/test/us1060.c')[[1.047966720736336, 0.48828918452368947], [0.7836792021989822, 0.2163208], [3.706504840031717, 1.3508575325934031], [7.724297310774476, 5.355657272368054]]\n",
      "2020-12-16 02:27:57,078 : INFO : Removed 573 and 1194 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:27:57,079 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:27:57,081 : INFO : built Dictionary(186 unique tokens: ['aan', 'accept', 'also', 'aph', 'app']...) from 2 documents (total 828 corpus positions)\n",
      "2020-12-16 02:27:57,310 : INFO : token count processed\n",
      "2020-12-16 02:27:57,319 : INFO : frequencies processed\n",
      "2020-12-16 02:27:57,940 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:27:57,941 : INFO : entropies processed\n",
      "2020-12-16 02:27:57,942 : INFO : extropies processed\n",
      "2020-12-16 02:27:57,947 : INFO : token count processed\n",
      "2020-12-16 02:27:57,949 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:27:57,952 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:27:57,952 : INFO : vocab #6957\n",
      "2020-12-16 02:27:57,954 : INFO : diff #set()\n",
      "2020-12-16 02:27:59,197 : INFO : alphabet #6957\n",
      "2020-12-16 02:27:59,820 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ58.txt', 'test_data/LibEST_semeru_format/test/us903.c')[[1.0112582858506483, 0.4972011834755757], [0.7746642678976059, 0.22533573], [4.762143689773634, 1.3998289401732493], [9.503464343347172, 8.24927407794544]]\n",
      "2020-12-16 02:27:59,823 : INFO : Removed 39 and 939 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:27:59,824 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:27:59,825 : INFO : built Dictionary(146 unique tokens: ['attribut', 'bound', 'client', 'defin', 'document']...) from 2 documents (total 708 corpus positions)\n",
      "2020-12-16 02:27:59,909 : INFO : token count processed\n",
      "2020-12-16 02:27:59,918 : INFO : frequencies processed\n",
      "2020-12-16 02:28:00,539 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:28:00,540 : INFO : entropies processed\n",
      "2020-12-16 02:28:00,541 : INFO : extropies processed\n",
      "2020-12-16 02:28:00,543 : INFO : token count processed\n",
      "2020-12-16 02:28:00,544 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:28:00,546 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:28:00,546 : INFO : vocab #6957\n",
      "2020-12-16 02:28:00,548 : INFO : diff #set()\n",
      "2020-12-16 02:28:01,796 : INFO : alphabet #6957\n",
      "2020-12-16 02:28:02,415 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ45.txt', 'test_data/LibEST_semeru_format/test/us1060.c')[[1.0415243117603876, 0.4898300716966281], [0.8720030635595322, 0.12799694], [3.478670650299986, 1.345652510582805], [7.6795338636468475, 4.591176439355959]]\n",
      "2020-12-16 02:28:02,418 : INFO : Removed 13 and 939 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:28:02,419 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:28:02,420 : INFO : built Dictionary(141 unique tokens: ['access', 'afford', 'defin', 'est', 'full']...) from 2 documents (total 676 corpus positions)\n",
      "2020-12-16 02:28:02,470 : INFO : token count processed\n",
      "2020-12-16 02:28:02,476 : INFO : frequencies processed\n",
      "2020-12-16 02:28:03,100 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:28:03,101 : INFO : entropies processed\n",
      "2020-12-16 02:28:03,105 : INFO : extropies processed\n",
      "2020-12-16 02:28:03,107 : INFO : token count processed\n",
      "2020-12-16 02:28:03,108 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:28:03,110 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:28:03,111 : INFO : vocab #6957\n",
      "2020-12-16 02:28:03,114 : INFO : diff #set()\n",
      "2020-12-16 02:28:04,384 : INFO : alphabet #6957\n",
      "2020-12-16 02:28:05,001 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ11.txt', 'test_data/LibEST_semeru_format/test/us1060.c')[[1.1123976114858203, 0.47339572557867976], [0.8932280093431473, 0.10677199], [2.9139770731827523, 1.3356231683419404], [7.642070411915824, 4.115322261499974]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 02:28:05,005 : INFO : Removed 59 and 2062 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:28:05,006 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:28:05,008 : INFO : built Dictionary(159 unique tokens: ['anchor', 'author', 'avail', 'behind', 'browser']...) from 2 documents (total 746 corpus positions)\n",
      "2020-12-16 02:28:05,167 : INFO : token count processed\n",
      "2020-12-16 02:28:05,173 : INFO : frequencies processed\n",
      "2020-12-16 02:28:05,802 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:28:05,803 : INFO : entropies processed\n",
      "2020-12-16 02:28:05,804 : INFO : extropies processed\n",
      "2020-12-16 02:28:05,807 : INFO : token count processed\n",
      "2020-12-16 02:28:05,808 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:28:05,809 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:28:05,810 : INFO : vocab #6957\n",
      "2020-12-16 02:28:05,812 : INFO : diff #set()\n",
      "2020-12-16 02:28:07,060 : INFO : alphabet #6957\n",
      "2020-12-16 02:28:07,679 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ2.txt', 'test_data/LibEST_semeru_format/test/us900.c')[[1.133492480133739, 0.4687150338290925], [0.933919683098793, 0.06608032], [3.1515225587212936, 1.3176949708866887], [7.379172125523214, 5.845305817659298]]\n",
      "2020-12-16 02:28:07,682 : INFO : Removed 13 and 957 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:28:07,683 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:28:07,684 : INFO : built Dictionary(126 unique tokens: ['access', 'afford', 'defin', 'est', 'full']...) from 2 documents (total 782 corpus positions)\n",
      "2020-12-16 02:28:07,722 : INFO : token count processed\n",
      "2020-12-16 02:28:07,727 : INFO : frequencies processed\n",
      "2020-12-16 02:28:08,358 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:28:08,359 : INFO : entropies processed\n",
      "2020-12-16 02:28:08,359 : INFO : extropies processed\n",
      "2020-12-16 02:28:08,362 : INFO : token count processed\n",
      "2020-12-16 02:28:08,363 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:28:08,364 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:28:08,365 : INFO : vocab #6957\n",
      "2020-12-16 02:28:08,367 : INFO : diff #set()\n",
      "2020-12-16 02:28:09,614 : INFO : alphabet #6957\n",
      "2020-12-16 02:28:10,235 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ11.txt', 'test_data/LibEST_semeru_format/test/us3496.c')[[1.1429239055513774, 0.4666521276884531], [0.901836097240448, 0.0981639], [2.7219280948873625, 1.3198385641318495], [7.352026223045901, 4.1056311782207535]]\n",
      "2020-12-16 02:28:10,239 : INFO : Removed 39 and 1105 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:28:10,240 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:28:10,242 : INFO : built Dictionary(153 unique tokens: ['attribut', 'bound', 'client', 'defin', 'document']...) from 2 documents (total 1189 corpus positions)\n",
      "2020-12-16 02:28:10,337 : INFO : token count processed\n",
      "2020-12-16 02:28:10,344 : INFO : frequencies processed\n",
      "2020-12-16 02:28:10,966 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:28:10,967 : INFO : entropies processed\n",
      "2020-12-16 02:28:10,967 : INFO : extropies processed\n",
      "2020-12-16 02:28:10,970 : INFO : token count processed\n",
      "2020-12-16 02:28:10,971 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:28:10,973 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:28:10,974 : INFO : vocab #6957\n",
      "2020-12-16 02:28:10,975 : INFO : diff #set()\n",
      "2020-12-16 02:28:12,332 : INFO : alphabet #6957\n",
      "2020-12-16 02:28:12,954 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ45.txt', 'test_data/LibEST_semeru_format/test/us901.c')[[1.0338298325201951, 0.4916832195154019], [0.7924119979143143, 0.207588], [3.289275520881484, 1.2943756935909785], [7.709345830839427, 4.617141590691163]]\n",
      "2020-12-16 02:28:12,958 : INFO : Removed 26 and 1682 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:28:12,959 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:28:12,960 : INFO : built Dictionary(228 unique tokens: ['administr', 'advis', 'burden', 'check', 'client']...) from 2 documents (total 1350 corpus positions)\n",
      "2020-12-16 02:28:13,230 : INFO : token count processed\n",
      "2020-12-16 02:28:13,239 : INFO : frequencies processed\n",
      "2020-12-16 02:28:13,861 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:28:13,862 : INFO : entropies processed\n",
      "2020-12-16 02:28:13,862 : INFO : extropies processed\n",
      "2020-12-16 02:28:13,865 : INFO : token count processed\n",
      "2020-12-16 02:28:13,867 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:28:13,869 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:28:13,870 : INFO : vocab #6957\n",
      "2020-12-16 02:28:13,873 : INFO : diff #set()\n",
      "2020-12-16 02:28:15,136 : INFO : alphabet #6957\n",
      "2020-12-16 02:28:15,762 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ16.txt', 'test_data/LibEST_semeru_format/test/us1883.c')[[0.9546155572388956, 0.5116095573354627], [0.5928142368793488, 0.40718576], [4.978373781480403, 1.4147104852892824], [9.0521931295105, 5.911112196821236]]\n",
      "2020-12-16 02:28:15,765 : INFO : Removed 5 and 1349 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:28:15,766 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:28:15,768 : INFO : built Dictionary(137 unique tokens: ['client', 'est', 'full', 'function', 'https']...) from 2 documents (total 1273 corpus positions)\n",
      "2020-12-16 02:28:15,822 : INFO : token count processed\n",
      "2020-12-16 02:28:15,828 : INFO : frequencies processed\n",
      "2020-12-16 02:28:16,455 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:28:16,456 : INFO : entropies processed\n",
      "2020-12-16 02:28:16,457 : INFO : extropies processed\n",
      "2020-12-16 02:28:16,459 : INFO : token count processed\n",
      "2020-12-16 02:28:16,461 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:28:16,462 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:28:16,463 : INFO : vocab #6957\n",
      "2020-12-16 02:28:16,465 : INFO : diff #set()\n",
      "2020-12-16 02:28:17,727 : INFO : alphabet #6957\n",
      "2020-12-16 02:28:18,347 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ39.txt', 'test_data/LibEST_semeru_format/test/us897.c')[[1.0846006959173289, 0.47970817718639863], [0.7666733711957932, 0.23332663], [2.913977073182752, 1.3356231683419404], [6.81735632558079, 4.070845545316719]]\n",
      "2020-12-16 02:28:18,350 : INFO : Removed 242 and 664 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:28:18,351 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:28:18,352 : INFO : built Dictionary(208 unique tokens: [')\",', '106', '2005', '2006', '2008']...) from 2 documents (total 632 corpus positions)\n",
      "2020-12-16 02:28:18,654 : INFO : token count processed\n",
      "2020-12-16 02:28:18,664 : INFO : frequencies processed\n",
      "2020-12-16 02:28:19,285 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:28:19,286 : INFO : entropies processed\n",
      "2020-12-16 02:28:19,287 : INFO : extropies processed\n",
      "2020-12-16 02:28:19,289 : INFO : token count processed\n",
      "2020-12-16 02:28:19,290 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:28:19,292 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:28:19,293 : INFO : vocab #6957\n",
      "2020-12-16 02:28:19,294 : INFO : diff #set()\n",
      "2020-12-16 02:28:20,546 : INFO : alphabet #6957\n",
      "2020-12-16 02:28:21,169 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ52.txt', 'test_data/LibEST_semeru_format/test/us1864.c')[[1.1467762075749648, 0.46581473954829095], [0.895891971886158, 0.10410803], [4.297299717551716, 1.3971827366819467], [9.051688682166944, 6.925247239481147]]\n",
      "2020-12-16 02:28:21,172 : INFO : Removed 51 and 2029 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:28:21,173 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:28:21,175 : INFO : built Dictionary(204 unique tokens: ['abil', 'abl', 'along', 'also', 'attribut']...) from 2 documents (total 1614 corpus positions)\n",
      "2020-12-16 02:28:21,373 : INFO : token count processed\n",
      "2020-12-16 02:28:21,384 : INFO : frequencies processed\n",
      "2020-12-16 02:28:22,004 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:28:22,005 : INFO : entropies processed\n",
      "2020-12-16 02:28:22,006 : INFO : extropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 02:28:22,012 : INFO : token count processed\n",
      "2020-12-16 02:28:22,015 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:28:22,017 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:28:22,018 : INFO : vocab #6957\n",
      "2020-12-16 02:28:22,020 : INFO : diff #set()\n",
      "2020-12-16 02:28:23,270 : INFO : alphabet #6957\n",
      "2020-12-16 02:28:23,889 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ14.txt', 'test_data/LibEST_semeru_format/test/us899.c')[[1.0634199184552562, 0.4846323286190979], [0.7152673006057739, 0.2847327], [4.101681489107106, 1.3817417127226523], [8.271135450796873, 5.953870679314438]]\n",
      "2020-12-16 02:28:23,893 : INFO : Removed 33 and 1630 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:28:23,893 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:28:23,895 : INFO : built Dictionary(193 unique tokens: ['accept', 'access', 'act', 'author', 'avail']...) from 2 documents (total 891 corpus positions)\n",
      "2020-12-16 02:28:24,045 : INFO : token count processed\n",
      "2020-12-16 02:28:24,055 : INFO : frequencies processed\n",
      "2020-12-16 02:28:24,694 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:28:24,694 : INFO : entropies processed\n",
      "2020-12-16 02:28:24,695 : INFO : extropies processed\n",
      "2020-12-16 02:28:24,697 : INFO : token count processed\n",
      "2020-12-16 02:28:24,699 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:28:24,700 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:28:24,700 : INFO : vocab #6957\n",
      "2020-12-16 02:28:24,702 : INFO : diff #set()\n",
      "2020-12-16 02:28:25,968 : INFO : alphabet #6957\n",
      "2020-12-16 02:28:26,590 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ29.txt', 'test_data/LibEST_semeru_format/test/us2174.c')[[1.0314075830356633, 0.49226950236428463], [0.6378704011440277, 0.3621296], [3.5552208323774015, 1.3430401984142322], [9.268493352731923, 5.459893592901803]]\n",
      "2020-12-16 02:28:26,593 : INFO : Removed 242 and 939 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:28:26,594 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:28:26,596 : INFO : built Dictionary(214 unique tokens: [')\",', '106', '2005', '2006', '2008']...) from 2 documents (total 909 corpus positions)\n",
      "2020-12-16 02:28:26,958 : INFO : token count processed\n",
      "2020-12-16 02:28:26,968 : INFO : frequencies processed\n",
      "2020-12-16 02:28:27,589 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:28:27,590 : INFO : entropies processed\n",
      "2020-12-16 02:28:27,591 : INFO : extropies processed\n",
      "2020-12-16 02:28:27,595 : INFO : token count processed\n",
      "2020-12-16 02:28:27,598 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:28:27,600 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:28:27,601 : INFO : vocab #6957\n",
      "2020-12-16 02:28:27,605 : INFO : diff #set()\n",
      "2020-12-16 02:28:28,907 : INFO : alphabet #6957\n",
      "2020-12-16 02:28:29,528 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ52.txt', 'test_data/LibEST_semeru_format/test/us1060.c')[[1.1208065061216195, 0.47151873455383214], [0.9151381030678749, 0.0848619], [4.318242691024847, 1.3937520438848123], [8.257454207095364, 6.696264317740006]]\n",
      "2020-12-16 02:28:29,532 : INFO : Removed 321 and 957 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:28:29,533 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:28:29,534 : INFO : built Dictionary(160 unique tokens: ['200', 'accept', 'along', 'attempt', 'auth']...) from 2 documents (total 873 corpus positions)\n",
      "2020-12-16 02:28:29,689 : INFO : token count processed\n",
      "2020-12-16 02:28:29,694 : INFO : frequencies processed\n",
      "2020-12-16 02:28:30,313 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:28:30,314 : INFO : entropies processed\n",
      "2020-12-16 02:28:30,315 : INFO : extropies processed\n",
      "2020-12-16 02:28:30,320 : INFO : token count processed\n",
      "2020-12-16 02:28:30,323 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:28:30,325 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:28:30,326 : INFO : vocab #6957\n",
      "2020-12-16 02:28:30,329 : INFO : diff #set()\n",
      "2020-12-16 02:28:31,611 : INFO : alphabet #6957\n",
      "2020-12-16 02:28:32,364 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ57.txt', 'test_data/LibEST_semeru_format/test/us3496.c')[[1.0641414404279674, 0.4844629250758445], [0.8577366173267365, 0.14226338], [4.429275070710713, 1.3981227802434169], [8.118968608147561, 7.31810782081679]]\n",
      "2020-12-16 02:28:32,368 : INFO : Removed 39 and 1444 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:28:32,369 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:28:32,370 : INFO : built Dictionary(167 unique tokens: ['attribut', 'bound', 'client', 'defin', 'document']...) from 2 documents (total 1078 corpus positions)\n",
      "2020-12-16 02:28:32,468 : INFO : token count processed\n",
      "2020-12-16 02:28:32,478 : INFO : frequencies processed\n",
      "2020-12-16 02:28:33,105 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:28:33,106 : INFO : entropies processed\n",
      "2020-12-16 02:28:33,107 : INFO : extropies processed\n",
      "2020-12-16 02:28:33,112 : INFO : token count processed\n",
      "2020-12-16 02:28:33,114 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:28:33,116 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:28:33,117 : INFO : vocab #6957\n",
      "2020-12-16 02:28:33,119 : INFO : diff #set()\n",
      "2020-12-16 02:28:34,386 : INFO : alphabet #6957\n",
      "2020-12-16 02:28:35,008 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ45.txt', 'test_data/LibEST_semeru_format/test/us893.c')[[1.0116889109284224, 0.4970947518612538], [0.7757646590471268, 0.22423534], [3.281107642994674, 1.2896921273296342], [7.962557157135516, 4.626104122209638]]\n",
      "2020-12-16 02:28:35,011 : INFO : Removed 56 and 939 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:28:35,012 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:28:35,013 : INFO : built Dictionary(171 unique tokens: [')\",', '180', '2012', '800', '://']...) from 2 documents (total 742 corpus positions)\n",
      "2020-12-16 02:28:35,178 : INFO : token count processed\n",
      "2020-12-16 02:28:35,184 : INFO : frequencies processed\n",
      "2020-12-16 02:28:35,820 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:28:35,821 : INFO : entropies processed\n",
      "2020-12-16 02:28:35,822 : INFO : extropies processed\n",
      "2020-12-16 02:28:35,824 : INFO : token count processed\n",
      "2020-12-16 02:28:35,826 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:28:35,827 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:28:35,828 : INFO : vocab #6957\n",
      "2020-12-16 02:28:35,830 : INFO : diff #set()\n",
      "2020-12-16 02:28:37,076 : INFO : alphabet #6957\n",
      "2020-12-16 02:28:37,700 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ53.txt', 'test_data/LibEST_semeru_format/test/us1060.c')[[1.144906525665974, 0.4662207830662966], [0.9156595766544342, 0.08434042], [3.7256507561120933, 1.3761145168664612], [7.842477208311367, 5.897995135656219]]\n",
      "2020-12-16 02:28:37,703 : INFO : Removed 21 and 541 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:28:37,704 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:28:37,705 : INFO : built Dictionary(110 unique tokens: ['author', 'base', 'check', 'client', 'date']...) from 2 documents (total 345 corpus positions)\n",
      "2020-12-16 02:28:37,758 : INFO : token count processed\n",
      "2020-12-16 02:28:37,764 : INFO : frequencies processed\n",
      "2020-12-16 02:28:38,393 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:28:38,393 : INFO : entropies processed\n",
      "2020-12-16 02:28:38,394 : INFO : extropies processed\n",
      "2020-12-16 02:28:38,398 : INFO : token count processed\n",
      "2020-12-16 02:28:38,399 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:28:38,401 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:28:38,402 : INFO : vocab #6957\n",
      "2020-12-16 02:28:38,405 : INFO : diff #set()\n",
      "2020-12-16 02:28:39,679 : INFO : alphabet #6957\n",
      "2020-12-16 02:28:40,297 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ33.txt', 'test_data/LibEST_semeru_format/test/us896.c')[[1.0841823810333326, 0.4798044591012243], [0.8002514839172363, 0.19974852], [3.3450739957852544, 1.351139104883511], [7.493552170164024, 4.81976242210971]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 02:28:40,301 : INFO : Removed 145 and 1444 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:28:40,301 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:28:40,303 : INFO : built Dictionary(205 unique tokens: ['802', 'also', 'anchor', 'aspect', 'attribut']...) from 2 documents (total 1325 corpus positions)\n",
      "2020-12-16 02:28:40,606 : INFO : token count processed\n",
      "2020-12-16 02:28:40,616 : INFO : frequencies processed\n",
      "2020-12-16 02:28:41,237 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:28:41,238 : INFO : entropies processed\n",
      "2020-12-16 02:28:41,238 : INFO : extropies processed\n",
      "2020-12-16 02:28:41,244 : INFO : token count processed\n",
      "2020-12-16 02:28:41,246 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:28:41,248 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:28:41,249 : INFO : vocab #6957\n",
      "2020-12-16 02:28:41,252 : INFO : diff #set()\n",
      "2020-12-16 02:28:42,516 : INFO : alphabet #6957\n",
      "2020-12-16 02:28:43,135 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ13.txt', 'test_data/LibEST_semeru_format/test/us893.c')[[0.969778340013247, 0.5076713352393116], [0.6707086563110352, 0.32929134], [4.379941419875077, 1.3833518186390923], [8.120117104916005, 6.011653276874149]]\n",
      "2020-12-16 02:28:43,138 : INFO : Removed 39 and 1349 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:28:43,139 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:28:43,142 : INFO : built Dictionary(143 unique tokens: ['attribut', 'bound', 'client', 'defin', 'document']...) from 2 documents (total 1314 corpus positions)\n",
      "2020-12-16 02:28:43,231 : INFO : token count processed\n",
      "2020-12-16 02:28:43,238 : INFO : frequencies processed\n",
      "2020-12-16 02:28:43,861 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:28:43,862 : INFO : entropies processed\n",
      "2020-12-16 02:28:43,863 : INFO : extropies processed\n",
      "2020-12-16 02:28:43,865 : INFO : token count processed\n",
      "2020-12-16 02:28:43,867 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:28:43,868 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:28:43,869 : INFO : vocab #6957\n",
      "2020-12-16 02:28:43,871 : INFO : diff #set()\n",
      "2020-12-16 02:28:45,123 : INFO : alphabet #6957\n",
      "2020-12-16 02:28:45,746 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ45.txt', 'test_data/LibEST_semeru_format/test/us897.c')[[1.0015624683276363, 0.49960968784328236], [0.8011851459741592, 0.19881485], [3.1749454606850165, 1.2757194808980619], [6.881267630045975, 4.5942235777786555]]\n",
      "2020-12-16 02:28:45,749 : INFO : Removed 50 and 1630 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:28:45,750 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:28:45,751 : INFO : built Dictionary(185 unique tokens: ['accept', 'agent', 'also', 'attribut', 'base']...) from 2 documents (total 884 corpus positions)\n",
      "2020-12-16 02:28:45,894 : INFO : token count processed\n",
      "2020-12-16 02:28:45,904 : INFO : frequencies processed\n",
      "2020-12-16 02:28:46,526 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:28:46,527 : INFO : entropies processed\n",
      "2020-12-16 02:28:46,528 : INFO : extropies processed\n",
      "2020-12-16 02:28:46,533 : INFO : token count processed\n",
      "2020-12-16 02:28:46,535 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:28:46,538 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:28:46,539 : INFO : vocab #6957\n",
      "2020-12-16 02:28:46,542 : INFO : diff #set()\n",
      "2020-12-16 02:28:47,801 : INFO : alphabet #6957\n",
      "2020-12-16 02:28:48,421 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ56.txt', 'test_data/LibEST_semeru_format/test/us2174.c')[[0.9981631620942624, 0.5004596316108171], [0.6915339827537537, 0.30846602], [4.703055907333277, 1.4085640256204517], [9.294211974754894, 6.013526102601386]]\n",
      "2020-12-16 02:28:48,425 : INFO : Removed 39 and 1630 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:28:48,426 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:28:48,428 : INFO : built Dictionary(179 unique tokens: ['attribut', 'bound', 'client', 'defin', 'document']...) from 2 documents (total 884 corpus positions)\n",
      "2020-12-16 02:28:48,538 : INFO : token count processed\n",
      "2020-12-16 02:28:48,548 : INFO : frequencies processed\n",
      "2020-12-16 02:28:49,184 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:28:49,185 : INFO : entropies processed\n",
      "2020-12-16 02:28:49,186 : INFO : extropies processed\n",
      "2020-12-16 02:28:49,189 : INFO : token count processed\n",
      "2020-12-16 02:28:49,190 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:28:49,191 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:28:49,192 : INFO : vocab #6957\n",
      "2020-12-16 02:28:49,194 : INFO : diff #set()\n",
      "2020-12-16 02:28:50,443 : INFO : alphabet #6957\n",
      "2020-12-16 02:28:51,084 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ45.txt', 'test_data/LibEST_semeru_format/test/us2174.c')[[0.9832748168951714, 0.504216557121169], [0.7249681055545807, 0.2750319], [3.2616777013202896, 1.2838148898824462], [9.235660389404902, 4.676381722240537]]\n",
      "2020-12-16 02:28:51,087 : INFO : Removed 60 and 1391 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:28:51,088 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:28:51,091 : INFO : built Dictionary(190 unique tokens: ['add', 'address', 'arc', 'attribut', 'author']...) from 2 documents (total 1359 corpus positions)\n",
      "2020-12-16 02:28:51,299 : INFO : token count processed\n",
      "2020-12-16 02:28:51,305 : INFO : frequencies processed\n",
      "2020-12-16 02:28:52,030 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:28:52,031 : INFO : entropies processed\n",
      "2020-12-16 02:28:52,032 : INFO : extropies processed\n",
      "2020-12-16 02:28:52,035 : INFO : token count processed\n",
      "2020-12-16 02:28:52,037 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:28:52,039 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:28:52,040 : INFO : vocab #6957\n",
      "2020-12-16 02:28:52,043 : INFO : diff #set()\n",
      "2020-12-16 02:28:53,312 : INFO : alphabet #6957\n",
      "2020-12-16 02:28:53,950 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ50.txt', 'test_data/LibEST_semeru_format/test/us898.c')[[1.0612669627451043, 0.48513851823843535], [0.825272798538208, 0.1747272], [4.734183719779191, 1.412436698218903], [7.394684920436502, 6.1179363154966975]]\n",
      "2020-12-16 02:28:53,953 : INFO : Removed 50 and 2029 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:28:53,954 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:28:53,956 : INFO : built Dictionary(183 unique tokens: ['accept', 'agent', 'also', 'attribut', 'base']...) from 2 documents (total 1558 corpus positions)\n",
      "2020-12-16 02:28:54,102 : INFO : token count processed\n",
      "2020-12-16 02:28:54,108 : INFO : frequencies processed\n",
      "2020-12-16 02:28:54,748 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:28:54,749 : INFO : entropies processed\n",
      "2020-12-16 02:28:54,750 : INFO : extropies processed\n",
      "2020-12-16 02:28:54,753 : INFO : token count processed\n",
      "2020-12-16 02:28:54,754 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:28:54,756 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:28:54,756 : INFO : vocab #6957\n",
      "2020-12-16 02:28:54,758 : INFO : diff #set()\n",
      "2020-12-16 02:28:56,009 : INFO : alphabet #6957\n",
      "2020-12-16 02:28:56,632 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ56.txt', 'test_data/LibEST_semeru_format/test/us899.c')[[1.008715874337328, 0.49783048602127383], [0.6746743023395538, 0.3253257], [4.708758439731455, 1.4091155790224204], [8.23537081739042, 6.001606628386158]]\n",
      "2020-12-16 02:28:56,635 : INFO : Removed 138 and 1181 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:28:56,636 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:28:56,637 : INFO : built Dictionary(196 unique tokens: ['132', 'access', 'address', 'attr', 'attribut']...) from 2 documents (total 911 corpus positions)\n",
      "2020-12-16 02:28:56,870 : INFO : token count processed\n",
      "2020-12-16 02:28:56,876 : INFO : frequencies processed\n",
      "2020-12-16 02:28:57,516 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:28:57,517 : INFO : entropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 02:28:57,518 : INFO : extropies processed\n",
      "2020-12-16 02:28:57,523 : INFO : token count processed\n",
      "2020-12-16 02:28:57,525 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:28:57,528 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:28:57,529 : INFO : vocab #6957\n",
      "2020-12-16 02:28:57,530 : INFO : diff #set()\n",
      "2020-12-16 02:28:58,780 : INFO : alphabet #6957\n",
      "2020-12-16 02:28:59,424 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ49.txt', 'test_data/LibEST_semeru_format/test/us748.c')[[1.0330329954968793, 0.491875932272116], [0.7348989546298981, 0.26510105], [4.739485167910035, 1.406924120604358], [8.83056177220314, 6.325418002026327]]\n",
      "2020-12-16 02:28:59,428 : INFO : Removed 19 and 1194 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:28:59,428 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:28:59,430 : INFO : built Dictionary(168 unique tokens: ['base', 'connect', 'correspond', 'cover', 'data']...) from 2 documents (total 772 corpus positions)\n",
      "2020-12-16 02:28:59,539 : INFO : token count processed\n",
      "2020-12-16 02:28:59,546 : INFO : frequencies processed\n",
      "2020-12-16 02:29:00,195 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:29:00,196 : INFO : entropies processed\n",
      "2020-12-16 02:29:00,197 : INFO : extropies processed\n",
      "2020-12-16 02:29:00,199 : INFO : token count processed\n",
      "2020-12-16 02:29:00,200 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:29:00,202 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:29:00,203 : INFO : vocab #6957\n",
      "2020-12-16 02:29:00,204 : INFO : diff #set()\n",
      "2020-12-16 02:29:01,459 : INFO : alphabet #6957\n",
      "2020-12-16 02:29:02,081 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ24.txt', 'test_data/LibEST_semeru_format/test/us903.c')[[1.095140941865264, 0.47729485879347056], [0.7395311295986176, 0.26046887], [3.8553885422075336, 1.3858306737380488], [8.676638119501682, 5.144281720491115]]\n",
      "2020-12-16 02:29:02,084 : INFO : Removed 66 and 1105 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:29:02,085 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:29:02,086 : INFO : built Dictionary(174 unique tokens: ['attribut', 'author', 'build', 'channel', 'client']...) from 2 documents (total 1244 corpus positions)\n",
      "2020-12-16 02:29:02,261 : INFO : token count processed\n",
      "2020-12-16 02:29:02,268 : INFO : frequencies processed\n",
      "2020-12-16 02:29:02,888 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:29:02,889 : INFO : entropies processed\n",
      "2020-12-16 02:29:02,890 : INFO : extropies processed\n",
      "2020-12-16 02:29:02,895 : INFO : token count processed\n",
      "2020-12-16 02:29:02,898 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:29:02,900 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:29:02,901 : INFO : vocab #6957\n",
      "2020-12-16 02:29:02,903 : INFO : diff #set()\n",
      "2020-12-16 02:29:04,166 : INFO : alphabet #6957\n",
      "2020-12-16 02:29:04,792 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ1.txt', 'test_data/LibEST_semeru_format/test/us901.c')[[0.9734579455665402, 0.5067247580555461], [0.6626050174236298, 0.33739498], [4.38758143671724, 1.395898967583022], [7.807124712749208, 5.9276438279264]]\n",
      "2020-12-16 02:29:04,795 : INFO : Removed 138 and 541 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:29:04,796 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:29:04,797 : INFO : built Dictionary(147 unique tokens: ['132', 'access', 'address', 'attr', 'attribut']...) from 2 documents (total 511 corpus positions)\n",
      "2020-12-16 02:29:04,943 : INFO : token count processed\n",
      "2020-12-16 02:29:04,949 : INFO : frequencies processed\n",
      "2020-12-16 02:29:05,593 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:29:05,594 : INFO : entropies processed\n",
      "2020-12-16 02:29:05,595 : INFO : extropies processed\n",
      "2020-12-16 02:29:05,599 : INFO : token count processed\n",
      "2020-12-16 02:29:05,601 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:29:05,604 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:29:05,605 : INFO : vocab #6957\n",
      "2020-12-16 02:29:05,608 : INFO : diff #set()\n",
      "2020-12-16 02:29:06,868 : INFO : alphabet #6957\n",
      "2020-12-16 02:29:07,514 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ49.txt', 'test_data/LibEST_semeru_format/test/us896.c')[[1.0426703207312682, 0.4895552600196412], [0.8312796503305435, 0.16872035], [4.118002190272557, 1.3917085014563888], [7.850495304465646, 6.003220733692146]]\n",
      "2020-12-16 02:29:07,518 : INFO : Removed 51 and 1682 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:29:07,519 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:29:07,522 : INFO : built Dictionary(232 unique tokens: ['abil', 'abl', 'along', 'also', 'attribut']...) from 2 documents (total 1370 corpus positions)\n",
      "2020-12-16 02:29:07,806 : INFO : token count processed\n",
      "2020-12-16 02:29:07,812 : INFO : frequencies processed\n",
      "2020-12-16 02:29:08,445 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:29:08,446 : INFO : entropies processed\n",
      "2020-12-16 02:29:08,447 : INFO : extropies processed\n",
      "2020-12-16 02:29:08,450 : INFO : token count processed\n",
      "2020-12-16 02:29:08,451 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:29:08,453 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:29:08,453 : INFO : vocab #6957\n",
      "2020-12-16 02:29:08,455 : INFO : diff #set()\n",
      "2020-12-16 02:29:09,706 : INFO : alphabet #6957\n",
      "2020-12-16 02:29:10,328 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ14.txt', 'test_data/LibEST_semeru_format/test/us1883.c')[[1.038049669605462, 0.4906651760815948], [0.674924373626709, 0.32507563], [4.693436422142243, 1.4007812816671141], [9.076848178094664, 5.9964863651717195]]\n",
      "2020-12-16 02:29:10,332 : INFO : Removed 45 and 1921 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:29:10,333 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:29:10,335 : INFO : built Dictionary(131 unique tokens: ['archiv', 'associ', 'author', 'base', 'cannot']...) from 2 documents (total 631 corpus positions)\n",
      "2020-12-16 02:29:10,444 : INFO : token count processed\n",
      "2020-12-16 02:29:10,451 : INFO : frequencies processed\n",
      "2020-12-16 02:29:11,089 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:29:11,090 : INFO : entropies processed\n",
      "2020-12-16 02:29:11,091 : INFO : extropies processed\n",
      "2020-12-16 02:29:11,096 : INFO : token count processed\n",
      "2020-12-16 02:29:11,098 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:29:11,100 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:29:11,101 : INFO : vocab #6957\n",
      "2020-12-16 02:29:11,104 : INFO : diff #set()\n",
      "2020-12-16 02:29:12,471 : INFO : alphabet #6957\n",
      "2020-12-16 02:29:13,102 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ42.txt', 'test_data/LibEST_semeru_format/test/us895.c')[[1.0938339152453245, 0.47759279889342837], [0.9462894275784492, 0.053710572], [3.6939427079182683, 1.364846846285613], [7.288415989964152, 5.26903434283844]]\n",
      "2020-12-16 02:29:13,105 : INFO : Removed 60 and 664 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:29:13,106 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:29:13,108 : INFO : built Dictionary(164 unique tokens: ['add', 'address', 'arc', 'attribut', 'author']...) from 2 documents (total 474 corpus positions)\n",
      "2020-12-16 02:29:13,275 : INFO : token count processed\n",
      "2020-12-16 02:29:13,285 : INFO : frequencies processed\n",
      "2020-12-16 02:29:13,910 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:29:13,911 : INFO : entropies processed\n",
      "2020-12-16 02:29:13,912 : INFO : extropies processed\n",
      "2020-12-16 02:29:13,914 : INFO : token count processed\n",
      "2020-12-16 02:29:13,915 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:29:13,917 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:29:13,917 : INFO : vocab #6957\n",
      "2020-12-16 02:29:13,919 : INFO : diff #set()\n",
      "2020-12-16 02:29:15,174 : INFO : alphabet #6957\n",
      "2020-12-16 02:29:15,796 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ50.txt', 'test_data/LibEST_semeru_format/test/us1864.c')[[1.071019931669089, 0.48285387538210406], [0.7783112674951553, 0.22168873], [4.594672032363178, 1.4096534295013252], [8.788632771278113, 6.151682808405848]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 02:29:15,800 : INFO : Removed 19 and 1630 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:29:15,801 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:29:15,802 : INFO : built Dictionary(185 unique tokens: ['base', 'connect', 'correspond', 'cover', 'data']...) from 2 documents (total 869 corpus positions)\n",
      "2020-12-16 02:29:15,922 : INFO : token count processed\n",
      "2020-12-16 02:29:15,928 : INFO : frequencies processed\n",
      "2020-12-16 02:29:16,548 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:29:16,549 : INFO : entropies processed\n",
      "2020-12-16 02:29:16,550 : INFO : extropies processed\n",
      "2020-12-16 02:29:16,555 : INFO : token count processed\n",
      "2020-12-16 02:29:16,557 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:29:16,560 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:29:16,561 : INFO : vocab #6957\n",
      "2020-12-16 02:29:16,562 : INFO : diff #set()\n",
      "2020-12-16 02:29:17,822 : INFO : alphabet #6957\n",
      "2020-12-16 02:29:18,447 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ24.txt', 'test_data/LibEST_semeru_format/test/us2174.c')[[1.0900672313506463, 0.47845350857626645], [0.7261197865009308, 0.2738802], [3.9500637564362107, 1.3899817399173775], [9.260929156849482, 5.162234101016653]]\n",
      "2020-12-16 02:29:18,450 : INFO : Removed 6 and 541 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:29:18,451 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:29:18,453 : INFO : built Dictionary(99 unique tokens: ['client', 'est', 'less', 'mutual', 'section']...) from 2 documents (total 308 corpus positions)\n",
      "2020-12-16 02:29:18,486 : INFO : token count processed\n",
      "2020-12-16 02:29:18,492 : INFO : frequencies processed\n",
      "2020-12-16 02:29:19,111 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:29:19,112 : INFO : entropies processed\n",
      "2020-12-16 02:29:19,112 : INFO : extropies processed\n",
      "2020-12-16 02:29:19,114 : INFO : token count processed\n",
      "2020-12-16 02:29:19,116 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:29:19,117 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:29:19,118 : INFO : vocab #6957\n",
      "2020-12-16 02:29:19,119 : INFO : diff #set()\n",
      "2020-12-16 02:29:20,399 : INFO : alphabet #6957\n",
      "2020-12-16 02:29:21,021 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ7.txt', 'test_data/LibEST_semeru_format/test/us896.c')[[1.0940830661225103, 0.47753597561516065], [0.8428313285112381, 0.15716867], [2.75, 1.3226647836567114], [7.408689862452206, 3.6859865593702317]]\n",
      "2020-12-16 02:29:21,024 : INFO : Removed 59 and 730 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:29:21,025 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:29:21,027 : INFO : built Dictionary(151 unique tokens: ['anchor', 'author', 'avail', 'behind', 'browser']...) from 2 documents (total 862 corpus positions)\n",
      "2020-12-16 02:29:21,179 : INFO : token count processed\n",
      "2020-12-16 02:29:21,187 : INFO : frequencies processed\n",
      "2020-12-16 02:29:21,809 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:29:21,810 : INFO : entropies processed\n",
      "2020-12-16 02:29:21,811 : INFO : extropies processed\n",
      "2020-12-16 02:29:21,813 : INFO : token count processed\n",
      "2020-12-16 02:29:21,814 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:29:21,815 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:29:21,816 : INFO : vocab #6957\n",
      "2020-12-16 02:29:21,818 : INFO : diff #set()\n",
      "2020-12-16 02:29:23,096 : INFO : alphabet #6957\n",
      "2020-12-16 02:29:23,716 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ2.txt', 'test_data/LibEST_semeru_format/test/us1005.c')[[0.9924497496823489, 0.5018947153670639], [0.6384263634681702, 0.36157364], [3.7237593638992093, 1.3709903369584204], [7.544784459773864, 5.830487317458691]]\n",
      "2020-12-16 02:29:23,720 : INFO : Removed 22 and 1105 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:29:23,720 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:29:23,722 : INFO : built Dictionary(160 unique tokens: ['also', 'associ', 'avail', 'client', 'connect']...) from 2 documents (total 1196 corpus positions)\n",
      "2020-12-16 02:29:23,833 : INFO : token count processed\n",
      "2020-12-16 02:29:23,839 : INFO : frequencies processed\n",
      "2020-12-16 02:29:24,475 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:29:24,476 : INFO : entropies processed\n",
      "2020-12-16 02:29:24,477 : INFO : extropies processed\n",
      "2020-12-16 02:29:24,480 : INFO : token count processed\n",
      "2020-12-16 02:29:24,481 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:29:24,482 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:29:24,483 : INFO : vocab #6957\n",
      "2020-12-16 02:29:24,485 : INFO : diff #set()\n",
      "2020-12-16 02:29:25,737 : INFO : alphabet #6957\n",
      "2020-12-16 02:29:26,365 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ15.txt', 'test_data/LibEST_semeru_format/test/us901.c')[[1.0415709491237257, 0.4898188820864716], [0.66816046833992, 0.33183953], [3.8967216165616274, 1.3823399492367783], [7.710479274553963, 5.15120799981702]]\n",
      "2020-12-16 02:29:26,368 : INFO : Removed 23 and 1194 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:29:26,369 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:29:26,370 : INFO : built Dictionary(161 unique tokens: ['alt', 'altern', 'cannot', 'chang', 'client']...) from 2 documents (total 779 corpus positions)\n",
      "2020-12-16 02:29:26,443 : INFO : token count processed\n",
      "2020-12-16 02:29:26,448 : INFO : frequencies processed\n",
      "2020-12-16 02:29:27,082 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:29:27,082 : INFO : entropies processed\n",
      "2020-12-16 02:29:27,083 : INFO : extropies processed\n",
      "2020-12-16 02:29:27,085 : INFO : token count processed\n",
      "2020-12-16 02:29:27,087 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:29:27,088 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:29:27,089 : INFO : vocab #6957\n",
      "2020-12-16 02:29:27,091 : INFO : diff #set()\n",
      "2020-12-16 02:29:28,356 : INFO : alphabet #6957\n",
      "2020-12-16 02:29:28,978 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ9.txt', 'test_data/LibEST_semeru_format/test/us903.c')[[1.059142564500368, 0.48563903113849766], [0.6577008068561554, 0.3422992], [3.318688134718482, 1.3433288127073835], [8.633733967215143, 4.454137388133198]]\n",
      "2020-12-16 02:29:28,981 : INFO : Removed 45 and 577 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:29:28,982 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:29:28,984 : INFO : built Dictionary(132 unique tokens: ['archiv', 'associ', 'author', 'base', 'cannot']...) from 2 documents (total 627 corpus positions)\n",
      "2020-12-16 02:29:29,087 : INFO : token count processed\n",
      "2020-12-16 02:29:29,092 : INFO : frequencies processed\n",
      "2020-12-16 02:29:29,722 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:29:29,723 : INFO : entropies processed\n",
      "2020-12-16 02:29:29,724 : INFO : extropies processed\n",
      "2020-12-16 02:29:29,726 : INFO : token count processed\n",
      "2020-12-16 02:29:29,728 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:29:29,729 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:29:29,730 : INFO : vocab #6957\n",
      "2020-12-16 02:29:29,732 : INFO : diff #set()\n",
      "2020-12-16 02:29:31,092 : INFO : alphabet #6957\n",
      "2020-12-16 02:29:31,713 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ42.txt', 'test_data/LibEST_semeru_format/test/us3612.c')[[0.9555741549551454, 0.511358772801401], [0.6513151526451111, 0.34868485], [4.128297945113544, 1.3876227734656772], [7.483026958107821, 5.316957484167908]]\n",
      "2020-12-16 02:29:31,716 : INFO : Removed 321 and 577 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:29:31,717 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:29:31,718 : INFO : built Dictionary(154 unique tokens: ['200', 'accept', 'along', 'attempt', 'auth']...) from 2 documents (total 631 corpus positions)\n",
      "2020-12-16 02:29:31,857 : INFO : token count processed\n",
      "2020-12-16 02:29:31,863 : INFO : frequencies processed\n",
      "2020-12-16 02:29:32,484 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:29:32,485 : INFO : entropies processed\n",
      "2020-12-16 02:29:32,486 : INFO : extropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 02:29:32,488 : INFO : token count processed\n",
      "2020-12-16 02:29:32,489 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:29:32,490 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:29:32,491 : INFO : vocab #6957\n",
      "2020-12-16 02:29:32,493 : INFO : diff #set()\n",
      "2020-12-16 02:29:33,749 : INFO : alphabet #6957\n",
      "2020-12-16 02:29:34,376 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ57.txt', 'test_data/LibEST_semeru_format/test/us3612.c')[[1.0771117270418926, 0.481437751749707], [0.8207048326730728, 0.17929517], [4.003103797225431, 1.3871060048624124], [8.352713831612732, 7.141824602044302]]\n",
      "2020-12-16 02:29:34,379 : INFO : Removed 27 and 1921 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:29:34,380 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:29:34,382 : INFO : built Dictionary(117 unique tokens: ['accord', 'advis', 'allow', 'attribut', 'author']...) from 2 documents (total 556 corpus positions)\n",
      "2020-12-16 02:29:34,437 : INFO : token count processed\n",
      "2020-12-16 02:29:34,443 : INFO : frequencies processed\n",
      "2020-12-16 02:29:35,080 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:29:35,081 : INFO : entropies processed\n",
      "2020-12-16 02:29:35,082 : INFO : extropies processed\n",
      "2020-12-16 02:29:35,084 : INFO : token count processed\n",
      "2020-12-16 02:29:35,086 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:29:35,087 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:29:35,088 : INFO : vocab #6957\n",
      "2020-12-16 02:29:35,090 : INFO : diff #set()\n",
      "2020-12-16 02:29:36,339 : INFO : alphabet #6957\n",
      "2020-12-16 02:29:36,960 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ47.txt', 'test_data/LibEST_semeru_format/test/us895.c')[[1.1813323111580136, 0.4584354226473295], [0.9571227170526981, 0.042877283], [2.6535442970305683, 1.274535884146605], [7.205600389472044, 4.9665344207465765]]\n",
      "2020-12-16 02:29:36,964 : INFO : Removed 54 and 1111 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:29:36,965 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:29:36,967 : INFO : built Dictionary(157 unique tokens: ['attribut', 'client', 'consist', 'content', 'correspond']...) from 2 documents (total 1101 corpus positions)\n",
      "2020-12-16 02:29:37,065 : INFO : token count processed\n",
      "2020-12-16 02:29:37,077 : INFO : frequencies processed\n",
      "2020-12-16 02:29:37,714 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:29:37,715 : INFO : entropies processed\n",
      "2020-12-16 02:29:37,716 : INFO : extropies processed\n",
      "2020-12-16 02:29:37,721 : INFO : token count processed\n",
      "2020-12-16 02:29:37,724 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:29:37,726 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:29:37,727 : INFO : vocab #6957\n",
      "2020-12-16 02:29:37,729 : INFO : diff #set()\n",
      "2020-12-16 02:29:38,983 : INFO : alphabet #6957\n",
      "2020-12-16 02:29:39,605 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ19.txt', 'test_data/LibEST_semeru_format/test/us1159.c')[[1.0913042313020056, 0.478170504813362], [0.8649823665618896, 0.13501763], [3.8801799226757376, 1.3869870052426927], [7.474606139960734, 5.065986999972722]]\n",
      "2020-12-16 02:29:39,608 : INFO : Removed 19 and 730 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:29:39,609 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:29:39,611 : INFO : built Dictionary(112 unique tokens: ['base', 'content', 'full', 'http', 'must']...) from 2 documents (total 748 corpus positions)\n",
      "2020-12-16 02:29:39,655 : INFO : token count processed\n",
      "2020-12-16 02:29:39,661 : INFO : frequencies processed\n",
      "2020-12-16 02:29:40,291 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:29:40,292 : INFO : entropies processed\n",
      "2020-12-16 02:29:40,293 : INFO : extropies processed\n",
      "2020-12-16 02:29:40,295 : INFO : token count processed\n",
      "2020-12-16 02:29:40,296 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:29:40,298 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:29:40,299 : INFO : vocab #6957\n",
      "2020-12-16 02:29:40,300 : INFO : diff #set()\n",
      "2020-12-16 02:29:41,569 : INFO : alphabet #6957\n",
      "2020-12-16 02:29:42,205 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ40.txt', 'test_data/LibEST_semeru_format/test/us1005.c')[[1.0959113557801907, 0.477119415018273], [0.8056060820817947, 0.19439392], [2.9139770731827523, 1.3356231683419404], [7.3614918547739165, 4.64068785502954]]\n",
      "2020-12-16 02:29:42,208 : INFO : Removed 33 and 939 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:29:42,209 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:29:42,210 : INFO : built Dictionary(158 unique tokens: ['accept', 'access', 'act', 'author', 'avail']...) from 2 documents (total 715 corpus positions)\n",
      "2020-12-16 02:29:42,331 : INFO : token count processed\n",
      "2020-12-16 02:29:42,341 : INFO : frequencies processed\n",
      "2020-12-16 02:29:42,980 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:29:42,981 : INFO : entropies processed\n",
      "2020-12-16 02:29:42,982 : INFO : extropies processed\n",
      "2020-12-16 02:29:42,984 : INFO : token count processed\n",
      "2020-12-16 02:29:42,985 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:29:42,987 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:29:42,988 : INFO : vocab #6957\n",
      "2020-12-16 02:29:42,989 : INFO : diff #set()\n",
      "2020-12-16 02:29:44,250 : INFO : alphabet #6957\n",
      "2020-12-16 02:29:44,871 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ29.txt', 'test_data/LibEST_semeru_format/test/us1060.c')[[1.0486606149147912, 0.4881237979193506], [0.7860568463802338, 0.21394315], [3.771338029606077, 1.3598909741258163], [7.704311618984133, 5.3827435180069605]]\n",
      "2020-12-16 02:29:44,874 : INFO : Removed 5 and 939 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:29:44,875 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:29:44,876 : INFO : built Dictionary(136 unique tokens: ['client', 'current', 'est', 'function', 'general']...) from 2 documents (total 659 corpus positions)\n",
      "2020-12-16 02:29:44,907 : INFO : token count processed\n",
      "2020-12-16 02:29:44,919 : INFO : frequencies processed\n",
      "2020-12-16 02:29:45,545 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:29:45,546 : INFO : entropies processed\n",
      "2020-12-16 02:29:45,546 : INFO : extropies processed\n",
      "2020-12-16 02:29:45,549 : INFO : token count processed\n",
      "2020-12-16 02:29:45,550 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:29:45,551 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:29:45,552 : INFO : vocab #6957\n",
      "2020-12-16 02:29:45,554 : INFO : diff #set()\n",
      "2020-12-16 02:29:46,806 : INFO : alphabet #6957\n",
      "2020-12-16 02:29:47,426 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ31.txt', 'test_data/LibEST_semeru_format/test/us1060.c')[[1.2128904286182403, 0.4518976570495693], [0.9033494889736176, 0.09665051], [2.2359263506290326, 1.2653331222512112], [7.608296414652822, 3.358112789908107]]\n",
      "2020-12-16 02:29:47,430 : INFO : Removed 5 and 1391 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:29:47,431 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:29:47,433 : INFO : built Dictionary(158 unique tokens: ['client', 'est', 'full', 'function', 'https']...) from 2 documents (total 1275 corpus positions)\n",
      "2020-12-16 02:29:47,489 : INFO : token count processed\n",
      "2020-12-16 02:29:47,500 : INFO : frequencies processed\n",
      "2020-12-16 02:29:48,128 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:29:48,129 : INFO : entropies processed\n",
      "2020-12-16 02:29:48,130 : INFO : extropies processed\n",
      "2020-12-16 02:29:48,132 : INFO : token count processed\n",
      "2020-12-16 02:29:48,134 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:29:48,135 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:29:48,136 : INFO : vocab #6957\n",
      "2020-12-16 02:29:48,138 : INFO : diff #set()\n",
      "2020-12-16 02:29:49,400 : INFO : alphabet #6957\n",
      "2020-12-16 02:29:50,128 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ39.txt', 'test_data/LibEST_semeru_format/test/us898.c')[[1.1310329569660593, 0.46925599941152235], [0.801217794418335, 0.1987822], [3.238901256602631, 1.3579502728384498], [7.206828388113754, 4.072115631023889]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 02:29:50,132 : INFO : Removed 32 and 1161 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:29:50,133 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:29:50,135 : INFO : built Dictionary(147 unique tokens: ['accept', 'access', 'also', 'altern', 'author']...) from 2 documents (total 1019 corpus positions)\n",
      "2020-12-16 02:29:50,260 : INFO : token count processed\n",
      "2020-12-16 02:29:50,266 : INFO : frequencies processed\n",
      "2020-12-16 02:29:50,888 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:29:50,889 : INFO : entropies processed\n",
      "2020-12-16 02:29:50,890 : INFO : extropies processed\n",
      "2020-12-16 02:29:50,893 : INFO : token count processed\n",
      "2020-12-16 02:29:50,894 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:29:50,895 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:29:50,896 : INFO : vocab #6957\n",
      "2020-12-16 02:29:50,898 : INFO : diff #set()\n",
      "2020-12-16 02:29:52,147 : INFO : alphabet #6957\n",
      "2020-12-16 02:29:52,772 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ20.txt', 'test_data/LibEST_semeru_format/test/us4020.c')[[1.0691113345531658, 0.48329927118975186], [0.8054524660110474, 0.19454753], [3.4473384506127904, 1.3552826691071784], [7.13744395566391, 5.298562708612867]]\n",
      "2020-12-16 02:29:52,776 : INFO : Removed 123 and 1225 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:29:52,777 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:29:52,778 : INFO : built Dictionary(220 unique tokens: ['agre', 'associ', 'attribut', 'base', 'case']...) from 2 documents (total 1455 corpus positions)\n",
      "2020-12-16 02:29:53,092 : INFO : token count processed\n",
      "2020-12-16 02:29:53,101 : INFO : frequencies processed\n",
      "2020-12-16 02:29:53,723 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:29:53,724 : INFO : entropies processed\n",
      "2020-12-16 02:29:53,725 : INFO : extropies processed\n",
      "2020-12-16 02:29:53,730 : INFO : token count processed\n",
      "2020-12-16 02:29:53,733 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:29:53,734 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:29:53,735 : INFO : vocab #6957\n",
      "2020-12-16 02:29:53,737 : INFO : diff #set()\n",
      "2020-12-16 02:29:55,006 : INFO : alphabet #6957\n",
      "2020-12-16 02:29:55,627 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us3512.c')[[1.0328865900595916, 0.4919113564375896], [0.7323500216007233, 0.26764998], [4.4374097843716065, 1.3850938444730096], [8.271757852836256, 5.960671611807317]]\n",
      "2020-12-16 02:29:55,631 : INFO : Removed 66 and 1921 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:29:55,632 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:29:55,633 : INFO : built Dictionary(143 unique tokens: ['attribut', 'author', 'build', 'channel', 'client']...) from 2 documents (total 631 corpus positions)\n",
      "2020-12-16 02:29:55,749 : INFO : token count processed\n",
      "2020-12-16 02:29:55,755 : INFO : frequencies processed\n",
      "2020-12-16 02:29:56,375 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:29:56,376 : INFO : entropies processed\n",
      "2020-12-16 02:29:56,377 : INFO : extropies processed\n",
      "2020-12-16 02:29:56,379 : INFO : token count processed\n",
      "2020-12-16 02:29:56,380 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:29:56,382 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:29:56,382 : INFO : vocab #6957\n",
      "2020-12-16 02:29:56,384 : INFO : diff #set()\n",
      "2020-12-16 02:29:57,634 : INFO : alphabet #6957\n",
      "2020-12-16 02:29:58,270 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ1.txt', 'test_data/LibEST_semeru_format/test/us895.c')[[1.118535418140434, 0.4720242066463822], [0.9390732049942017, 0.060926795], [3.4088132572058147, 1.3351408769505002], [7.356866366423626, 5.8373176787946]]\n",
      "2020-12-16 02:29:58,273 : INFO : Removed 59 and 1682 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:29:58,274 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:29:58,277 : INFO : built Dictionary(244 unique tokens: ['anchor', 'author', 'avail', 'behind', 'browser']...) from 2 documents (total 1392 corpus positions)\n",
      "2020-12-16 02:29:58,589 : INFO : token count processed\n",
      "2020-12-16 02:29:58,602 : INFO : frequencies processed\n",
      "2020-12-16 02:29:59,221 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:29:59,222 : INFO : entropies processed\n",
      "2020-12-16 02:29:59,223 : INFO : extropies processed\n",
      "2020-12-16 02:29:59,229 : INFO : token count processed\n",
      "2020-12-16 02:29:59,231 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:29:59,234 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:29:59,235 : INFO : vocab #6957\n",
      "2020-12-16 02:29:59,238 : INFO : diff #set()\n",
      "2020-12-16 02:30:00,485 : INFO : alphabet #6957\n",
      "2020-12-16 02:30:01,106 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ2.txt', 'test_data/LibEST_semeru_format/test/us1883.c')[[1.0256803119180875, 0.49366131176597877], [0.6388160884380341, 0.3611839], [4.176071841686861, 1.386076708396141], [9.086160099146426, 6.016862932730099]]\n",
      "2020-12-16 02:30:01,109 : INFO : Removed 62 and 664 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:30:01,110 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:30:01,112 : INFO : built Dictionary(154 unique tokens: ['anon', 'aris', 'associ', 'author', 'avail']...) from 2 documents (total 516 corpus positions)\n",
      "2020-12-16 02:30:01,259 : INFO : token count processed\n",
      "2020-12-16 02:30:01,270 : INFO : frequencies processed\n",
      "2020-12-16 02:30:01,915 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:30:01,916 : INFO : entropies processed\n",
      "2020-12-16 02:30:01,917 : INFO : extropies processed\n",
      "2020-12-16 02:30:01,919 : INFO : token count processed\n",
      "2020-12-16 02:30:01,920 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:30:01,921 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:30:01,922 : INFO : vocab #6957\n",
      "2020-12-16 02:30:01,924 : INFO : diff #set()\n",
      "2020-12-16 02:30:03,174 : INFO : alphabet #6957\n",
      "2020-12-16 02:30:03,795 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ18.txt', 'test_data/LibEST_semeru_format/test/us1864.c')[[0.956124439105192, 0.5112149206915687], [0.6888520419597626, 0.31114796], [4.401750066893862, 1.3920022552210756], [8.618405602059793, 5.614616505315373]]\n",
      "2020-12-16 02:30:03,798 : INFO : Removed 123 and 1181 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:30:03,799 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:30:03,801 : INFO : built Dictionary(197 unique tokens: ['agre', 'associ', 'attribut', 'base', 'case']...) from 2 documents (total 993 corpus positions)\n",
      "2020-12-16 02:30:04,042 : INFO : token count processed\n",
      "2020-12-16 02:30:04,052 : INFO : frequencies processed\n",
      "2020-12-16 02:30:04,675 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:30:04,676 : INFO : entropies processed\n",
      "2020-12-16 02:30:04,677 : INFO : extropies processed\n",
      "2020-12-16 02:30:04,679 : INFO : token count processed\n",
      "2020-12-16 02:30:04,680 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:30:04,682 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:30:04,682 : INFO : vocab #6957\n",
      "2020-12-16 02:30:04,684 : INFO : diff #set()\n",
      "2020-12-16 02:30:05,945 : INFO : alphabet #6957\n",
      "2020-12-16 02:30:06,567 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us748.c')[[1.0338174459711043, 0.4916862140114652], [0.7922302484512329, 0.20776975], [4.682616108071052, 1.4058612061191562], [8.772834053667403, 6.053077447729077]]\n",
      "2020-12-16 02:30:06,570 : INFO : Removed 32 and 2062 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:30:06,571 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:30:06,573 : INFO : built Dictionary(142 unique tokens: ['accept', 'access', 'also', 'altern', 'author']...) from 2 documents (total 691 corpus positions)\n",
      "2020-12-16 02:30:06,694 : INFO : token count processed\n",
      "2020-12-16 02:30:06,703 : INFO : frequencies processed\n",
      "2020-12-16 02:30:07,324 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:30:07,324 : INFO : entropies processed\n",
      "2020-12-16 02:30:07,325 : INFO : extropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 02:30:07,328 : INFO : token count processed\n",
      "2020-12-16 02:30:07,329 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:30:07,331 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:30:07,331 : INFO : vocab #6957\n",
      "2020-12-16 02:30:07,333 : INFO : diff #set()\n",
      "2020-12-16 02:30:08,584 : INFO : alphabet #6957\n",
      "2020-12-16 02:30:09,210 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ20.txt', 'test_data/LibEST_semeru_format/test/us900.c')[[1.1447020552638465, 0.46626523136192805], [0.9458929188549519, 0.05410708], [3.272812967681858, 1.3375600856964762], [7.2755988615781275, 5.3029932265755635]]\n",
      "2020-12-16 02:30:09,213 : INFO : Removed 31 and 939 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:30:09,214 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:30:09,216 : INFO : built Dictionary(149 unique tokens: ['author', 'base', 'cbc', 'check', 'client']...) from 2 documents (total 718 corpus positions)\n",
      "2020-12-16 02:30:09,327 : INFO : token count processed\n",
      "2020-12-16 02:30:09,335 : INFO : frequencies processed\n",
      "2020-12-16 02:30:10,065 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:30:10,066 : INFO : entropies processed\n",
      "2020-12-16 02:30:10,067 : INFO : extropies processed\n",
      "2020-12-16 02:30:10,069 : INFO : token count processed\n",
      "2020-12-16 02:30:10,070 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:30:10,072 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:30:10,072 : INFO : vocab #6957\n",
      "2020-12-16 02:30:10,074 : INFO : diff #set()\n",
      "2020-12-16 02:30:11,355 : INFO : alphabet #6957\n",
      "2020-12-16 02:30:11,976 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ21.txt', 'test_data/LibEST_semeru_format/test/us1060.c')[[1.0217990248034223, 0.49460900303739597], [0.7953046411275864, 0.20469536], [4.068451161635844, 1.3890737067452985], [7.686387849043962, 5.187484513315168]]\n",
      "2020-12-16 02:30:11,979 : INFO : Removed 13 and 541 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-16 02:30:11,980 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-16 02:30:11,981 : INFO : built Dictionary(105 unique tokens: ['accord', 'author', 'check', 'client', 'est']...) from 2 documents (total 324 corpus positions)\n",
      "2020-12-16 02:30:12,025 : INFO : token count processed\n",
      "2020-12-16 02:30:12,031 : INFO : frequencies processed\n",
      "2020-12-16 02:30:12,658 : INFO : scalar_distribution processed\n",
      "2020-12-16 02:30:12,659 : INFO : entropies processed\n",
      "2020-12-16 02:30:12,660 : INFO : extropies processed\n",
      "2020-12-16 02:30:12,662 : INFO : token count processed\n",
      "2020-12-16 02:30:12,663 : INFO : alphabet_source #6957\n",
      "2020-12-16 02:30:12,665 : INFO : alphabet_target #6957\n",
      "2020-12-16 02:30:12,666 : INFO : vocab #6957\n",
      "2020-12-16 02:30:12,667 : INFO : diff #set()\n",
      "2020-12-16 02:30:13,923 : INFO : alphabet #6957\n",
      "2020-12-16 02:30:14,545 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ28.txt', 'test_data/LibEST_semeru_format/test/us896.c')[[1.113782305111712, 0.4730856141532279], [0.8524157702922821, 0.14758423], [2.8423709931771084, 1.3250943850700085], [7.472795572130023, 4.5423538103030126]]\n",
      "2020-12-16 02:30:14,548 : INFO : Non-groundtruth links computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>DistanceMetric.WMD</th>\n",
       "      <th>SimilarityMetric.WMD_sim</th>\n",
       "      <th>DistanceMetric.SCM</th>\n",
       "      <th>SimilarityMetric.SCM_sim</th>\n",
       "      <th>EntropyMetric.MSI_I</th>\n",
       "      <th>EntropyMetric.MSI_X</th>\n",
       "      <th>EntropyMetric.JI</th>\n",
       "      <th>EntropyMetric.MI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us894.c</td>\n",
       "      <td>1.055692</td>\n",
       "      <td>0.486454</td>\n",
       "      <td>0.803410</td>\n",
       "      <td>0.196590</td>\n",
       "      <td>4.685972</td>\n",
       "      <td>1.410394</td>\n",
       "      <td>7.920854</td>\n",
       "      <td>6.158280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us898.c</td>\n",
       "      <td>1.115621</td>\n",
       "      <td>0.472674</td>\n",
       "      <td>0.799947</td>\n",
       "      <td>0.200053</td>\n",
       "      <td>3.653757</td>\n",
       "      <td>1.376497</td>\n",
       "      <td>7.226829</td>\n",
       "      <td>4.598405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us900.c</td>\n",
       "      <td>1.182004</td>\n",
       "      <td>0.458294</td>\n",
       "      <td>0.953313</td>\n",
       "      <td>0.046687</td>\n",
       "      <td>2.873141</td>\n",
       "      <td>1.329330</td>\n",
       "      <td>7.190003</td>\n",
       "      <td>4.580062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us900.c</td>\n",
       "      <td>1.079827</td>\n",
       "      <td>0.480809</td>\n",
       "      <td>0.947533</td>\n",
       "      <td>0.052467</td>\n",
       "      <td>4.367133</td>\n",
       "      <td>1.395704</td>\n",
       "      <td>7.502431</td>\n",
       "      <td>5.776133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us3496.c</td>\n",
       "      <td>1.050227</td>\n",
       "      <td>0.487751</td>\n",
       "      <td>0.791415</td>\n",
       "      <td>0.208585</td>\n",
       "      <td>3.520787</td>\n",
       "      <td>1.357829</td>\n",
       "      <td>7.457979</td>\n",
       "      <td>5.326814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Source  \\\n",
       "0  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "1  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "2  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "3  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "4  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "\n",
       "                                         Target  DistanceMetric.WMD  \\\n",
       "0   test_data/LibEST_semeru_format/test/us894.c            1.055692   \n",
       "1   test_data/LibEST_semeru_format/test/us898.c            1.115621   \n",
       "2   test_data/LibEST_semeru_format/test/us900.c            1.182004   \n",
       "3   test_data/LibEST_semeru_format/test/us900.c            1.079827   \n",
       "4  test_data/LibEST_semeru_format/test/us3496.c            1.050227   \n",
       "\n",
       "   SimilarityMetric.WMD_sim  DistanceMetric.SCM  SimilarityMetric.SCM_sim  \\\n",
       "0                  0.486454            0.803410                  0.196590   \n",
       "1                  0.472674            0.799947                  0.200053   \n",
       "2                  0.458294            0.953313                  0.046687   \n",
       "3                  0.480809            0.947533                  0.052467   \n",
       "4                  0.487751            0.791415                  0.208585   \n",
       "\n",
       "   EntropyMetric.MSI_I  EntropyMetric.MSI_X  EntropyMetric.JI  \\\n",
       "0             4.685972             1.410394          7.920854   \n",
       "1             3.653757             1.376497          7.226829   \n",
       "2             2.873141             1.329330          7.190003   \n",
       "3             4.367133             1.395704          7.502431   \n",
       "4             3.520787             1.357829          7.457979   \n",
       "\n",
       "   EntropyMetric.MI  \n",
       "0          6.158280  \n",
       "1          4.598405  \n",
       "2          4.580062  \n",
       "3          5.776133  \n",
       "4          5.326814  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[step 2]NonGroundTruth Computation\n",
    "metric_list = [DistanceMetric.WMD,DistanceMetric.SCM,EntropyMetric.MSI_I,EntropyMetric.MI]\n",
    "#metric_list = [EntropyMetric.MSI_I,EntropyMetric.MI]\n",
    "word2vec.ComputeDistanceArtifacts( sampling=True, samples = 100, metric_list = metric_list )\n",
    "word2vec.df_nonground_link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>DistanceMetric.WMD</th>\n",
       "      <th>SimilarityMetric.WMD_sim</th>\n",
       "      <th>DistanceMetric.SCM</th>\n",
       "      <th>SimilarityMetric.SCM_sim</th>\n",
       "      <th>EntropyMetric.MSI_I</th>\n",
       "      <th>EntropyMetric.MSI_X</th>\n",
       "      <th>EntropyMetric.JI</th>\n",
       "      <th>EntropyMetric.MI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us894.c</td>\n",
       "      <td>1.055692</td>\n",
       "      <td>0.486454</td>\n",
       "      <td>0.803410</td>\n",
       "      <td>0.196590</td>\n",
       "      <td>4.685972</td>\n",
       "      <td>1.410394</td>\n",
       "      <td>7.920854</td>\n",
       "      <td>6.158280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us898.c</td>\n",
       "      <td>1.115621</td>\n",
       "      <td>0.472674</td>\n",
       "      <td>0.799947</td>\n",
       "      <td>0.200053</td>\n",
       "      <td>3.653757</td>\n",
       "      <td>1.376497</td>\n",
       "      <td>7.226829</td>\n",
       "      <td>4.598405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us900.c</td>\n",
       "      <td>1.182004</td>\n",
       "      <td>0.458294</td>\n",
       "      <td>0.953313</td>\n",
       "      <td>0.046687</td>\n",
       "      <td>2.873141</td>\n",
       "      <td>1.329330</td>\n",
       "      <td>7.190003</td>\n",
       "      <td>4.580062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us900.c</td>\n",
       "      <td>1.079827</td>\n",
       "      <td>0.480809</td>\n",
       "      <td>0.947533</td>\n",
       "      <td>0.052467</td>\n",
       "      <td>4.367133</td>\n",
       "      <td>1.395704</td>\n",
       "      <td>7.502431</td>\n",
       "      <td>5.776133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us3496.c</td>\n",
       "      <td>1.050227</td>\n",
       "      <td>0.487751</td>\n",
       "      <td>0.791415</td>\n",
       "      <td>0.208585</td>\n",
       "      <td>3.520787</td>\n",
       "      <td>1.357829</td>\n",
       "      <td>7.457979</td>\n",
       "      <td>5.326814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Source  \\\n",
       "0  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "1  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "2  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "3  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "4  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "\n",
       "                                         Target  DistanceMetric.WMD  \\\n",
       "0   test_data/LibEST_semeru_format/test/us894.c            1.055692   \n",
       "1   test_data/LibEST_semeru_format/test/us898.c            1.115621   \n",
       "2   test_data/LibEST_semeru_format/test/us900.c            1.182004   \n",
       "3   test_data/LibEST_semeru_format/test/us900.c            1.079827   \n",
       "4  test_data/LibEST_semeru_format/test/us3496.c            1.050227   \n",
       "\n",
       "   SimilarityMetric.WMD_sim  DistanceMetric.SCM  SimilarityMetric.SCM_sim  \\\n",
       "0                  0.486454            0.803410                  0.196590   \n",
       "1                  0.472674            0.799947                  0.200053   \n",
       "2                  0.458294            0.953313                  0.046687   \n",
       "3                  0.480809            0.947533                  0.052467   \n",
       "4                  0.487751            0.791415                  0.208585   \n",
       "\n",
       "   EntropyMetric.MSI_I  EntropyMetric.MSI_X  EntropyMetric.JI  \\\n",
       "0             4.685972             1.410394          7.920854   \n",
       "1             3.653757             1.376497          7.226829   \n",
       "2             2.873141             1.329330          7.190003   \n",
       "3             4.367133             1.395704          7.502431   \n",
       "4             3.520787             1.357829          7.457979   \n",
       "\n",
       "   EntropyMetric.MI  \n",
       "0          6.158280  \n",
       "1          4.598405  \n",
       "2          4.580062  \n",
       "3          5.776133  \n",
       "4          5.326814  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.df_nonground_link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 02:30:30,466 : INFO : Saving in...../dvc-ds4se/se-benchmarking/traceability[libest-VectorizationType.word2vec-LinkType.req2src-False-1608085830.45751].csv\n"
     ]
    }
   ],
   "source": [
    "#[step 3]Saving Non-GroundTruth Links\n",
    "word2vec.SaveLinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 02:31:43,782 : INFO : Loading computed links from... ../dvc-ds4se/se-benchmarking/traceability[libest-VectorizationType.word2vec-LinkType.req2src-False-1608085830.45751].csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>DistanceMetric.WMD</th>\n",
       "      <th>SimilarityMetric.WMD_sim</th>\n",
       "      <th>DistanceMetric.SCM</th>\n",
       "      <th>SimilarityMetric.SCM_sim</th>\n",
       "      <th>EntropyMetric.MSI_I</th>\n",
       "      <th>EntropyMetric.MSI_X</th>\n",
       "      <th>EntropyMetric.JI</th>\n",
       "      <th>EntropyMetric.MI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us894.c</td>\n",
       "      <td>1.055692</td>\n",
       "      <td>0.486454</td>\n",
       "      <td>0.803410</td>\n",
       "      <td>0.196590</td>\n",
       "      <td>4.685972</td>\n",
       "      <td>1.410394</td>\n",
       "      <td>7.920854</td>\n",
       "      <td>6.158280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us898.c</td>\n",
       "      <td>1.115621</td>\n",
       "      <td>0.472674</td>\n",
       "      <td>0.799947</td>\n",
       "      <td>0.200053</td>\n",
       "      <td>3.653757</td>\n",
       "      <td>1.376497</td>\n",
       "      <td>7.226829</td>\n",
       "      <td>4.598405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us900.c</td>\n",
       "      <td>1.182004</td>\n",
       "      <td>0.458294</td>\n",
       "      <td>0.953313</td>\n",
       "      <td>0.046687</td>\n",
       "      <td>2.873141</td>\n",
       "      <td>1.329330</td>\n",
       "      <td>7.190003</td>\n",
       "      <td>4.580062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us900.c</td>\n",
       "      <td>1.079827</td>\n",
       "      <td>0.480809</td>\n",
       "      <td>0.947533</td>\n",
       "      <td>0.052467</td>\n",
       "      <td>4.367133</td>\n",
       "      <td>1.395704</td>\n",
       "      <td>7.502431</td>\n",
       "      <td>5.776133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us3496.c</td>\n",
       "      <td>1.050227</td>\n",
       "      <td>0.487751</td>\n",
       "      <td>0.791415</td>\n",
       "      <td>0.208585</td>\n",
       "      <td>3.520787</td>\n",
       "      <td>1.357829</td>\n",
       "      <td>7.457979</td>\n",
       "      <td>5.326814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Source  \\\n",
       "0  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "1  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "2  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "3  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "4  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "\n",
       "                                         Target  DistanceMetric.WMD  \\\n",
       "0   test_data/LibEST_semeru_format/test/us894.c            1.055692   \n",
       "1   test_data/LibEST_semeru_format/test/us898.c            1.115621   \n",
       "2   test_data/LibEST_semeru_format/test/us900.c            1.182004   \n",
       "3   test_data/LibEST_semeru_format/test/us900.c            1.079827   \n",
       "4  test_data/LibEST_semeru_format/test/us3496.c            1.050227   \n",
       "\n",
       "   SimilarityMetric.WMD_sim  DistanceMetric.SCM  SimilarityMetric.SCM_sim  \\\n",
       "0                  0.486454            0.803410                  0.196590   \n",
       "1                  0.472674            0.799947                  0.200053   \n",
       "2                  0.458294            0.953313                  0.046687   \n",
       "3                  0.480809            0.947533                  0.052467   \n",
       "4                  0.487751            0.791415                  0.208585   \n",
       "\n",
       "   EntropyMetric.MSI_I  EntropyMetric.MSI_X  EntropyMetric.JI  \\\n",
       "0             4.685972             1.410394          7.920854   \n",
       "1             3.653757             1.376497          7.226829   \n",
       "2             2.873141             1.329330          7.190003   \n",
       "3             4.367133             1.395704          7.502431   \n",
       "4             3.520787             1.357829          7.457979   \n",
       "\n",
       "   EntropyMetric.MI  \n",
       "0          6.158280  \n",
       "1          4.598405  \n",
       "2          4.580062  \n",
       "3          5.776133  \n",
       "4          5.326814  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_nonglinks = ds.mining.ir.LoadLinks(timestamp=1608085830.45751, params=parameters)\n",
    "df_nonglinks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 02:32:12,562 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,566 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,570 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,574 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,578 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,582 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,585 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,587 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,590 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,593 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,595 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,598 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,600 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,602 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,605 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,607 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,610 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,612 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,614 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,617 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,619 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,622 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,624 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,626 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,629 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,631 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,634 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,636 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,638 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,641 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,643 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,645 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,648 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,650 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,652 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,654 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,657 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,659 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,661 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,664 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,666 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,669 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,671 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,673 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,675 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,682 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,686 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,688 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,691 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,694 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,697 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,699 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,701 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,704 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,707 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,711 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,713 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,716 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,718 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,720 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,723 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,725 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,727 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,730 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,732 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,735 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,737 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,740 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,742 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,744 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,747 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,749 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,752 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,754 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,756 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,759 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,761 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,764 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,766 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,769 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,771 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,774 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,777 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,779 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,781 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,784 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,786 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,789 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,791 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,794 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,796 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,799 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,801 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,803 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,806 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,808 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,810 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,812 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,814 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,816 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,819 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,821 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,823 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,825 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,827 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,830 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,832 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,834 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,836 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,839 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,841 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,843 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,845 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,848 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,850 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,852 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,854 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,856 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,858 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,860 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,862 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,864 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,867 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,869 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,871 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,873 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,875 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,877 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,879 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,884 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,886 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,889 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,891 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,893 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,895 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,898 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,900 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,902 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,904 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,907 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,909 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,911 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,913 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,915 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,917 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,919 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,921 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,924 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,926 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,928 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,930 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,933 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,935 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,937 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,939 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,942 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,944 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,946 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,948 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,951 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,953 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,955 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,958 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,960 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,962 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,965 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,967 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,969 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,971 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,974 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,976 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,978 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,980 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,983 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,985 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,988 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,990 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,993 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,995 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:12,998 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,000 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,002 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,004 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,006 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,009 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,011 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,013 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,016 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,018 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,020 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,022 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,025 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,029 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,033 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,036 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,040 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,043 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,045 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,047 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,050 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,052 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,054 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,056 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,059 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,061 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,063 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,065 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,067 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,069 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,071 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,074 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,076 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,078 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,080 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,083 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,085 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,087 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,089 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,092 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,094 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,096 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,099 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,101 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,103 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,105 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,108 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,110 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,113 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,115 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,117 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,120 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,122 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,125 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,127 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,129 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,131 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,134 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,136 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,138 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,140 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,144 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,147 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,151 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,154 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,156 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,158 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,160 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,162 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,164 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,167 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,169 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,171 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,173 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,175 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,177 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,180 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,182 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,184 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,186 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,188 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,191 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,193 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,196 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,200 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,203 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,206 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,209 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,212 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,215 : INFO : findDistInDF: semeru_format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 02:32:13,217 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,219 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,222 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,224 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,226 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,229 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,231 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,233 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,236 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,238 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,240 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,243 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,245 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,248 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,250 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,253 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,255 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,258 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,260 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,262 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,273 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,276 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,278 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,281 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,284 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,286 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,288 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,290 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,292 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,294 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,296 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,298 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,301 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,303 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,305 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,307 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,310 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,312 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,314 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,316 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,319 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,321 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,323 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,326 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,328 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,330 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,333 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,335 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,337 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,340 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,342 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,346 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,350 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,354 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,356 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,359 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,361 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,363 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,365 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,368 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,370 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,372 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,375 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,377 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,379 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,382 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,384 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,386 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,388 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,391 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,393 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,396 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,398 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,400 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,403 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,417 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,420 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,422 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,425 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,429 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,432 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,434 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,436 : INFO : findDistInDF: semeru_format\n",
      "2020-12-16 02:32:13,471 : INFO : Groundtruth links computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>DistanceMetric.WMD</th>\n",
       "      <th>SimilarityMetric.WMD_sim</th>\n",
       "      <th>DistanceMetric.SCM</th>\n",
       "      <th>SimilarityMetric.SCM_sim</th>\n",
       "      <th>EntropyMetric.MSI_I</th>\n",
       "      <th>EntropyMetric.MSI_X</th>\n",
       "      <th>EntropyMetric.JI</th>\n",
       "      <th>EntropyMetric.MI</th>\n",
       "      <th>Linked?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us894.c</td>\n",
       "      <td>1.055692</td>\n",
       "      <td>0.486454</td>\n",
       "      <td>0.803410</td>\n",
       "      <td>0.196590</td>\n",
       "      <td>4.685972</td>\n",
       "      <td>1.410394</td>\n",
       "      <td>7.920854</td>\n",
       "      <td>6.158280</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us898.c</td>\n",
       "      <td>1.115621</td>\n",
       "      <td>0.472674</td>\n",
       "      <td>0.799947</td>\n",
       "      <td>0.200053</td>\n",
       "      <td>3.653757</td>\n",
       "      <td>1.376497</td>\n",
       "      <td>7.226829</td>\n",
       "      <td>4.598405</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us900.c</td>\n",
       "      <td>1.182004</td>\n",
       "      <td>0.458294</td>\n",
       "      <td>0.953313</td>\n",
       "      <td>0.046687</td>\n",
       "      <td>2.873141</td>\n",
       "      <td>1.329330</td>\n",
       "      <td>7.190003</td>\n",
       "      <td>4.580062</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us900.c</td>\n",
       "      <td>1.079827</td>\n",
       "      <td>0.480809</td>\n",
       "      <td>0.947533</td>\n",
       "      <td>0.052467</td>\n",
       "      <td>4.367133</td>\n",
       "      <td>1.395704</td>\n",
       "      <td>7.502431</td>\n",
       "      <td>5.776133</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us3496.c</td>\n",
       "      <td>1.050227</td>\n",
       "      <td>0.487751</td>\n",
       "      <td>0.791415</td>\n",
       "      <td>0.208585</td>\n",
       "      <td>3.520787</td>\n",
       "      <td>1.357829</td>\n",
       "      <td>7.457979</td>\n",
       "      <td>5.326814</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us1864.c</td>\n",
       "      <td>0.956124</td>\n",
       "      <td>0.511215</td>\n",
       "      <td>0.688852</td>\n",
       "      <td>0.311148</td>\n",
       "      <td>4.401750</td>\n",
       "      <td>1.392002</td>\n",
       "      <td>8.618406</td>\n",
       "      <td>5.614617</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us748.c</td>\n",
       "      <td>1.033817</td>\n",
       "      <td>0.491686</td>\n",
       "      <td>0.792230</td>\n",
       "      <td>0.207770</td>\n",
       "      <td>4.682616</td>\n",
       "      <td>1.405861</td>\n",
       "      <td>8.772834</td>\n",
       "      <td>6.053077</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us900.c</td>\n",
       "      <td>1.144702</td>\n",
       "      <td>0.466265</td>\n",
       "      <td>0.945893</td>\n",
       "      <td>0.054107</td>\n",
       "      <td>3.272813</td>\n",
       "      <td>1.337560</td>\n",
       "      <td>7.275599</td>\n",
       "      <td>5.302993</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us1060.c</td>\n",
       "      <td>1.021799</td>\n",
       "      <td>0.494609</td>\n",
       "      <td>0.795305</td>\n",
       "      <td>0.204695</td>\n",
       "      <td>4.068451</td>\n",
       "      <td>1.389074</td>\n",
       "      <td>7.686388</td>\n",
       "      <td>5.187485</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us896.c</td>\n",
       "      <td>1.113782</td>\n",
       "      <td>0.473086</td>\n",
       "      <td>0.852416</td>\n",
       "      <td>0.147584</td>\n",
       "      <td>2.842371</td>\n",
       "      <td>1.325094</td>\n",
       "      <td>7.472796</td>\n",
       "      <td>4.542354</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Source  \\\n",
       "0   test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "1   test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "2   test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "3   test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "4   test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "..                                                ...   \n",
       "95  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "96  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "97  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "98  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "99  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "\n",
       "                                          Target  DistanceMetric.WMD  \\\n",
       "0    test_data/LibEST_semeru_format/test/us894.c            1.055692   \n",
       "1    test_data/LibEST_semeru_format/test/us898.c            1.115621   \n",
       "2    test_data/LibEST_semeru_format/test/us900.c            1.182004   \n",
       "3    test_data/LibEST_semeru_format/test/us900.c            1.079827   \n",
       "4   test_data/LibEST_semeru_format/test/us3496.c            1.050227   \n",
       "..                                           ...                 ...   \n",
       "95  test_data/LibEST_semeru_format/test/us1864.c            0.956124   \n",
       "96   test_data/LibEST_semeru_format/test/us748.c            1.033817   \n",
       "97   test_data/LibEST_semeru_format/test/us900.c            1.144702   \n",
       "98  test_data/LibEST_semeru_format/test/us1060.c            1.021799   \n",
       "99   test_data/LibEST_semeru_format/test/us896.c            1.113782   \n",
       "\n",
       "    SimilarityMetric.WMD_sim  DistanceMetric.SCM  SimilarityMetric.SCM_sim  \\\n",
       "0                   0.486454            0.803410                  0.196590   \n",
       "1                   0.472674            0.799947                  0.200053   \n",
       "2                   0.458294            0.953313                  0.046687   \n",
       "3                   0.480809            0.947533                  0.052467   \n",
       "4                   0.487751            0.791415                  0.208585   \n",
       "..                       ...                 ...                       ...   \n",
       "95                  0.511215            0.688852                  0.311148   \n",
       "96                  0.491686            0.792230                  0.207770   \n",
       "97                  0.466265            0.945893                  0.054107   \n",
       "98                  0.494609            0.795305                  0.204695   \n",
       "99                  0.473086            0.852416                  0.147584   \n",
       "\n",
       "    EntropyMetric.MSI_I  EntropyMetric.MSI_X  EntropyMetric.JI  \\\n",
       "0              4.685972             1.410394          7.920854   \n",
       "1              3.653757             1.376497          7.226829   \n",
       "2              2.873141             1.329330          7.190003   \n",
       "3              4.367133             1.395704          7.502431   \n",
       "4              3.520787             1.357829          7.457979   \n",
       "..                  ...                  ...               ...   \n",
       "95             4.401750             1.392002          8.618406   \n",
       "96             4.682616             1.405861          8.772834   \n",
       "97             3.272813             1.337560          7.275599   \n",
       "98             4.068451             1.389074          7.686388   \n",
       "99             2.842371             1.325094          7.472796   \n",
       "\n",
       "    EntropyMetric.MI  Linked?  \n",
       "0           6.158280      0.0  \n",
       "1           4.598405      0.0  \n",
       "2           4.580062      0.0  \n",
       "3           5.776133      0.0  \n",
       "4           5.326814      1.0  \n",
       "..               ...      ...  \n",
       "95          5.614617      1.0  \n",
       "96          6.053077      0.0  \n",
       "97          5.302993      1.0  \n",
       "98          5.187485      0.0  \n",
       "99          4.542354      0.0  \n",
       "\n",
       "[100 rows x 11 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[step 4]GroundTruthMatching Testing\n",
    "#TODO change the path for a param\n",
    "path_to_ground_truth = '/tf/main/benchmarking/traceability/testbeds/groundtruth/english/[libest-ground-req-to-tc].txt'\n",
    "word2vec.MatchWithGroundTruth(path_to_ground_truth, semeru_format=True)\n",
    "word2vec.df_ground_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 4.1]GroundTruthMatching Testing For CISCO Mappings\n",
    "word2vec.MatchWithGroundTruth(from_mappings=True)\n",
    "word2vec.df_ground_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('RQ33.txt', 'us894.c')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[optional]GroundTruth Direct Processing\n",
    "ground_links = word2vec.ground_truth_processing(path_to_ground_truth)\n",
    "ground_links[141] # A tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 02:32:42,445 : INFO : Saving in...../dvc-ds4se/se-benchmarking/traceability[libest-VectorizationType.word2vec-LinkType.req2src-True-1608085962.438394].csv\n"
     ]
    }
   ],
   "source": [
    "#[step 5]Saving GroundTruth Links\n",
    "word2vec.SaveLinks(grtruth = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-16 02:32:58,325 : INFO : Loading computed links from... ../dvc-ds4se/se-benchmarking/traceability[libest-VectorizationType.word2vec-LinkType.req2src-True-1608085962.438394].csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>DistanceMetric.WMD</th>\n",
       "      <th>SimilarityMetric.WMD_sim</th>\n",
       "      <th>DistanceMetric.SCM</th>\n",
       "      <th>SimilarityMetric.SCM_sim</th>\n",
       "      <th>EntropyMetric.MSI_I</th>\n",
       "      <th>EntropyMetric.MSI_X</th>\n",
       "      <th>EntropyMetric.JI</th>\n",
       "      <th>EntropyMetric.MI</th>\n",
       "      <th>Linked?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us894.c</td>\n",
       "      <td>1.055692</td>\n",
       "      <td>0.486454</td>\n",
       "      <td>0.803410</td>\n",
       "      <td>0.196590</td>\n",
       "      <td>4.685972</td>\n",
       "      <td>1.410394</td>\n",
       "      <td>7.920854</td>\n",
       "      <td>6.158280</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us898.c</td>\n",
       "      <td>1.115621</td>\n",
       "      <td>0.472674</td>\n",
       "      <td>0.799947</td>\n",
       "      <td>0.200053</td>\n",
       "      <td>3.653757</td>\n",
       "      <td>1.376497</td>\n",
       "      <td>7.226829</td>\n",
       "      <td>4.598405</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us900.c</td>\n",
       "      <td>1.182004</td>\n",
       "      <td>0.458294</td>\n",
       "      <td>0.953313</td>\n",
       "      <td>0.046687</td>\n",
       "      <td>2.873141</td>\n",
       "      <td>1.329330</td>\n",
       "      <td>7.190003</td>\n",
       "      <td>4.580062</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us900.c</td>\n",
       "      <td>1.079827</td>\n",
       "      <td>0.480809</td>\n",
       "      <td>0.947533</td>\n",
       "      <td>0.052467</td>\n",
       "      <td>4.367133</td>\n",
       "      <td>1.395704</td>\n",
       "      <td>7.502431</td>\n",
       "      <td>5.776133</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_data/LibEST_semeru_format/requirements/RQ...</td>\n",
       "      <td>test_data/LibEST_semeru_format/test/us3496.c</td>\n",
       "      <td>1.050227</td>\n",
       "      <td>0.487751</td>\n",
       "      <td>0.791415</td>\n",
       "      <td>0.208585</td>\n",
       "      <td>3.520787</td>\n",
       "      <td>1.357829</td>\n",
       "      <td>7.457979</td>\n",
       "      <td>5.326814</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Source  \\\n",
       "0  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "1  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "2  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "3  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "4  test_data/LibEST_semeru_format/requirements/RQ...   \n",
       "\n",
       "                                         Target  DistanceMetric.WMD  \\\n",
       "0   test_data/LibEST_semeru_format/test/us894.c            1.055692   \n",
       "1   test_data/LibEST_semeru_format/test/us898.c            1.115621   \n",
       "2   test_data/LibEST_semeru_format/test/us900.c            1.182004   \n",
       "3   test_data/LibEST_semeru_format/test/us900.c            1.079827   \n",
       "4  test_data/LibEST_semeru_format/test/us3496.c            1.050227   \n",
       "\n",
       "   SimilarityMetric.WMD_sim  DistanceMetric.SCM  SimilarityMetric.SCM_sim  \\\n",
       "0                  0.486454            0.803410                  0.196590   \n",
       "1                  0.472674            0.799947                  0.200053   \n",
       "2                  0.458294            0.953313                  0.046687   \n",
       "3                  0.480809            0.947533                  0.052467   \n",
       "4                  0.487751            0.791415                  0.208585   \n",
       "\n",
       "   EntropyMetric.MSI_I  EntropyMetric.MSI_X  EntropyMetric.JI  \\\n",
       "0             4.685972             1.410394          7.920854   \n",
       "1             3.653757             1.376497          7.226829   \n",
       "2             2.873141             1.329330          7.190003   \n",
       "3             4.367133             1.395704          7.502431   \n",
       "4             3.520787             1.357829          7.457979   \n",
       "\n",
       "   EntropyMetric.MI  Linked?  \n",
       "0          6.158280      0.0  \n",
       "1          4.598405      0.0  \n",
       "2          4.580062      0.0  \n",
       "3          5.776133      0.0  \n",
       "4          5.326814      1.0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_glinks = ds.mining.ir.LoadLinks(timestamp=1608085962.438394, params=parameters,grtruth = True)\n",
    "df_glinks.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
