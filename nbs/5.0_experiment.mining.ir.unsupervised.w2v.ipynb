{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiment.mining.ir.unsupervised.w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting Neural Unsupervised Approaches for Software Information Retrieval [w2v]\n",
    "\n",
    "> Just Paper. Full Experimentation. This module is dedicated to experiment with word2vec. Consider to Copy the entire notebook for a new and separeted empirical evaluation. \n",
    "> Implementing mutual information analysis\n",
    "> Author: @danaderp April 2020\n",
    "> Author: @danielrc Nov 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This copy is for Cisco purposes. It was adapted to process private github data from cisco. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds4se.mining.ir import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prg import prg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ds4se as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Similarity with BasicSequenceVectorization\n",
    "\n",
    "We test diferent similarities based on [blog](https://www.kdnuggets.com/2017/08/comparing-distance-measurements-python-scipy.html) and [blog2](https://www.kdnuggets.com/2019/01/comparison-text-distance-metrics.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experients Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../dvc-ds4se/' #dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_prefix = path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_8k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 0.0.0\n",
    "path_to_trained_model = path_data+'models/wv/conv/[word2vec-Py-Java-SK-500-20E-1592607739.629433].model'\n",
    "def libest_params():\n",
    "        return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.req2tc,\n",
    "        \"system\": 'libest',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_type\": SoftwareArtifacts.REQ.value,\n",
    "        \"target_type\": SoftwareArtifacts.TC.value,\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": path_data + 'se-benchmarking/traceability/cisco/libest_data/[libest-all-corpus-1596063103.098236].csv',\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','conv'],\n",
    "            \"prep\": Preprocessing.conv\n",
    "        },\n",
    "        \"path_mappings\": path_data + \"se-benchmarking/traceability/testbeds/groundtruth/english/[libest-ground-req-to-tc].txt\",\n",
    "        \"saving_path\": path_data + 'metrics/traceability/experiments0.0.x/',\n",
    "        \"names\": ['Source','Target','Linked?'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 2 with Libest BPE preprocessing\n",
    "def libest_params_bpe():\n",
    "        return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.req2src,\n",
    "        \"system\": 'libest',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_type\": 'req', #TODO Standardize the artifacts \n",
    "        \"target_type\": 'tc',\n",
    "        #\"path_mappings\": 'cisco/libest_data/sacp-pr-mappings.csv',\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": path_data + 'se-benchmarking/traceability/cisco/libest_data/[libest-all-corpus-1596063103.098236].csv',\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','bpe128k'],\n",
    "            \"prep\": Preprocessing.bpe\n",
    "        },\n",
    "        \"saving_path\": path_data + 'se-benchmarking/traceability',\n",
    "        \"names\": ['Source','Target','Linked?'],\n",
    "        \"model_prefix\":path_data + 'models/bpe/sentencepiece/wiki_py_java_bpe_8k' #For BPE Analysis    \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CISCO GitHub Parameters\n",
    "def sacp_params():\n",
    "    return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.issue2src,\n",
    "        \"system\": 'sacp-python-common',\n",
    "        \"path_to_trained_model\": path_data + 'models/wv/conv/[word2vec-Py-Java-Wiki-SK-500-20E[0]-1592979270.711115].model',\n",
    "        \"source_type\": 'pr', #TODO Standardize the artifacts \n",
    "        \"target_type\": 'py',\n",
    "        \"path_mappings\": '/tf/data/cisco/sacp_data/sacp-pr-mappings.csv',\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": '/tf/data/cisco/sacp_data/[sacp-python-common-all-corpus-1596383717.992744].csv', #MUST have bpe8k <----\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','conv'],\n",
    "            \"prep\": Preprocessing.conv\n",
    "        },\n",
    "        \"saving_path\":  path_data/'se-benchmarking/traceability/cisco/sacp',\n",
    "        \"names\": ['Source','Target','Linked?']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_trained_model = path_data + 'models/wv/bpe8k/[word2vec-Java-Py-Wiki-SK-500-20E-8k[12]-1594546477.788739].model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sacp_params_bpe():\n",
    "    return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.issue2src,\n",
    "        \"system\": 'sacp-python-common',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_type\": 'pr', #TODO Standardize the artifacts \n",
    "        \"target_type\": 'py',\n",
    "        \"path_mappings\": '/tf/data/cisco/sacp_data/sacp-pr-mappings.csv',\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": '/tf/data/cisco/sacp_data/[sacp-python-common-all-corpus-1596383717.992744].csv',\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','bpe8k'],\n",
    "            \"prep\": Preprocessing.bpe\n",
    "        },\n",
    "        \"saving_path\": path_data + 'se-benchmarking/traceability/cisco/sacp',\n",
    "        \"names\": ['Source','Target','Linked?'],\n",
    "        \"model_prefix\":path_data + 'models/bpe/sentencepiece/wiki_py_java_bpe_8k' #For BPE Analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiments 0.0.2 <<-- word2vec\n",
    "path_to_trained_model = path_data+'models/wv/conv/[word2vec-Py-Java-SK-500-20E-1592607739.629433].model'\n",
    "def sacp_params():\n",
    "        return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.issue2src,\n",
    "        \"system\": 'sacp-python-common',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_type\": SoftwareArtifacts.PR.value,\n",
    "        \"target_type\": SoftwareArtifacts.PY.value,\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": '/tf/data/cisco/sacp_data/[sacp-python-common-all-corpus-1596383717.992744].csv',\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','conv'],\n",
    "            \"prep\": Preprocessing.conv\n",
    "        },\n",
    "        \"path_mappings\": \"/tf/data/cisco/sacp_data/sacp-pr-mappings.csv\",\n",
    "        \"saving_path\": path_data + 'metrics/traceability/experiments0.0.x/',\n",
    "        \"names\": ['Source','Target','Linked?']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectorizationType': <VectorizationType.word2vec: 1>,\n",
       " 'linkType': <LinkType.issue2src: 3>,\n",
       " 'system': 'sacp-python-common',\n",
       " 'path_to_trained_model': '../dvc-ds4se/models/wv/conv/[word2vec-Py-Java-SK-500-20E-1592607739.629433].model',\n",
       " 'source_type': 'pr',\n",
       " 'target_type': 'py',\n",
       " 'system_path_config': {'system_path': '/tf/data/cisco/sacp_data/[sacp-python-common-all-corpus-1596383717.992744].csv',\n",
       "  'sep': '~',\n",
       "  'names': ['ids', 'conv'],\n",
       "  'prep': <Preprocessing.conv: 1>},\n",
       " 'path_mappings': '/tf/data/cisco/sacp_data/sacp-pr-mappings.csv',\n",
       " 'saving_path': '../dvc-ds4se/metrics/traceability/experiments0.0.x/',\n",
       " 'names': ['Source', 'Target', 'Linked?']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parameters = default_params()\n",
    "parameters = sacp_params()\n",
    "#parameters = _params()\n",
    "#parameters = sacp_params_bpe()\n",
    "#parameters = libest_params_bpe()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Similarity with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:39:56,629 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:39:56,662 : INFO : built Dictionary(2480 unique tokens: ['allow', 'append', 'bom', 'dare', 'feed']...) from 362 documents (total 49209 corpus positions)\n",
      "2020-12-23 02:39:56,663 : INFO : conventional preprocessing documents, dictionary, and vocab for the test corpus\n",
      "2020-12-23 02:39:56,664 : INFO : loading Word2Vec object from ../dvc-ds4se/models/wv/conv/[word2vec-Py-Java-SK-500-20E-1592607739.629433].model\n",
      "2020-12-23 02:39:56,907 : INFO : loading wv recursively from ../dvc-ds4se/models/wv/conv/[word2vec-Py-Java-SK-500-20E-1592607739.629433].model.wv.* with mmap=None\n",
      "2020-12-23 02:39:56,908 : INFO : loading vectors from ../dvc-ds4se/models/wv/conv/[word2vec-Py-Java-SK-500-20E-1592607739.629433].model.wv.vectors.npy with mmap=None\n",
      "2020-12-23 02:39:56,951 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-12-23 02:39:56,953 : INFO : loading vocabulary recursively from ../dvc-ds4se/models/wv/conv/[word2vec-Py-Java-SK-500-20E-1592607739.629433].model.vocabulary.* with mmap=None\n",
      "2020-12-23 02:39:56,954 : INFO : loading trainables recursively from ../dvc-ds4se/models/wv/conv/[word2vec-Py-Java-SK-500-20E-1592607739.629433].model.trainables.* with mmap=None\n",
      "2020-12-23 02:39:56,954 : INFO : loading syn1neg from ../dvc-ds4se/models/wv/conv/[word2vec-Py-Java-SK-500-20E-1592607739.629433].model.trainables.syn1neg.npy with mmap=None\n",
      "2020-12-23 02:39:56,992 : INFO : setting ignored attribute cum_table to None\n",
      "2020-12-23 02:39:56,993 : INFO : loaded ../dvc-ds4se/models/wv/conv/[word2vec-Py-Java-SK-500-20E-1592607739.629433].model\n",
      "2020-12-23 02:39:57,134 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-12-23 02:39:57,221 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7f2f006c96d8>\n",
      "2020-12-23 02:39:57,223 : INFO : iterating over columns in dictionary order\n",
      "2020-12-23 02:39:57,229 : INFO : PROGRESS: at 0.04% columns (1 / 2480, 0.040323% density, 0.040323% projected density)\n",
      "2020-12-23 02:40:02,470 : INFO : PROGRESS: at 40.36% columns (1001 / 2480, 0.242066% density, 0.540145% projected density)\n",
      "2020-12-23 02:40:04,672 : INFO : PROGRESS: at 80.69% columns (2001 / 2480, 0.270259% density, 0.325301% projected density)\n",
      "2020-12-23 02:40:05,620 : INFO : constructed a sparse term similarity matrix with 0.281380% density\n"
     ]
    }
   ],
   "source": [
    "#[step 1]Creating the Vectorization Class\n",
    "word2vec = ds.mining.ir.Word2VecSeqVect( params = parameters, logging = logging )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:40:05,640 : INFO : Removed 1 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:05,640 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:05,642 : INFO : built Dictionary(124 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 419 corpus positions)\n",
      "2020-12-23 02:40:05,670 : INFO : token count processed\n",
      "2020-12-23 02:40:05,673 : INFO : frequencies processed\n",
      "2020-12-23 02:40:05,808 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:05,809 : INFO : entropies processed\n",
      "2020-12-23 02:40:05,809 : INFO : extropies processed\n",
      "2020-12-23 02:40:05,810 : INFO : token count processed\n",
      "2020-12-23 02:40:05,811 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:05,812 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:05,812 : INFO : vocab #2480\n",
      "2020-12-23 02:40:05,813 : INFO : diff #set()\n",
      "2020-12-23 02:40:06,068 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:06,195 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.226182203376558, 0.44919953024655923], [0.8896235525608063, 0.11037645], [1.584962500721156, 1.1699250014423124], [3.7004397181410926, 6.301552355933639, 6.4008173220932, 3.6011747519815316, 2.7003776039521075, 0.09926496615956104]]\n",
      "2020-12-23 02:40:06,197 : INFO : Removed 1 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:06,198 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:06,199 : INFO : built Dictionary(161 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 655 corpus positions)\n",
      "2020-12-23 02:40:06,244 : INFO : token count processed\n",
      "2020-12-23 02:40:06,246 : INFO : frequencies processed\n",
      "2020-12-23 02:40:06,374 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:06,374 : INFO : entropies processed\n",
      "2020-12-23 02:40:06,375 : INFO : extropies processed\n",
      "2020-12-23 02:40:06,376 : INFO : token count processed\n",
      "2020-12-23 02:40:06,377 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:06,378 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:06,379 : INFO : vocab #2480\n",
      "2020-12-23 02:40:06,380 : INFO : diff #set()\n",
      "2020-12-23 02:40:06,640 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:06,769 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.2231280632715442, 0.4498166419294824], [0.8597878664731979, 0.14021213], [1.584962500721156, 1.1699250014423124], [3.7004397181410926, 6.739005504021667, 6.8056422649648765, 3.633802957197884, 3.105202546823784, 0.0666367609432097]]\n",
      "2020-12-23 02:40:06,772 : INFO : Removed 1 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:06,773 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:06,774 : INFO : built Dictionary(106 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 506 corpus positions)\n",
      "2020-12-23 02:40:06,804 : INFO : token count processed\n",
      "2020-12-23 02:40:06,806 : INFO : frequencies processed\n",
      "2020-12-23 02:40:06,934 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:06,935 : INFO : entropies processed\n",
      "2020-12-23 02:40:06,935 : INFO : extropies processed\n",
      "2020-12-23 02:40:06,936 : INFO : token count processed\n",
      "2020-12-23 02:40:06,937 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:06,939 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:06,939 : INFO : vocab #2480\n",
      "2020-12-23 02:40:06,940 : INFO : diff #set()\n",
      "2020-12-23 02:40:07,198 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:07,325 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.219856556884352, 0.45047955774382814], [0.9148126170039177, 0.08518738], [1.0, 1.0], [3.7004397181410926, 5.870833373337847, 5.95821349610202, 3.61305959537692, 2.257773777960927, 0.08738012276417262]]\n",
      "2020-12-23 02:40:07,327 : INFO : Removed 1 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:07,328 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:07,330 : INFO : built Dictionary(66 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 189 corpus positions)\n",
      "2020-12-23 02:40:07,351 : INFO : token count processed\n",
      "2020-12-23 02:40:07,353 : INFO : frequencies processed\n",
      "2020-12-23 02:40:07,482 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:07,482 : INFO : entropies processed\n",
      "2020-12-23 02:40:07,483 : INFO : extropies processed\n",
      "2020-12-23 02:40:07,484 : INFO : token count processed\n",
      "2020-12-23 02:40:07,485 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:07,486 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:07,487 : INFO : vocab #2480\n",
      "2020-12-23 02:40:07,488 : INFO : diff #set()\n",
      "2020-12-23 02:40:07,743 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:07,871 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.1713350802314637, 0.4605461446758371], [0.8754657432436943, 0.12453426], [1.0, 1.0], [3.7004397181410926, 5.371881234145534, 5.572606746477421, 3.499714205809207, 1.872167028336328, 0.2007255123318865]]\n",
      "2020-12-23 02:40:07,874 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:07,874 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:07,875 : INFO : built Dictionary(57 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 141 corpus positions)\n",
      "2020-12-23 02:40:07,888 : INFO : token count processed\n",
      "2020-12-23 02:40:07,890 : INFO : frequencies processed\n",
      "2020-12-23 02:40:08,018 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:08,019 : INFO : entropies processed\n",
      "2020-12-23 02:40:08,020 : INFO : extropies processed\n",
      "2020-12-23 02:40:08,021 : INFO : token count processed\n",
      "2020-12-23 02:40:08,022 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:08,023 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:08,023 : INFO : vocab #2480\n",
      "2020-12-23 02:40:08,024 : INFO : diff #set()\n",
      "2020-12-23 02:40:08,280 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:08,407 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.160970064075761, 0.4627551379003931], [0.8834090307354927, 0.11659097], [1.0, 1.0], [3.7004397181410926, 4.85108279267097, 5.140316248250741, 3.411206262561322, 1.4398765301096486, 0.28923345557977154]]\n",
      "2020-12-23 02:40:08,410 : INFO : Removed 1 and 127 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:08,411 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:08,412 : INFO : built Dictionary(96 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 414 corpus positions)\n",
      "2020-12-23 02:40:08,442 : INFO : token count processed\n",
      "2020-12-23 02:40:08,447 : INFO : frequencies processed\n",
      "2020-12-23 02:40:08,572 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:08,573 : INFO : entropies processed\n",
      "2020-12-23 02:40:08,573 : INFO : extropies processed\n",
      "2020-12-23 02:40:08,574 : INFO : token count processed\n",
      "2020-12-23 02:40:08,575 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:08,576 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:08,576 : INFO : vocab #2480\n",
      "2020-12-23 02:40:08,577 : INFO : diff #set()\n",
      "2020-12-23 02:40:08,838 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:08,965 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.213807468007911, 0.45171046464119463], [0.9002867341041565, 0.099713266], [1.584962500721156, 1.1699250014423124], [3.7004397181410926, 6.139571208108155, 6.224414517550855, 3.6155964086983925, 2.5239747994097623, 0.08484330944270013]]\n",
      "2020-12-23 02:40:08,968 : INFO : Removed 1 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:08,969 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:08,970 : INFO : built Dictionary(84 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 424 corpus positions)\n",
      "2020-12-23 02:40:08,993 : INFO : token count processed\n",
      "2020-12-23 02:40:08,995 : INFO : frequencies processed\n",
      "2020-12-23 02:40:09,124 : INFO : scalar_distribution processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:40:09,124 : INFO : entropies processed\n",
      "2020-12-23 02:40:09,125 : INFO : extropies processed\n",
      "2020-12-23 02:40:09,126 : INFO : token count processed\n",
      "2020-12-23 02:40:09,127 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:09,127 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:09,128 : INFO : vocab #2480\n",
      "2020-12-23 02:40:09,129 : INFO : diff #set()\n",
      "2020-12-23 02:40:09,386 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:09,513 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.2138943523907912, 0.45169273724380615], [0.866939976811409, 0.13306002], [0.0, 0.0], [3.7004397181410926, 5.609710627339259, 5.731815546985773, 3.578334798494579, 2.0313758288446806, 0.12210491964651382]]\n",
      "2020-12-23 02:40:09,516 : INFO : Removed 1 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:09,517 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:09,518 : INFO : built Dictionary(172 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 1084 corpus positions)\n",
      "2020-12-23 02:40:09,565 : INFO : token count processed\n",
      "2020-12-23 02:40:09,568 : INFO : frequencies processed\n",
      "2020-12-23 02:40:09,694 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:09,695 : INFO : entropies processed\n",
      "2020-12-23 02:40:09,696 : INFO : extropies processed\n",
      "2020-12-23 02:40:09,698 : INFO : token count processed\n",
      "2020-12-23 02:40:09,699 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:09,700 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:09,701 : INFO : vocab #2480\n",
      "2020-12-23 02:40:09,702 : INFO : diff #set()\n",
      "2020-12-23 02:40:09,967 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:10,095 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.1816937108588639, 0.4583594823703881], [0.8278666734695435, 0.17213333], [2.321928094887362, 1.2877123795494492], [3.7004397181410926, 7.2441902753576075, 7.2705967431090315, 3.6740332503896695, 3.570157024967939, 0.026406467751423968]]\n",
      "2020-12-23 02:40:10,098 : INFO : Removed 1 and 136 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:10,099 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:10,101 : INFO : built Dictionary(137 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 691 corpus positions)\n",
      "2020-12-23 02:40:10,147 : INFO : token count processed\n",
      "2020-12-23 02:40:10,151 : INFO : frequencies processed\n",
      "2020-12-23 02:40:10,280 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:10,280 : INFO : entropies processed\n",
      "2020-12-23 02:40:10,281 : INFO : extropies processed\n",
      "2020-12-23 02:40:10,282 : INFO : token count processed\n",
      "2020-12-23 02:40:10,283 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:10,283 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:10,284 : INFO : vocab #2480\n",
      "2020-12-23 02:40:10,285 : INFO : diff #set()\n",
      "2020-12-23 02:40:10,541 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:10,668 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.2141706637119427, 0.45163636949445973], [0.8624442666769028, 0.13755573], [1.0, 1.0], [3.7004397181410926, 6.2567074920449475, 6.326013078822047, 3.631134131363993, 2.6255733606809546, 0.06930558677709975]]\n",
      "2020-12-23 02:40:10,671 : INFO : Removed 1 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:10,672 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:10,673 : INFO : built Dictionary(84 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 201 corpus positions)\n",
      "2020-12-23 02:40:10,692 : INFO : token count processed\n",
      "2020-12-23 02:40:10,694 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:40:10,695 : INFO : frequencies processed\n",
      "2020-12-23 02:40:10,696 : INFO : token count processed\n",
      "2020-12-23 02:40:10,697 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:10,698 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:10,699 : INFO : vocab #2480\n",
      "2020-12-23 02:40:10,701 : INFO : diff #set()\n",
      "2020-12-23 02:40:10,980 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:11,108 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.2306724444898764, 0.4482953122365243], [0.8955796882510185, 0.10442031], [nan, nan], [3.7004397181410926, 5.7680018917339435, 5.970171904795007, 3.4982697050800295, 2.269732186653914, 0.20217001306106308]]\n",
      "2020-12-23 02:40:11,111 : INFO : Removed 1 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:11,112 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:11,113 : INFO : built Dictionary(178 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 742 corpus positions)\n",
      "2020-12-23 02:40:11,164 : INFO : token count processed\n",
      "2020-12-23 02:40:11,167 : INFO : frequencies processed\n",
      "2020-12-23 02:40:11,294 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:11,295 : INFO : entropies processed\n",
      "2020-12-23 02:40:11,295 : INFO : extropies processed\n",
      "2020-12-23 02:40:11,296 : INFO : token count processed\n",
      "2020-12-23 02:40:11,297 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:11,298 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:11,298 : INFO : vocab #2480\n",
      "2020-12-23 02:40:11,299 : INFO : diff #set()\n",
      "2020-12-23 02:40:11,556 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:11,683 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.1996529210008338, 0.4546171763975399], [0.8354210704565048, 0.16457893], [2.0, 1.2451124978365313], [3.7004397181410926, 6.846479111193757, 6.895592608920091, 3.6513262204147585, 3.1951528907789983, 0.04911349772633411]]\n",
      "2020-12-23 02:40:11,686 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:11,687 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:11,688 : INFO : built Dictionary(32 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 45 corpus positions)\n",
      "2020-12-23 02:40:11,696 : INFO : token count processed\n",
      "2020-12-23 02:40:11,699 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:40:11,700 : INFO : frequencies processed\n",
      "2020-12-23 02:40:11,701 : INFO : token count processed\n",
      "2020-12-23 02:40:11,702 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:11,703 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:11,704 : INFO : vocab #2480\n",
      "2020-12-23 02:40:11,705 : INFO : diff #set()\n",
      "2020-12-23 02:40:11,972 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:12,100 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2459964116815332, 0.44523668639849684], [0.9470943138003349, 0.052905686], [nan, nan], [3.7004397181410926, 4.165013816065912, 4.89270217326019, 2.9727513609468144, 1.1922624551190975, 0.7276883571942783]]\n",
      "2020-12-23 02:40:12,102 : INFO : Removed 1 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:12,103 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:12,105 : INFO : built Dictionary(60 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 152 corpus positions)\n",
      "2020-12-23 02:40:12,125 : INFO : token count processed\n",
      "2020-12-23 02:40:12,128 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:40:12,129 : INFO : frequencies processed\n",
      "2020-12-23 02:40:12,130 : INFO : token count processed\n",
      "2020-12-23 02:40:12,131 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:12,132 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:12,133 : INFO : vocab #2480\n",
      "2020-12-23 02:40:12,134 : INFO : diff #set()\n",
      "2020-12-23 02:40:12,395 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:12,522 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.2305685083071976, 0.4483162011279854], [0.8965431377291679, 0.10345686], [nan, nan], [3.7004397181410926, 5.449968864419248, 5.705800666360268, 3.4446079162000736, 2.0053609482191757, 0.25583180194101995]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:40:12,525 : INFO : Removed 1 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:12,526 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:12,527 : INFO : built Dictionary(151 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 589 corpus positions)\n",
      "2020-12-23 02:40:12,577 : INFO : token count processed\n",
      "2020-12-23 02:40:12,582 : INFO : frequencies processed\n",
      "2020-12-23 02:40:12,710 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:12,710 : INFO : entropies processed\n",
      "2020-12-23 02:40:12,711 : INFO : extropies processed\n",
      "2020-12-23 02:40:12,713 : INFO : token count processed\n",
      "2020-12-23 02:40:12,714 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:12,715 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:12,716 : INFO : vocab #2480\n",
      "2020-12-23 02:40:12,717 : INFO : diff #set()\n",
      "2020-12-23 02:40:12,976 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:13,104 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.2090412048461452, 0.45268508247207984], [0.8743438124656677, 0.12565619], [1.0, 1.0], [3.7004397181410926, 6.530294129310484, 6.596891799654061, 3.6338420477975157, 2.896452081512968, 0.06659767034357689]]\n",
      "2020-12-23 02:40:13,107 : INFO : Removed 1 and 142 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:13,108 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:13,109 : INFO : built Dictionary(123 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 605 corpus positions)\n",
      "2020-12-23 02:40:13,143 : INFO : token count processed\n",
      "2020-12-23 02:40:13,149 : INFO : frequencies processed\n",
      "2020-12-23 02:40:13,279 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:13,280 : INFO : entropies processed\n",
      "2020-12-23 02:40:13,280 : INFO : extropies processed\n",
      "2020-12-23 02:40:13,281 : INFO : token count processed\n",
      "2020-12-23 02:40:13,282 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:13,283 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:13,284 : INFO : vocab #2480\n",
      "2020-12-23 02:40:13,285 : INFO : diff #set()\n",
      "2020-12-23 02:40:13,551 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:13,679 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.115960309159703, 0.4725986568231628], [0.7778140604496002, 0.22218594], [1.584962500721156, 1.1699250014423124], [3.7004397181410926, 6.470272233491701, 6.529503487085383, 3.64120846454741, 2.82906376894429, 0.05923125359368164]]\n",
      "2020-12-23 02:40:13,682 : INFO : Removed 1 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:13,683 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:13,685 : INFO : built Dictionary(126 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 394 corpus positions)\n",
      "2020-12-23 02:40:13,726 : INFO : token count processed\n",
      "2020-12-23 02:40:13,729 : INFO : frequencies processed\n",
      "2020-12-23 02:40:13,858 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:13,858 : INFO : entropies processed\n",
      "2020-12-23 02:40:13,859 : INFO : extropies processed\n",
      "2020-12-23 02:40:13,860 : INFO : token count processed\n",
      "2020-12-23 02:40:13,860 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:13,861 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:13,862 : INFO : vocab #2480\n",
      "2020-12-23 02:40:13,863 : INFO : diff #set()\n",
      "2020-12-23 02:40:14,124 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:14,251 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.2281987358690896, 0.4487930021241838], [0.8838725239038467, 0.116127476], [0.0, 0.0], [3.7004397181410926, 6.550038223589686, 6.652326987440002, 3.5981509542907766, 2.9518872692989095, 0.10228876385031604]]\n",
      "2020-12-23 02:40:14,254 : INFO : Removed 1 and 70 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:14,254 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:14,255 : INFO : built Dictionary(79 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 310 corpus positions)\n",
      "2020-12-23 02:40:14,275 : INFO : token count processed\n",
      "2020-12-23 02:40:14,280 : INFO : frequencies processed\n",
      "2020-12-23 02:40:14,408 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:14,408 : INFO : entropies processed\n",
      "2020-12-23 02:40:14,409 : INFO : extropies processed\n",
      "2020-12-23 02:40:14,409 : INFO : token count processed\n",
      "2020-12-23 02:40:14,410 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:14,411 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:14,412 : INFO : vocab #2480\n",
      "2020-12-23 02:40:14,412 : INFO : diff #set()\n",
      "2020-12-23 02:40:14,682 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:14,810 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.1964375042771707, 0.455282701216255], [0.8905801102519035, 0.10941989], [1.584962500721156, 1.1699250014423124], [3.7004397181410926, 5.860525481261383, 5.975296554485895, 3.5856686449165798, 2.274856836344802, 0.11477107322451197]]\n",
      "2020-12-23 02:40:14,813 : INFO : Removed 1 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:14,813 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:14,815 : INFO : built Dictionary(53 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 348 corpus positions)\n",
      "2020-12-23 02:40:14,836 : INFO : token count processed\n",
      "2020-12-23 02:40:14,838 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:40:14,841 : INFO : frequencies processed\n",
      "2020-12-23 02:40:14,842 : INFO : token count processed\n",
      "2020-12-23 02:40:14,845 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:14,846 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:14,847 : INFO : vocab #2480\n",
      "2020-12-23 02:40:14,847 : INFO : diff #set()\n",
      "2020-12-23 02:40:15,100 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:15,228 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.2831520823092395, 0.4379909721075496], [0.9779806733131409, 0.022019327], [nan, nan], [3.7004397181410926, 5.945464049777852, 6.058439811986268, 3.5874639559326766, 2.3580000938451757, 0.11297576220841599]]\n",
      "2020-12-23 02:40:15,231 : INFO : Removed 1 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:15,232 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:15,233 : INFO : built Dictionary(198 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 895 corpus positions)\n",
      "2020-12-23 02:40:15,287 : INFO : token count processed\n",
      "2020-12-23 02:40:15,290 : INFO : frequencies processed\n",
      "2020-12-23 02:40:15,417 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:15,417 : INFO : entropies processed\n",
      "2020-12-23 02:40:15,418 : INFO : extropies processed\n",
      "2020-12-23 02:40:15,419 : INFO : token count processed\n",
      "2020-12-23 02:40:15,420 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:15,421 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:15,421 : INFO : vocab #2480\n",
      "2020-12-23 02:40:15,422 : INFO : diff #set()\n",
      "2020-12-23 02:40:15,682 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:15,810 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.213052495940949, 0.45186456346342496], [0.8533613085746765, 0.14663869], [2.0, 1.2451124978365313], [3.7004397181410926, 6.811563897304216, 6.857673499935295, 3.654330115510014, 3.1572337817942024, 0.04610960263107877]]\n",
      "2020-12-23 02:40:15,813 : INFO : Removed 1 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:15,814 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:15,817 : INFO : built Dictionary(217 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 990 corpus positions)\n",
      "2020-12-23 02:40:15,891 : INFO : token count processed\n",
      "2020-12-23 02:40:15,897 : INFO : frequencies processed\n",
      "2020-12-23 02:40:16,024 : INFO : scalar_distribution processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:40:16,025 : INFO : entropies processed\n",
      "2020-12-23 02:40:16,026 : INFO : extropies processed\n",
      "2020-12-23 02:40:16,027 : INFO : token count processed\n",
      "2020-12-23 02:40:16,028 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:16,029 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:16,029 : INFO : vocab #2480\n",
      "2020-12-23 02:40:16,030 : INFO : diff #set()\n",
      "2020-12-23 02:40:16,286 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:16,422 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.1778531962194627, 0.45916777206833814], [0.8085208684206009, 0.19147913], [2.584962500721156, 1.315172029168969], [3.7004397181410926, 7.502034948968415, 7.525847072110963, 3.6766275949985445, 3.8254073539698705, 0.023812123142548103]]\n",
      "2020-12-23 02:40:16,425 : INFO : Removed 1 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:16,426 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:16,428 : INFO : built Dictionary(262 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 1563 corpus positions)\n",
      "2020-12-23 02:40:16,519 : INFO : token count processed\n",
      "2020-12-23 02:40:16,522 : INFO : frequencies processed\n",
      "2020-12-23 02:40:16,652 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:16,652 : INFO : entropies processed\n",
      "2020-12-23 02:40:16,653 : INFO : extropies processed\n",
      "2020-12-23 02:40:16,655 : INFO : token count processed\n",
      "2020-12-23 02:40:16,656 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:16,657 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:16,658 : INFO : vocab #2480\n",
      "2020-12-23 02:40:16,659 : INFO : diff #set()\n",
      "2020-12-23 02:40:16,914 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:17,042 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.2162456294807524, 0.4512135237619358], [0.8605232983827591, 0.1394767], [2.0, 1.2451124978365313], [3.7004397181410926, 7.39180093901977, 7.417770662561376, 3.6744699945994865, 3.717330944420284, 0.025969723541606093]]\n",
      "2020-12-23 02:40:17,044 : INFO : Removed 1 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:17,045 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:17,047 : INFO : built Dictionary(50 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 118 corpus positions)\n",
      "2020-12-23 02:40:17,063 : INFO : token count processed\n",
      "2020-12-23 02:40:17,065 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:40:17,066 : INFO : frequencies processed\n",
      "2020-12-23 02:40:17,067 : INFO : token count processed\n",
      "2020-12-23 02:40:17,068 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:17,069 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:17,070 : INFO : vocab #2480\n",
      "2020-12-23 02:40:17,072 : INFO : diff #set()\n",
      "2020-12-23 02:40:17,333 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:17,462 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.2265556850981711, 0.44912418166442974], [0.9101010039448738, 0.089898996], [nan, nan], [3.7004397181410926, 4.927561309677364, 5.28786134646055, 3.340139681357906, 1.5874216283194578, 0.3603000367831868]]\n",
      "2020-12-23 02:40:17,464 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:17,465 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:17,466 : INFO : built Dictionary(18 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 19 corpus positions)\n",
      "2020-12-23 02:40:17,469 : INFO : token count processed\n",
      "2020-12-23 02:40:17,471 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:40:17,472 : INFO : frequencies processed\n",
      "2020-12-23 02:40:17,473 : INFO : token count processed\n",
      "2020-12-23 02:40:17,474 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:17,475 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:17,475 : INFO : vocab #2480\n",
      "2020-12-23 02:40:17,476 : INFO : diff #set()\n",
      "2020-12-23 02:40:17,848 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:17,977 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2507655172610148, 0.4442932825881004], [0.9437739178538322, 0.056226082], [nan, nan], [3.7004397181410926, 2.5216406363433186, 4.221928094887363, 2.0001522595970487, 0.5214883767462704, 1.7002874585440444]]\n",
      "2020-12-23 02:40:17,980 : INFO : Removed 1 and 530 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:17,981 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:17,984 : INFO : built Dictionary(333 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 2891 corpus positions)\n",
      "2020-12-23 02:40:18,109 : INFO : token count processed\n",
      "2020-12-23 02:40:18,115 : INFO : frequencies processed\n",
      "2020-12-23 02:40:18,241 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:18,242 : INFO : entropies processed\n",
      "2020-12-23 02:40:18,242 : INFO : extropies processed\n",
      "2020-12-23 02:40:18,245 : INFO : token count processed\n",
      "2020-12-23 02:40:18,246 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:18,247 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:18,248 : INFO : vocab #2480\n",
      "2020-12-23 02:40:18,249 : INFO : diff #set()\n",
      "2020-12-23 02:40:18,518 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:18,645 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.1481931333527635, 0.46550749300611693], [0.7652618736028671, 0.23473813], [2.807354922057604, 1.3343545280186873], [3.7004397181410926, 7.480007711014331, 7.489532540509739, 3.6909148886456844, 3.789092822368646, 0.009524829495408227]]\n",
      "2020-12-23 02:40:18,649 : INFO : Removed 1 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:18,649 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:18,651 : INFO : built Dictionary(210 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 1040 corpus positions)\n",
      "2020-12-23 02:40:18,711 : INFO : token count processed\n",
      "2020-12-23 02:40:18,717 : INFO : frequencies processed\n",
      "2020-12-23 02:40:18,844 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:18,845 : INFO : entropies processed\n",
      "2020-12-23 02:40:18,845 : INFO : extropies processed\n",
      "2020-12-23 02:40:18,847 : INFO : token count processed\n",
      "2020-12-23 02:40:18,847 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:18,848 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:18,849 : INFO : vocab #2480\n",
      "2020-12-23 02:40:18,850 : INFO : diff #set()\n",
      "2020-12-23 02:40:19,114 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:19,242 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.1293893505774424, 0.46961820285652434], [0.7477023601531982, 0.25229764], [2.807354922057604, 1.3343545280186873], [3.7004397181410926, 7.131331012509435, 7.151895971713092, 3.6798747589374354, 3.4514562535719993, 0.020564959203657196]]\n",
      "2020-12-23 02:40:19,244 : INFO : Removed 1 and 166 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:19,245 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:19,247 : INFO : built Dictionary(201 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 859 corpus positions)\n",
      "2020-12-23 02:40:19,309 : INFO : token count processed\n",
      "2020-12-23 02:40:19,312 : INFO : frequencies processed\n",
      "2020-12-23 02:40:19,439 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:19,440 : INFO : entropies processed\n",
      "2020-12-23 02:40:19,441 : INFO : extropies processed\n",
      "2020-12-23 02:40:19,442 : INFO : token count processed\n",
      "2020-12-23 02:40:19,443 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:19,444 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:19,445 : INFO : vocab #2480\n",
      "2020-12-23 02:40:19,447 : INFO : diff #set()\n",
      "2020-12-23 02:40:19,709 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:19,837 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.2037318719017174, 0.4537757123497274], [0.8317358493804932, 0.16826415], [2.321928094887362, 1.2877123795494492], [3.7004397181410926, 7.203742744794778, 7.241462986089435, 3.662719476846436, 3.5410232679483427, 0.03772024129465734]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:40:19,840 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:19,841 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:19,842 : INFO : built Dictionary(60 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 191 corpus positions)\n",
      "2020-12-23 02:40:19,861 : INFO : token count processed\n",
      "2020-12-23 02:40:19,863 : INFO : frequencies processed\n",
      "2020-12-23 02:40:19,995 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:19,995 : INFO : entropies processed\n",
      "2020-12-23 02:40:19,996 : INFO : extropies processed\n",
      "2020-12-23 02:40:19,998 : INFO : token count processed\n",
      "2020-12-23 02:40:20,000 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:20,001 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:20,002 : INFO : vocab #2480\n",
      "2020-12-23 02:40:20,004 : INFO : diff #set()\n",
      "2020-12-23 02:40:20,267 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:20,399 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.0285922540505215, 0.49295268578655194], [0.6988546848297119, 0.30114532], [2.0, 1.2451124978365313], [3.7004397181410926, 5.195502554608948, 5.3700378329405964, 3.5259044398094446, 1.6695981147995038, 0.17453527833164806]]\n",
      "2020-12-23 02:40:20,401 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:20,402 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:20,403 : INFO : built Dictionary(65 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 252 corpus positions)\n",
      "2020-12-23 02:40:20,418 : INFO : token count processed\n",
      "2020-12-23 02:40:20,420 : INFO : frequencies processed\n",
      "2020-12-23 02:40:20,547 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:20,548 : INFO : entropies processed\n",
      "2020-12-23 02:40:20,548 : INFO : extropies processed\n",
      "2020-12-23 02:40:20,550 : INFO : token count processed\n",
      "2020-12-23 02:40:20,551 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:20,552 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:20,552 : INFO : vocab #2480\n",
      "2020-12-23 02:40:20,553 : INFO : diff #set()\n",
      "2020-12-23 02:40:20,812 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:20,941 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.0643899595738056, 0.4844046035790886], [0.7238581478595734, 0.27614185], [2.321928094887362, 1.2877123795494492], [3.7004397181410926, 5.32027245610305, 5.461776242402169, 3.5589359318419733, 1.7613365242610763, 0.1415037862991193]]\n",
      "2020-12-23 02:40:20,943 : INFO : Removed 1 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:20,944 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:20,946 : INFO : built Dictionary(165 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 492 corpus positions)\n",
      "2020-12-23 02:40:21,001 : INFO : token count processed\n",
      "2020-12-23 02:40:21,004 : INFO : frequencies processed\n",
      "2020-12-23 02:40:21,130 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:21,130 : INFO : entropies processed\n",
      "2020-12-23 02:40:21,131 : INFO : extropies processed\n",
      "2020-12-23 02:40:21,132 : INFO : token count processed\n",
      "2020-12-23 02:40:21,132 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:21,133 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:21,133 : INFO : vocab #2480\n",
      "2020-12-23 02:40:21,134 : INFO : diff #set()\n",
      "2020-12-23 02:40:21,392 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:21,521 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.1890884073722017, 0.4568111532783674], [0.8100150376558304, 0.18998496], [2.321928094887362, 1.2877123795494492], [3.7004397181410926, 6.898202761357263, 6.955460475136203, 3.643182004362153, 3.25502075699511, 0.057257713778939845]]\n",
      "2020-12-23 02:40:21,523 : INFO : Removed 1 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:21,524 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:21,525 : INFO : built Dictionary(129 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 514 corpus positions)\n",
      "2020-12-23 02:40:21,563 : INFO : token count processed\n",
      "2020-12-23 02:40:21,566 : INFO : frequencies processed\n",
      "2020-12-23 02:40:21,694 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:21,695 : INFO : entropies processed\n",
      "2020-12-23 02:40:21,695 : INFO : extropies processed\n",
      "2020-12-23 02:40:21,696 : INFO : token count processed\n",
      "2020-12-23 02:40:21,697 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:21,698 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:21,699 : INFO : vocab #2480\n",
      "2020-12-23 02:40:21,700 : INFO : diff #set()\n",
      "2020-12-23 02:40:21,960 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:22,087 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.1414563863674512, 0.4669719198420372], [0.8033781051635742, 0.1966219], [2.321928094887362, 1.2877123795494492], [3.7004397181410926, 6.388500481644799, 6.4484759714065785, 3.6404642283793134, 2.748036253265486, 0.059975489761779244]]\n",
      "2020-12-23 02:40:22,090 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:22,091 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:22,092 : INFO : built Dictionary(53 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 178 corpus positions)\n",
      "2020-12-23 02:40:22,109 : INFO : token count processed\n",
      "2020-12-23 02:40:22,111 : INFO : frequencies processed\n",
      "2020-12-23 02:40:22,245 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:22,246 : INFO : entropies processed\n",
      "2020-12-23 02:40:22,247 : INFO : extropies processed\n",
      "2020-12-23 02:40:22,249 : INFO : token count processed\n",
      "2020-12-23 02:40:22,251 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:22,252 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:22,253 : INFO : vocab #2480\n",
      "2020-12-23 02:40:22,254 : INFO : diff #set()\n",
      "2020-12-23 02:40:22,513 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:22,641 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.1472151026900168, 0.46571952607226297], [0.8216089159250259, 0.17839108], [1.0, 1.0], [3.7004397181410926, 4.8191513650620195, 5.069183052231953, 3.4504080309711593, 1.3687433340908601, 0.2500316871699333]]\n",
      "2020-12-23 02:40:22,644 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:22,645 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:22,646 : INFO : built Dictionary(58 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 233 corpus positions)\n",
      "2020-12-23 02:40:22,659 : INFO : token count processed\n",
      "2020-12-23 02:40:22,662 : INFO : frequencies processed\n",
      "2020-12-23 02:40:22,789 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:22,790 : INFO : entropies processed\n",
      "2020-12-23 02:40:22,791 : INFO : extropies processed\n",
      "2020-12-23 02:40:22,792 : INFO : token count processed\n",
      "2020-12-23 02:40:22,793 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:22,793 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:22,794 : INFO : vocab #2480\n",
      "2020-12-23 02:40:22,795 : INFO : diff #set()\n",
      "2020-12-23 02:40:23,052 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:23,180 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.1796840992983568, 0.4587820777891169], [0.8393341153860092, 0.16066588], [1.0, 1.0], [3.7004397181410926, 5.062480936779194, 5.265305025745981, 3.4976156291743052, 1.5648653076048884, 0.20282408896678739]]\n",
      "2020-12-23 02:40:23,183 : INFO : Removed 1 and 223 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:23,184 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:23,185 : INFO : built Dictionary(246 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 1787 corpus positions)\n",
      "2020-12-23 02:40:23,269 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:40:23,272 : INFO : frequencies processed\n",
      "2020-12-23 02:40:23,400 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:23,400 : INFO : entropies processed\n",
      "2020-12-23 02:40:23,401 : INFO : extropies processed\n",
      "2020-12-23 02:40:23,403 : INFO : token count processed\n",
      "2020-12-23 02:40:23,404 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:23,405 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:23,406 : INFO : vocab #2480\n",
      "2020-12-23 02:40:23,407 : INFO : diff #set()\n",
      "2020-12-23 02:40:23,677 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:23,806 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.2118322014829586, 0.4521138625839401], [0.8373184502124786, 0.16268155], [2.321928094887362, 1.2877123795494492], [3.7004397181410926, 7.185085743102134, 7.210800749279268, 3.6747247119639583, 3.510361031138175, 0.025715006177133404]]\n",
      "2020-12-23 02:40:23,809 : INFO : Removed 1 and 148 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:23,810 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:23,811 : INFO : built Dictionary(164 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 762 corpus positions)\n",
      "2020-12-23 02:40:23,860 : INFO : token count processed\n",
      "2020-12-23 02:40:23,863 : INFO : frequencies processed\n",
      "2020-12-23 02:40:23,992 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:23,992 : INFO : entropies processed\n",
      "2020-12-23 02:40:23,993 : INFO : extropies processed\n",
      "2020-12-23 02:40:23,994 : INFO : token count processed\n",
      "2020-12-23 02:40:23,994 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:23,995 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:23,996 : INFO : vocab #2480\n",
      "2020-12-23 02:40:23,997 : INFO : diff #set()\n",
      "2020-12-23 02:40:24,256 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:24,385 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.1964261729024748, 0.4552850500222125], [0.8373012989759445, 0.1626987], [2.0, 1.2451124978365313], [3.7004397181410926, 6.591225336124281, 6.6425970850009515, 3.6490679692644212, 2.942157366859859, 0.05137174887667051]]\n",
      "2020-12-23 02:40:24,387 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:24,388 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:24,389 : INFO : built Dictionary(45 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 100 corpus positions)\n",
      "2020-12-23 02:40:24,398 : INFO : token count processed\n",
      "2020-12-23 02:40:24,401 : INFO : frequencies processed\n",
      "2020-12-23 02:40:24,529 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:24,530 : INFO : entropies processed\n",
      "2020-12-23 02:40:24,530 : INFO : extropies processed\n",
      "2020-12-23 02:40:24,531 : INFO : token count processed\n",
      "2020-12-23 02:40:24,532 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:24,533 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:24,534 : INFO : vocab #2480\n",
      "2020-12-23 02:40:24,535 : INFO : diff #set()\n",
      "2020-12-23 02:40:24,792 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:24,919 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.2091568835365032, 0.4526613783984239], [0.9003702476620674, 0.09962975], [0.0, 0.0], [3.7004397181410926, 4.7032114441396695, 5.098804319130886, 3.3048468431498774, 1.398364600989793, 0.3955928749912161]]\n",
      "2020-12-23 02:40:24,922 : INFO : Removed 1 and 9 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:24,923 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:24,924 : INFO : built Dictionary(123 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 340 corpus positions)\n",
      "2020-12-23 02:40:24,956 : INFO : token count processed\n",
      "2020-12-23 02:40:24,962 : INFO : frequencies processed\n",
      "2020-12-23 02:40:25,090 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:25,091 : INFO : entropies processed\n",
      "2020-12-23 02:40:25,091 : INFO : extropies processed\n",
      "2020-12-23 02:40:25,092 : INFO : token count processed\n",
      "2020-12-23 02:40:25,093 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:25,094 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:25,094 : INFO : vocab #2480\n",
      "2020-12-23 02:40:25,095 : INFO : diff #set()\n",
      "2020-12-23 02:40:25,355 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:25,482 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.2156720998224722, 0.4513303209803128], [0.8760339543223381, 0.123966046], [0.0, 0.0], [3.7004397181410926, 6.14228447828618, 6.274908420187062, 3.567815776240211, 2.5744687020459693, 0.13262394190088234]]\n",
      "2020-12-23 02:40:25,485 : INFO : Removed 1 and 184 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:25,486 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:25,488 : INFO : built Dictionary(258 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 1130 corpus positions)\n",
      "2020-12-23 02:40:25,573 : INFO : token count processed\n",
      "2020-12-23 02:40:25,576 : INFO : frequencies processed\n",
      "2020-12-23 02:40:25,703 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:25,703 : INFO : entropies processed\n",
      "2020-12-23 02:40:25,704 : INFO : extropies processed\n",
      "2020-12-23 02:40:25,706 : INFO : token count processed\n",
      "2020-12-23 02:40:25,707 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:25,708 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:25,709 : INFO : vocab #2480\n",
      "2020-12-23 02:40:25,711 : INFO : diff #set()\n",
      "2020-12-23 02:40:25,969 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:26,098 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.1956408345812295, 0.45544789669150426], [0.8245578408241272, 0.17544216], [2.0, 1.2451124978365313], [3.7004397181410926, 7.450178124335845, 7.481233931567202, 3.669383910909735, 3.780794213426109, 0.031055807231356702]]\n",
      "2020-12-23 02:40:26,100 : INFO : Removed 1 and 16 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:26,101 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:26,103 : INFO : built Dictionary(61 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 178 corpus positions)\n",
      "2020-12-23 02:40:26,122 : INFO : token count processed\n",
      "2020-12-23 02:40:26,125 : INFO : frequencies processed\n",
      "2020-12-23 02:40:26,256 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:26,257 : INFO : entropies processed\n",
      "2020-12-23 02:40:26,258 : INFO : extropies processed\n",
      "2020-12-23 02:40:26,259 : INFO : token count processed\n",
      "2020-12-23 02:40:26,260 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:26,261 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:26,262 : INFO : vocab #2480\n",
      "2020-12-23 02:40:26,263 : INFO : diff #set()\n",
      "2020-12-23 02:40:26,529 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:26,657 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.119295550014533, 0.4718549048022786], [0.7916130572557449, 0.20838694], [1.584962500721156, 1.1699250014423124], [3.7004397181410926, 5.20665021947654, 5.406860039536353, 3.5002298980812796, 1.7064203213952602, 0.20020982005981303]]\n",
      "2020-12-23 02:40:26,660 : INFO : Removed 1 and 89 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:26,661 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:26,666 : INFO : built Dictionary(130 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 516 corpus positions)\n",
      "2020-12-23 02:40:26,702 : INFO : token count processed\n",
      "2020-12-23 02:40:26,704 : INFO : frequencies processed\n",
      "2020-12-23 02:40:26,832 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:26,833 : INFO : entropies processed\n",
      "2020-12-23 02:40:26,833 : INFO : extropies processed\n",
      "2020-12-23 02:40:26,835 : INFO : token count processed\n",
      "2020-12-23 02:40:26,836 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:26,836 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:26,837 : INFO : vocab #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:40:26,838 : INFO : diff #set()\n",
      "2020-12-23 02:40:27,096 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:27,223 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.2034652238039258, 0.4538306251430925], [0.8549590408802032, 0.14504096], [1.584962500721156, 1.1699250014423124], [3.7004397181410926, 6.524718477352, 6.5977922308813985, 3.627365964611694, 2.897352512740306, 0.07307375352939882]]\n",
      "2020-12-23 02:40:27,226 : INFO : Removed 1 and 8 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:27,227 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:27,228 : INFO : built Dictionary(67 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 223 corpus positions)\n",
      "2020-12-23 02:40:27,243 : INFO : token count processed\n",
      "2020-12-23 02:40:27,245 : INFO : frequencies processed\n",
      "2020-12-23 02:40:27,383 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:27,384 : INFO : entropies processed\n",
      "2020-12-23 02:40:27,385 : INFO : extropies processed\n",
      "2020-12-23 02:40:27,386 : INFO : token count processed\n",
      "2020-12-23 02:40:27,387 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:27,388 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:27,389 : INFO : vocab #2480\n",
      "2020-12-23 02:40:27,390 : INFO : diff #set()\n",
      "2020-12-23 02:40:27,658 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:27,785 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.159690156863753, 0.4630293826278185], [0.8192497789859772, 0.18075022], [1.584962500721156, 1.1699250014423124], [3.7004397181410926, 5.321859380715434, 5.500259310921724, 3.522039787934803, 1.799819592780631, 0.17839993020628953]]\n",
      "2020-12-23 02:40:27,788 : INFO : Removed 1 and 119 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:27,789 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:27,790 : INFO : built Dictionary(143 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 841 corpus positions)\n",
      "2020-12-23 02:40:27,832 : INFO : token count processed\n",
      "2020-12-23 02:40:27,840 : INFO : frequencies processed\n",
      "2020-12-23 02:40:27,971 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:27,972 : INFO : entropies processed\n",
      "2020-12-23 02:40:27,972 : INFO : extropies processed\n",
      "2020-12-23 02:40:27,974 : INFO : token count processed\n",
      "2020-12-23 02:40:27,974 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:27,975 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:27,976 : INFO : vocab #2480\n",
      "2020-12-23 02:40:27,976 : INFO : diff #set()\n",
      "2020-12-23 02:40:28,243 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:28,372 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.1051346993506026, 0.4750289852276353], [0.7314820289611816, 0.26851797], [2.584962500721156, 1.315172029168969], [3.7004397181410926, 6.500767808767801, 6.534044638532466, 3.667162888376428, 2.833604920391373, 0.03327682976466484]]\n",
      "2020-12-23 02:40:28,375 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:28,375 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:28,376 : INFO : built Dictionary(42 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 63 corpus positions)\n",
      "2020-12-23 02:40:28,385 : INFO : token count processed\n",
      "2020-12-23 02:40:28,387 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:40:28,388 : INFO : frequencies processed\n",
      "2020-12-23 02:40:28,389 : INFO : token count processed\n",
      "2020-12-23 02:40:28,390 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:28,391 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:28,391 : INFO : vocab #2480\n",
      "2020-12-23 02:40:28,392 : INFO : diff #set()\n",
      "2020-12-23 02:40:28,652 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:28,780 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.2312754859542578, 0.4481741525396297], [0.9431033656001091, 0.056896634], [nan, nan], [3.7004397181410926, 4.736228843383063, 5.25099911322203, 3.185669448302124, 1.5505593950809375, 0.5147702698389676]]\n",
      "2020-12-23 02:40:28,782 : INFO : Removed 1 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:28,783 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:28,785 : INFO : built Dictionary(103 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 404 corpus positions)\n",
      "2020-12-23 02:40:28,820 : INFO : token count processed\n",
      "2020-12-23 02:40:28,823 : INFO : frequencies processed\n",
      "2020-12-23 02:40:28,954 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:28,955 : INFO : entropies processed\n",
      "2020-12-23 02:40:28,955 : INFO : extropies processed\n",
      "2020-12-23 02:40:28,957 : INFO : token count processed\n",
      "2020-12-23 02:40:28,958 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:28,959 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:28,960 : INFO : vocab #2480\n",
      "2020-12-23 02:40:28,961 : INFO : diff #set()\n",
      "2020-12-23 02:40:29,221 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:29,349 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.221084013035987, 0.45023060547498417], [0.8815841749310493, 0.118415825], [0.0, 0.0], [3.7004397181410926, 5.788442787590127, 5.915427619586161, 3.5734548861450577, 2.2149879014450686, 0.12698483199603405]]\n",
      "2020-12-23 02:40:29,352 : INFO : Removed 1 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:29,353 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:29,354 : INFO : built Dictionary(60 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 90 corpus positions)\n",
      "2020-12-23 02:40:29,367 : INFO : token count processed\n",
      "2020-12-23 02:40:29,369 : INFO : frequencies processed\n",
      "2020-12-23 02:40:29,497 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:29,498 : INFO : entropies processed\n",
      "2020-12-23 02:40:29,499 : INFO : extropies processed\n",
      "2020-12-23 02:40:29,500 : INFO : token count processed\n",
      "2020-12-23 02:40:29,501 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:29,502 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:29,503 : INFO : vocab #2480\n",
      "2020-12-23 02:40:29,504 : INFO : diff #set()\n",
      "2020-12-23 02:40:29,763 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:29,892 : INFO : Computed distances or similarities ('295', 'sacp-python-common/setup.py')[[1.2163106844529366, 0.4512002793718585], [0.9048701822757721, 0.09512982], [0.0, 0.0], [3.7004397181410926, 5.370004292053436, 5.693992192899218, 3.37645181729531, 1.9935524747581255, 0.3239879008457818]]\n",
      "2020-12-23 02:40:29,895 : INFO : Removed 1 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:29,896 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:29,897 : INFO : built Dictionary(86 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 335 corpus positions)\n",
      "2020-12-23 02:40:29,921 : INFO : token count processed\n",
      "2020-12-23 02:40:29,925 : INFO : frequencies processed\n",
      "2020-12-23 02:40:30,061 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:30,061 : INFO : entropies processed\n",
      "2020-12-23 02:40:30,062 : INFO : extropies processed\n",
      "2020-12-23 02:40:30,064 : INFO : token count processed\n",
      "2020-12-23 02:40:30,064 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:30,065 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:30,066 : INFO : vocab #2480\n",
      "2020-12-23 02:40:30,067 : INFO : diff #set()\n",
      "2020-12-23 02:40:30,334 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:30,461 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.1744018964166008, 0.45989658197410194], [0.8578390330076218, 0.14216097], [0.0, 0.0], [3.7004397181410926, 5.695663584743922, 5.831387677790055, 3.5647156250949603, 2.130947959648962, 0.1357240930461323]]\n",
      "2020-12-23 02:40:30,464 : INFO : Removed 1 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:30,464 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:40:30,465 : INFO : built Dictionary(46 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 110 corpus positions)\n",
      "2020-12-23 02:40:30,475 : INFO : token count processed\n",
      "2020-12-23 02:40:30,477 : INFO : frequencies processed\n",
      "2020-12-23 02:40:30,605 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:30,605 : INFO : entropies processed\n",
      "2020-12-23 02:40:30,606 : INFO : extropies processed\n",
      "2020-12-23 02:40:30,607 : INFO : token count processed\n",
      "2020-12-23 02:40:30,608 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:30,609 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:30,610 : INFO : vocab #2480\n",
      "2020-12-23 02:40:30,611 : INFO : diff #set()\n",
      "2020-12-23 02:40:30,871 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:31,000 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.161630667536713, 0.4626137179759531], [0.8687168508768082, 0.13128315], [1.0, 1.0], [3.7004397181410926, 4.9004417692112465, 5.22251214701885, 3.3783693403334896, 1.522072428877757, 0.32207037780760306]]\n",
      "2020-12-23 02:40:31,002 : INFO : Removed 1 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:31,003 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:31,004 : INFO : built Dictionary(44 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 117 corpus positions)\n",
      "2020-12-23 02:40:31,013 : INFO : token count processed\n",
      "2020-12-23 02:40:31,016 : INFO : frequencies processed\n",
      "2020-12-23 02:40:31,144 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:31,145 : INFO : entropies processed\n",
      "2020-12-23 02:40:31,146 : INFO : extropies processed\n",
      "2020-12-23 02:40:31,147 : INFO : token count processed\n",
      "2020-12-23 02:40:31,148 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:31,149 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:31,149 : INFO : vocab #2480\n",
      "2020-12-23 02:40:31,150 : INFO : diff #set()\n",
      "2020-12-23 02:40:31,407 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:31,535 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.1747117548775954, 0.4598310547396133], [0.8762188479304314, 0.12378115], [1.0, 1.0], [3.7004397181410926, 4.778624108914332, 5.106164220739084, 3.3728996063163414, 1.405724502597991, 0.3275401118247512]]\n",
      "2020-12-23 02:40:31,538 : INFO : Removed 1 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:31,539 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:31,540 : INFO : built Dictionary(45 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 148 corpus positions)\n",
      "2020-12-23 02:40:31,549 : INFO : token count processed\n",
      "2020-12-23 02:40:31,552 : INFO : frequencies processed\n",
      "2020-12-23 02:40:31,680 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:31,681 : INFO : entropies processed\n",
      "2020-12-23 02:40:31,681 : INFO : extropies processed\n",
      "2020-12-23 02:40:31,682 : INFO : token count processed\n",
      "2020-12-23 02:40:31,683 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:31,684 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:31,685 : INFO : vocab #2480\n",
      "2020-12-23 02:40:31,686 : INFO : diff #set()\n",
      "2020-12-23 02:40:31,944 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:32,073 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.1719620030313718, 0.46041321100659977], [0.891217015683651, 0.108782984], [1.0, 1.0], [3.7004397181410926, 4.773880192225086, 5.056389022169278, 3.4179308881969, 1.3559493040281856, 0.28250882994419246]]\n",
      "2020-12-23 02:40:32,076 : INFO : Removed 1 and 346 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:32,077 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:32,080 : INFO : built Dictionary(157 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 1973 corpus positions)\n",
      "2020-12-23 02:40:32,132 : INFO : token count processed\n",
      "2020-12-23 02:40:32,136 : INFO : frequencies processed\n",
      "2020-12-23 02:40:32,262 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:32,262 : INFO : entropies processed\n",
      "2020-12-23 02:40:32,263 : INFO : extropies processed\n",
      "2020-12-23 02:40:32,264 : INFO : token count processed\n",
      "2020-12-23 02:40:32,265 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:32,266 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:32,266 : INFO : vocab #2480\n",
      "2020-12-23 02:40:32,267 : INFO : diff #set()\n",
      "2020-12-23 02:40:32,527 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:32,654 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.212919178213824, 0.4518917861280222], [0.8879570215940475, 0.11204298], [0.0, 0.0], [3.7004397181410926, 6.620773041953877, 6.651104337073959, 3.67010842302101, 2.9506646189328665, 0.03033129512008248]]\n",
      "2020-12-23 02:40:32,657 : INFO : Removed 1 and 132 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:32,658 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:32,659 : INFO : built Dictionary(87 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 536 corpus positions)\n",
      "2020-12-23 02:40:32,682 : INFO : token count processed\n",
      "2020-12-23 02:40:32,687 : INFO : frequencies processed\n",
      "2020-12-23 02:40:32,817 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:32,818 : INFO : entropies processed\n",
      "2020-12-23 02:40:32,818 : INFO : extropies processed\n",
      "2020-12-23 02:40:32,820 : INFO : token count processed\n",
      "2020-12-23 02:40:32,821 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:32,822 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:32,822 : INFO : vocab #2480\n",
      "2020-12-23 02:40:32,824 : INFO : diff #set()\n",
      "2020-12-23 02:40:33,089 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:33,217 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.1465012189536827, 0.4658744151505549], [0.8353558033704758, 0.1646442], [0.0, 0.0], [3.7004397181410926, 5.828370634755606, 5.915467561149163, 3.6133427917475354, 2.2150278430080705, 0.08709692639355726]]\n",
      "2020-12-23 02:40:33,220 : INFO : Removed 1 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:33,221 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:33,222 : INFO : built Dictionary(86 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 299 corpus positions)\n",
      "2020-12-23 02:40:33,248 : INFO : token count processed\n",
      "2020-12-23 02:40:33,250 : INFO : frequencies processed\n",
      "2020-12-23 02:40:33,378 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:33,379 : INFO : entropies processed\n",
      "2020-12-23 02:40:33,380 : INFO : extropies processed\n",
      "2020-12-23 02:40:33,382 : INFO : token count processed\n",
      "2020-12-23 02:40:33,384 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:33,385 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:33,386 : INFO : vocab #2480\n",
      "2020-12-23 02:40:33,388 : INFO : diff #set()\n",
      "2020-12-23 02:40:33,648 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:33,776 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.153092628429755, 0.46444820199365855], [0.8216769248247147, 0.17832308], [1.584962500721156, 1.1699250014423124], [3.7004397181410926, 5.774409284925443, 5.898755645950303, 3.576093357116231, 2.198315927809211, 0.12434636102486074]]\n",
      "2020-12-23 02:40:33,779 : INFO : Removed 1 and 51 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:33,780 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:33,781 : INFO : built Dictionary(99 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 348 corpus positions)\n",
      "2020-12-23 02:40:33,814 : INFO : token count processed\n",
      "2020-12-23 02:40:33,818 : INFO : frequencies processed\n",
      "2020-12-23 02:40:33,947 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:33,948 : INFO : entropies processed\n",
      "2020-12-23 02:40:33,948 : INFO : extropies processed\n",
      "2020-12-23 02:40:33,949 : INFO : token count processed\n",
      "2020-12-23 02:40:33,950 : INFO : alphabet_source #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:40:33,951 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:33,951 : INFO : vocab #2480\n",
      "2020-12-23 02:40:33,952 : INFO : diff #set()\n",
      "2020-12-23 02:40:34,210 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:34,339 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.178362049544595, 0.4590605130166762], [0.8608332723379135, 0.13916673], [0.0, 0.0], [3.7004397181410926, 5.977819040873918, 6.097090920085843, 3.5811678389291677, 2.39665120194475, 0.11927187921192495]]\n",
      "2020-12-23 02:40:34,341 : INFO : Removed 1 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:34,342 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:34,343 : INFO : built Dictionary(80 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 295 corpus positions)\n",
      "2020-12-23 02:40:34,361 : INFO : token count processed\n",
      "2020-12-23 02:40:34,363 : INFO : frequencies processed\n",
      "2020-12-23 02:40:34,491 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:34,492 : INFO : entropies processed\n",
      "2020-12-23 02:40:34,492 : INFO : extropies processed\n",
      "2020-12-23 02:40:34,494 : INFO : token count processed\n",
      "2020-12-23 02:40:34,494 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:34,495 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:34,496 : INFO : vocab #2480\n",
      "2020-12-23 02:40:34,497 : INFO : diff #set()\n",
      "2020-12-23 02:40:34,755 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:34,883 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.1718245055833592, 0.46044235960556895], [0.8309000581502914, 0.16909994], [0.0, 0.0], [3.7004397181410926, 5.901812829596593, 6.0367295209422, 3.5655230267954847, 2.336289802801107, 0.13491669134560702]]\n",
      "2020-12-23 02:40:34,885 : INFO : Removed 1 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:34,886 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:34,887 : INFO : built Dictionary(82 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 295 corpus positions)\n",
      "2020-12-23 02:40:34,906 : INFO : token count processed\n",
      "2020-12-23 02:40:34,908 : INFO : frequencies processed\n",
      "2020-12-23 02:40:35,036 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:35,037 : INFO : entropies processed\n",
      "2020-12-23 02:40:35,037 : INFO : extropies processed\n",
      "2020-12-23 02:40:35,039 : INFO : token count processed\n",
      "2020-12-23 02:40:35,039 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:35,040 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:35,041 : INFO : vocab #2480\n",
      "2020-12-23 02:40:35,043 : INFO : diff #set()\n",
      "2020-12-23 02:40:35,315 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:35,445 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.2009452494593023, 0.45435023894650095], [0.897495836019516, 0.102504164], [0.0, 0.0], [3.7004397181410926, 5.643202320803383, 5.79614412927, 3.547497909674475, 2.0957044111289074, 0.15294180846661742]]\n",
      "2020-12-23 02:40:35,447 : INFO : Removed 1 and 30 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:35,448 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:35,449 : INFO : built Dictionary(94 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 329 corpus positions)\n",
      "2020-12-23 02:40:35,480 : INFO : token count processed\n",
      "2020-12-23 02:40:35,485 : INFO : frequencies processed\n",
      "2020-12-23 02:40:35,613 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:35,614 : INFO : entropies processed\n",
      "2020-12-23 02:40:35,614 : INFO : extropies processed\n",
      "2020-12-23 02:40:35,616 : INFO : token count processed\n",
      "2020-12-23 02:40:35,616 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:35,617 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:35,618 : INFO : vocab #2480\n",
      "2020-12-23 02:40:35,619 : INFO : diff #set()\n",
      "2020-12-23 02:40:35,880 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:36,007 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.2071079896028005, 0.45308159125461], [0.8969336003065109, 0.1030664], [1.0, 1.0], [3.7004397181410926, 5.925214310725336, 6.051983328349005, 3.5736707005174235, 2.351543610207912, 0.1267690176236691]]\n",
      "2020-12-23 02:40:36,010 : INFO : Removed 1 and 293 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:36,011 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:36,012 : INFO : built Dictionary(167 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 1724 corpus positions)\n",
      "2020-12-23 02:40:36,067 : INFO : token count processed\n",
      "2020-12-23 02:40:36,069 : INFO : frequencies processed\n",
      "2020-12-23 02:40:36,197 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:36,197 : INFO : entropies processed\n",
      "2020-12-23 02:40:36,198 : INFO : extropies processed\n",
      "2020-12-23 02:40:36,200 : INFO : token count processed\n",
      "2020-12-23 02:40:36,200 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:36,201 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:36,201 : INFO : vocab #2480\n",
      "2020-12-23 02:40:36,202 : INFO : diff #set()\n",
      "2020-12-23 02:40:36,460 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:36,588 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.2084698811089098, 0.45280219058178106], [0.8887243494391441, 0.11127565], [1.0, 1.0], [3.7004397181410926, 6.551685682764175, 6.584740738758599, 3.6673846621466684, 2.8843010206175066, 0.033055055994424265]]\n",
      "2020-12-23 02:40:36,591 : INFO : Removed 1 and 88 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:36,592 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:36,594 : INFO : built Dictionary(144 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 565 corpus positions)\n",
      "2020-12-23 02:40:36,641 : INFO : token count processed\n",
      "2020-12-23 02:40:36,644 : INFO : frequencies processed\n",
      "2020-12-23 02:40:36,771 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:36,772 : INFO : entropies processed\n",
      "2020-12-23 02:40:36,772 : INFO : extropies processed\n",
      "2020-12-23 02:40:36,773 : INFO : token count processed\n",
      "2020-12-23 02:40:36,774 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:36,775 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:36,776 : INFO : vocab #2480\n",
      "2020-12-23 02:40:36,777 : INFO : diff #set()\n",
      "2020-12-23 02:40:37,036 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:37,164 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.187171993314336, 0.4572114141259864], [0.8447419106960297, 0.15525809], [0.0, 0.0], [3.7004397181410926, 6.642985062562557, 6.716488959408888, 3.6269358212947616, 3.0160492412677957, 0.07350389684633107]]\n",
      "2020-12-23 02:40:37,166 : INFO : Removed 1 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:37,167 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:37,168 : INFO : built Dictionary(57 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 160 corpus positions)\n",
      "2020-12-23 02:40:37,180 : INFO : token count processed\n",
      "2020-12-23 02:40:37,183 : INFO : frequencies processed\n",
      "2020-12-23 02:40:37,310 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:37,311 : INFO : entropies processed\n",
      "2020-12-23 02:40:37,311 : INFO : extropies processed\n",
      "2020-12-23 02:40:37,312 : INFO : token count processed\n",
      "2020-12-23 02:40:37,313 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:37,314 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:37,315 : INFO : vocab #2480\n",
      "2020-12-23 02:40:37,316 : INFO : diff #set()\n",
      "2020-12-23 02:40:37,573 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:37,701 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.1652454030573414, 0.46184141464426764], [0.8468579351902008, 0.15314206], [0.0, 0.0], [3.7004397181410926, 5.2461980344571995, 5.488238915084674, 3.4583988375136183, 1.7877991969435811, 0.2420408806274743]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:40:37,703 : INFO : Removed 1 and 23 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:37,704 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:37,705 : INFO : built Dictionary(79 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 220 corpus positions)\n",
      "2020-12-23 02:40:37,723 : INFO : token count processed\n",
      "2020-12-23 02:40:37,725 : INFO : frequencies processed\n",
      "2020-12-23 02:40:37,853 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:37,854 : INFO : entropies processed\n",
      "2020-12-23 02:40:37,855 : INFO : extropies processed\n",
      "2020-12-23 02:40:37,856 : INFO : token count processed\n",
      "2020-12-23 02:40:37,857 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:37,858 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:37,859 : INFO : vocab #2480\n",
      "2020-12-23 02:40:37,860 : INFO : diff #set()\n",
      "2020-12-23 02:40:38,119 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:38,245 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/test_auth_utility.py')[[1.2042117666586083, 0.4536769175839725], [0.8829321563243866, 0.11706784], [0.0, 0.0], [3.7004397181410926, 5.903090303960449, 6.071106973519105, 3.5324230485824364, 2.370667255378012, 0.16801666955865535]]\n",
      "2020-12-23 02:40:38,249 : INFO : Removed 1 and 442 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:38,249 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:38,251 : INFO : built Dictionary(111 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 1219 corpus positions)\n",
      "2020-12-23 02:40:38,284 : INFO : token count processed\n",
      "2020-12-23 02:40:38,288 : INFO : frequencies processed\n",
      "2020-12-23 02:40:38,415 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:38,416 : INFO : entropies processed\n",
      "2020-12-23 02:40:38,417 : INFO : extropies processed\n",
      "2020-12-23 02:40:38,419 : INFO : token count processed\n",
      "2020-12-23 02:40:38,420 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:38,421 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:38,422 : INFO : vocab #2480\n",
      "2020-12-23 02:40:38,423 : INFO : diff #set()\n",
      "2020-12-23 02:40:38,682 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:38,810 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.2201541823332216, 0.4504191681629392], [0.9122363105416298, 0.08776369], [1.584962500721156, 1.1699250014423124], [3.7004397181410926, 6.16659449033757, 6.2053196477761325, 3.6617145607025297, 2.50487992963504, 0.038725157438562974]]\n",
      "2020-12-23 02:40:38,813 : INFO : Removed 1 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:38,813 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:38,815 : INFO : built Dictionary(69 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 254 corpus positions)\n",
      "2020-12-23 02:40:38,839 : INFO : token count processed\n",
      "2020-12-23 02:40:38,844 : INFO : frequencies processed\n",
      "2020-12-23 02:40:38,973 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:38,974 : INFO : entropies processed\n",
      "2020-12-23 02:40:38,974 : INFO : extropies processed\n",
      "2020-12-23 02:40:38,975 : INFO : token count processed\n",
      "2020-12-23 02:40:38,976 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:38,976 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:38,977 : INFO : vocab #2480\n",
      "2020-12-23 02:40:38,978 : INFO : diff #set()\n",
      "2020-12-23 02:40:39,242 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:39,370 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.0985504040501448, 0.47651940981261515], [0.7618568390607834, 0.23814316], [2.321928094887362, 1.2877123795494492], [3.7004397181410926, 5.906856253399655, 6.000969775758306, 3.606326195782441, 2.3005300576172134, 0.09411352235865067]]\n",
      "2020-12-23 02:40:39,373 : INFO : Removed 1 and 68 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:39,373 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:39,374 : INFO : built Dictionary(85 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 253 corpus positions)\n",
      "2020-12-23 02:40:39,395 : INFO : token count processed\n",
      "2020-12-23 02:40:39,397 : INFO : frequencies processed\n",
      "2020-12-23 02:40:39,523 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:39,523 : INFO : entropies processed\n",
      "2020-12-23 02:40:39,524 : INFO : extropies processed\n",
      "2020-12-23 02:40:39,525 : INFO : token count processed\n",
      "2020-12-23 02:40:39,526 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:39,526 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:39,527 : INFO : vocab #2480\n",
      "2020-12-23 02:40:39,528 : INFO : diff #set()\n",
      "2020-12-23 02:40:39,792 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:39,919 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.1294582800586375, 0.46960300155420925], [0.8127002418041229, 0.18729976], [1.0, 1.0], [3.7004397181410926, 5.965115449163356, 6.091574449348764, 3.573980717955685, 2.3911347312076714, 0.1264590001854078]]\n",
      "2020-12-23 02:40:39,921 : INFO : Removed 1 and 13 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:39,922 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:39,923 : INFO : built Dictionary(88 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 367 corpus positions)\n",
      "2020-12-23 02:40:39,950 : INFO : token count processed\n",
      "2020-12-23 02:40:39,953 : INFO : frequencies processed\n",
      "2020-12-23 02:40:40,079 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:40,080 : INFO : entropies processed\n",
      "2020-12-23 02:40:40,081 : INFO : extropies processed\n",
      "2020-12-23 02:40:40,083 : INFO : token count processed\n",
      "2020-12-23 02:40:40,084 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:40,085 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:40,086 : INFO : vocab #2480\n",
      "2020-12-23 02:40:40,087 : INFO : diff #set()\n",
      "2020-12-23 02:40:40,348 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:40,476 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.0269200593977528, 0.493359368251121], [0.6517134606838226, 0.34828654], [2.584962500721156, 1.315172029168969], [3.7004397181410926, 5.791362404253194, 5.8683043954419825, 3.623497726952304, 2.16786467730089, 0.07694199118878853]]\n",
      "2020-12-23 02:40:40,478 : INFO : Removed 1 and 41 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:40,479 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:40,480 : INFO : built Dictionary(78 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 379 corpus positions)\n",
      "2020-12-23 02:40:40,498 : INFO : token count processed\n",
      "2020-12-23 02:40:40,501 : INFO : frequencies processed\n",
      "2020-12-23 02:40:40,628 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:40,629 : INFO : entropies processed\n",
      "2020-12-23 02:40:40,629 : INFO : extropies processed\n",
      "2020-12-23 02:40:40,631 : INFO : token count processed\n",
      "2020-12-23 02:40:40,632 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:40,633 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:40,633 : INFO : vocab #2480\n",
      "2020-12-23 02:40:40,634 : INFO : diff #set()\n",
      "2020-12-23 02:40:40,893 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:41,021 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.0225123724927019, 0.49443455258942226], [0.6793330907821655, 0.3206669], [2.584962500721156, 1.315172029168969], [3.7004397181410926, 5.651670454631116, 5.738021303327633, 3.6140888694445756, 2.0375815851865404, 0.08635084869651699]]\n",
      "2020-12-23 02:40:41,023 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:41,024 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:41,026 : INFO : built Dictionary(45 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 89 corpus positions)\n",
      "2020-12-23 02:40:41,042 : INFO : token count processed\n",
      "2020-12-23 02:40:41,044 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:40:41,176 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:41,177 : INFO : entropies processed\n",
      "2020-12-23 02:40:41,178 : INFO : extropies processed\n",
      "2020-12-23 02:40:41,179 : INFO : token count processed\n",
      "2020-12-23 02:40:41,180 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:41,181 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:41,182 : INFO : vocab #2480\n",
      "2020-12-23 02:40:41,183 : INFO : diff #set()\n",
      "2020-12-23 02:40:41,457 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:41,587 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.1549787606567141, 0.46404169649229265], [0.8261016607284546, 0.17389834], [1.0, 1.0], [3.7004397181410926, 4.8226207261920235, 5.19233753137766, 3.330722912955455, 1.4918978132365677, 0.36971680518563677]]\n",
      "2020-12-23 02:40:41,589 : INFO : Removed 1 and 50 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:41,590 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:41,591 : INFO : built Dictionary(86 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 259 corpus positions)\n",
      "2020-12-23 02:40:41,622 : INFO : token count processed\n",
      "2020-12-23 02:40:41,629 : INFO : frequencies processed\n",
      "2020-12-23 02:40:41,765 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:41,766 : INFO : entropies processed\n",
      "2020-12-23 02:40:41,769 : INFO : extropies processed\n",
      "2020-12-23 02:40:41,770 : INFO : token count processed\n",
      "2020-12-23 02:40:41,770 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:41,771 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:41,771 : INFO : vocab #2480\n",
      "2020-12-23 02:40:41,772 : INFO : diff #set()\n",
      "2020-12-23 02:40:42,036 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:42,164 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.1235061797676669, 0.4709192794105314], [0.7767381519079208, 0.22326185], [2.584962500721156, 1.315172029168969], [3.7004397181410926, 6.24862851613934, 6.328298205224409, 3.6207700290560227, 2.6278584870833166, 0.079669689085069]]\n",
      "2020-12-23 02:40:42,166 : INFO : Removed 1 and 32 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:42,167 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:42,168 : INFO : built Dictionary(88 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 375 corpus positions)\n",
      "2020-12-23 02:40:42,192 : INFO : token count processed\n",
      "2020-12-23 02:40:42,196 : INFO : frequencies processed\n",
      "2020-12-23 02:40:42,325 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:42,326 : INFO : entropies processed\n",
      "2020-12-23 02:40:42,327 : INFO : extropies processed\n",
      "2020-12-23 02:40:42,328 : INFO : token count processed\n",
      "2020-12-23 02:40:42,329 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:42,330 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:42,330 : INFO : vocab #2480\n",
      "2020-12-23 02:40:42,331 : INFO : diff #set()\n",
      "2020-12-23 02:40:42,590 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:42,718 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.143270055613639, 0.46657676076834387], [0.8055054545402527, 0.19449455], [2.321928094887362, 1.2877123795494492], [3.7004397181410926, 5.850156917433494, 5.937835646256872, 3.6127609893177137, 2.237395928115779, 0.08767872882337802]]\n",
      "2020-12-23 02:40:42,720 : INFO : Removed 1 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:42,721 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:42,722 : INFO : built Dictionary(83 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 373 corpus positions)\n",
      "2020-12-23 02:40:42,742 : INFO : token count processed\n",
      "2020-12-23 02:40:42,745 : INFO : frequencies processed\n",
      "2020-12-23 02:40:42,872 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:42,873 : INFO : entropies processed\n",
      "2020-12-23 02:40:42,874 : INFO : extropies processed\n",
      "2020-12-23 02:40:42,875 : INFO : token count processed\n",
      "2020-12-23 02:40:42,876 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:42,877 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:42,877 : INFO : vocab #2480\n",
      "2020-12-23 02:40:42,878 : INFO : diff #set()\n",
      "2020-12-23 02:40:43,136 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:43,263 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.1108547429062383, 0.4737417405724439], [0.7659479528665543, 0.23405205], [2.321928094887362, 1.2877123795494492], [3.7004397181410926, 5.6831976040360095, 5.782046648588882, 3.601590673588219, 2.0816069304477898, 0.09884904455287291]]\n",
      "2020-12-23 02:40:43,266 : INFO : Removed 1 and 52 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:43,267 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:43,268 : INFO : built Dictionary(72 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 351 corpus positions)\n",
      "2020-12-23 02:40:43,284 : INFO : token count processed\n",
      "2020-12-23 02:40:43,286 : INFO : frequencies processed\n",
      "2020-12-23 02:40:43,414 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:43,414 : INFO : entropies processed\n",
      "2020-12-23 02:40:43,415 : INFO : extropies processed\n",
      "2020-12-23 02:40:43,418 : INFO : token count processed\n",
      "2020-12-23 02:40:43,419 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:43,421 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:43,422 : INFO : vocab #2480\n",
      "2020-12-23 02:40:43,423 : INFO : diff #set()\n",
      "2020-12-23 02:40:43,693 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:43,825 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.1290698851067482, 0.4696886687445967], [0.8046217262744904, 0.19537827], [1.584962500721156, 1.1699250014423124], [3.7004397181410926, 5.749308601266266, 5.854745810861914, 3.595002508545445, 2.1543060927208213, 0.10543720959564773]]\n",
      "2020-12-23 02:40:43,828 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:43,828 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:43,829 : INFO : built Dictionary(66 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 232 corpus positions)\n",
      "2020-12-23 02:40:43,844 : INFO : token count processed\n",
      "2020-12-23 02:40:43,846 : INFO : frequencies processed\n",
      "2020-12-23 02:40:43,974 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:43,975 : INFO : entropies processed\n",
      "2020-12-23 02:40:43,976 : INFO : extropies processed\n",
      "2020-12-23 02:40:43,978 : INFO : token count processed\n",
      "2020-12-23 02:40:43,979 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:43,981 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:43,981 : INFO : vocab #2480\n",
      "2020-12-23 02:40:43,982 : INFO : diff #set()\n",
      "2020-12-23 02:40:44,248 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:44,375 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.1509230864534776, 0.4649166705671644], [0.8452837765216827, 0.15471622], [1.0, 1.0], [3.7004397181410926, 5.015422548793484, 5.2207131282694395, 3.495149138665136, 1.5202734101283468, 0.20529057947595586]]\n",
      "2020-12-23 02:40:44,378 : INFO : Removed 1 and 81 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:44,379 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:44,380 : INFO : built Dictionary(92 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 358 corpus positions)\n",
      "2020-12-23 02:40:44,409 : INFO : token count processed\n",
      "2020-12-23 02:40:44,411 : INFO : frequencies processed\n",
      "2020-12-23 02:40:44,538 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:44,539 : INFO : entropies processed\n",
      "2020-12-23 02:40:44,540 : INFO : extropies processed\n",
      "2020-12-23 02:40:44,541 : INFO : token count processed\n",
      "2020-12-23 02:40:44,542 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:44,543 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:44,544 : INFO : vocab #2480\n",
      "2020-12-23 02:40:44,545 : INFO : diff #set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:40:44,802 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:44,930 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.0845757140440502, 0.4797139260823551], [0.7504666894674301, 0.24953331], [2.321928094887362, 1.2877123795494492], [3.7004397181410926, 6.030001281822029, 6.106123697292933, 3.624317302670189, 2.4056839791518403, 0.0761224154709037]]\n",
      "2020-12-23 02:40:44,933 : INFO : Removed 1 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:44,934 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:44,935 : INFO : built Dictionary(78 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 321 corpus positions)\n",
      "2020-12-23 02:40:44,954 : INFO : token count processed\n",
      "2020-12-23 02:40:44,956 : INFO : frequencies processed\n",
      "2020-12-23 02:40:45,083 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:45,084 : INFO : entropies processed\n",
      "2020-12-23 02:40:45,085 : INFO : extropies processed\n",
      "2020-12-23 02:40:45,086 : INFO : token count processed\n",
      "2020-12-23 02:40:45,087 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:45,088 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:45,089 : INFO : vocab #2480\n",
      "2020-12-23 02:40:45,090 : INFO : diff #set()\n",
      "2020-12-23 02:40:45,358 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:45,486 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[0.9945097495697526, 0.501376340835494], [0.6342703104019165, 0.3657297], [2.807354922057604, 1.3343545280186873], [3.7004397181410926, 5.9537092545441395, 6.0207562042636615, 3.6333927684215697, 2.320316486122569, 0.06704694971952208]]\n",
      "2020-12-23 02:40:45,489 : INFO : Removed 1 and 90 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:45,490 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:45,491 : INFO : built Dictionary(93 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 322 corpus positions)\n",
      "2020-12-23 02:40:45,523 : INFO : token count processed\n",
      "2020-12-23 02:40:45,527 : INFO : frequencies processed\n",
      "2020-12-23 02:40:45,656 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:45,656 : INFO : entropies processed\n",
      "2020-12-23 02:40:45,657 : INFO : extropies processed\n",
      "2020-12-23 02:40:45,658 : INFO : token count processed\n",
      "2020-12-23 02:40:45,659 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:45,660 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:45,660 : INFO : vocab #2480\n",
      "2020-12-23 02:40:45,661 : INFO : diff #set()\n",
      "2020-12-23 02:40:45,930 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:46,058 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.1123408665044912, 0.4734084426699577], [0.7882837057113647, 0.2117163], [1.584962500721156, 1.1699250014423124], [3.7004397181410926, 6.184756445474906, 6.276792222074843, 3.6084039415411544, 2.5763525039337507, 0.09203577659993734]]\n",
      "2020-12-23 02:40:46,060 : INFO : Removed 1 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:46,061 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:46,062 : INFO : built Dictionary(100 unique tokens: ['allow', 'append', 'bom', 'feed', 'field']...) from 2 documents (total 435 corpus positions)\n",
      "2020-12-23 02:40:46,093 : INFO : token count processed\n",
      "2020-12-23 02:40:46,096 : INFO : frequencies processed\n",
      "2020-12-23 02:40:46,225 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:46,226 : INFO : entropies processed\n",
      "2020-12-23 02:40:46,226 : INFO : extropies processed\n",
      "2020-12-23 02:40:46,228 : INFO : token count processed\n",
      "2020-12-23 02:40:46,229 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:46,230 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:46,230 : INFO : vocab #2480\n",
      "2020-12-23 02:40:46,232 : INFO : diff #set()\n",
      "2020-12-23 02:40:46,499 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:46,626 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.1404361734886423, 0.4671944963302155], [0.7819007635116577, 0.21809924], [2.321928094887362, 1.2877123795494492], [3.7004397181410926, 6.212221456585881, 6.281892089391902, 3.6307690853350723, 2.5814523712508093, 0.06967063280602126]]\n",
      "2020-12-23 02:40:46,629 : INFO : Removed 10 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:46,629 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:46,631 : INFO : built Dictionary(173 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 500 corpus positions)\n",
      "2020-12-23 02:40:46,804 : INFO : token count processed\n",
      "2020-12-23 02:40:46,807 : INFO : frequencies processed\n",
      "2020-12-23 02:40:46,940 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:46,941 : INFO : entropies processed\n",
      "2020-12-23 02:40:46,942 : INFO : extropies processed\n",
      "2020-12-23 02:40:46,943 : INFO : token count processed\n",
      "2020-12-23 02:40:46,944 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:46,946 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:46,946 : INFO : vocab #2480\n",
      "2020-12-23 02:40:46,948 : INFO : diff #set()\n",
      "2020-12-23 02:40:47,221 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:47,351 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.0907828047455712, 0.4782897571810146], [0.7582869827747345, 0.24171302], [3.852168723603281, 1.3887904911367783], [6.130567275446907, 6.301552355933639, 6.874928077097023, 5.5571915542835235, 0.7443608016501155, 0.5733757211633836]]\n",
      "2020-12-23 02:40:47,353 : INFO : Removed 10 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:47,354 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:47,356 : INFO : built Dictionary(197 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 736 corpus positions)\n",
      "2020-12-23 02:40:47,593 : INFO : token count processed\n",
      "2020-12-23 02:40:47,596 : INFO : frequencies processed\n",
      "2020-12-23 02:40:47,729 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:47,729 : INFO : entropies processed\n",
      "2020-12-23 02:40:47,730 : INFO : extropies processed\n",
      "2020-12-23 02:40:47,731 : INFO : token count processed\n",
      "2020-12-23 02:40:47,731 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:47,732 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:47,732 : INFO : vocab #2480\n",
      "2020-12-23 02:40:47,733 : INFO : diff #set()\n",
      "2020-12-23 02:40:47,990 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:48,118 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.025047278184649, 0.4938156312560015], [0.7333950400352478, 0.26660496], [4.781036112553423, 1.4144533941290538], [6.130567275446907, 6.739005504021667, 7.072982199929296, 5.796590579539277, 0.9424149244823887, 0.33397669590762913]]\n",
      "2020-12-23 02:40:48,121 : INFO : Removed 10 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:48,121 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:48,123 : INFO : built Dictionary(143 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 587 corpus positions)\n",
      "2020-12-23 02:40:48,259 : INFO : token count processed\n",
      "2020-12-23 02:40:48,262 : INFO : frequencies processed\n",
      "2020-12-23 02:40:48,389 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:48,390 : INFO : entropies processed\n",
      "2020-12-23 02:40:48,393 : INFO : extropies processed\n",
      "2020-12-23 02:40:48,394 : INFO : token count processed\n",
      "2020-12-23 02:40:48,395 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:48,396 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:48,396 : INFO : vocab #2480\n",
      "2020-12-23 02:40:48,397 : INFO : diff #set()\n",
      "2020-12-23 02:40:48,656 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:48,784 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[0.9990549283339866, 0.5002363796143413], [0.7682823836803436, 0.23171762], [4.473825694390371, 1.3967767317837956], [6.130567275446907, 5.870833373337847, 6.334563131772876, 5.666837517011878, 0.20399585632596917, 0.4637297584350293]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:40:48,786 : INFO : Removed 10 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:48,787 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:48,789 : INFO : built Dictionary(111 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 270 corpus positions)\n",
      "2020-12-23 02:40:48,880 : INFO : token count processed\n",
      "2020-12-23 02:40:48,883 : INFO : frequencies processed\n",
      "2020-12-23 02:40:49,013 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:49,013 : INFO : entropies processed\n",
      "2020-12-23 02:40:49,014 : INFO : extropies processed\n",
      "2020-12-23 02:40:49,015 : INFO : token count processed\n",
      "2020-12-23 02:40:49,016 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:49,018 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:49,019 : INFO : vocab #2480\n",
      "2020-12-23 02:40:49,020 : INFO : diff #set()\n",
      "2020-12-23 02:40:49,279 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:49,406 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[0.9754928200379317, 0.506202801577785], [0.6300491988658905, 0.3699508], [3.790767239125583, 1.3599792870306666], [6.130567275446907, 5.371881234145534, 6.312186749068844, 5.190261760523597, 0.18161947362193676, 0.9403055149233097]]\n",
      "2020-12-23 02:40:49,408 : INFO : Removed 10 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:49,409 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:49,410 : INFO : built Dictionary(103 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 222 corpus positions)\n",
      "2020-12-23 02:40:49,486 : INFO : token count processed\n",
      "2020-12-23 02:40:49,492 : INFO : frequencies processed\n",
      "2020-12-23 02:40:49,621 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:49,622 : INFO : entropies processed\n",
      "2020-12-23 02:40:49,623 : INFO : extropies processed\n",
      "2020-12-23 02:40:49,624 : INFO : token count processed\n",
      "2020-12-23 02:40:49,625 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:49,626 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:49,627 : INFO : vocab #2480\n",
      "2020-12-23 02:40:49,628 : INFO : diff #set()\n",
      "2020-12-23 02:40:49,896 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:50,022 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[0.9713195668105528, 0.5072744251293183], [0.6772065460681915, 0.32279345], [3.6828173151210217, 1.350098396915759], [6.130567275446907, 4.85108279267097, 6.133286118713087, 4.8483639494047885, 0.0027188432661802153, 1.2822033260421177]]\n",
      "2020-12-23 02:40:50,025 : INFO : Removed 10 and 127 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:50,026 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:50,027 : INFO : built Dictionary(141 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 495 corpus positions)\n",
      "2020-12-23 02:40:50,150 : INFO : token count processed\n",
      "2020-12-23 02:40:50,153 : INFO : frequencies processed\n",
      "2020-12-23 02:40:50,284 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:50,284 : INFO : entropies processed\n",
      "2020-12-23 02:40:50,285 : INFO : extropies processed\n",
      "2020-12-23 02:40:50,287 : INFO : token count processed\n",
      "2020-12-23 02:40:50,288 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:50,289 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:50,289 : INFO : vocab #2480\n",
      "2020-12-23 02:40:50,290 : INFO : diff #set()\n",
      "2020-12-23 02:40:50,558 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:50,687 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.0302445537869558, 0.4925514998351944], [0.8022589087486267, 0.19774109], [4.186704345910025, 1.400026419702932], [6.130567275446907, 6.139571208108155, 6.654067057627109, 5.616071425927953, 0.5234997821802017, 0.514495849518954]]\n",
      "2020-12-23 02:40:50,689 : INFO : Removed 10 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:50,690 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:50,691 : INFO : built Dictionary(139 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 505 corpus positions)\n",
      "2020-12-23 02:40:50,805 : INFO : token count processed\n",
      "2020-12-23 02:40:50,807 : INFO : frequencies processed\n",
      "2020-12-23 02:40:50,938 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:50,938 : INFO : entropies processed\n",
      "2020-12-23 02:40:50,939 : INFO : extropies processed\n",
      "2020-12-23 02:40:50,940 : INFO : token count processed\n",
      "2020-12-23 02:40:50,941 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:50,942 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:50,943 : INFO : vocab #2480\n",
      "2020-12-23 02:40:50,944 : INFO : diff #set()\n",
      "2020-12-23 02:40:51,203 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:51,331 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.1854445229593331, 0.4575728139032738], [0.8489009886980057, 0.15109901], [2.725480556997868, 1.3192201298976014], [6.130567275446907, 5.609710627339259, 6.3595584483778875, 5.380719454408279, 0.22899117293098037, 0.7498478210386281]]\n",
      "2020-12-23 02:40:51,334 : INFO : Removed 10 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:51,335 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:51,338 : INFO : built Dictionary(206 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 1165 corpus positions)\n",
      "2020-12-23 02:40:51,604 : INFO : token count processed\n",
      "2020-12-23 02:40:51,612 : INFO : frequencies processed\n",
      "2020-12-23 02:40:51,741 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:51,742 : INFO : entropies processed\n",
      "2020-12-23 02:40:51,743 : INFO : extropies processed\n",
      "2020-12-23 02:40:51,745 : INFO : token count processed\n",
      "2020-12-23 02:40:51,746 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:51,747 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:51,748 : INFO : vocab #2480\n",
      "2020-12-23 02:40:51,749 : INFO : diff #set()\n",
      "2020-12-23 02:40:52,007 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:52,134 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[0.9746072650301097, 0.5064298190884816], [0.6787796020507812, 0.3212204], [4.698633691197925, 1.4046301626135864], [6.130567275446907, 7.2441902753576075, 7.411303946878773, 5.963453603925741, 1.2807366714318658, 0.1671136715211654]]\n",
      "2020-12-23 02:40:52,137 : INFO : Removed 10 and 136 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:52,138 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:52,139 : INFO : built Dictionary(180 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 772 corpus positions)\n",
      "2020-12-23 02:40:52,341 : INFO : token count processed\n",
      "2020-12-23 02:40:52,343 : INFO : frequencies processed\n",
      "2020-12-23 02:40:52,474 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:52,475 : INFO : entropies processed\n",
      "2020-12-23 02:40:52,475 : INFO : extropies processed\n",
      "2020-12-23 02:40:52,477 : INFO : token count processed\n",
      "2020-12-23 02:40:52,478 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:52,479 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:52,480 : INFO : vocab #2480\n",
      "2020-12-23 02:40:52,483 : INFO : diff #set()\n",
      "2020-12-23 02:40:52,748 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:52,878 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.129873346408014, 0.46951148606393833], [0.8177186697721481, 0.18228133], [4.3104430577190245, 1.403308615816235], [6.130567275446907, 6.2567074920449475, 6.665432850643646, 5.721841916848208, 0.534865575196739, 0.40872535859869874]]\n",
      "2020-12-23 02:40:52,881 : INFO : Removed 10 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:52,882 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:52,884 : INFO : built Dictionary(137 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 282 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:40:52,996 : INFO : token count processed\n",
      "2020-12-23 02:40:53,003 : INFO : frequencies processed\n",
      "2020-12-23 02:40:53,135 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:53,136 : INFO : entropies processed\n",
      "2020-12-23 02:40:53,136 : INFO : extropies processed\n",
      "2020-12-23 02:40:53,137 : INFO : token count processed\n",
      "2020-12-23 02:40:53,138 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:53,139 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:53,139 : INFO : vocab #2480\n",
      "2020-12-23 02:40:53,140 : INFO : diff #set()\n",
      "2020-12-23 02:40:53,406 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:53,533 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.1457465723755471, 0.466038260470296], [0.8361074030399323, 0.1638926], [2.94770277922009, 1.3393100707180505], [6.130567275446907, 5.7680018917339435, 6.734775740499872, 5.163793426680979, 0.6042084650529649, 0.9667738487659285]]\n",
      "2020-12-23 02:40:53,536 : INFO : Removed 10 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:53,537 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:53,538 : INFO : built Dictionary(215 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 823 corpus positions)\n",
      "2020-12-23 02:40:53,821 : INFO : token count processed\n",
      "2020-12-23 02:40:53,823 : INFO : frequencies processed\n",
      "2020-12-23 02:40:53,957 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:53,957 : INFO : entropies processed\n",
      "2020-12-23 02:40:53,958 : INFO : extropies processed\n",
      "2020-12-23 02:40:53,959 : INFO : token count processed\n",
      "2020-12-23 02:40:53,960 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:53,961 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:53,961 : INFO : vocab #2480\n",
      "2020-12-23 02:40:53,962 : INFO : diff #set()\n",
      "2020-12-23 02:40:54,230 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:54,358 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.0154879089482753, 0.4961577767647445], [0.6990088522434235, 0.30099115], [4.772581706041736, 1.4143028976562844], [6.130567275446907, 6.846479111193757, 7.131278530027905, 5.845767856612759, 1.000711254580998, 0.2847994188341483]]\n",
      "2020-12-23 02:40:54,360 : INFO : Removed 10 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:54,361 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:54,363 : INFO : built Dictionary(89 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 126 corpus positions)\n",
      "2020-12-23 02:40:54,401 : INFO : token count processed\n",
      "2020-12-23 02:40:54,404 : INFO : frequencies processed\n",
      "2020-12-23 02:40:54,531 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:54,532 : INFO : entropies processed\n",
      "2020-12-23 02:40:54,532 : INFO : extropies processed\n",
      "2020-12-23 02:40:54,534 : INFO : token count processed\n",
      "2020-12-23 02:40:54,535 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:54,536 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:54,536 : INFO : vocab #2480\n",
      "2020-12-23 02:40:54,538 : INFO : diff #set()\n",
      "2020-12-23 02:40:54,799 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:54,927 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2026084340543177, 0.4540071601193821], [0.8683491051197052, 0.1316509], [1.584962500721156, 1.1699250014423124], [6.130567275446907, 4.165013816065912, 6.394222584357002, 3.901358507155817, 0.2636553089100948, 2.22920876829109]]\n",
      "2020-12-23 02:40:54,929 : INFO : Removed 10 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:54,930 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:54,931 : INFO : built Dictionary(111 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 233 corpus positions)\n",
      "2020-12-23 02:40:55,007 : INFO : token count processed\n",
      "2020-12-23 02:40:55,009 : INFO : frequencies processed\n",
      "2020-12-23 02:40:55,138 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:55,139 : INFO : entropies processed\n",
      "2020-12-23 02:40:55,140 : INFO : extropies processed\n",
      "2020-12-23 02:40:55,141 : INFO : token count processed\n",
      "2020-12-23 02:40:55,142 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:55,143 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:55,144 : INFO : vocab #2480\n",
      "2020-12-23 02:40:55,145 : INFO : diff #set()\n",
      "2020-12-23 02:40:55,402 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:55,530 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.095250320815318, 0.47726994243383447], [0.7873522192239761, 0.21264778], [3.2776134368191157, 1.3618978811135465], [6.130567275446907, 5.449968864419248, 6.568395773606401, 5.012140366259754, 0.43782849815949376, 1.1184269091871526]]\n",
      "2020-12-23 02:40:55,532 : INFO : Removed 10 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:55,533 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:55,534 : INFO : built Dictionary(186 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 670 corpus positions)\n",
      "2020-12-23 02:40:55,749 : INFO : token count processed\n",
      "2020-12-23 02:40:55,752 : INFO : frequencies processed\n",
      "2020-12-23 02:40:55,878 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:55,879 : INFO : entropies processed\n",
      "2020-12-23 02:40:55,880 : INFO : extropies processed\n",
      "2020-12-23 02:40:55,881 : INFO : token count processed\n",
      "2020-12-23 02:40:55,882 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:55,883 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:55,884 : INFO : vocab #2480\n",
      "2020-12-23 02:40:55,885 : INFO : diff #set()\n",
      "2020-12-23 02:40:56,140 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:56,267 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[0.9678211910268076, 0.5081762532896603], [0.7025412917137146, 0.2974587], [4.585564578900931, 1.400368828887893], [6.130567275446907, 6.530294129310484, 6.8604363275093, 5.800425077248091, 0.7298690520623925, 0.33014219819881596]]\n",
      "2020-12-23 02:40:56,270 : INFO : Removed 10 and 142 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:56,271 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:56,273 : INFO : built Dictionary(169 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 686 corpus positions)\n",
      "2020-12-23 02:40:56,459 : INFO : token count processed\n",
      "2020-12-23 02:40:56,462 : INFO : frequencies processed\n",
      "2020-12-23 02:40:56,588 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:56,589 : INFO : entropies processed\n",
      "2020-12-23 02:40:56,589 : INFO : extropies processed\n",
      "2020-12-23 02:40:56,590 : INFO : token count processed\n",
      "2020-12-23 02:40:56,591 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:56,592 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:56,592 : INFO : vocab #2480\n",
      "2020-12-23 02:40:56,593 : INFO : diff #set()\n",
      "2020-12-23 02:40:56,855 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:56,982 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[0.9879827006931412, 0.503022485885483], [0.5815452337265015, 0.41845477], [3.7661506484543548, 1.3527127989451628], [6.130567275446907, 6.470272233491701, 6.847806595247379, 5.75303291369123, 0.7172393198004716, 0.37753436175567767]]\n",
      "2020-12-23 02:40:56,985 : INFO : Removed 10 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:56,986 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:56,987 : INFO : built Dictionary(169 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 475 corpus positions)\n",
      "2020-12-23 02:40:57,159 : INFO : token count processed\n",
      "2020-12-23 02:40:57,162 : INFO : frequencies processed\n",
      "2020-12-23 02:40:57,290 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:57,291 : INFO : entropies processed\n",
      "2020-12-23 02:40:57,291 : INFO : extropies processed\n",
      "2020-12-23 02:40:57,292 : INFO : token count processed\n",
      "2020-12-23 02:40:57,293 : INFO : alphabet_source #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:40:57,294 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:57,294 : INFO : vocab #2480\n",
      "2020-12-23 02:40:57,295 : INFO : diff #set()\n",
      "2020-12-23 02:40:57,554 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:57,682 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.0943879484381394, 0.4774664601874433], [0.7923142164945602, 0.20768578], [4.243856189774725, 1.4014225974728614], [6.130567275446907, 6.550038223589686, 7.04942522942759, 5.631180269609003, 0.9188579539806829, 0.49938700583790396]]\n",
      "2020-12-23 02:40:57,685 : INFO : Removed 10 and 70 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:57,686 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:57,687 : INFO : built Dictionary(131 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 391 corpus positions)\n",
      "2020-12-23 02:40:57,786 : INFO : token count processed\n",
      "2020-12-23 02:40:57,789 : INFO : frequencies processed\n",
      "2020-12-23 02:40:57,916 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:57,917 : INFO : entropies processed\n",
      "2020-12-23 02:40:57,918 : INFO : extropies processed\n",
      "2020-12-23 02:40:57,919 : INFO : token count processed\n",
      "2020-12-23 02:40:57,920 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:57,922 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:57,922 : INFO : vocab #2480\n",
      "2020-12-23 02:40:57,924 : INFO : diff #set()\n",
      "2020-12-23 02:40:58,178 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:58,305 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.060880427157331, 0.4852295100785381], [0.7753611207008362, 0.22463888], [3.3921472236645345, 1.3581797203736243], [6.130567275446907, 5.860525481261383, 6.558586646832024, 5.432506109876266, 0.42801937138511725, 0.6980611655706417]]\n",
      "2020-12-23 02:40:58,307 : INFO : Removed 10 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:58,308 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:58,309 : INFO : built Dictionary(107 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 429 corpus positions)\n",
      "2020-12-23 02:40:58,373 : INFO : token count processed\n",
      "2020-12-23 02:40:58,375 : INFO : frequencies processed\n",
      "2020-12-23 02:40:58,502 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:58,502 : INFO : entropies processed\n",
      "2020-12-23 02:40:58,503 : INFO : extropies processed\n",
      "2020-12-23 02:40:58,504 : INFO : token count processed\n",
      "2020-12-23 02:40:58,505 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:58,505 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:58,506 : INFO : vocab #2480\n",
      "2020-12-23 02:40:58,507 : INFO : diff #set()\n",
      "2020-12-23 02:40:58,770 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:58,897 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.1876564301145107, 0.45711016877895033], [0.8282322436571121, 0.17176776], [2.75, 1.3226647836567116], [6.130567275446907, 5.945464049777852, 6.57103033388144, 5.505000991343319, 0.44046305843453304, 0.6255662841035878]]\n",
      "2020-12-23 02:40:58,899 : INFO : Removed 10 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:58,900 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:58,903 : INFO : built Dictionary(235 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 976 corpus positions)\n",
      "2020-12-23 02:40:59,223 : INFO : token count processed\n",
      "2020-12-23 02:40:59,227 : INFO : frequencies processed\n",
      "2020-12-23 02:40:59,355 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:40:59,355 : INFO : entropies processed\n",
      "2020-12-23 02:40:59,356 : INFO : extropies processed\n",
      "2020-12-23 02:40:59,357 : INFO : token count processed\n",
      "2020-12-23 02:40:59,358 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:40:59,358 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:40:59,359 : INFO : vocab #2480\n",
      "2020-12-23 02:40:59,360 : INFO : diff #set()\n",
      "2020-12-23 02:40:59,617 : INFO : alphabet #2480\n",
      "2020-12-23 02:40:59,745 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.1020695238925104, 0.47572165841035], [0.7886465340852737, 0.21135347], [4.774243302917271, 1.4143074545063914], [6.130567275446907, 6.811563897304216, 7.090331516926527, 5.851799655824596, 0.9597642414796201, 0.278767619622311]]\n",
      "2020-12-23 02:40:59,748 : INFO : Removed 10 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:40:59,749 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:40:59,752 : INFO : built Dictionary(249 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 1071 corpus positions)\n",
      "2020-12-23 02:41:00,104 : INFO : token count processed\n",
      "2020-12-23 02:41:00,111 : INFO : frequencies processed\n",
      "2020-12-23 02:41:00,241 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:00,241 : INFO : entropies processed\n",
      "2020-12-23 02:41:00,242 : INFO : extropies processed\n",
      "2020-12-23 02:41:00,243 : INFO : token count processed\n",
      "2020-12-23 02:41:00,244 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:00,245 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:00,246 : INFO : vocab #2480\n",
      "2020-12-23 02:41:00,247 : INFO : diff #set()\n",
      "2020-12-23 02:41:00,505 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:00,633 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[0.9616345936456094, 0.5097789380546889], [0.6260070204734802, 0.37399298], [4.89197769118546, 1.410126775176589], [6.130567275446907, 7.502034948968415, 7.648225293000693, 5.984376931414629, 1.517658017553786, 0.14619034403227804]]\n",
      "2020-12-23 02:41:00,636 : INFO : Removed 10 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:00,637 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:00,638 : INFO : built Dictionary(287 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 1644 corpus positions)\n",
      "2020-12-23 02:41:01,064 : INFO : token count processed\n",
      "2020-12-23 02:41:01,072 : INFO : frequencies processed\n",
      "2020-12-23 02:41:01,200 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:01,200 : INFO : entropies processed\n",
      "2020-12-23 02:41:01,201 : INFO : extropies processed\n",
      "2020-12-23 02:41:01,202 : INFO : token count processed\n",
      "2020-12-23 02:41:01,203 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:01,203 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:01,204 : INFO : vocab #2480\n",
      "2020-12-23 02:41:01,205 : INFO : diff #set()\n",
      "2020-12-23 02:41:01,462 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:01,589 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[0.9883404716251081, 0.5029319748155009], [0.6719739735126495, 0.32802603], [5.139851952810274, 1.4159377733382585], [6.130567275446907, 7.39180093901977, 7.518767883856466, 6.003600330610212, 1.3882006084095586, 0.12696694483669546]]\n",
      "2020-12-23 02:41:01,592 : INFO : Removed 10 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:01,593 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:01,595 : INFO : built Dictionary(106 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 199 corpus positions)\n",
      "2020-12-23 02:41:01,661 : INFO : token count processed\n",
      "2020-12-23 02:41:01,664 : INFO : frequencies processed\n",
      "2020-12-23 02:41:01,790 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:01,791 : INFO : entropies processed\n",
      "2020-12-23 02:41:01,791 : INFO : extropies processed\n",
      "2020-12-23 02:41:01,792 : INFO : token count processed\n",
      "2020-12-23 02:41:01,793 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:01,793 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:01,794 : INFO : vocab #2480\n",
      "2020-12-23 02:41:01,796 : INFO : diff #set()\n",
      "2020-12-23 02:41:02,047 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:02,173 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.1589897975008898, 0.4631795857291854], [0.8362987339496613, 0.16370127], [2.2516291673878226, 1.2667563532600834], [6.130567275446907, 4.927561309677364, 6.4408995265191935, 4.617229058605077, 0.31033225107228635, 1.51333821684183]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:41:02,175 : INFO : Removed 10 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:02,176 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:02,177 : INFO : built Dictionary(76 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 100 corpus positions)\n",
      "2020-12-23 02:41:02,191 : INFO : token count processed\n",
      "2020-12-23 02:41:02,194 : INFO : frequencies processed\n",
      "2020-12-23 02:41:02,321 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:02,322 : INFO : entropies processed\n",
      "2020-12-23 02:41:02,323 : INFO : extropies processed\n",
      "2020-12-23 02:41:02,324 : INFO : token count processed\n",
      "2020-12-23 02:41:02,325 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:02,326 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:02,327 : INFO : vocab #2480\n",
      "2020-12-23 02:41:02,328 : INFO : diff #set()\n",
      "2020-12-23 02:41:02,586 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:02,714 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2335247707604107, 0.44772281601316055], [0.8492907285690308, 0.15070927], [1.0, 1.0], [6.130567275446907, 2.5216406363433186, 6.1993959641519, 2.4528119476383266, 0.06882868870499248, 3.677755327808581]]\n",
      "2020-12-23 02:41:02,718 : INFO : Removed 10 and 530 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:02,719 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:02,721 : INFO : built Dictionary(373 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 2972 corpus positions)\n",
      "2020-12-23 02:41:03,394 : INFO : token count processed\n",
      "2020-12-23 02:41:03,397 : INFO : frequencies processed\n",
      "2020-12-23 02:41:03,524 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:03,525 : INFO : entropies processed\n",
      "2020-12-23 02:41:03,525 : INFO : extropies processed\n",
      "2020-12-23 02:41:03,527 : INFO : token count processed\n",
      "2020-12-23 02:41:03,528 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:03,529 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:03,529 : INFO : vocab #2480\n",
      "2020-12-23 02:41:03,530 : INFO : diff #set()\n",
      "2020-12-23 02:41:03,788 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:03,916 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.0869644414009543, 0.47916484831371253], [0.7418538331985474, 0.25814617], [4.7413638163281515, 1.4137797648004946], [6.130567275446907, 7.480007711014331, 7.591517054899068, 6.01905793156217, 1.4609497794521609, 0.11150934388473743]]\n",
      "2020-12-23 02:41:03,918 : INFO : Removed 10 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:03,919 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:03,921 : INFO : built Dictionary(250 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 1121 corpus positions)\n",
      "2020-12-23 02:41:04,277 : INFO : token count processed\n",
      "2020-12-23 02:41:04,279 : INFO : frequencies processed\n",
      "2020-12-23 02:41:04,404 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:04,405 : INFO : entropies processed\n",
      "2020-12-23 02:41:04,405 : INFO : extropies processed\n",
      "2020-12-23 02:41:04,406 : INFO : token count processed\n",
      "2020-12-23 02:41:04,407 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:04,408 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:04,408 : INFO : vocab #2480\n",
      "2020-12-23 02:41:04,409 : INFO : diff #set()\n",
      "2020-12-23 02:41:04,665 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:04,794 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.0823601803660612, 0.48022431922618136], [0.7753843814134598, 0.22461562], [4.781036112553423, 1.4144533941290538], [6.130567275446907, 7.131331012509435, 7.363945036839759, 5.897953251116583, 1.2333777613928518, 0.2326140243303243]]\n",
      "2020-12-23 02:41:04,796 : INFO : Removed 10 and 166 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:04,797 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:04,800 : INFO : built Dictionary(240 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 940 corpus positions)\n",
      "2020-12-23 02:41:05,148 : INFO : token count processed\n",
      "2020-12-23 02:41:05,154 : INFO : frequencies processed\n",
      "2020-12-23 02:41:05,283 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:05,284 : INFO : entropies processed\n",
      "2020-12-23 02:41:05,284 : INFO : extropies processed\n",
      "2020-12-23 02:41:05,286 : INFO : token count processed\n",
      "2020-12-23 02:41:05,286 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:05,287 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:05,288 : INFO : vocab #2480\n",
      "2020-12-23 02:41:05,289 : INFO : diff #set()\n",
      "2020-12-23 02:41:05,551 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:05,678 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.0720266103619902, 0.48261928442381175], [0.7635292112827301, 0.23647079], [4.729283016944966, 1.4133983082413781], [6.130567275446907, 7.203742744794778, 7.4512953479626205, 5.883014672279064, 1.3207280725157133, 0.24755260316784256]]\n",
      "2020-12-23 02:41:05,681 : INFO : Removed 10 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:05,682 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:05,683 : INFO : built Dictionary(116 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 272 corpus positions)\n",
      "2020-12-23 02:41:05,766 : INFO : token count processed\n",
      "2020-12-23 02:41:05,771 : INFO : frequencies processed\n",
      "2020-12-23 02:41:05,900 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:05,901 : INFO : entropies processed\n",
      "2020-12-23 02:41:05,902 : INFO : extropies processed\n",
      "2020-12-23 02:41:05,903 : INFO : token count processed\n",
      "2020-12-23 02:41:05,904 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:05,905 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:05,905 : INFO : vocab #2480\n",
      "2020-12-23 02:41:05,907 : INFO : diff #set()\n",
      "2020-12-23 02:41:06,165 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:06,293 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.1492778430024935, 0.4652725580621174], [0.8140331655740738, 0.18596683], [3.169925001442312, 1.3594000115384994], [6.130567275446907, 5.195502554608948, 6.394420553924574, 4.931649276131282, 0.2638532784776668, 1.1989179993156256]]\n",
      "2020-12-23 02:41:06,296 : INFO : Removed 10 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:06,296 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:06,297 : INFO : built Dictionary(123 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 333 corpus positions)\n",
      "2020-12-23 02:41:06,387 : INFO : token count processed\n",
      "2020-12-23 02:41:06,392 : INFO : frequencies processed\n",
      "2020-12-23 02:41:06,520 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:06,521 : INFO : entropies processed\n",
      "2020-12-23 02:41:06,521 : INFO : extropies processed\n",
      "2020-12-23 02:41:06,522 : INFO : token count processed\n",
      "2020-12-23 02:41:06,523 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:06,524 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:06,525 : INFO : vocab #2480\n",
      "2020-12-23 02:41:06,526 : INFO : diff #set()\n",
      "2020-12-23 02:41:06,784 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:06,912 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.1180210333064393, 0.4721388429457198], [0.7727638185024261, 0.22723618], [2.9219280948873623, 1.3359016564230495], [6.130567275446907, 5.32027245610305, 6.340608152886167, 5.1102315786637895, 0.21004087743926014, 1.0203356967831176]]\n",
      "2020-12-23 02:41:06,914 : INFO : Removed 10 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:06,915 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:06,916 : INFO : built Dictionary(210 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 573 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:41:07,179 : INFO : token count processed\n",
      "2020-12-23 02:41:07,187 : INFO : frequencies processed\n",
      "2020-12-23 02:41:07,316 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:07,317 : INFO : entropies processed\n",
      "2020-12-23 02:41:07,318 : INFO : extropies processed\n",
      "2020-12-23 02:41:07,319 : INFO : token count processed\n",
      "2020-12-23 02:41:07,320 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:07,321 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:07,322 : INFO : vocab #2480\n",
      "2020-12-23 02:41:07,323 : INFO : diff #set()\n",
      "2020-12-23 02:41:07,587 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:07,717 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.0923812097997532, 0.47792438362400647], [0.7652539014816284, 0.2347461], [4.378783493486175, 1.405167757330086], [6.130567275446907, 6.898202761357263, 7.29289951260597, 5.7358705241982, 1.1623322371590632, 0.3946967512487074]]\n",
      "2020-12-23 02:41:07,720 : INFO : Removed 10 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:07,721 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:07,722 : INFO : built Dictionary(181 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 595 corpus positions)\n",
      "2020-12-23 02:41:07,917 : INFO : token count processed\n",
      "2020-12-23 02:41:07,920 : INFO : frequencies processed\n",
      "2020-12-23 02:41:08,047 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:08,048 : INFO : entropies processed\n",
      "2020-12-23 02:41:08,049 : INFO : extropies processed\n",
      "2020-12-23 02:41:08,050 : INFO : token count processed\n",
      "2020-12-23 02:41:08,051 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:08,052 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:08,052 : INFO : vocab #2480\n",
      "2020-12-23 02:41:08,053 : INFO : diff #set()\n",
      "2020-12-23 02:41:08,311 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:08,438 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.0936200933546407, 0.4776415755533202], [0.7633165717124939, 0.23668343], [3.7345216647797517, 1.383483006702452], [6.130567275446907, 6.388500481644799, 6.896768913039779, 5.622298844051928, 0.7662016375928715, 0.5082684313949795]]\n",
      "2020-12-23 02:41:08,441 : INFO : Removed 10 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:08,442 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:08,443 : INFO : built Dictionary(108 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 259 corpus positions)\n",
      "2020-12-23 02:41:08,502 : INFO : token count processed\n",
      "2020-12-23 02:41:08,504 : INFO : frequencies processed\n",
      "2020-12-23 02:41:08,632 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:08,632 : INFO : entropies processed\n",
      "2020-12-23 02:41:08,633 : INFO : extropies processed\n",
      "2020-12-23 02:41:08,634 : INFO : token count processed\n",
      "2020-12-23 02:41:08,635 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:08,636 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:08,637 : INFO : vocab #2480\n",
      "2020-12-23 02:41:08,638 : INFO : diff #set()\n",
      "2020-12-23 02:41:08,896 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:09,024 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.1602323714042866, 0.46291316306399816], [0.8369157463312149, 0.16308425], [3.0, 1.3485155455967714], [6.130567275446907, 4.8191513650620195, 6.202168775351805, 4.747549865157121, 0.07160149990489817, 1.3830174102897859]]\n",
      "2020-12-23 02:41:09,026 : INFO : Removed 10 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:09,027 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:09,029 : INFO : built Dictionary(113 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 314 corpus positions)\n",
      "2020-12-23 02:41:09,108 : INFO : token count processed\n",
      "2020-12-23 02:41:09,111 : INFO : frequencies processed\n",
      "2020-12-23 02:41:09,240 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:09,240 : INFO : entropies processed\n",
      "2020-12-23 02:41:09,241 : INFO : extropies processed\n",
      "2020-12-23 02:41:09,243 : INFO : token count processed\n",
      "2020-12-23 02:41:09,244 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:09,245 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:09,246 : INFO : vocab #2480\n",
      "2020-12-23 02:41:09,247 : INFO : diff #set()\n",
      "2020-12-23 02:41:09,508 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:09,635 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.1178713599691232, 0.4721722097486503], [0.7810858637094498, 0.21891414], [2.9219280948873623, 1.3359016564230495], [6.130567275446907, 5.062480936779194, 6.196680946489946, 4.996367265736155, 0.06611367104303856, 1.134200009710752]]\n",
      "2020-12-23 02:41:09,638 : INFO : Removed 10 and 223 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:09,639 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:09,642 : INFO : built Dictionary(287 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 1868 corpus positions)\n",
      "2020-12-23 02:41:10,070 : INFO : token count processed\n",
      "2020-12-23 02:41:10,077 : INFO : frequencies processed\n",
      "2020-12-23 02:41:10,205 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:10,206 : INFO : entropies processed\n",
      "2020-12-23 02:41:10,206 : INFO : extropies processed\n",
      "2020-12-23 02:41:10,209 : INFO : token count processed\n",
      "2020-12-23 02:41:10,210 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:10,210 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:10,211 : INFO : vocab #2480\n",
      "2020-12-23 02:41:10,212 : INFO : diff #set()\n",
      "2020-12-23 02:41:10,479 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:10,608 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.0583836430297509, 0.4858180851690466], [0.7349319159984589, 0.26506808], [4.593069207771888, 1.4108940472358729], [6.130567275446907, 7.185085743102134, 7.3579432209957005, 5.957709797553342, 1.2273759455487934, 0.1728574778935661]]\n",
      "2020-12-23 02:41:10,610 : INFO : Removed 10 and 148 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:10,611 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:10,613 : INFO : built Dictionary(212 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 843 corpus positions)\n",
      "2020-12-23 02:41:10,880 : INFO : token count processed\n",
      "2020-12-23 02:41:10,883 : INFO : frequencies processed\n",
      "2020-12-23 02:41:11,010 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:11,011 : INFO : entropies processed\n",
      "2020-12-23 02:41:11,011 : INFO : extropies processed\n",
      "2020-12-23 02:41:11,013 : INFO : token count processed\n",
      "2020-12-23 02:41:11,014 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:11,015 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:11,016 : INFO : vocab #2480\n",
      "2020-12-23 02:41:11,017 : INFO : diff #set()\n",
      "2020-12-23 02:41:11,280 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:11,409 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.1254156995559563, 0.4704961952661405], [0.8041769862174988, 0.19582301], [4.084962500721156, 1.396526115098872], [6.130567275446907, 6.591225336124281, 6.948412673571035, 5.773379938000154, 0.8178453981241276, 0.3571873374467538]]\n",
      "2020-12-23 02:41:11,411 : INFO : Removed 10 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:11,412 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:11,413 : INFO : built Dictionary(102 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 181 corpus positions)\n",
      "2020-12-23 02:41:11,464 : INFO : token count processed\n",
      "2020-12-23 02:41:11,469 : INFO : frequencies processed\n",
      "2020-12-23 02:41:11,597 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:11,598 : INFO : entropies processed\n",
      "2020-12-23 02:41:11,598 : INFO : extropies processed\n",
      "2020-12-23 02:41:11,599 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:41:11,599 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:11,600 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:11,601 : INFO : vocab #2480\n",
      "2020-12-23 02:41:11,602 : INFO : diff #set()\n",
      "2020-12-23 02:41:11,859 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:11,986 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.167146837840726, 0.46143619921775453], [0.8542697429656982, 0.14573026], [2.2516291673878226, 1.2667563532600834], [6.130567275446907, 4.7032114441396695, 6.390794561851838, 4.442984157734738, 0.2602272864049304, 1.687583117712168]]\n",
      "2020-12-23 02:41:11,989 : INFO : Removed 10 and 9 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:11,990 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:11,992 : INFO : built Dictionary(169 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 421 corpus positions)\n",
      "2020-12-23 02:41:12,179 : INFO : token count processed\n",
      "2020-12-23 02:41:12,181 : INFO : frequencies processed\n",
      "2020-12-23 02:41:12,310 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:12,310 : INFO : entropies processed\n",
      "2020-12-23 02:41:12,311 : INFO : extropies processed\n",
      "2020-12-23 02:41:12,312 : INFO : token count processed\n",
      "2020-12-23 02:41:12,313 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:12,314 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:12,314 : INFO : vocab #2480\n",
      "2020-12-23 02:41:12,315 : INFO : diff #set()\n",
      "2020-12-23 02:41:12,578 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:12,705 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.0908313184252219, 0.47827865939619785], [0.7485226392745972, 0.25147736], [3.875, 1.3906253758377911], [6.130567275446907, 6.14228447828618, 6.8137805702965615, 5.459071183436524, 0.6832132948496543, 0.6714960920103819]]\n",
      "2020-12-23 02:41:12,708 : INFO : Removed 10 and 184 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:12,709 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:12,712 : INFO : built Dictionary(297 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 1211 corpus positions)\n",
      "2020-12-23 02:41:13,171 : INFO : token count processed\n",
      "2020-12-23 02:41:13,178 : INFO : frequencies processed\n",
      "2020-12-23 02:41:13,307 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:13,308 : INFO : entropies processed\n",
      "2020-12-23 02:41:13,308 : INFO : extropies processed\n",
      "2020-12-23 02:41:13,310 : INFO : token count processed\n",
      "2020-12-23 02:41:13,310 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:13,311 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:13,312 : INFO : vocab #2480\n",
      "2020-12-23 02:41:13,313 : INFO : diff #set()\n",
      "2020-12-23 02:41:13,573 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:13,701 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.0741409352821658, 0.482127314971468], [0.7511664927005768, 0.2488335], [4.640223928941852, 1.4117440181515406], [6.130567275446907, 7.450178124335845, 7.657975839161588, 5.922769560621165, 1.5274085637146806, 0.20779771482574283]]\n",
      "2020-12-23 02:41:13,703 : INFO : Removed 10 and 16 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:13,704 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:13,705 : INFO : built Dictionary(115 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 259 corpus positions)\n",
      "2020-12-23 02:41:13,780 : INFO : token count processed\n",
      "2020-12-23 02:41:13,782 : INFO : frequencies processed\n",
      "2020-12-23 02:41:13,910 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:13,911 : INFO : entropies processed\n",
      "2020-12-23 02:41:13,912 : INFO : extropies processed\n",
      "2020-12-23 02:41:13,913 : INFO : token count processed\n",
      "2020-12-23 02:41:13,914 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:13,915 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:13,916 : INFO : vocab #2480\n",
      "2020-12-23 02:41:13,917 : INFO : diff #set()\n",
      "2020-12-23 02:41:14,176 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:14,303 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.1398895046612627, 0.4673138485990643], [0.8122074455022812, 0.18779255], [3.321928094887362, 1.3680278410054498], [6.130567275446907, 5.20665021947654, 6.38802042300076, 4.949197071922687, 0.2574531475538526, 1.18137020352422]]\n",
      "2020-12-23 02:41:14,306 : INFO : Removed 10 and 89 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:14,307 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:14,308 : INFO : built Dictionary(176 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 597 corpus positions)\n",
      "2020-12-23 02:41:14,499 : INFO : token count processed\n",
      "2020-12-23 02:41:14,501 : INFO : frequencies processed\n",
      "2020-12-23 02:41:14,629 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:14,630 : INFO : entropies processed\n",
      "2020-12-23 02:41:14,633 : INFO : extropies processed\n",
      "2020-12-23 02:41:14,634 : INFO : token count processed\n",
      "2020-12-23 02:41:14,635 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:14,635 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:14,636 : INFO : vocab #2480\n",
      "2020-12-23 02:41:14,637 : INFO : diff #set()\n",
      "2020-12-23 02:41:14,895 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:15,023 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.1189288875701189, 0.47193655524077055], [0.8147562891244888, 0.18524371], [4.163856189774725, 1.3990174095103045], [6.130567275446907, 6.524718477352, 6.974402239284134, 5.680883513514773, 0.8438349638372271, 0.44968376193213455]]\n",
      "2020-12-23 02:41:15,025 : INFO : Removed 10 and 8 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:15,026 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:15,028 : INFO : built Dictionary(121 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 304 corpus positions)\n",
      "2020-12-23 02:41:15,120 : INFO : token count processed\n",
      "2020-12-23 02:41:15,122 : INFO : frequencies processed\n",
      "2020-12-23 02:41:15,250 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:15,250 : INFO : entropies processed\n",
      "2020-12-23 02:41:15,251 : INFO : extropies processed\n",
      "2020-12-23 02:41:15,252 : INFO : token count processed\n",
      "2020-12-23 02:41:15,253 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:15,254 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:15,255 : INFO : vocab #2480\n",
      "2020-12-23 02:41:15,256 : INFO : diff #set()\n",
      "2020-12-23 02:41:15,526 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:15,656 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.1323172630357328, 0.4689733640182241], [0.8228376060724258, 0.1771624], [3.121928094887362, 1.3519647487142497], [6.130567275446907, 5.321859380715434, 6.398800398508323, 5.0536262576540185, 0.26823312306141567, 1.0769410177928886]]\n",
      "2020-12-23 02:41:15,659 : INFO : Removed 10 and 119 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:15,660 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:15,662 : INFO : built Dictionary(192 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 922 corpus positions)\n",
      "2020-12-23 02:41:15,883 : INFO : token count processed\n",
      "2020-12-23 02:41:15,890 : INFO : frequencies processed\n",
      "2020-12-23 02:41:16,022 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:16,022 : INFO : entropies processed\n",
      "2020-12-23 02:41:16,023 : INFO : extropies processed\n",
      "2020-12-23 02:41:16,024 : INFO : token count processed\n",
      "2020-12-23 02:41:16,025 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:16,026 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:16,026 : INFO : vocab #2480\n",
      "2020-12-23 02:41:16,027 : INFO : diff #set()\n",
      "2020-12-23 02:41:16,286 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:16,413 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.1106940725501844, 0.4737778027640829], [0.7797829806804657, 0.22021702], [4.163856189774725, 1.3990174095103045], [6.130567275446907, 6.500767808767801, 6.851481686585611, 5.779853397629097, 0.720914411138704, 0.35071387781781027]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:41:16,416 : INFO : Removed 10 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:16,416 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:16,417 : INFO : built Dictionary(95 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 144 corpus positions)\n",
      "2020-12-23 02:41:16,467 : INFO : token count processed\n",
      "2020-12-23 02:41:16,470 : INFO : frequencies processed\n",
      "2020-12-23 02:41:16,598 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:16,598 : INFO : entropies processed\n",
      "2020-12-23 02:41:16,599 : INFO : extropies processed\n",
      "2020-12-23 02:41:16,600 : INFO : token count processed\n",
      "2020-12-23 02:41:16,601 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:16,601 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:16,602 : INFO : vocab #2480\n",
      "2020-12-23 02:41:16,603 : INFO : diff #set()\n",
      "2020-12-23 02:41:16,859 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:16,987 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.1217045956414957, 0.47131914690397836], [0.7700831592082977, 0.22991684], [2.75, 1.3226647836567116], [6.130567275446907, 4.736228843383063, 6.453544044717027, 4.413252074112943, 0.3229767692701202, 1.7173152013339648]]\n",
      "2020-12-23 02:41:16,989 : INFO : Removed 10 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:16,990 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:16,991 : INFO : built Dictionary(154 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 485 corpus positions)\n",
      "2020-12-23 02:41:17,133 : INFO : token count processed\n",
      "2020-12-23 02:41:17,135 : INFO : frequencies processed\n",
      "2020-12-23 02:41:17,264 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:17,265 : INFO : entropies processed\n",
      "2020-12-23 02:41:17,265 : INFO : extropies processed\n",
      "2020-12-23 02:41:17,267 : INFO : token count processed\n",
      "2020-12-23 02:41:17,268 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:17,269 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:17,269 : INFO : vocab #2480\n",
      "2020-12-23 02:41:17,270 : INFO : diff #set()\n",
      "2020-12-23 02:41:17,537 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:17,673 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.1521815553015797, 0.46464481471679214], [0.847894936800003, 0.15210506], [3.4182958340544896, 1.369895090630202], [6.130567275446907, 5.788442787590127, 6.516594315532979, 5.402415747504056, 0.3860270400860717, 0.7281515279428517]]\n",
      "2020-12-23 02:41:17,675 : INFO : Removed 10 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:17,676 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:17,677 : INFO : built Dictionary(109 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 171 corpus positions)\n",
      "2020-12-23 02:41:17,748 : INFO : token count processed\n",
      "2020-12-23 02:41:17,754 : INFO : frequencies processed\n",
      "2020-12-23 02:41:17,882 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:17,883 : INFO : entropies processed\n",
      "2020-12-23 02:41:17,884 : INFO : extropies processed\n",
      "2020-12-23 02:41:17,885 : INFO : token count processed\n",
      "2020-12-23 02:41:17,886 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:17,887 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:17,887 : INFO : vocab #2480\n",
      "2020-12-23 02:41:17,888 : INFO : diff #set()\n",
      "2020-12-23 02:41:18,145 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:18,273 : INFO : Computed distances or similarities ('294', 'sacp-python-common/setup.py')[[1.0619897382409063, 0.48496846587272785], [0.7681682705879211, 0.23183173], [3.6644977792004623, 1.381962919072374], [6.130567275446907, 5.370004292053436, 6.59734960719973, 4.903221960300614, 0.4667823317528228, 1.2273453151462936]]\n",
      "2020-12-23 02:41:18,276 : INFO : Removed 10 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:18,276 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:18,277 : INFO : built Dictionary(136 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 416 corpus positions)\n",
      "2020-12-23 02:41:18,392 : INFO : token count processed\n",
      "2020-12-23 02:41:18,394 : INFO : frequencies processed\n",
      "2020-12-23 02:41:18,521 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:18,522 : INFO : entropies processed\n",
      "2020-12-23 02:41:18,522 : INFO : extropies processed\n",
      "2020-12-23 02:41:18,524 : INFO : token count processed\n",
      "2020-12-23 02:41:18,524 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:18,525 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:18,526 : INFO : vocab #2480\n",
      "2020-12-23 02:41:18,527 : INFO : diff #set()\n",
      "2020-12-23 02:41:18,795 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:18,923 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.0636654801969936, 0.4845746607655335], [0.7387974560260773, 0.26120254], [3.139353872167201, 1.3102779844304584], [6.130567275446907, 5.695663584743922, 6.433315704916055, 5.392915155274775, 0.3027484294691476, 0.7376521201721324]]\n",
      "2020-12-23 02:41:18,926 : INFO : Removed 10 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:18,926 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:18,927 : INFO : built Dictionary(100 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 191 corpus positions)\n",
      "2020-12-23 02:41:18,980 : INFO : token count processed\n",
      "2020-12-23 02:41:18,982 : INFO : frequencies processed\n",
      "2020-12-23 02:41:19,108 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:19,109 : INFO : entropies processed\n",
      "2020-12-23 02:41:19,109 : INFO : extropies processed\n",
      "2020-12-23 02:41:19,110 : INFO : token count processed\n",
      "2020-12-23 02:41:19,111 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:19,112 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:19,112 : INFO : vocab #2480\n",
      "2020-12-23 02:41:19,113 : INFO : diff #set()\n",
      "2020-12-23 02:41:19,373 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:19,501 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.0577842821493273, 0.4859595870542435], [0.7298550307750702, 0.27014497], [2.835237745312117, 1.2953472784269702], [6.130567275446907, 4.9004417692112465, 6.345960514604782, 4.685048530053372, 0.21539323915787456, 1.4455187453935352]]\n",
      "2020-12-23 02:41:19,503 : INFO : Removed 10 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:19,504 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:19,506 : INFO : built Dictionary(97 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 198 corpus positions)\n",
      "2020-12-23 02:41:19,565 : INFO : token count processed\n",
      "2020-12-23 02:41:19,568 : INFO : frequencies processed\n",
      "2020-12-23 02:41:19,699 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:19,699 : INFO : entropies processed\n",
      "2020-12-23 02:41:19,700 : INFO : extropies processed\n",
      "2020-12-23 02:41:19,701 : INFO : token count processed\n",
      "2020-12-23 02:41:19,702 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:19,703 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:19,703 : INFO : vocab #2480\n",
      "2020-12-23 02:41:19,704 : INFO : diff #set()\n",
      "2020-12-23 02:41:19,963 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:20,091 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.0638349112254852, 0.4845348794910198], [0.7465223670005798, 0.25347763], [2.9780948881692604, 1.3032820102692515], [6.130567275446907, 4.778624108914332, 6.259818148610087, 4.649373235751153, 0.12925087316317985, 1.4811940396957546]]\n",
      "2020-12-23 02:41:20,093 : INFO : Removed 10 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:20,094 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:20,095 : INFO : built Dictionary(98 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 229 corpus positions)\n",
      "2020-12-23 02:41:20,144 : INFO : token count processed\n",
      "2020-12-23 02:41:20,146 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:41:20,274 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:20,275 : INFO : entropies processed\n",
      "2020-12-23 02:41:20,275 : INFO : extropies processed\n",
      "2020-12-23 02:41:20,277 : INFO : token count processed\n",
      "2020-12-23 02:41:20,277 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:20,278 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:20,279 : INFO : vocab #2480\n",
      "2020-12-23 02:41:20,280 : INFO : diff #set()\n",
      "2020-12-23 02:41:20,538 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:20,666 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.062381716869204, 0.48487629221134126], [0.7435103356838226, 0.25648966], [2.771782221599798, 1.2525277862418243], [6.130567275446907, 4.773880192225086, 6.147496090233274, 4.756951377438719, 0.01692881478636643, 1.3736158980081878]]\n",
      "2020-12-23 02:41:20,669 : INFO : Removed 10 and 346 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:20,670 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:20,671 : INFO : built Dictionary(183 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 2054 corpus positions)\n",
      "2020-12-23 02:41:20,881 : INFO : token count processed\n",
      "2020-12-23 02:41:20,884 : INFO : frequencies processed\n",
      "2020-12-23 02:41:21,010 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:21,011 : INFO : entropies processed\n",
      "2020-12-23 02:41:21,012 : INFO : extropies processed\n",
      "2020-12-23 02:41:21,014 : INFO : token count processed\n",
      "2020-12-23 02:41:21,015 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:21,016 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:21,017 : INFO : vocab #2480\n",
      "2020-12-23 02:41:21,018 : INFO : diff #set()\n",
      "2020-12-23 02:41:21,274 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:21,402 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[0.9024018813353016, 0.5256512884113093], [0.598390132188797, 0.40160987], [4.973376627179274, 1.4116276874872296], [6.130567275446907, 6.620773041953877, 6.756149584399816, 5.9951907330009675, 0.6255823089529091, 0.13537654244593966]]\n",
      "2020-12-23 02:41:21,404 : INFO : Removed 10 and 132 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:21,405 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:21,406 : INFO : built Dictionary(139 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 617 corpus positions)\n",
      "2020-12-23 02:41:21,515 : INFO : token count processed\n",
      "2020-12-23 02:41:21,517 : INFO : frequencies processed\n",
      "2020-12-23 02:41:21,646 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:21,647 : INFO : entropies processed\n",
      "2020-12-23 02:41:21,648 : INFO : extropies processed\n",
      "2020-12-23 02:41:21,649 : INFO : token count processed\n",
      "2020-12-23 02:41:21,650 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:21,651 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:21,652 : INFO : vocab #2480\n",
      "2020-12-23 02:41:21,653 : INFO : diff #set()\n",
      "2020-12-23 02:41:21,910 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:22,038 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.0370866344271832, 0.49089713863897305], [0.7109765410423279, 0.28902346], [2.8978493842644677, 1.2928184068769577], [6.130567275446907, 5.828370634755606, 6.350299132852705, 5.608638777349808, 0.21973185740579826, 0.5219284980970995]]\n",
      "2020-12-23 02:41:22,040 : INFO : Removed 10 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:22,041 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:22,042 : INFO : built Dictionary(132 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 380 corpus positions)\n",
      "2020-12-23 02:41:22,147 : INFO : token count processed\n",
      "2020-12-23 02:41:22,150 : INFO : frequencies processed\n",
      "2020-12-23 02:41:22,280 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:22,281 : INFO : entropies processed\n",
      "2020-12-23 02:41:22,284 : INFO : extropies processed\n",
      "2020-12-23 02:41:22,285 : INFO : token count processed\n",
      "2020-12-23 02:41:22,285 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:22,286 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:22,286 : INFO : vocab #2480\n",
      "2020-12-23 02:41:22,287 : INFO : diff #set()\n",
      "2020-12-23 02:41:22,543 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:22,670 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.0117284549583478, 0.49708498059729667], [0.6769754886627197, 0.3230245], [3.8048325223707566, 1.3631677762526997], [6.130567275446907, 5.774409284925443, 6.4867595626569265, 5.418216997715424, 0.3561922872100194, 0.7123502777314838]]\n",
      "2020-12-23 02:41:22,673 : INFO : Removed 10 and 51 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:22,674 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:22,676 : INFO : built Dictionary(146 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 429 corpus positions)\n",
      "2020-12-23 02:41:22,815 : INFO : token count processed\n",
      "2020-12-23 02:41:22,820 : INFO : frequencies processed\n",
      "2020-12-23 02:41:22,950 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:22,950 : INFO : entropies processed\n",
      "2020-12-23 02:41:22,951 : INFO : extropies processed\n",
      "2020-12-23 02:41:22,951 : INFO : token count processed\n",
      "2020-12-23 02:41:22,952 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:22,953 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:22,953 : INFO : vocab #2480\n",
      "2020-12-23 02:41:22,954 : INFO : diff #set()\n",
      "2020-12-23 02:41:23,212 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:23,340 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.023262386544126, 0.49425126797719504], [0.6904554069042206, 0.3095446], [3.4952365449960037, 1.3399856858001622], [6.130567275446907, 5.977819040873918, 6.605817347806957, 5.502568968513868, 0.47525007236004946, 0.6279983069330388]]\n",
      "2020-12-23 02:41:23,343 : INFO : Removed 10 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:23,343 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:23,344 : INFO : built Dictionary(128 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 376 corpus positions)\n",
      "2020-12-23 02:41:23,443 : INFO : token count processed\n",
      "2020-12-23 02:41:23,446 : INFO : frequencies processed\n",
      "2020-12-23 02:41:23,575 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:23,576 : INFO : entropies processed\n",
      "2020-12-23 02:41:23,577 : INFO : extropies processed\n",
      "2020-12-23 02:41:23,578 : INFO : token count processed\n",
      "2020-12-23 02:41:23,579 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:23,580 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:23,581 : INFO : vocab #2480\n",
      "2020-12-23 02:41:23,582 : INFO : diff #set()\n",
      "2020-12-23 02:41:23,839 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:23,966 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.0097633480040855, 0.49757102048512786], [0.646233469247818, 0.35376653], [3.4082800232568733, 1.3371335272078064], [6.130567275446907, 5.901812829596593, 6.586106098202107, 5.446274006841394, 0.45553882275519975, 0.6842932686055141]]\n",
      "2020-12-23 02:41:23,969 : INFO : Removed 10 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:23,970 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:23,972 : INFO : built Dictionary(132 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 376 corpus positions)\n",
      "2020-12-23 02:41:24,083 : INFO : token count processed\n",
      "2020-12-23 02:41:24,088 : INFO : frequencies processed\n",
      "2020-12-23 02:41:24,216 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:24,217 : INFO : entropies processed\n",
      "2020-12-23 02:41:24,217 : INFO : extropies processed\n",
      "2020-12-23 02:41:24,218 : INFO : token count processed\n",
      "2020-12-23 02:41:24,219 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:24,220 : INFO : alphabet_target #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:41:24,221 : INFO : vocab #2480\n",
      "2020-12-23 02:41:24,222 : INFO : diff #set()\n",
      "2020-12-23 02:41:24,490 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:24,618 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.0764333894785922, 0.4815950297597109], [0.7622981071472168, 0.2377019], [3.1083757000539407, 1.3012595652728154], [6.130567275446907, 5.643202320803383, 6.446041237711114, 5.327728358539176, 0.3154739622642069, 0.8028389169077315]]\n",
      "2020-12-23 02:41:24,621 : INFO : Removed 10 and 30 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:24,622 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:24,623 : INFO : built Dictionary(145 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 410 corpus positions)\n",
      "2020-12-23 02:41:24,757 : INFO : token count processed\n",
      "2020-12-23 02:41:24,759 : INFO : frequencies processed\n",
      "2020-12-23 02:41:24,887 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:24,887 : INFO : entropies processed\n",
      "2020-12-23 02:41:24,888 : INFO : extropies processed\n",
      "2020-12-23 02:41:24,889 : INFO : token count processed\n",
      "2020-12-23 02:41:24,890 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:24,891 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:24,891 : INFO : vocab #2480\n",
      "2020-12-23 02:41:24,892 : INFO : diff #set()\n",
      "2020-12-23 02:41:25,151 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:25,279 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.0619462161662194, 0.4849787022375889], [0.7197576463222504, 0.28024235], [3.221097250057956, 1.326818270405738], [6.130567275446907, 5.925214310725336, 6.612481689180445, 5.443299896991798, 0.4819144137335378, 0.6872673784551093]]\n",
      "2020-12-23 02:41:25,282 : INFO : Removed 10 and 293 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:25,283 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:25,284 : INFO : built Dictionary(191 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 1805 corpus positions)\n",
      "2020-12-23 02:41:25,511 : INFO : token count processed\n",
      "2020-12-23 02:41:25,514 : INFO : frequencies processed\n",
      "2020-12-23 02:41:25,640 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:25,640 : INFO : entropies processed\n",
      "2020-12-23 02:41:25,641 : INFO : extropies processed\n",
      "2020-12-23 02:41:25,643 : INFO : token count processed\n",
      "2020-12-23 02:41:25,644 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:25,645 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:25,646 : INFO : vocab #2480\n",
      "2020-12-23 02:41:25,647 : INFO : diff #set()\n",
      "2020-12-23 02:41:25,913 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:26,041 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[0.8906875822208785, 0.5289081122674744], [0.5872830748558044, 0.41271693], [5.0967872688991465, 1.4145922753511695], [6.130567275446907, 6.551685682764175, 6.697200357923494, 5.985052600287588, 0.5666330824765868, 0.14551467515931904]]\n",
      "2020-12-23 02:41:26,044 : INFO : Removed 10 and 88 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:26,044 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:26,047 : INFO : built Dictionary(182 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 646 corpus positions)\n",
      "2020-12-23 02:41:26,255 : INFO : token count processed\n",
      "2020-12-23 02:41:26,257 : INFO : frequencies processed\n",
      "2020-12-23 02:41:26,384 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:26,385 : INFO : entropies processed\n",
      "2020-12-23 02:41:26,388 : INFO : extropies processed\n",
      "2020-12-23 02:41:26,389 : INFO : token count processed\n",
      "2020-12-23 02:41:26,389 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:26,390 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:26,391 : INFO : vocab #2480\n",
      "2020-12-23 02:41:26,391 : INFO : diff #set()\n",
      "2020-12-23 02:41:26,645 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:26,772 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.006472556551446, 0.498387080717772], [0.6628981530666351, 0.33710185], [4.290717099931111, 1.3902954899502997], [6.130567275446907, 6.642985062562557, 7.004148636598488, 5.769403701410976, 0.8735813611515812, 0.36116357403593113]]\n",
      "2020-12-23 02:41:26,775 : INFO : Removed 10 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:26,776 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:26,777 : INFO : built Dictionary(111 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 241 corpus positions)\n",
      "2020-12-23 02:41:26,854 : INFO : token count processed\n",
      "2020-12-23 02:41:26,860 : INFO : frequencies processed\n",
      "2020-12-23 02:41:26,991 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:26,992 : INFO : entropies processed\n",
      "2020-12-23 02:41:26,992 : INFO : extropies processed\n",
      "2020-12-23 02:41:26,993 : INFO : token count processed\n",
      "2020-12-23 02:41:26,994 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:26,995 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:26,996 : INFO : vocab #2480\n",
      "2020-12-23 02:41:26,997 : INFO : diff #set()\n",
      "2020-12-23 02:41:27,266 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:27,393 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.0764007083347955, 0.4816026097399893], [0.7069624960422516, 0.2930375], [2.521782221599798, 1.2404964061729615], [6.130567275446907, 5.2461980344571995, 6.40719379768694, 4.969571512217167, 0.2766265222400328, 1.1609957632297405]]\n",
      "2020-12-23 02:41:27,396 : INFO : Removed 10 and 23 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:27,397 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:27,399 : INFO : built Dictionary(128 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 301 corpus positions)\n",
      "2020-12-23 02:41:27,503 : INFO : token count processed\n",
      "2020-12-23 02:41:27,507 : INFO : frequencies processed\n",
      "2020-12-23 02:41:27,636 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:27,637 : INFO : entropies processed\n",
      "2020-12-23 02:41:27,637 : INFO : extropies processed\n",
      "2020-12-23 02:41:27,639 : INFO : token count processed\n",
      "2020-12-23 02:41:27,639 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:27,640 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:27,641 : INFO : vocab #2480\n",
      "2020-12-23 02:41:27,642 : INFO : diff #set()\n",
      "2020-12-23 02:41:27,908 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:28,036 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/test_auth_utility.py')[[1.0680784633292588, 0.4835406478679576], [0.7282074391841888, 0.27179256], [3.503258334775645, 1.362897132787518], [6.130567275446907, 5.903090303960449, 6.710351172618779, 5.3233064067885785, 0.5797838971718718, 0.8072608686583296]]\n",
      "2020-12-23 02:41:28,039 : INFO : Removed 10 and 442 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:28,040 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:28,041 : INFO : built Dictionary(165 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 1300 corpus positions)\n",
      "2020-12-23 02:41:28,196 : INFO : token count processed\n",
      "2020-12-23 02:41:28,199 : INFO : frequencies processed\n",
      "2020-12-23 02:41:28,326 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:28,327 : INFO : entropies processed\n",
      "2020-12-23 02:41:28,328 : INFO : extropies processed\n",
      "2020-12-23 02:41:28,329 : INFO : token count processed\n",
      "2020-12-23 02:41:28,330 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:28,331 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:28,332 : INFO : vocab #2480\n",
      "2020-12-23 02:41:28,333 : INFO : diff #set()\n",
      "2020-12-23 02:41:28,591 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:28,718 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.152695751072268, 0.46453382903826296], [0.8686713427305222, 0.13132866], [2.8978493842644677, 1.2928184068769577], [6.130567275446907, 6.16659449033757, 6.451400710721325, 5.845761055063152, 0.32083343527441777, 0.28480622038375536]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:41:28,721 : INFO : Removed 10 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:28,722 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:28,724 : INFO : built Dictionary(126 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 335 corpus positions)\n",
      "2020-12-23 02:41:28,822 : INFO : token count processed\n",
      "2020-12-23 02:41:28,825 : INFO : frequencies processed\n",
      "2020-12-23 02:41:28,956 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:28,956 : INFO : entropies processed\n",
      "2020-12-23 02:41:28,957 : INFO : extropies processed\n",
      "2020-12-23 02:41:28,958 : INFO : token count processed\n",
      "2020-12-23 02:41:28,959 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:28,960 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:28,961 : INFO : vocab #2480\n",
      "2020-12-23 02:41:28,962 : INFO : diff #set()\n",
      "2020-12-23 02:41:29,219 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:29,347 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.090354074179873, 0.4783878541688392], [0.7571056932210922, 0.2428943], [2.6961990498148554, 1.2628792701715872], [6.130567275446907, 5.906856253399655, 6.67457335913423, 5.362850169712333, 0.5440060836873233, 0.7677171057345751]]\n",
      "2020-12-23 02:41:29,350 : INFO : Removed 10 and 68 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:29,351 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:29,352 : INFO : built Dictionary(138 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 334 corpus positions)\n",
      "2020-12-23 02:41:29,460 : INFO : token count processed\n",
      "2020-12-23 02:41:29,462 : INFO : frequencies processed\n",
      "2020-12-23 02:41:29,591 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:29,592 : INFO : entropies processed\n",
      "2020-12-23 02:41:29,594 : INFO : extropies processed\n",
      "2020-12-23 02:41:29,595 : INFO : token count processed\n",
      "2020-12-23 02:41:29,595 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:29,596 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:29,596 : INFO : vocab #2480\n",
      "2020-12-23 02:41:29,597 : INFO : diff #set()\n",
      "2020-12-23 02:41:29,852 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:29,980 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.0474879824796857, 0.48840335501696736], [0.6848832964897156, 0.3151167], [2.771782221599798, 1.2525277862418243], [6.130567275446907, 5.965115449163356, 6.704573562324833, 5.391109162285431, 0.5740062868779257, 0.7394581131614766]]\n",
      "2020-12-23 02:41:29,982 : INFO : Removed 10 and 13 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:29,983 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:29,984 : INFO : built Dictionary(142 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 448 corpus positions)\n",
      "2020-12-23 02:41:30,110 : INFO : token count processed\n",
      "2020-12-23 02:41:30,112 : INFO : frequencies processed\n",
      "2020-12-23 02:41:30,239 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:30,240 : INFO : entropies processed\n",
      "2020-12-23 02:41:30,240 : INFO : extropies processed\n",
      "2020-12-23 02:41:30,241 : INFO : token count processed\n",
      "2020-12-23 02:41:30,242 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:30,243 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:30,243 : INFO : vocab #2480\n",
      "2020-12-23 02:41:30,244 : INFO : diff #set()\n",
      "2020-12-23 02:41:30,502 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:30,629 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.0712957363037996, 0.48278958068271177], [0.7363177835941315, 0.26368222], [3.239353872167201, 1.3140763050730895], [6.130567275446907, 5.791362404253194, 6.496378165993917, 5.425551513706184, 0.3658108905470101, 0.7050157617407233]]\n",
      "2020-12-23 02:41:30,631 : INFO : Removed 10 and 41 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:30,632 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:30,634 : INFO : built Dictionary(131 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 460 corpus positions)\n",
      "2020-12-23 02:41:30,752 : INFO : token count processed\n",
      "2020-12-23 02:41:30,755 : INFO : frequencies processed\n",
      "2020-12-23 02:41:30,885 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:30,885 : INFO : entropies processed\n",
      "2020-12-23 02:41:30,886 : INFO : extropies processed\n",
      "2020-12-23 02:41:30,887 : INFO : token count processed\n",
      "2020-12-23 02:41:30,888 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:30,889 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:30,890 : INFO : vocab #2480\n",
      "2020-12-23 02:41:30,891 : INFO : diff #set()\n",
      "2020-12-23 02:41:31,150 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:31,277 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.0190779944028547, 0.49527556774534187], [0.6114147305488586, 0.38858527], [3.38436414343715, 1.331437979034141], [6.130567275446907, 5.651670454631116, 6.329863918417642, 5.452373811660381, 0.1992966429707348, 0.6781934637865259]]\n",
      "2020-12-23 02:41:31,280 : INFO : Removed 10 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:31,280 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:31,282 : INFO : built Dictionary(102 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 170 corpus positions)\n",
      "2020-12-23 02:41:31,346 : INFO : token count processed\n",
      "2020-12-23 02:41:31,350 : INFO : frequencies processed\n",
      "2020-12-23 02:41:31,478 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:31,479 : INFO : entropies processed\n",
      "2020-12-23 02:41:31,480 : INFO : extropies processed\n",
      "2020-12-23 02:41:31,481 : INFO : token count processed\n",
      "2020-12-23 02:41:31,482 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:31,483 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:31,484 : INFO : vocab #2480\n",
      "2020-12-23 02:41:31,485 : INFO : diff #set()\n",
      "2020-12-23 02:41:31,745 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:31,874 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.1092498051522188, 0.4741022128139216], [0.7400577664375305, 0.25994223], [2.2810361125534233, 1.2263316211630273], [6.130567275446907, 4.8226207261920235, 6.421005446685129, 4.532182554953803, 0.2904381712382218, 1.5983847204931054]]\n",
      "2020-12-23 02:41:31,876 : INFO : Removed 10 and 50 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:31,877 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:31,878 : INFO : built Dictionary(142 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 340 corpus positions)\n",
      "2020-12-23 02:41:32,007 : INFO : token count processed\n",
      "2020-12-23 02:41:32,012 : INFO : frequencies processed\n",
      "2020-12-23 02:41:32,141 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:32,141 : INFO : entropies processed\n",
      "2020-12-23 02:41:32,142 : INFO : extropies processed\n",
      "2020-12-23 02:41:32,143 : INFO : token count processed\n",
      "2020-12-23 02:41:32,144 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:32,144 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:32,145 : INFO : vocab #2480\n",
      "2020-12-23 02:41:32,146 : INFO : diff #set()\n",
      "2020-12-23 02:41:32,406 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:32,533 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.0908370035447839, 0.4782773589259279], [0.7240416407585144, 0.27595836], [3.2210972500579564, 1.3268182704057379], [6.130567275446907, 6.24862851613934, 6.911123032551763, 5.468072759034485, 0.7805557571048558, 0.6624945164124227]]\n",
      "2020-12-23 02:41:32,536 : INFO : Removed 10 and 32 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:32,537 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:32,539 : INFO : built Dictionary(139 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 456 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:41:32,670 : INFO : token count processed\n",
      "2020-12-23 02:41:32,673 : INFO : frequencies processed\n",
      "2020-12-23 02:41:32,803 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:32,804 : INFO : entropies processed\n",
      "2020-12-23 02:41:32,804 : INFO : extropies processed\n",
      "2020-12-23 02:41:32,806 : INFO : token count processed\n",
      "2020-12-23 02:41:32,807 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:32,808 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:32,809 : INFO : vocab #2480\n",
      "2020-12-23 02:41:32,810 : INFO : diff #set()\n",
      "2020-12-23 02:41:33,078 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:33,205 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.0367881125043927, 0.49096908699571135], [0.7381086051464081, 0.2618914], [3.4752732343462402, 1.334561876809589], [6.130567275446907, 5.850156917433494, 6.506745172283841, 5.473979020596561, 0.3761778968369338, 0.6565882548503472]]\n",
      "2020-12-23 02:41:33,208 : INFO : Removed 10 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:33,209 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:33,210 : INFO : built Dictionary(132 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 454 corpus positions)\n",
      "2020-12-23 02:41:33,316 : INFO : token count processed\n",
      "2020-12-23 02:41:33,319 : INFO : frequencies processed\n",
      "2020-12-23 02:41:33,447 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:33,448 : INFO : entropies processed\n",
      "2020-12-23 02:41:33,449 : INFO : extropies processed\n",
      "2020-12-23 02:41:33,450 : INFO : token count processed\n",
      "2020-12-23 02:41:33,451 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:33,453 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:33,454 : INFO : vocab #2480\n",
      "2020-12-23 02:41:33,455 : INFO : diff #set()\n",
      "2020-12-23 02:41:33,716 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:33,844 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.0057810993279244, 0.4985588907658315], [0.6071562170982361, 0.39284378], [3.6977968115985957, 1.354103026519888], [6.130567275446907, 5.6831976040360095, 6.346512757064931, 5.467252122417986, 0.21594548161802418, 0.6633151530289219]]\n",
      "2020-12-23 02:41:33,847 : INFO : Removed 10 and 52 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:33,848 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:33,849 : INFO : built Dictionary(127 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 432 corpus positions)\n",
      "2020-12-23 02:41:33,947 : INFO : token count processed\n",
      "2020-12-23 02:41:33,952 : INFO : frequencies processed\n",
      "2020-12-23 02:41:34,087 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:34,087 : INFO : entropies processed\n",
      "2020-12-23 02:41:34,088 : INFO : extropies processed\n",
      "2020-12-23 02:41:34,089 : INFO : token count processed\n",
      "2020-12-23 02:41:34,090 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:34,091 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:34,091 : INFO : vocab #2480\n",
      "2020-12-23 02:41:34,092 : INFO : diff #set()\n",
      "2020-12-23 02:41:34,354 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:34,482 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.0479419145672404, 0.4882950990391319], [0.6805132329463959, 0.31948677], [2.744842531753244, 1.2767283831335483], [6.130567275446907, 5.749308601266266, 6.444310007291593, 5.4355658694215805, 0.3137427318446857, 0.6950014060253267]]\n",
      "2020-12-23 02:41:34,484 : INFO : Removed 10 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:34,485 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:34,486 : INFO : built Dictionary(121 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 313 corpus positions)\n",
      "2020-12-23 02:41:34,573 : INFO : token count processed\n",
      "2020-12-23 02:41:34,576 : INFO : frequencies processed\n",
      "2020-12-23 02:41:34,701 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:34,702 : INFO : entropies processed\n",
      "2020-12-23 02:41:34,702 : INFO : extropies processed\n",
      "2020-12-23 02:41:34,703 : INFO : token count processed\n",
      "2020-12-23 02:41:34,704 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:34,705 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:34,705 : INFO : vocab #2480\n",
      "2020-12-23 02:41:34,706 : INFO : diff #set()\n",
      "2020-12-23 02:41:34,963 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:35,091 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.0814697309081516, 0.48042975843021124], [0.7324268221855164, 0.26757318], [2.403677461028802, 1.1949488254573275], [6.130567275446907, 5.015422548793484, 6.143717784717085, 5.002272039523307, 0.01315050927017758, 1.1282952359236011]]\n",
      "2020-12-23 02:41:35,093 : INFO : Removed 10 and 81 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:35,094 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:35,096 : INFO : built Dictionary(143 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 439 corpus positions)\n",
      "2020-12-23 02:41:35,233 : INFO : token count processed\n",
      "2020-12-23 02:41:35,236 : INFO : frequencies processed\n",
      "2020-12-23 02:41:35,362 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:35,363 : INFO : entropies processed\n",
      "2020-12-23 02:41:35,364 : INFO : extropies processed\n",
      "2020-12-23 02:41:35,365 : INFO : token count processed\n",
      "2020-12-23 02:41:35,366 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:35,367 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:35,368 : INFO : vocab #2480\n",
      "2020-12-23 02:41:35,369 : INFO : diff #set()\n",
      "2020-12-23 02:41:35,628 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:35,756 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.0274734306507969, 0.49322471253248973], [0.7195025682449341, 0.28049743], [3.4952365449960037, 1.3399856858001622], [6.130567275446907, 6.030001281822029, 6.6231678433658105, 5.537400713903126, 0.4926005679189034, 0.5931665615437813]]\n",
      "2020-12-23 02:41:35,758 : INFO : Removed 10 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:35,759 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:35,761 : INFO : built Dictionary(138 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 402 corpus positions)\n",
      "2020-12-23 02:41:35,883 : INFO : token count processed\n",
      "2020-12-23 02:41:35,888 : INFO : frequencies processed\n",
      "2020-12-23 02:41:36,017 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:36,018 : INFO : entropies processed\n",
      "2020-12-23 02:41:36,018 : INFO : extropies processed\n",
      "2020-12-23 02:41:36,020 : INFO : token count processed\n",
      "2020-12-23 02:41:36,020 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:36,021 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:36,022 : INFO : vocab #2480\n",
      "2020-12-23 02:41:36,023 : INFO : diff #set()\n",
      "2020-12-23 02:41:36,281 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:36,408 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.105007332721393, 0.4750577275696143], [0.7427363693714142, 0.25726363], [2.521782221599798, 1.2404964061729615], [6.130567275446907, 5.9537092545441395, 6.663825652082162, 5.420450877908886, 0.5332583766352545, 0.7101163975380222]]\n",
      "2020-12-23 02:41:36,411 : INFO : Removed 10 and 90 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:36,412 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:36,413 : INFO : built Dictionary(147 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 403 corpus positions)\n",
      "2020-12-23 02:41:36,534 : INFO : token count processed\n",
      "2020-12-23 02:41:36,537 : INFO : frequencies processed\n",
      "2020-12-23 02:41:36,665 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:36,666 : INFO : entropies processed\n",
      "2020-12-23 02:41:36,667 : INFO : extropies processed\n",
      "2020-12-23 02:41:36,668 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:41:36,670 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:36,671 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:36,672 : INFO : vocab #2480\n",
      "2020-12-23 02:41:36,673 : INFO : diff #set()\n",
      "2020-12-23 02:41:36,934 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:37,062 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.0470969132582326, 0.488496657644002], [0.6885788440704346, 0.31142116], [2.771782221599798, 1.2525277862418243], [6.130567275446907, 6.184756445474906, 6.804521589386617, 5.510802131535197, 0.67395431393971, 0.6197651439117111]]\n",
      "2020-12-23 02:41:37,064 : INFO : Removed 10 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:37,065 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:37,067 : INFO : built Dictionary(152 unique tokens: ['add', 'addit', 'analysi', 'artifact', 'asset']...) from 2 documents (total 516 corpus positions)\n",
      "2020-12-23 02:41:37,214 : INFO : token count processed\n",
      "2020-12-23 02:41:37,217 : INFO : frequencies processed\n",
      "2020-12-23 02:41:37,342 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:37,343 : INFO : entropies processed\n",
      "2020-12-23 02:41:37,344 : INFO : extropies processed\n",
      "2020-12-23 02:41:37,345 : INFO : token count processed\n",
      "2020-12-23 02:41:37,346 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:37,347 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:37,348 : INFO : vocab #2480\n",
      "2020-12-23 02:41:37,349 : INFO : diff #set()\n",
      "2020-12-23 02:41:37,607 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:37,735 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.0432616628697353, 0.4894135774052123], [0.7174924612045288, 0.28250754], [3.38436414343715, 1.331437979034141], [6.130567275446907, 6.212221456585881, 6.74965442462034, 5.593134307412447, 0.6190871491734331, 0.5374329680344596]]\n",
      "2020-12-23 02:41:37,738 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:37,739 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:37,740 : INFO : built Dictionary(152 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 472 corpus positions)\n",
      "2020-12-23 02:41:37,851 : INFO : token count processed\n",
      "2020-12-23 02:41:37,853 : INFO : frequencies processed\n",
      "2020-12-23 02:41:37,982 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:37,983 : INFO : entropies processed\n",
      "2020-12-23 02:41:37,983 : INFO : extropies processed\n",
      "2020-12-23 02:41:37,985 : INFO : token count processed\n",
      "2020-12-23 02:41:37,986 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:37,987 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:37,987 : INFO : vocab #2480\n",
      "2020-12-23 02:41:37,988 : INFO : diff #set()\n",
      "2020-12-23 02:41:38,257 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:38,387 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.1251663331260946, 0.4705514031595878], [0.7304078936576843, 0.2695921], [2.625814583693911, 1.3020314880872434], [5.310603774467064, 6.301552355933639, 6.66350872908173, 4.948647401318972, 1.352904954614666, 0.36195637314809126]]\n",
      "2020-12-23 02:41:38,390 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:38,391 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:38,392 : INFO : built Dictionary(189 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 708 corpus positions)\n",
      "2020-12-23 02:41:38,559 : INFO : token count processed\n",
      "2020-12-23 02:41:38,561 : INFO : frequencies processed\n",
      "2020-12-23 02:41:38,688 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:38,688 : INFO : entropies processed\n",
      "2020-12-23 02:41:38,689 : INFO : extropies processed\n",
      "2020-12-23 02:41:38,690 : INFO : token count processed\n",
      "2020-12-23 02:41:38,691 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:38,692 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:38,693 : INFO : vocab #2480\n",
      "2020-12-23 02:41:38,694 : INFO : diff #set()\n",
      "2020-12-23 02:41:38,948 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:39,076 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.1306463266546496, 0.4693411513163288], [0.7043557465076447, 0.29564425], [2.8453509366224363, 1.3210203571681218], [5.310603774467064, 6.739005504021667, 6.9936170520262575, 5.0559922264624735, 1.6830132775591933, 0.25461154800459074]]\n",
      "2020-12-23 02:41:39,078 : INFO : Removed 0 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:39,079 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:39,080 : INFO : built Dictionary(136 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 559 corpus positions)\n",
      "2020-12-23 02:41:39,177 : INFO : token count processed\n",
      "2020-12-23 02:41:39,179 : INFO : frequencies processed\n",
      "2020-12-23 02:41:39,314 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:39,314 : INFO : entropies processed\n",
      "2020-12-23 02:41:39,315 : INFO : extropies processed\n",
      "2020-12-23 02:41:39,316 : INFO : token count processed\n",
      "2020-12-23 02:41:39,317 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:39,318 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:39,319 : INFO : vocab #2480\n",
      "2020-12-23 02:41:39,320 : INFO : diff #set()\n",
      "2020-12-23 02:41:39,579 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:39,707 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.1542160312878524, 0.46420599674126983], [0.8334151655435562, 0.16658483], [2.170950594454669, 1.2515504860726414], [5.310603774467064, 5.870833373337847, 6.226499993510962, 4.954937154293948, 0.9158962190438977, 0.3556666201731149]]\n",
      "2020-12-23 02:41:39,709 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:39,710 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:39,712 : INFO : built Dictionary(99 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 242 corpus positions)\n",
      "2020-12-23 02:41:39,774 : INFO : token count processed\n",
      "2020-12-23 02:41:39,776 : INFO : frequencies processed\n",
      "2020-12-23 02:41:39,905 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:39,905 : INFO : entropies processed\n",
      "2020-12-23 02:41:39,906 : INFO : extropies processed\n",
      "2020-12-23 02:41:39,907 : INFO : token count processed\n",
      "2020-12-23 02:41:39,908 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:39,908 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:39,909 : INFO : vocab #2480\n",
      "2020-12-23 02:41:39,910 : INFO : diff #set()\n",
      "2020-12-23 02:41:40,171 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:40,299 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.1719743335233155, 0.46041059719975064], [0.8629540503025055, 0.13704595], [0.9182958340544896, 0.9182958340544896], [5.310603774467064, 5.371881234145534, 6.141449549039017, 4.541035459573582, 0.8308457745719524, 0.7695683148934824]]\n",
      "2020-12-23 02:41:40,301 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:40,302 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:40,304 : INFO : built Dictionary(89 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 194 corpus positions)\n",
      "2020-12-23 02:41:40,351 : INFO : token count processed\n",
      "2020-12-23 02:41:40,353 : INFO : frequencies processed\n",
      "2020-12-23 02:41:40,487 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:40,488 : INFO : entropies processed\n",
      "2020-12-23 02:41:40,489 : INFO : extropies processed\n",
      "2020-12-23 02:41:40,490 : INFO : token count processed\n",
      "2020-12-23 02:41:40,491 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:40,492 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:40,492 : INFO : vocab #2480\n",
      "2020-12-23 02:41:40,493 : INFO : diff #set()\n",
      "2020-12-23 02:41:40,750 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:40,877 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.168816262019235, 0.46108101341372687], [0.8819161877036095, 0.11808381], [1.5, 1.1225562489182657], [5.310603774467064, 4.85108279267097, 5.871055651529458, 4.290630915608576, 0.5604518770623939, 1.0199728588584884]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:41:40,879 : INFO : Removed 0 and 127 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:40,880 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:40,881 : INFO : built Dictionary(125 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 467 corpus positions)\n",
      "2020-12-23 02:41:40,964 : INFO : token count processed\n",
      "2020-12-23 02:41:40,967 : INFO : frequencies processed\n",
      "2020-12-23 02:41:41,099 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:41,100 : INFO : entropies processed\n",
      "2020-12-23 02:41:41,101 : INFO : extropies processed\n",
      "2020-12-23 02:41:41,102 : INFO : token count processed\n",
      "2020-12-23 02:41:41,104 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:41,105 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:41,106 : INFO : vocab #2480\n",
      "2020-12-23 02:41:41,107 : INFO : diff #set()\n",
      "2020-12-23 02:41:41,374 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:41,507 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.1468182537959934, 0.4658056163961736], [0.8360424339771271, 0.16395757], [2.6635327548042547, 1.307883000782319], [5.310603774467064, 6.139571208108155, 6.492349250696228, 4.957825731878992, 1.181745476229164, 0.3527780425880733]]\n",
      "2020-12-23 02:41:41,509 : INFO : Removed 0 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:41,510 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:41,512 : INFO : built Dictionary(114 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 477 corpus positions)\n",
      "2020-12-23 02:41:41,584 : INFO : token count processed\n",
      "2020-12-23 02:41:41,603 : INFO : frequencies processed\n",
      "2020-12-23 02:41:41,736 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:41,736 : INFO : entropies processed\n",
      "2020-12-23 02:41:41,737 : INFO : extropies processed\n",
      "2020-12-23 02:41:41,738 : INFO : token count processed\n",
      "2020-12-23 02:41:41,739 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:41,740 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:41,741 : INFO : vocab #2480\n",
      "2020-12-23 02:41:41,742 : INFO : diff #set()\n",
      "2020-12-23 02:41:42,011 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:42,139 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.1578527290787213, 0.46342365562034554], [0.749984472990036, 0.25001553], [1.7527152789797045, 1.1631732844642984], [5.310603774467064, 5.609710627339259, 6.067769828298548, 4.852544573507775, 0.7571660538314839, 0.45805920095928876]]\n",
      "2020-12-23 02:41:42,142 : INFO : Removed 0 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:42,143 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:42,145 : INFO : built Dictionary(200 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 1137 corpus positions)\n",
      "2020-12-23 02:41:42,311 : INFO : token count processed\n",
      "2020-12-23 02:41:42,313 : INFO : frequencies processed\n",
      "2020-12-23 02:41:42,441 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:42,441 : INFO : entropies processed\n",
      "2020-12-23 02:41:42,442 : INFO : extropies processed\n",
      "2020-12-23 02:41:42,444 : INFO : token count processed\n",
      "2020-12-23 02:41:42,445 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:42,446 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:42,447 : INFO : vocab #2480\n",
      "2020-12-23 02:41:42,449 : INFO : diff #set()\n",
      "2020-12-23 02:41:42,705 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:42,832 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.1541131842169792, 0.4642281600274873], [0.7851119637489319, 0.21488804], [3.2086949695628415, 1.3559991902913526], [5.310603774467064, 7.2441902753576075, 7.375403454659063, 5.179390595165609, 2.0647996801919986, 0.13121317930145526]]\n",
      "2020-12-23 02:41:42,835 : INFO : Removed 0 and 136 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:42,836 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:42,838 : INFO : built Dictionary(165 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 744 corpus positions)\n",
      "2020-12-23 02:41:42,967 : INFO : token count processed\n",
      "2020-12-23 02:41:42,973 : INFO : frequencies processed\n",
      "2020-12-23 02:41:43,099 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:43,100 : INFO : entropies processed\n",
      "2020-12-23 02:41:43,101 : INFO : extropies processed\n",
      "2020-12-23 02:41:43,102 : INFO : token count processed\n",
      "2020-12-23 02:41:43,103 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:43,103 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:43,104 : INFO : vocab #2480\n",
      "2020-12-23 02:41:43,105 : INFO : diff #set()\n",
      "2020-12-23 02:41:43,366 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:43,494 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.1619776915174154, 0.4625394627907264], [0.7499873340129852, 0.25001267], [2.5503407095463886, 1.2768429653008606], [5.310603774467064, 6.2567074920449475, 6.5345369083947356, 5.032774358117275, 1.2239331339276713, 0.2778294163497881]]\n",
      "2020-12-23 02:41:43,497 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:43,498 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:43,499 : INFO : built Dictionary(113 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 254 corpus positions)\n",
      "2020-12-23 02:41:43,563 : INFO : token count processed\n",
      "2020-12-23 02:41:43,565 : INFO : frequencies processed\n",
      "2020-12-23 02:41:43,694 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:43,695 : INFO : entropies processed\n",
      "2020-12-23 02:41:43,696 : INFO : extropies processed\n",
      "2020-12-23 02:41:43,697 : INFO : token count processed\n",
      "2020-12-23 02:41:43,698 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:43,699 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:43,700 : INFO : vocab #2480\n",
      "2020-12-23 02:41:43,700 : INFO : diff #set()\n",
      "2020-12-23 02:41:43,959 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:44,087 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.1400063603712565, 0.46728833078165066], [0.7334270179271698, 0.26657298], [1.9219280948873623, 1.2148067842293933], [5.310603774467064, 5.7680018917339435, 6.402868517798603, 4.675737148402404, 1.0922647433315387, 0.6348666260646594]]\n",
      "2020-12-23 02:41:44,089 : INFO : Removed 0 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:44,090 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:44,092 : INFO : built Dictionary(204 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 795 corpus positions)\n",
      "2020-12-23 02:41:44,271 : INFO : token count processed\n",
      "2020-12-23 02:41:44,276 : INFO : frequencies processed\n",
      "2020-12-23 02:41:44,405 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:44,405 : INFO : entropies processed\n",
      "2020-12-23 02:41:44,406 : INFO : extropies processed\n",
      "2020-12-23 02:41:44,407 : INFO : token count processed\n",
      "2020-12-23 02:41:44,408 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:44,409 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:44,409 : INFO : vocab #2480\n",
      "2020-12-23 02:41:44,410 : INFO : diff #set()\n",
      "2020-12-23 02:41:44,677 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:44,804 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.1194509596858773, 0.47182030583439855], [0.7087017595767975, 0.29129824], [3.373557262275186, 1.3659961090863855], [5.310603774467064, 6.846479111193757, 7.056728241139888, 5.100354644520932, 1.746124466672824, 0.21024912994613132]]\n",
      "2020-12-23 02:41:44,806 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:44,807 : INFO : built Dictionary(64 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 98 corpus positions)\n",
      "2020-12-23 02:41:44,827 : INFO : token count processed\n",
      "2020-12-23 02:41:44,829 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:41:44,957 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:44,958 : INFO : entropies processed\n",
      "2020-12-23 02:41:44,958 : INFO : extropies processed\n",
      "2020-12-23 02:41:44,960 : INFO : token count processed\n",
      "2020-12-23 02:41:44,960 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:44,961 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:44,962 : INFO : vocab #2480\n",
      "2020-12-23 02:41:44,964 : INFO : diff #set()\n",
      "2020-12-23 02:41:45,227 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:45,356 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2067834830620165, 0.453148216703368], [0.8995235711336136, 0.10047643], [0.0, 0.0], [5.310603774467064, 4.165013816065912, 5.82609543069537, 3.6495221598376073, 0.5154916562283054, 1.6610816146294578]]\n",
      "2020-12-23 02:41:45,358 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:45,359 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:45,360 : INFO : built Dictionary(89 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 205 corpus positions)\n",
      "2020-12-23 02:41:45,402 : INFO : token count processed\n",
      "2020-12-23 02:41:45,404 : INFO : frequencies processed\n",
      "2020-12-23 02:41:45,532 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:45,533 : INFO : entropies processed\n",
      "2020-12-23 02:41:45,534 : INFO : extropies processed\n",
      "2020-12-23 02:41:45,535 : INFO : token count processed\n",
      "2020-12-23 02:41:45,536 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:45,537 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:45,537 : INFO : vocab #2480\n",
      "2020-12-23 02:41:45,538 : INFO : diff #set()\n",
      "2020-12-23 02:41:45,796 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:45,925 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.1040583034354237, 0.4752720009551253], [0.7325253784656525, 0.26747462], [1.811278124459133, 1.18471876778999], [5.310603774467064, 5.449968864419248, 6.185590547053719, 4.574982091832593, 0.8749867725866549, 0.7356216826344708]]\n",
      "2020-12-23 02:41:45,927 : INFO : Removed 0 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:45,928 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:45,929 : INFO : built Dictionary(177 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 642 corpus positions)\n",
      "2020-12-23 02:41:46,074 : INFO : token count processed\n",
      "2020-12-23 02:41:46,077 : INFO : frequencies processed\n",
      "2020-12-23 02:41:46,207 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:46,207 : INFO : entropies processed\n",
      "2020-12-23 02:41:46,208 : INFO : extropies processed\n",
      "2020-12-23 02:41:46,209 : INFO : token count processed\n",
      "2020-12-23 02:41:46,210 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:46,211 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:46,212 : INFO : vocab #2480\n",
      "2020-12-23 02:41:46,213 : INFO : diff #set()\n",
      "2020-12-23 02:41:46,471 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:46,600 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.1203533185937906, 0.4716195132343301], [0.7433137595653534, 0.25668624], [3.0269868333592873, 1.3385887750658594], [5.310603774467064, 6.530294129310484, 6.791032894646065, 5.049865009131484, 1.480429120179001, 0.2607387653355815]]\n",
      "2020-12-23 02:41:46,602 : INFO : Removed 0 and 142 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:46,603 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:46,604 : INFO : built Dictionary(154 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 658 corpus positions)\n",
      "2020-12-23 02:41:46,718 : INFO : token count processed\n",
      "2020-12-23 02:41:46,720 : INFO : frequencies processed\n",
      "2020-12-23 02:41:46,851 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:46,852 : INFO : entropies processed\n",
      "2020-12-23 02:41:46,853 : INFO : extropies processed\n",
      "2020-12-23 02:41:46,855 : INFO : token count processed\n",
      "2020-12-23 02:41:46,856 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:46,857 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:46,858 : INFO : vocab #2480\n",
      "2020-12-23 02:41:46,859 : INFO : diff #set()\n",
      "2020-12-23 02:41:47,117 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:47,244 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.1832350841779915, 0.45803587861291145], [0.8553186804056168, 0.14468132], [2.2516291673878226, 1.2667563532600834], [5.310603774467064, 6.470272233491701, 6.757214217170146, 5.023661790788619, 1.446610442703082, 0.28694198367844503]]\n",
      "2020-12-23 02:41:47,247 : INFO : Removed 0 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:47,248 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:47,249 : INFO : built Dictionary(154 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 447 corpus positions)\n",
      "2020-12-23 02:41:47,359 : INFO : token count processed\n",
      "2020-12-23 02:41:47,362 : INFO : frequencies processed\n",
      "2020-12-23 02:41:47,490 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:47,491 : INFO : entropies processed\n",
      "2020-12-23 02:41:47,492 : INFO : extropies processed\n",
      "2020-12-23 02:41:47,493 : INFO : token count processed\n",
      "2020-12-23 02:41:47,494 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:47,495 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:47,496 : INFO : vocab #2480\n",
      "2020-12-23 02:41:47,497 : INFO : diff #set()\n",
      "2020-12-23 02:41:47,766 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:47,901 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.1538764298819133, 0.4642791880381109], [0.7383697926998138, 0.2616302], [2.521640636343318, 1.2998438251349493], [5.310603774467064, 6.550038223589686, 6.910020336158975, 4.950621661897775, 1.5994165616919105, 0.3599821125692886]]\n",
      "2020-12-23 02:41:47,904 : INFO : Removed 0 and 70 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:47,905 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:47,906 : INFO : built Dictionary(109 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 363 corpus positions)\n",
      "2020-12-23 02:41:47,975 : INFO : token count processed\n",
      "2020-12-23 02:41:47,978 : INFO : frequencies processed\n",
      "2020-12-23 02:41:48,104 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:48,105 : INFO : entropies processed\n",
      "2020-12-23 02:41:48,106 : INFO : extropies processed\n",
      "2020-12-23 02:41:48,107 : INFO : token count processed\n",
      "2020-12-23 02:41:48,108 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:48,109 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:48,110 : INFO : vocab #2480\n",
      "2020-12-23 02:41:48,111 : INFO : diff #set()\n",
      "2020-12-23 02:41:48,372 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:48,500 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.1523295488124947, 0.4646128658837259], [0.8193911164999008, 0.18060888], [2.4464393446710155, 1.2856945251022456], [5.310603774467064, 5.860525481261383, 6.328958259286263, 4.842170996442184, 1.0183544848191985, 0.46843277802488004]]\n",
      "2020-12-23 02:41:48,503 : INFO : Removed 0 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:48,504 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:48,505 : INFO : built Dictionary(86 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 401 corpus positions)\n",
      "2020-12-23 02:41:48,547 : INFO : token count processed\n",
      "2020-12-23 02:41:48,552 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:41:48,553 : INFO : frequencies processed\n",
      "2020-12-23 02:41:48,554 : INFO : token count processed\n",
      "2020-12-23 02:41:48,555 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:48,555 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:48,556 : INFO : vocab #2480\n",
      "2020-12-23 02:41:48,557 : INFO : diff #set()\n",
      "2020-12-23 02:41:48,809 : INFO : alphabet #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:41:48,936 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.2625218862517174, 0.44198467474570313], [0.9240255355834961, 0.075974464], [nan, nan], [5.310603774467064, 5.945464049777852, 6.385501252038724, 4.870566572206192, 1.0748974775716595, 0.4400372022608714]]\n",
      "2020-12-23 02:41:48,939 : INFO : Removed 0 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:48,940 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:48,942 : INFO : built Dictionary(226 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 948 corpus positions)\n",
      "2020-12-23 02:41:49,152 : INFO : token count processed\n",
      "2020-12-23 02:41:49,155 : INFO : frequencies processed\n",
      "2020-12-23 02:41:49,282 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:49,282 : INFO : entropies processed\n",
      "2020-12-23 02:41:49,283 : INFO : extropies processed\n",
      "2020-12-23 02:41:49,284 : INFO : token count processed\n",
      "2020-12-23 02:41:49,285 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:49,286 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:49,287 : INFO : vocab #2480\n",
      "2020-12-23 02:41:49,288 : INFO : diff #set()\n",
      "2020-12-23 02:41:49,546 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:49,674 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.1698905500649825, 0.460852737466437], [0.7425298988819122, 0.2574701], [2.753434386188785, 1.3071158514240315], [5.310603774467064, 6.811563897304216, 7.0161234023166354, 5.106044269454644, 1.7055196278495712, 0.20455950501241915]]\n",
      "2020-12-23 02:41:49,677 : INFO : Removed 0 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:49,678 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:49,680 : INFO : built Dictionary(245 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 1043 corpus positions)\n",
      "2020-12-23 02:41:49,920 : INFO : token count processed\n",
      "2020-12-23 02:41:49,927 : INFO : frequencies processed\n",
      "2020-12-23 02:41:50,058 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:50,059 : INFO : entropies processed\n",
      "2020-12-23 02:41:50,060 : INFO : extropies processed\n",
      "2020-12-23 02:41:50,061 : INFO : token count processed\n",
      "2020-12-23 02:41:50,062 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:50,063 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:50,064 : INFO : vocab #2480\n",
      "2020-12-23 02:41:50,065 : INFO : diff #set()\n",
      "2020-12-23 02:41:50,325 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:50,453 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.1436129786921705, 0.4665021204574462], [0.7429656088352203, 0.2570344], [3.1279868068776753, 1.3437322713578626], [5.310603774467064, 7.502034948968415, 7.6319272756229175, 5.180711447812563, 2.3213235011558533, 0.12989232665450245]]\n",
      "2020-12-23 02:41:50,456 : INFO : Removed 0 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:50,457 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:50,460 : INFO : built Dictionary(288 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 1616 corpus positions)\n",
      "2020-12-23 02:41:50,790 : INFO : token count processed\n",
      "2020-12-23 02:41:50,796 : INFO : frequencies processed\n",
      "2020-12-23 02:41:50,924 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:50,924 : INFO : entropies processed\n",
      "2020-12-23 02:41:50,925 : INFO : extropies processed\n",
      "2020-12-23 02:41:50,926 : INFO : token count processed\n",
      "2020-12-23 02:41:50,927 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:50,927 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:50,928 : INFO : vocab #2480\n",
      "2020-12-23 02:41:50,929 : INFO : diff #set()\n",
      "2020-12-23 02:41:51,190 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:51,319 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.1329489384108855, 0.4688344769964496], [0.7315226793289185, 0.26847732], [3.30827083453526, 1.3588095879903364], [5.310603774467064, 7.39180093901977, 7.501298469927519, 5.201106243559316, 2.190694695460455, 0.10949753090774905]]\n",
      "2020-12-23 02:41:51,321 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:51,322 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:51,323 : INFO : built Dictionary(80 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 171 corpus positions)\n",
      "2020-12-23 02:41:51,359 : INFO : token count processed\n",
      "2020-12-23 02:41:51,364 : INFO : frequencies processed\n",
      "2020-12-23 02:41:51,499 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:51,500 : INFO : entropies processed\n",
      "2020-12-23 02:41:51,501 : INFO : extropies processed\n",
      "2020-12-23 02:41:51,502 : INFO : token count processed\n",
      "2020-12-23 02:41:51,503 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:51,504 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:51,505 : INFO : vocab #2480\n",
      "2020-12-23 02:41:51,506 : INFO : diff #set()\n",
      "2020-12-23 02:41:51,764 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:51,892 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.150676422303984, 0.46496999252389465], [0.7984671145677567, 0.20153289], [1.3709505944546687, 1.0438561897747245], [5.310603774467064, 4.927561309677364, 5.954823309598846, 4.283341774545581, 0.6442195351317821, 1.0272619999214827]]\n",
      "2020-12-23 02:41:51,894 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:51,895 : INFO : built Dictionary(51 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 72 corpus positions)\n",
      "2020-12-23 02:41:51,904 : INFO : token count processed\n",
      "2020-12-23 02:41:51,906 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:41:51,907 : INFO : frequencies processed\n",
      "2020-12-23 02:41:51,908 : INFO : token count processed\n",
      "2020-12-23 02:41:51,909 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:51,910 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:51,910 : INFO : vocab #2480\n",
      "2020-12-23 02:41:51,911 : INFO : diff #set()\n",
      "2020-12-23 02:41:52,170 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:52,298 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.245791663368988, 0.4452772785254115], [0.9161864891648293, 0.08381351], [nan, nan], [5.310603774467064, 2.5216406363433186, 5.4995824666299455, 2.332661944180437, 0.18897869216288132, 2.977941830286627]]\n",
      "2020-12-23 02:41:52,302 : INFO : Removed 0 and 530 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:52,303 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:52,305 : INFO : built Dictionary(354 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 2944 corpus positions)\n",
      "2020-12-23 02:41:52,735 : INFO : token count processed\n",
      "2020-12-23 02:41:52,742 : INFO : frequencies processed\n",
      "2020-12-23 02:41:52,870 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:52,871 : INFO : entropies processed\n",
      "2020-12-23 02:41:52,872 : INFO : extropies processed\n",
      "2020-12-23 02:41:52,875 : INFO : token count processed\n",
      "2020-12-23 02:41:52,876 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:52,878 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:52,878 : INFO : vocab #2480\n",
      "2020-12-23 02:41:52,880 : INFO : diff #set()\n",
      "2020-12-23 02:41:53,148 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:53,277 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.0755860254740852, 0.48179164232501015], [0.6324464678764343, 0.36755353], [4.036006945330954, 1.391163990026039], [5.310603774467064, 7.480007711014331, 7.531285926091664, 5.2593255593897315, 2.2206821516246, 0.051278215077333655]]\n",
      "2020-12-23 02:41:53,280 : INFO : Removed 0 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:53,281 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:41:53,282 : INFO : built Dictionary(238 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 1093 corpus positions)\n",
      "2020-12-23 02:41:53,502 : INFO : token count processed\n",
      "2020-12-23 02:41:53,505 : INFO : frequencies processed\n",
      "2020-12-23 02:41:53,632 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:53,633 : INFO : entropies processed\n",
      "2020-12-23 02:41:53,636 : INFO : extropies processed\n",
      "2020-12-23 02:41:53,638 : INFO : token count processed\n",
      "2020-12-23 02:41:53,638 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:53,639 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:53,640 : INFO : vocab #2480\n",
      "2020-12-23 02:41:53,641 : INFO : diff #set()\n",
      "2020-12-23 02:41:53,899 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:54,027 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.0904418234975737, 0.4783677731470535], [0.6526997983455658, 0.3473002], [3.4116022179746714, 1.364803813191739], [5.310603774467064, 7.131331012509435, 7.264398174735837, 5.1775366122406625, 1.953794400268773, 0.13306716222640258]]\n",
      "2020-12-23 02:41:54,030 : INFO : Removed 0 and 166 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:54,031 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:54,033 : INFO : built Dictionary(226 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 912 corpus positions)\n",
      "2020-12-23 02:41:54,234 : INFO : token count processed\n",
      "2020-12-23 02:41:54,237 : INFO : frequencies processed\n",
      "2020-12-23 02:41:54,370 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:54,370 : INFO : entropies processed\n",
      "2020-12-23 02:41:54,371 : INFO : extropies processed\n",
      "2020-12-23 02:41:54,372 : INFO : token count processed\n",
      "2020-12-23 02:41:54,373 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:54,374 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:54,375 : INFO : vocab #2480\n",
      "2020-12-23 02:41:54,375 : INFO : diff #set()\n",
      "2020-12-23 02:41:54,632 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:54,759 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.1524780787561637, 0.4645808056627747], [0.7303984463214874, 0.26960155], [3.621928094887362, 1.3794228029333433], [5.310603774467064, 7.203742744794778, 7.372673239445702, 5.14167327981614, 2.062069464978638, 0.16893049465092425]]\n",
      "2020-12-23 02:41:54,761 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:54,762 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:54,763 : INFO : built Dictionary(90 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 244 corpus positions)\n",
      "2020-12-23 02:41:54,808 : INFO : token count processed\n",
      "2020-12-23 02:41:54,813 : INFO : frequencies processed\n",
      "2020-12-23 02:41:54,947 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:54,948 : INFO : entropies processed\n",
      "2020-12-23 02:41:54,949 : INFO : extropies processed\n",
      "2020-12-23 02:41:54,951 : INFO : token count processed\n",
      "2020-12-23 02:41:54,953 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:54,954 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:54,955 : INFO : vocab #2480\n",
      "2020-12-23 02:41:54,956 : INFO : diff #set()\n",
      "2020-12-23 02:41:55,222 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:55,349 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.012489452927006, 0.4968970140666225], [0.5943929553031921, 0.40560704], [2.577819531114783, 1.2963836209792667], [5.310603774467064, 5.195502554608948, 5.869631570012494, 4.636474759063518, 0.5590277955454299, 0.6741290154035458]]\n",
      "2020-12-23 02:41:55,352 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:55,352 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:55,354 : INFO : built Dictionary(97 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 305 corpus positions)\n",
      "2020-12-23 02:41:55,406 : INFO : token count processed\n",
      "2020-12-23 02:41:55,415 : INFO : frequencies processed\n",
      "2020-12-23 02:41:55,548 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:55,549 : INFO : entropies processed\n",
      "2020-12-23 02:41:55,549 : INFO : extropies processed\n",
      "2020-12-23 02:41:55,550 : INFO : token count processed\n",
      "2020-12-23 02:41:55,551 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:55,552 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:55,552 : INFO : vocab #2480\n",
      "2020-12-23 02:41:55,553 : INFO : diff #set()\n",
      "2020-12-23 02:41:55,809 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:55,935 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.099057692514207, 0.4764042472802265], [0.6859059035778046, 0.3140941], [2.446439344671015, 1.2856945251022456], [5.310603774467064, 5.32027245610305, 5.95348488169962, 4.677391348870493, 0.6428811072325562, 0.6332124255965708]]\n",
      "2020-12-23 02:41:55,938 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:55,939 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:55,940 : INFO : built Dictionary(191 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 545 corpus positions)\n",
      "2020-12-23 02:41:56,099 : INFO : token count processed\n",
      "2020-12-23 02:41:56,102 : INFO : frequencies processed\n",
      "2020-12-23 02:41:56,230 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:56,230 : INFO : entropies processed\n",
      "2020-12-23 02:41:56,231 : INFO : extropies processed\n",
      "2020-12-23 02:41:56,232 : INFO : token count processed\n",
      "2020-12-23 02:41:56,233 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:56,234 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:56,234 : INFO : vocab #2480\n",
      "2020-12-23 02:41:56,235 : INFO : diff #set()\n",
      "2020-12-23 02:41:56,495 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:56,622 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.0713103359002945, 0.4827861777483721], [0.6124171912670135, 0.3875828], [3.36030597770208, 1.3582780947232875], [5.310603774467064, 6.898202761357263, 7.118022429440336, 5.090784106383991, 1.8074186549732714, 0.21981966808307263]]\n",
      "2020-12-23 02:41:56,624 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:56,625 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:56,627 : INFO : built Dictionary(159 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 567 corpus positions)\n",
      "2020-12-23 02:41:56,754 : INFO : token count processed\n",
      "2020-12-23 02:41:56,757 : INFO : frequencies processed\n",
      "2020-12-23 02:41:56,883 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:56,883 : INFO : entropies processed\n",
      "2020-12-23 02:41:56,884 : INFO : extropies processed\n",
      "2020-12-23 02:41:56,885 : INFO : token count processed\n",
      "2020-12-23 02:41:56,886 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:56,887 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:56,888 : INFO : vocab #2480\n",
      "2020-12-23 02:41:56,889 : INFO : diff #set()\n",
      "2020-12-23 02:41:57,160 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:57,290 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.0891265934253727, 0.4786689342556214], [0.7141637802124023, 0.28583622], [2.9053077763737467, 1.3349935327833893], [5.310603774467064, 6.388500481644799, 6.67588997139085, 5.023214284721012, 1.365286196923786, 0.28738948974605094]]\n",
      "2020-12-23 02:41:57,293 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:57,294 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:57,295 : INFO : built Dictionary(83 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 231 corpus positions)\n",
      "2020-12-23 02:41:57,338 : INFO : token count processed\n",
      "2020-12-23 02:41:57,341 : INFO : frequencies processed\n",
      "2020-12-23 02:41:57,467 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:57,468 : INFO : entropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:41:57,469 : INFO : extropies processed\n",
      "2020-12-23 02:41:57,471 : INFO : token count processed\n",
      "2020-12-23 02:41:57,473 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:57,474 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:57,474 : INFO : vocab #2480\n",
      "2020-12-23 02:41:57,475 : INFO : diff #set()\n",
      "2020-12-23 02:41:57,735 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:57,861 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.0706399890862044, 0.48294247443821015], [0.6717905402183533, 0.32820946], [2.103909910282364, 1.2389103347072152], [5.310603774467064, 4.8191513650620195, 5.663514743501507, 4.466240396027578, 0.3529109690344425, 0.8443633784394873]]\n",
      "2020-12-23 02:41:57,864 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:57,865 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:57,865 : INFO : built Dictionary(88 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 286 corpus positions)\n",
      "2020-12-23 02:41:57,910 : INFO : token count processed\n",
      "2020-12-23 02:41:57,915 : INFO : frequencies processed\n",
      "2020-12-23 02:41:58,050 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:58,051 : INFO : entropies processed\n",
      "2020-12-23 02:41:58,052 : INFO : extropies processed\n",
      "2020-12-23 02:41:58,053 : INFO : token count processed\n",
      "2020-12-23 02:41:58,054 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:58,055 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:58,055 : INFO : vocab #2480\n",
      "2020-12-23 02:41:58,057 : INFO : diff #set()\n",
      "2020-12-23 02:41:58,320 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:58,448 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.1420582552991967, 0.4668407115101185], [0.7544108629226685, 0.24558914], [2.1556390622295662, 1.2407663947533207], [5.310603774467064, 5.062480936779194, 5.814748720297755, 4.558335990948502, 0.5041449458306904, 0.7522677835185609]]\n",
      "2020-12-23 02:41:58,451 : INFO : Removed 0 and 223 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:58,451 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:58,453 : INFO : built Dictionary(268 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 1840 corpus positions)\n",
      "2020-12-23 02:41:58,736 : INFO : token count processed\n",
      "2020-12-23 02:41:58,741 : INFO : frequencies processed\n",
      "2020-12-23 02:41:58,871 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:58,871 : INFO : entropies processed\n",
      "2020-12-23 02:41:58,872 : INFO : extropies processed\n",
      "2020-12-23 02:41:58,874 : INFO : token count processed\n",
      "2020-12-23 02:41:58,874 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:58,875 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:58,876 : INFO : vocab #2480\n",
      "2020-12-23 02:41:58,877 : INFO : diff #set()\n",
      "2020-12-23 02:41:59,134 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:59,262 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.1265277122243025, 0.4702501614493523], [0.6799613535404205, 0.32003865], [3.8561962982193796, 1.3866167862627314], [5.310603774467064, 7.185085743102134, 7.284528092787112, 5.2111614247820865, 1.9739243183200479, 0.09944234968497767]]\n",
      "2020-12-23 02:41:59,265 : INFO : Removed 0 and 148 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:59,266 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:59,267 : INFO : built Dictionary(191 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 815 corpus positions)\n",
      "2020-12-23 02:41:59,431 : INFO : token count processed\n",
      "2020-12-23 02:41:59,434 : INFO : frequencies processed\n",
      "2020-12-23 02:41:59,561 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:41:59,562 : INFO : entropies processed\n",
      "2020-12-23 02:41:59,562 : INFO : extropies processed\n",
      "2020-12-23 02:41:59,563 : INFO : token count processed\n",
      "2020-12-23 02:41:59,564 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:41:59,565 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:41:59,565 : INFO : vocab #2480\n",
      "2020-12-23 02:41:59,566 : INFO : diff #set()\n",
      "2020-12-23 02:41:59,823 : INFO : alphabet #2480\n",
      "2020-12-23 02:41:59,950 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.14279554854433, 0.4666800809248144], [0.7369188666343689, 0.26308113], [3.016875695766564, 1.328359742951307], [5.310603774467064, 6.591225336124281, 6.811248052538898, 5.0905810580524475, 1.5006442780718334, 0.22002271641461668]]\n",
      "2020-12-23 02:41:59,952 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:41:59,953 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:41:59,955 : INFO : built Dictionary(75 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 153 corpus positions)\n",
      "2020-12-23 02:41:59,991 : INFO : token count processed\n",
      "2020-12-23 02:41:59,993 : INFO : frequencies processed\n",
      "2020-12-23 02:42:00,134 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:00,135 : INFO : entropies processed\n",
      "2020-12-23 02:42:00,136 : INFO : extropies processed\n",
      "2020-12-23 02:42:00,138 : INFO : token count processed\n",
      "2020-12-23 02:42:00,139 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:00,140 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:00,141 : INFO : vocab #2480\n",
      "2020-12-23 02:42:00,142 : INFO : diff #set()\n",
      "2020-12-23 02:42:00,405 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:00,532 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.1411983409551016, 0.46702819672181356], [0.8129349797964096, 0.18706502], [1.8423709931771088, 1.1893232685884285], [5.310603774467064, 4.7032114441396695, 5.846176106693391, 4.167639111913343, 0.5355723322263266, 1.1429646625537213]]\n",
      "2020-12-23 02:42:00,535 : INFO : Removed 0 and 9 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:00,536 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:00,537 : INFO : built Dictionary(147 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 393 corpus positions)\n",
      "2020-12-23 02:42:00,639 : INFO : token count processed\n",
      "2020-12-23 02:42:00,642 : INFO : frequencies processed\n",
      "2020-12-23 02:42:00,768 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:00,769 : INFO : entropies processed\n",
      "2020-12-23 02:42:00,769 : INFO : extropies processed\n",
      "2020-12-23 02:42:00,770 : INFO : token count processed\n",
      "2020-12-23 02:42:00,771 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:00,771 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:00,772 : INFO : vocab #2480\n",
      "2020-12-23 02:42:00,773 : INFO : diff #set()\n",
      "2020-12-23 02:42:01,039 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:01,168 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.0921093424270658, 0.4779864893867809], [0.6743512153625488, 0.32564878], [3.189898095464287, 1.351714529210319], [5.310603774467064, 6.14228447828618, 6.550517581942841, 4.902370670810403, 1.2399138074757765, 0.4082331036566611]]\n",
      "2020-12-23 02:42:01,171 : INFO : Removed 0 and 184 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:01,172 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:01,175 : INFO : built Dictionary(279 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 1183 corpus positions)\n",
      "2020-12-23 02:42:01,483 : INFO : token count processed\n",
      "2020-12-23 02:42:01,486 : INFO : frequencies processed\n",
      "2020-12-23 02:42:01,613 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:01,614 : INFO : entropies processed\n",
      "2020-12-23 02:42:01,617 : INFO : extropies processed\n",
      "2020-12-23 02:42:01,618 : INFO : token count processed\n",
      "2020-12-23 02:42:01,619 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:01,619 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:01,620 : INFO : vocab #2480\n",
      "2020-12-23 02:42:01,621 : INFO : diff #set()\n",
      "2020-12-23 02:42:01,879 : INFO : alphabet #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:42:02,006 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.0798804793470171, 0.4807968582473316], [0.6449548006057739, 0.3550452], [3.855388542207534, 1.3858306737380488], [5.310603774467064, 7.450178124335845, 7.5638797992840345, 5.196902099518875, 2.2532760248169703, 0.11370167494818961]]\n",
      "2020-12-23 02:42:02,009 : INFO : Removed 0 and 16 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:02,010 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:02,012 : INFO : built Dictionary(91 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 231 corpus positions)\n",
      "2020-12-23 02:42:02,062 : INFO : token count processed\n",
      "2020-12-23 02:42:02,064 : INFO : frequencies processed\n",
      "2020-12-23 02:42:02,204 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:02,205 : INFO : entropies processed\n",
      "2020-12-23 02:42:02,205 : INFO : extropies processed\n",
      "2020-12-23 02:42:02,207 : INFO : token count processed\n",
      "2020-12-23 02:42:02,208 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:02,209 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:02,210 : INFO : vocab #2480\n",
      "2020-12-23 02:42:02,211 : INFO : diff #set()\n",
      "2020-12-23 02:42:02,478 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:02,605 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.0400010365804926, 0.49019582934929695], [0.6498239636421204, 0.35017604], [2.3248629576173565, 1.2646704166980127], [5.310603774467064, 5.20665021947654, 5.911182145054003, 4.606071848889602, 0.6005783705869385, 0.7045319255774629]]\n",
      "2020-12-23 02:42:02,607 : INFO : Removed 0 and 89 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:02,608 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:02,609 : INFO : built Dictionary(157 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 569 corpus positions)\n",
      "2020-12-23 02:42:02,723 : INFO : token count processed\n",
      "2020-12-23 02:42:02,725 : INFO : frequencies processed\n",
      "2020-12-23 02:42:02,853 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:02,854 : INFO : entropies processed\n",
      "2020-12-23 02:42:02,855 : INFO : extropies processed\n",
      "2020-12-23 02:42:02,856 : INFO : token count processed\n",
      "2020-12-23 02:42:02,857 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:02,858 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:02,859 : INFO : vocab #2480\n",
      "2020-12-23 02:42:02,860 : INFO : diff #set()\n",
      "2020-12-23 02:42:03,117 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:03,244 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.1420522432849878, 0.4668420217736752], [0.744693249464035, 0.25530675], [3.0849625007211556, 1.3480058660457088], [5.310603774467064, 6.524718477352, 6.824359377757169, 5.010962874061896, 1.5137556032901047, 0.2996409004051692]]\n",
      "2020-12-23 02:42:03,246 : INFO : Removed 0 and 8 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:03,247 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:03,248 : INFO : built Dictionary(97 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 276 corpus positions)\n",
      "2020-12-23 02:42:03,300 : INFO : token count processed\n",
      "2020-12-23 02:42:03,302 : INFO : frequencies processed\n",
      "2020-12-23 02:42:03,430 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:03,430 : INFO : entropies processed\n",
      "2020-12-23 02:42:03,431 : INFO : extropies processed\n",
      "2020-12-23 02:42:03,433 : INFO : token count processed\n",
      "2020-12-23 02:42:03,433 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:03,434 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:03,435 : INFO : vocab #2480\n",
      "2020-12-23 02:42:03,436 : INFO : diff #set()\n",
      "2020-12-23 02:42:03,710 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:03,841 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.0822676030712612, 0.4802456699249607], [0.6814606487751007, 0.31853935], [2.3248629576173565, 1.2646704166980127], [5.310603774467064, 5.321859380715434, 5.964394403914889, 4.668068751267611, 0.6537906294478244, 0.6425350231994544]]\n",
      "2020-12-23 02:42:03,844 : INFO : Removed 0 and 119 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:03,844 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:03,846 : INFO : built Dictionary(169 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 894 corpus positions)\n",
      "2020-12-23 02:42:03,979 : INFO : token count processed\n",
      "2020-12-23 02:42:03,981 : INFO : frequencies processed\n",
      "2020-12-23 02:42:04,109 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:04,110 : INFO : entropies processed\n",
      "2020-12-23 02:42:04,111 : INFO : extropies processed\n",
      "2020-12-23 02:42:04,113 : INFO : token count processed\n",
      "2020-12-23 02:42:04,114 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:04,115 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:04,116 : INFO : vocab #2480\n",
      "2020-12-23 02:42:04,117 : INFO : diff #set()\n",
      "2020-12-23 02:42:04,385 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:04,515 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.0644642670662723, 0.48438716811556154], [0.6469522416591644, 0.35304776], [3.480083695187448, 1.3641183294486805], [5.310603774467064, 6.500767808767801, 6.691292373946533, 5.120079209288333, 1.3806885994794689, 0.19052456517873217]]\n",
      "2020-12-23 02:42:04,518 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:04,518 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:04,520 : INFO : built Dictionary(73 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 116 corpus positions)\n",
      "2020-12-23 02:42:04,551 : INFO : token count processed\n",
      "2020-12-23 02:42:04,554 : INFO : frequencies processed\n",
      "2020-12-23 02:42:04,686 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:04,687 : INFO : entropies processed\n",
      "2020-12-23 02:42:04,688 : INFO : extropies processed\n",
      "2020-12-23 02:42:04,690 : INFO : token count processed\n",
      "2020-12-23 02:42:04,691 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:04,693 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:04,694 : INFO : vocab #2480\n",
      "2020-12-23 02:42:04,695 : INFO : diff #set()\n",
      "2020-12-23 02:42:04,965 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:05,097 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.136582877080426, 0.4680370748671678], [0.7910764813423157, 0.20892352], [0.9182958340544896, 0.9182958340544896], [5.310603774467064, 4.736228843383063, 5.9813624249821675, 4.065470192867959, 0.6707586505151033, 1.245133581599105]]\n",
      "2020-12-23 02:42:05,100 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:05,101 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:05,102 : INFO : built Dictionary(132 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 457 corpus positions)\n",
      "2020-12-23 02:42:05,186 : INFO : token count processed\n",
      "2020-12-23 02:42:05,188 : INFO : frequencies processed\n",
      "2020-12-23 02:42:05,318 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:05,318 : INFO : entropies processed\n",
      "2020-12-23 02:42:05,319 : INFO : extropies processed\n",
      "2020-12-23 02:42:05,320 : INFO : token count processed\n",
      "2020-12-23 02:42:05,321 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:05,322 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:05,323 : INFO : vocab #2480\n",
      "2020-12-23 02:42:05,324 : INFO : diff #set()\n",
      "2020-12-23 02:42:05,584 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:05,713 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.1374577761079692, 0.46784549906799516], [0.7554992586374283, 0.24450074], [2.1556390622295662, 1.2407663947533205], [5.310603774467064, 5.788442787590127, 6.250792761150647, 4.848253800906544, 0.9401889866835829, 0.4623499735605199]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:42:05,715 : INFO : Removed 0 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:05,716 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:05,718 : INFO : built Dictionary(90 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 143 corpus positions)\n",
      "2020-12-23 02:42:05,767 : INFO : token count processed\n",
      "2020-12-23 02:42:05,769 : INFO : frequencies processed\n",
      "2020-12-23 02:42:05,911 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:05,911 : INFO : entropies processed\n",
      "2020-12-23 02:42:05,912 : INFO : extropies processed\n",
      "2020-12-23 02:42:05,914 : INFO : token count processed\n",
      "2020-12-23 02:42:05,916 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:05,917 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:05,918 : INFO : vocab #2480\n",
      "2020-12-23 02:42:05,920 : INFO : diff #set()\n",
      "2020-12-23 02:42:06,183 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:06,313 : INFO : Computed distances or similarities ('293', 'sacp-python-common/setup.py')[[1.1371086699231368, 0.46792192370637187], [0.8340601176023483, 0.16593988], [1.9219280948873623, 1.2148067842293933], [5.310603774467064, 5.370004292053436, 6.254412775751732, 4.426195290768769, 0.9438090012846674, 0.8844084836982953]]\n",
      "2020-12-23 02:42:06,315 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:06,316 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:06,317 : INFO : built Dictionary(117 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 388 corpus positions)\n",
      "2020-12-23 02:42:06,385 : INFO : token count processed\n",
      "2020-12-23 02:42:06,388 : INFO : frequencies processed\n",
      "2020-12-23 02:42:06,517 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:06,518 : INFO : entropies processed\n",
      "2020-12-23 02:42:06,519 : INFO : extropies processed\n",
      "2020-12-23 02:42:06,521 : INFO : token count processed\n",
      "2020-12-23 02:42:06,522 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:06,523 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:06,523 : INFO : vocab #2480\n",
      "2020-12-23 02:42:06,525 : INFO : diff #set()\n",
      "2020-12-23 02:42:06,785 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:06,912 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.1834894270201244, 0.4579825244973734], [0.8399754464626312, 0.16002455], [1.5219280948873621, 1.1419011889093373], [5.310603774467064, 5.695663584743922, 6.233961694765451, 4.772305664445536, 0.923357920298387, 0.5382981100215289]]\n",
      "2020-12-23 02:42:06,915 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:06,916 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:06,917 : INFO : built Dictionary(76 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 163 corpus positions)\n",
      "2020-12-23 02:42:06,948 : INFO : token count processed\n",
      "2020-12-23 02:42:06,950 : INFO : frequencies processed\n",
      "2020-12-23 02:42:07,077 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:07,078 : INFO : entropies processed\n",
      "2020-12-23 02:42:07,079 : INFO : extropies processed\n",
      "2020-12-23 02:42:07,080 : INFO : token count processed\n",
      "2020-12-23 02:42:07,081 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:07,082 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:07,083 : INFO : vocab #2480\n",
      "2020-12-23 02:42:07,084 : INFO : diff #set()\n",
      "2020-12-23 02:42:07,341 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:07,468 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.1540936155715908, 0.4642323772612125], [0.8550165146589279, 0.14498349], [2.321928094887362, 1.2877123795494492], [5.310603774467064, 4.9004417692112465, 5.946419640158544, 4.264625903519766, 0.6358158656914794, 1.045977870947297]]\n",
      "2020-12-23 02:42:07,470 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:07,471 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:07,472 : INFO : built Dictionary(75 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 170 corpus positions)\n",
      "2020-12-23 02:42:07,502 : INFO : token count processed\n",
      "2020-12-23 02:42:07,505 : INFO : frequencies processed\n",
      "2020-12-23 02:42:07,632 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:07,633 : INFO : entropies processed\n",
      "2020-12-23 02:42:07,634 : INFO : extropies processed\n",
      "2020-12-23 02:42:07,635 : INFO : token count processed\n",
      "2020-12-23 02:42:07,636 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:07,637 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:07,638 : INFO : vocab #2480\n",
      "2020-12-23 02:42:07,639 : INFO : diff #set()\n",
      "2020-12-23 02:42:07,898 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:08,026 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.1641167778098742, 0.46208227312576833], [0.8614997863769531, 0.13850021], [1.9219280948873623, 1.2148067842293933], [5.310603774467064, 4.778624108914332, 5.862571745070899, 4.226656138310498, 0.551967970603835, 1.0839476361565668]]\n",
      "2020-12-23 02:42:08,029 : INFO : Removed 0 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:08,030 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:08,031 : INFO : built Dictionary(77 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 201 corpus positions)\n",
      "2020-12-23 02:42:08,068 : INFO : token count processed\n",
      "2020-12-23 02:42:08,070 : INFO : frequencies processed\n",
      "2020-12-23 02:42:08,211 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:08,212 : INFO : entropies processed\n",
      "2020-12-23 02:42:08,213 : INFO : extropies processed\n",
      "2020-12-23 02:42:08,215 : INFO : token count processed\n",
      "2020-12-23 02:42:08,216 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:08,217 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:08,218 : INFO : vocab #2480\n",
      "2020-12-23 02:42:08,220 : INFO : diff #set()\n",
      "2020-12-23 02:42:08,482 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:08,609 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.1912131359219693, 0.4563682024383459], [0.9036672785878181, 0.09633272], [1.5, 1.1225562489182657], [5.310603774467064, 4.773880192225086, 5.791924472074073, 4.292559494618078, 0.4813206976070088, 1.0180442798489873]]\n",
      "2020-12-23 02:42:08,613 : INFO : Removed 0 and 346 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:08,613 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:08,615 : INFO : built Dictionary(187 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 2026 corpus positions)\n",
      "2020-12-23 02:42:08,787 : INFO : token count processed\n",
      "2020-12-23 02:42:08,790 : INFO : frequencies processed\n",
      "2020-12-23 02:42:08,916 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:08,916 : INFO : entropies processed\n",
      "2020-12-23 02:42:08,917 : INFO : extropies processed\n",
      "2020-12-23 02:42:08,919 : INFO : token count processed\n",
      "2020-12-23 02:42:08,920 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:08,921 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:08,921 : INFO : vocab #2480\n",
      "2020-12-23 02:42:08,923 : INFO : diff #set()\n",
      "2020-12-23 02:42:09,178 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:09,306 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.1357303691489284, 0.4682238986930228], [0.7806393951177597, 0.2193606], [1.9056390622295665, 1.2149156328132604], [5.310603774467064, 6.620773041953877, 6.747897492379924, 5.1834793240410155, 1.4372937179128602, 0.1271244504260478]]\n",
      "2020-12-23 02:42:09,308 : INFO : Removed 0 and 132 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:09,309 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:09,311 : INFO : built Dictionary(115 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 589 corpus positions)\n",
      "2020-12-23 02:42:09,393 : INFO : token count processed\n",
      "2020-12-23 02:42:09,395 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:42:09,528 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:09,528 : INFO : entropies processed\n",
      "2020-12-23 02:42:09,529 : INFO : extropies processed\n",
      "2020-12-23 02:42:09,530 : INFO : token count processed\n",
      "2020-12-23 02:42:09,531 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:09,532 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:09,532 : INFO : vocab #2480\n",
      "2020-12-23 02:42:09,533 : INFO : diff #set()\n",
      "2020-12-23 02:42:09,798 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:09,926 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.1489486694406854, 0.46534382799393414], [0.7928782999515533, 0.2071217], [2.4193819456463714, 1.2761517340193214], [5.310603774467064, 5.828370634755606, 6.180919357583194, 4.958055051639475, 0.8703155831161302, 0.35254872282758853]]\n",
      "2020-12-23 02:42:09,929 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:09,930 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:09,931 : INFO : built Dictionary(118 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 352 corpus positions)\n",
      "2020-12-23 02:42:09,998 : INFO : token count processed\n",
      "2020-12-23 02:42:10,001 : INFO : frequencies processed\n",
      "2020-12-23 02:42:10,129 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:10,130 : INFO : entropies processed\n",
      "2020-12-23 02:42:10,130 : INFO : extropies processed\n",
      "2020-12-23 02:42:10,132 : INFO : token count processed\n",
      "2020-12-23 02:42:10,133 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:10,134 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:10,134 : INFO : vocab #2480\n",
      "2020-12-23 02:42:10,135 : INFO : diff #set()\n",
      "2020-12-23 02:42:10,392 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:10,520 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.1624439307980796, 0.4624397357812354], [0.7896097898483276, 0.21039021], [1.9182958340544896, 1.2183406773511978], [5.310603774467064, 5.774409284925443, 6.3143528111367555, 4.770660248255751, 1.0037490366696913, 0.5399435262113128]]\n",
      "2020-12-23 02:42:10,522 : INFO : Removed 0 and 51 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:10,523 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:10,524 : INFO : built Dictionary(126 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 401 corpus positions)\n",
      "2020-12-23 02:42:10,606 : INFO : token count processed\n",
      "2020-12-23 02:42:10,608 : INFO : frequencies processed\n",
      "2020-12-23 02:42:10,736 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:10,737 : INFO : entropies processed\n",
      "2020-12-23 02:42:10,738 : INFO : extropies processed\n",
      "2020-12-23 02:42:10,739 : INFO : token count processed\n",
      "2020-12-23 02:42:10,740 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:10,741 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:10,742 : INFO : vocab #2480\n",
      "2020-12-23 02:42:10,743 : INFO : diff #set()\n",
      "2020-12-23 02:42:10,999 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:11,126 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.1162378551825292, 0.47253667519039266], [0.7658755034208298, 0.2341245], [2.75, 1.3226647836567116], [5.310603774467064, 5.977819040873918, 6.416153130660847, 4.872269684680136, 1.1055493561937828, 0.4383340897869292]]\n",
      "2020-12-23 02:42:11,128 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:11,129 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:11,131 : INFO : built Dictionary(109 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 348 corpus positions)\n",
      "2020-12-23 02:42:11,204 : INFO : token count processed\n",
      "2020-12-23 02:42:11,207 : INFO : frequencies processed\n",
      "2020-12-23 02:42:11,335 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:11,336 : INFO : entropies processed\n",
      "2020-12-23 02:42:11,336 : INFO : extropies processed\n",
      "2020-12-23 02:42:11,337 : INFO : token count processed\n",
      "2020-12-23 02:42:11,338 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:11,339 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:11,339 : INFO : vocab #2480\n",
      "2020-12-23 02:42:11,340 : INFO : diff #set()\n",
      "2020-12-23 02:42:11,613 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:11,744 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.0912391253792424, 0.47818539155279616], [0.6915747821331024, 0.30842522], [2.1709505944546685, 1.2515504860726414], [5.310603774467064, 5.901812829596593, 6.371412150337743, 4.841004453725914, 1.0608083758706792, 0.4695993207411506]]\n",
      "2020-12-23 02:42:11,746 : INFO : Removed 0 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:11,747 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:11,748 : INFO : built Dictionary(113 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 348 corpus positions)\n",
      "2020-12-23 02:42:11,819 : INFO : token count processed\n",
      "2020-12-23 02:42:11,821 : INFO : frequencies processed\n",
      "2020-12-23 02:42:11,950 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:11,951 : INFO : entropies processed\n",
      "2020-12-23 02:42:11,954 : INFO : extropies processed\n",
      "2020-12-23 02:42:11,955 : INFO : token count processed\n",
      "2020-12-23 02:42:11,956 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:11,957 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:11,957 : INFO : vocab #2480\n",
      "2020-12-23 02:42:11,958 : INFO : diff #set()\n",
      "2020-12-23 02:42:12,217 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:12,344 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.1867719336411726, 0.4572950588106871], [0.8437814563512802, 0.15621854], [1.5, 1.1225562489182657], [5.310603774467064, 5.643202320803383, 6.232474272804907, 4.72133182246554, 0.9218704983378432, 0.5892719520015248]]\n",
      "2020-12-23 02:42:12,347 : INFO : Removed 0 and 30 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:12,348 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:12,350 : INFO : built Dictionary(123 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 382 corpus positions)\n",
      "2020-12-23 02:42:12,435 : INFO : token count processed\n",
      "2020-12-23 02:42:12,439 : INFO : frequencies processed\n",
      "2020-12-23 02:42:12,566 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:12,567 : INFO : entropies processed\n",
      "2020-12-23 02:42:12,567 : INFO : extropies processed\n",
      "2020-12-23 02:42:12,569 : INFO : token count processed\n",
      "2020-12-23 02:42:12,570 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:12,571 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:12,571 : INFO : vocab #2480\n",
      "2020-12-23 02:42:12,572 : INFO : diff #set()\n",
      "2020-12-23 02:42:12,832 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:12,959 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.1377521776042347, 0.46778106951606224], [0.8014210611581802, 0.19857894], [2.521640636343318, 1.2998438251349493], [5.310603774467064, 5.925214310725336, 6.402769727975148, 4.833048357217251, 1.0921659535080837, 0.4775554172498122]]\n",
      "2020-12-23 02:42:12,962 : INFO : Removed 0 and 293 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:12,963 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:12,966 : INFO : built Dictionary(196 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 1777 corpus positions)\n",
      "2020-12-23 02:42:13,154 : INFO : token count processed\n",
      "2020-12-23 02:42:13,162 : INFO : frequencies processed\n",
      "2020-12-23 02:42:13,296 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:13,297 : INFO : entropies processed\n",
      "2020-12-23 02:42:13,297 : INFO : extropies processed\n",
      "2020-12-23 02:42:13,299 : INFO : token count processed\n",
      "2020-12-23 02:42:13,300 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:13,301 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:13,302 : INFO : vocab #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:42:13,303 : INFO : diff #set()\n",
      "2020-12-23 02:42:13,573 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:13,701 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.139001781929869, 0.46750779192795777], [0.7852564603090286, 0.21474354], [2.4464393446710155, 1.2856945251022456], [5.310603774467064, 6.551685682764175, 6.691533794029336, 5.170755663201902, 1.380930019562272, 0.13984811126516128]]\n",
      "2020-12-23 02:42:13,703 : INFO : Removed 0 and 88 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:13,704 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:13,706 : INFO : built Dictionary(171 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 618 corpus positions)\n",
      "2020-12-23 02:42:13,849 : INFO : token count processed\n",
      "2020-12-23 02:42:13,852 : INFO : frequencies processed\n",
      "2020-12-23 02:42:13,983 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:13,984 : INFO : entropies processed\n",
      "2020-12-23 02:42:13,985 : INFO : extropies processed\n",
      "2020-12-23 02:42:13,987 : INFO : token count processed\n",
      "2020-12-23 02:42:13,988 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:13,989 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:13,989 : INFO : vocab #2480\n",
      "2020-12-23 02:42:13,990 : INFO : diff #set()\n",
      "2020-12-23 02:42:14,248 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:14,376 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.157792366923181, 0.4634366194491226], [0.7769713550806046, 0.22302864], [2.6635327548042547, 1.307883000782319], [5.310603774467064, 6.642985062562557, 6.919117096303152, 5.0344717407264685, 1.6085133218360879, 0.2761320337405948]]\n",
      "2020-12-23 02:42:14,378 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:14,379 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:14,380 : INFO : built Dictionary(87 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 213 corpus positions)\n",
      "2020-12-23 02:42:14,424 : INFO : token count processed\n",
      "2020-12-23 02:42:14,429 : INFO : frequencies processed\n",
      "2020-12-23 02:42:14,567 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:14,568 : INFO : entropies processed\n",
      "2020-12-23 02:42:14,569 : INFO : extropies processed\n",
      "2020-12-23 02:42:14,571 : INFO : token count processed\n",
      "2020-12-23 02:42:14,572 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:14,573 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:14,574 : INFO : vocab #2480\n",
      "2020-12-23 02:42:14,575 : INFO : diff #set()\n",
      "2020-12-23 02:42:14,838 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:14,966 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.1301951583006262, 0.4694405562341785], [0.7588147073984146, 0.2411853], [1.7924812503605778, 1.1575860145844845], [5.310603774467064, 5.2461980344571995, 6.049059371886759, 4.507742437037503, 0.7384555974196951, 0.8028613374295599]]\n",
      "2020-12-23 02:42:14,968 : INFO : Removed 0 and 23 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:14,969 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:14,970 : INFO : built Dictionary(107 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 273 corpus positions)\n",
      "2020-12-23 02:42:15,029 : INFO : token count processed\n",
      "2020-12-23 02:42:15,032 : INFO : frequencies processed\n",
      "2020-12-23 02:42:15,160 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:15,161 : INFO : entropies processed\n",
      "2020-12-23 02:42:15,164 : INFO : extropies processed\n",
      "2020-12-23 02:42:15,165 : INFO : token count processed\n",
      "2020-12-23 02:42:15,166 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:15,166 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:15,167 : INFO : vocab #2480\n",
      "2020-12-23 02:42:15,168 : INFO : diff #set()\n",
      "2020-12-23 02:42:15,425 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:15,553 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/test_auth_utility.py')[[1.1097359904671515, 0.4739929567104619], [0.7084678709506989, 0.29153213], [2.4193819456463714, 1.2761517340193214], [5.310603774467064, 5.903090303960449, 6.4485970610143895, 4.765097017413124, 1.1379932865473252, 0.54550675705394]]\n",
      "2020-12-23 02:42:15,556 : INFO : Removed 0 and 442 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:15,557 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:15,558 : INFO : built Dictionary(137 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 1272 corpus positions)\n",
      "2020-12-23 02:42:15,655 : INFO : token count processed\n",
      "2020-12-23 02:42:15,658 : INFO : frequencies processed\n",
      "2020-12-23 02:42:15,785 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:15,786 : INFO : entropies processed\n",
      "2020-12-23 02:42:15,786 : INFO : extropies processed\n",
      "2020-12-23 02:42:15,788 : INFO : token count processed\n",
      "2020-12-23 02:42:15,789 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:15,790 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:15,790 : INFO : vocab #2480\n",
      "2020-12-23 02:42:15,791 : INFO : diff #set()\n",
      "2020-12-23 02:42:16,058 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:16,187 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.0127829496148855, 0.496824558351577], [0.7021737098693848, 0.2978263], [3.1180782093497093, 1.345323982658864], [5.310603774467064, 6.16659449033757, 6.313464766764559, 5.1637334980400755, 1.002860992297495, 0.14687027642698958]]\n",
      "2020-12-23 02:42:16,189 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:16,190 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:16,192 : INFO : built Dictionary(98 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 307 corpus positions)\n",
      "2020-12-23 02:42:16,256 : INFO : token count processed\n",
      "2020-12-23 02:42:16,259 : INFO : frequencies processed\n",
      "2020-12-23 02:42:16,387 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:16,387 : INFO : entropies processed\n",
      "2020-12-23 02:42:16,388 : INFO : extropies processed\n",
      "2020-12-23 02:42:16,389 : INFO : token count processed\n",
      "2020-12-23 02:42:16,389 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:16,390 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:16,391 : INFO : vocab #2480\n",
      "2020-12-23 02:42:16,392 : INFO : diff #set()\n",
      "2020-12-23 02:42:16,652 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:16,781 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.0422105789550138, 0.4896654685393383], [0.6694062650203705, 0.33059373], [2.984769618706743, 1.334465310119026], [5.310603774467064, 5.906856253399655, 6.3426787898052615, 4.874781238061458, 1.0320750153381972, 0.4358225364056061]]\n",
      "2020-12-23 02:42:16,783 : INFO : Removed 0 and 68 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:16,784 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:16,785 : INFO : built Dictionary(112 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 306 corpus positions)\n",
      "2020-12-23 02:42:16,850 : INFO : token count processed\n",
      "2020-12-23 02:42:16,852 : INFO : frequencies processed\n",
      "2020-12-23 02:42:16,979 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:16,980 : INFO : entropies processed\n",
      "2020-12-23 02:42:16,981 : INFO : extropies processed\n",
      "2020-12-23 02:42:16,983 : INFO : token count processed\n",
      "2020-12-23 02:42:16,984 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:16,985 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:16,986 : INFO : vocab #2480\n",
      "2020-12-23 02:42:16,988 : INFO : diff #set()\n",
      "2020-12-23 02:42:17,250 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:17,377 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.1173357200566296, 0.47229165905407494], [0.7878037989139557, 0.2121962], [2.931208948910323, 1.3394327153712626], [5.310603774467064, 5.965115449163356, 6.436932440428587, 4.838786783201835, 1.1263286659615224, 0.47181699126523036]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:42:17,379 : INFO : Removed 0 and 13 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:17,380 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:17,382 : INFO : built Dictionary(118 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 420 corpus positions)\n",
      "2020-12-23 02:42:17,463 : INFO : token count processed\n",
      "2020-12-23 02:42:17,465 : INFO : frequencies processed\n",
      "2020-12-23 02:42:17,592 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:17,593 : INFO : entropies processed\n",
      "2020-12-23 02:42:17,596 : INFO : extropies processed\n",
      "2020-12-23 02:42:17,597 : INFO : token count processed\n",
      "2020-12-23 02:42:17,598 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:17,599 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:17,599 : INFO : vocab #2480\n",
      "2020-12-23 02:42:17,600 : INFO : diff #set()\n",
      "2020-12-23 02:42:17,858 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:17,985 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.0626338944257787, 0.4848170112507495], [0.6383461654186249, 0.36165383], [3.046439344671015, 1.3439456482381849], [5.310603774467064, 5.791362404253194, 6.196073980502739, 4.9058921982175185, 0.8854702060356745, 0.4047115762495448]]\n",
      "2020-12-23 02:42:17,988 : INFO : Removed 0 and 41 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:17,989 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:17,991 : INFO : built Dictionary(108 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 432 corpus positions)\n",
      "2020-12-23 02:42:18,064 : INFO : token count processed\n",
      "2020-12-23 02:42:18,067 : INFO : frequencies processed\n",
      "2020-12-23 02:42:18,195 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:18,195 : INFO : entropies processed\n",
      "2020-12-23 02:42:18,196 : INFO : extropies processed\n",
      "2020-12-23 02:42:18,197 : INFO : token count processed\n",
      "2020-12-23 02:42:18,198 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:18,199 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:18,199 : INFO : vocab #2480\n",
      "2020-12-23 02:42:18,200 : INFO : diff #set()\n",
      "2020-12-23 02:42:18,459 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:18,587 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.058075926618951, 0.4858907230127415], [0.6879459023475647, 0.3120541], [3.0168756957665637, 1.339014858943969], [5.310603774467064, 5.651670454631116, 6.0667749882766895, 4.895499240821492, 0.7561712138096253, 0.4151045336455734]]\n",
      "2020-12-23 02:42:18,590 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:18,591 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:18,592 : INFO : built Dictionary(76 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 142 corpus positions)\n",
      "2020-12-23 02:42:18,625 : INFO : token count processed\n",
      "2020-12-23 02:42:18,630 : INFO : frequencies processed\n",
      "2020-12-23 02:42:18,769 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:18,769 : INFO : entropies processed\n",
      "2020-12-23 02:42:18,770 : INFO : extropies processed\n",
      "2020-12-23 02:42:18,772 : INFO : token count processed\n",
      "2020-12-23 02:42:18,772 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:18,774 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:18,775 : INFO : vocab #2480\n",
      "2020-12-23 02:42:18,776 : INFO : diff #set()\n",
      "2020-12-23 02:42:19,042 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:19,169 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.0880608590002507, 0.4789132441660695], [0.6685514450073242, 0.33144855], [1.6644977792004614, 1.0957486925807722], [5.310603774467064, 4.8226207261920235, 5.913202404682571, 4.220022095976517, 0.6025986302155069, 1.0905816784905475]]\n",
      "2020-12-23 02:42:19,172 : INFO : Removed 0 and 50 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:19,173 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:19,174 : INFO : built Dictionary(118 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 312 corpus positions)\n",
      "2020-12-23 02:42:19,251 : INFO : token count processed\n",
      "2020-12-23 02:42:19,256 : INFO : frequencies processed\n",
      "2020-12-23 02:42:19,385 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:19,385 : INFO : entropies processed\n",
      "2020-12-23 02:42:19,386 : INFO : extropies processed\n",
      "2020-12-23 02:42:19,387 : INFO : token count processed\n",
      "2020-12-23 02:42:19,388 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:19,389 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:19,389 : INFO : vocab #2480\n",
      "2020-12-23 02:42:19,390 : INFO : diff #set()\n",
      "2020-12-23 02:42:19,657 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:19,785 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.0752951755400113, 0.4818591648003954], [0.6739406287670135, 0.32605937], [2.656564762130954, 1.3082512643991342], [5.310603774467064, 6.24862851613934, 6.645666483869505, 4.9135658067369, 1.3350627094024405, 0.39703796773016453]]\n",
      "2020-12-23 02:42:19,787 : INFO : Removed 0 and 32 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:19,788 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:19,790 : INFO : built Dictionary(118 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 428 corpus positions)\n",
      "2020-12-23 02:42:19,881 : INFO : token count processed\n",
      "2020-12-23 02:42:19,883 : INFO : frequencies processed\n",
      "2020-12-23 02:42:20,010 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:20,011 : INFO : entropies processed\n",
      "2020-12-23 02:42:20,011 : INFO : extropies processed\n",
      "2020-12-23 02:42:20,013 : INFO : token count processed\n",
      "2020-12-23 02:42:20,013 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:20,014 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:20,015 : INFO : vocab #2480\n",
      "2020-12-23 02:42:20,016 : INFO : diff #set()\n",
      "2020-12-23 02:42:20,283 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:20,411 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.115353432432721, 0.4727342413177591], [0.7557662725448608, 0.24423373], [2.8638263900908156, 1.3292848223421005], [5.310603774467064, 5.850156917433494, 6.263166576901844, 4.897594114998714, 0.9525628024347794, 0.4130096594683499]]\n",
      "2020-12-23 02:42:20,413 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:20,414 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:20,416 : INFO : built Dictionary(113 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 426 corpus positions)\n",
      "2020-12-23 02:42:20,495 : INFO : token count processed\n",
      "2020-12-23 02:42:20,500 : INFO : frequencies processed\n",
      "2020-12-23 02:42:20,628 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:20,629 : INFO : entropies processed\n",
      "2020-12-23 02:42:20,629 : INFO : extropies processed\n",
      "2020-12-23 02:42:20,630 : INFO : token count processed\n",
      "2020-12-23 02:42:20,631 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:20,632 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:20,633 : INFO : vocab #2480\n",
      "2020-12-23 02:42:20,634 : INFO : diff #set()\n",
      "2020-12-23 02:42:20,895 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:21,022 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.101363301323159, 0.47588153812828704], [0.748575359582901, 0.25142464], [2.827819531114783, 1.3222343829193264], [5.310603774467064, 5.6831976040360095, 6.118468264954816, 4.875333113548257, 0.8078644904877521, 0.43527066091880684]]\n",
      "2020-12-23 02:42:21,025 : INFO : Removed 0 and 52 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:21,026 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:21,027 : INFO : built Dictionary(102 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 404 corpus positions)\n",
      "2020-12-23 02:42:21,092 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:42:21,095 : INFO : frequencies processed\n",
      "2020-12-23 02:42:21,230 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:21,230 : INFO : entropies processed\n",
      "2020-12-23 02:42:21,231 : INFO : extropies processed\n",
      "2020-12-23 02:42:21,232 : INFO : token count processed\n",
      "2020-12-23 02:42:21,233 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:21,233 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:21,234 : INFO : vocab #2480\n",
      "2020-12-23 02:42:21,235 : INFO : diff #set()\n",
      "2020-12-23 02:42:21,491 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:21,619 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.1557639919266773, 0.46387267054510317], [0.8103837668895721, 0.18961623], [2.321928094887362, 1.2469329688117936], [5.310603774467064, 5.749308601266266, 6.223843917297337, 4.836068458435992, 0.913240142830273, 0.474535316031071]]\n",
      "2020-12-23 02:42:21,622 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:21,623 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:21,624 : INFO : built Dictionary(95 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 285 corpus positions)\n",
      "2020-12-23 02:42:21,678 : INFO : token count processed\n",
      "2020-12-23 02:42:21,681 : INFO : frequencies processed\n",
      "2020-12-23 02:42:21,808 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:21,809 : INFO : entropies processed\n",
      "2020-12-23 02:42:21,809 : INFO : extropies processed\n",
      "2020-12-23 02:42:21,811 : INFO : token count processed\n",
      "2020-12-23 02:42:21,812 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:21,813 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:21,814 : INFO : vocab #2480\n",
      "2020-12-23 02:42:21,815 : INFO : diff #set()\n",
      "2020-12-23 02:42:22,073 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:22,200 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.126768368179584, 0.47019694996496214], [0.7985779643058777, 0.20142204], [2.4591479170272446, 1.2910868757949967], [5.310603774467064, 5.015422548793484, 5.756322566309276, 4.569703756951272, 0.44571879184221164, 0.7409000175157923]]\n",
      "2020-12-23 02:42:22,203 : INFO : Removed 0 and 81 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:22,203 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:22,204 : INFO : built Dictionary(122 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 411 corpus positions)\n",
      "2020-12-23 02:42:22,285 : INFO : token count processed\n",
      "2020-12-23 02:42:22,287 : INFO : frequencies processed\n",
      "2020-12-23 02:42:22,415 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:22,416 : INFO : entropies processed\n",
      "2020-12-23 02:42:22,416 : INFO : extropies processed\n",
      "2020-12-23 02:42:22,418 : INFO : token count processed\n",
      "2020-12-23 02:42:22,419 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:22,420 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:22,421 : INFO : vocab #2480\n",
      "2020-12-23 02:42:22,422 : INFO : diff #set()\n",
      "2020-12-23 02:42:22,682 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:22,810 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.0676358735323166, 0.4836441526290679], [0.6983420252799988, 0.30165797], [2.8638263900908156, 1.3292848223421005], [5.310603774467064, 6.030001281822029, 6.397588786880303, 4.94301626940879, 1.0869850124132387, 0.36758750505827376]]\n",
      "2020-12-23 02:42:22,812 : INFO : Removed 0 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:22,813 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:22,815 : INFO : built Dictionary(107 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 374 corpus positions)\n",
      "2020-12-23 02:42:22,883 : INFO : token count processed\n",
      "2020-12-23 02:42:22,885 : INFO : frequencies processed\n",
      "2020-12-23 02:42:23,013 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:23,013 : INFO : entropies processed\n",
      "2020-12-23 02:42:23,014 : INFO : extropies processed\n",
      "2020-12-23 02:42:23,016 : INFO : token count processed\n",
      "2020-12-23 02:42:23,017 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:23,018 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:23,019 : INFO : vocab #2480\n",
      "2020-12-23 02:42:23,020 : INFO : diff #set()\n",
      "2020-12-23 02:42:23,290 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:23,419 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.0229913914300028, 0.49431747670123527], [0.6424818634986877, 0.35751814], [3.3087513512471665, 1.3587513851520319], [5.310603774467064, 5.9537092545441395, 6.327454900765749, 4.936858128245455, 1.0168511262986843, 0.3737456462216091]]\n",
      "2020-12-23 02:42:23,421 : INFO : Removed 0 and 90 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:23,422 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:23,424 : INFO : built Dictionary(119 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 375 corpus positions)\n",
      "2020-12-23 02:42:23,504 : INFO : token count processed\n",
      "2020-12-23 02:42:23,507 : INFO : frequencies processed\n",
      "2020-12-23 02:42:23,636 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:23,637 : INFO : entropies processed\n",
      "2020-12-23 02:42:23,638 : INFO : extropies processed\n",
      "2020-12-23 02:42:23,639 : INFO : token count processed\n",
      "2020-12-23 02:42:23,640 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:23,641 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:23,642 : INFO : vocab #2480\n",
      "2020-12-23 02:42:23,643 : INFO : diff #set()\n",
      "2020-12-23 02:42:23,901 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:24,029 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.0970507462726646, 0.4768601817468738], [0.7597578912973404, 0.24024211], [3.2359263506290326, 1.3581587235455306], [5.310603774467064, 6.184756445474906, 6.564878262495656, 4.930481957446315, 1.2542744880285914, 0.38012181702074965]]\n",
      "2020-12-23 02:42:24,032 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:24,033 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:24,034 : INFO : built Dictionary(130 unique tokens: ['access', 'add', 'addit', 'allow', 'api']...) from 2 documents (total 488 corpus positions)\n",
      "2020-12-23 02:42:24,120 : INFO : token count processed\n",
      "2020-12-23 02:42:24,123 : INFO : frequencies processed\n",
      "2020-12-23 02:42:24,252 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:24,252 : INFO : entropies processed\n",
      "2020-12-23 02:42:24,253 : INFO : extropies processed\n",
      "2020-12-23 02:42:24,254 : INFO : token count processed\n",
      "2020-12-23 02:42:24,255 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:24,256 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:24,257 : INFO : vocab #2480\n",
      "2020-12-23 02:42:24,257 : INFO : diff #set()\n",
      "2020-12-23 02:42:24,514 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:24,642 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.1015499527598847, 0.4758392721937152], [0.7110881209373474, 0.28891188], [2.836591668108979, 1.323937542311495], [5.310603774467064, 6.212221456585881, 6.542705857943762, 4.980119373109183, 1.2321020834766978, 0.3304844013578814]]\n",
      "2020-12-23 02:42:24,644 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:24,645 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:24,647 : INFO : built Dictionary(142 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 464 corpus positions)\n",
      "2020-12-23 02:42:24,736 : INFO : token count processed\n",
      "2020-12-23 02:42:24,744 : INFO : frequencies processed\n",
      "2020-12-23 02:42:24,872 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:24,872 : INFO : entropies processed\n",
      "2020-12-23 02:42:24,873 : INFO : extropies processed\n",
      "2020-12-23 02:42:24,874 : INFO : token count processed\n",
      "2020-12-23 02:42:24,875 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:24,876 : INFO : alphabet_target #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:42:24,877 : INFO : vocab #2480\n",
      "2020-12-23 02:42:24,878 : INFO : diff #set()\n",
      "2020-12-23 02:42:25,148 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:25,277 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.168083751569553, 0.46123679460079176], [0.7913572192192078, 0.20864278], [2.321928094887362, 1.2877123795494492], [4.770972031480867, 6.301552355933639, 6.611421965518408, 4.461102421896099, 1.840449934037541, 0.3098696095847693]]\n",
      "2020-12-23 02:42:25,280 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:25,281 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:25,282 : INFO : built Dictionary(179 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 700 corpus positions)\n",
      "2020-12-23 02:42:25,401 : INFO : token count processed\n",
      "2020-12-23 02:42:25,404 : INFO : frequencies processed\n",
      "2020-12-23 02:42:25,531 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:25,531 : INFO : entropies processed\n",
      "2020-12-23 02:42:25,532 : INFO : extropies processed\n",
      "2020-12-23 02:42:25,534 : INFO : token count processed\n",
      "2020-12-23 02:42:25,535 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:25,536 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:25,537 : INFO : vocab #2480\n",
      "2020-12-23 02:42:25,538 : INFO : diff #set()\n",
      "2020-12-23 02:42:25,803 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:25,931 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.1637572971957195, 0.46215904218833764], [0.7666780650615692, 0.23332193], [2.5216406363433186, 1.2998438251349493], [4.770972031480867, 6.739005504021667, 6.950130657346317, 4.559846878156217, 2.1791586258654494, 0.21112515332464987]]\n",
      "2020-12-23 02:42:25,934 : INFO : Removed 0 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:25,935 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:25,937 : INFO : built Dictionary(124 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 551 corpus positions)\n",
      "2020-12-23 02:42:26,008 : INFO : token count processed\n",
      "2020-12-23 02:42:26,010 : INFO : frequencies processed\n",
      "2020-12-23 02:42:26,136 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:26,137 : INFO : entropies processed\n",
      "2020-12-23 02:42:26,137 : INFO : extropies processed\n",
      "2020-12-23 02:42:26,139 : INFO : token count processed\n",
      "2020-12-23 02:42:26,140 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:26,140 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:26,141 : INFO : vocab #2480\n",
      "2020-12-23 02:42:26,142 : INFO : diff #set()\n",
      "2020-12-23 02:42:26,400 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:26,528 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.1985689778095026, 0.45484131273257966], [0.8955808281898499, 0.10441917], [2.25, 1.2709632597765914], [4.770972031480867, 5.870833373337847, 6.1655355109364125, 4.476269893882303, 1.3945634794555453, 0.29470213759856545]]\n",
      "2020-12-23 02:42:26,530 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:26,531 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:26,532 : INFO : built Dictionary(85 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 234 corpus positions)\n",
      "2020-12-23 02:42:26,568 : INFO : token count processed\n",
      "2020-12-23 02:42:26,571 : INFO : frequencies processed\n",
      "2020-12-23 02:42:26,699 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:26,700 : INFO : entropies processed\n",
      "2020-12-23 02:42:26,701 : INFO : extropies processed\n",
      "2020-12-23 02:42:26,706 : INFO : token count processed\n",
      "2020-12-23 02:42:26,708 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:26,709 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:26,710 : INFO : vocab #2480\n",
      "2020-12-23 02:42:26,711 : INFO : diff #set()\n",
      "2020-12-23 02:42:26,971 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:27,099 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.1428470822370225, 0.4666688576564462], [0.8224719762802124, 0.17752802], [1.8423709931771086, 1.1893232685884285], [4.770972031480867, 5.371881234145534, 5.936212641398694, 4.206640624227707, 1.1652406099178272, 0.5643314072531602]]\n",
      "2020-12-23 02:42:27,101 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:27,102 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:27,103 : INFO : built Dictionary(75 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 186 corpus positions)\n",
      "2020-12-23 02:42:27,134 : INFO : token count processed\n",
      "2020-12-23 02:42:27,136 : INFO : frequencies processed\n",
      "2020-12-23 02:42:27,270 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:27,271 : INFO : entropies processed\n",
      "2020-12-23 02:42:27,272 : INFO : extropies processed\n",
      "2020-12-23 02:42:27,274 : INFO : token count processed\n",
      "2020-12-23 02:42:27,276 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:27,277 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:27,278 : INFO : vocab #2480\n",
      "2020-12-23 02:42:27,280 : INFO : diff #set()\n",
      "2020-12-23 02:42:27,545 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:27,675 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.127765290945138, 0.46997664839048436], [0.8507680743932724, 0.14923193], [2.128085278891394, 1.2238339714721664], [4.770972031480867, 4.85108279267097, 5.6022360073115545, 4.019818816840282, 0.8312639758306872, 0.7511532146405848]]\n",
      "2020-12-23 02:42:27,678 : INFO : Removed 0 and 127 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:27,678 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:27,679 : INFO : built Dictionary(115 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 459 corpus positions)\n",
      "2020-12-23 02:42:27,739 : INFO : token count processed\n",
      "2020-12-23 02:42:27,744 : INFO : frequencies processed\n",
      "2020-12-23 02:42:27,880 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:27,881 : INFO : entropies processed\n",
      "2020-12-23 02:42:27,881 : INFO : extropies processed\n",
      "2020-12-23 02:42:27,882 : INFO : token count processed\n",
      "2020-12-23 02:42:27,883 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:27,883 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:27,884 : INFO : vocab #2480\n",
      "2020-12-23 02:42:27,885 : INFO : diff #set()\n",
      "2020-12-23 02:42:28,148 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:28,275 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.153626387444487, 0.4643330922345399], [0.8631530255079269, 0.13684697], [2.197159723424149, 1.2560617931988722], [4.770972031480867, 6.139571208108155, 6.419045789674904, 4.491497449914117, 1.6480737581940366, 0.279474581566749]]\n",
      "2020-12-23 02:42:28,278 : INFO : Removed 0 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:28,279 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:28,280 : INFO : built Dictionary(102 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 469 corpus positions)\n",
      "2020-12-23 02:42:28,328 : INFO : token count processed\n",
      "2020-12-23 02:42:28,331 : INFO : frequencies processed\n",
      "2020-12-23 02:42:28,458 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:28,459 : INFO : entropies processed\n",
      "2020-12-23 02:42:28,460 : INFO : extropies processed\n",
      "2020-12-23 02:42:28,461 : INFO : token count processed\n",
      "2020-12-23 02:42:28,462 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:28,463 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:28,464 : INFO : vocab #2480\n",
      "2020-12-23 02:42:28,465 : INFO : diff #set()\n",
      "2020-12-23 02:42:28,722 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:28,850 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.1404476282125728, 0.4671919961130148], [0.7273076772689819, 0.27269232], [1.6171895725927152, 1.1011855493497047], [4.770972031480867, 5.609710627339259, 5.967952651126723, 4.412730007693405, 1.1969806196458554, 0.3582420237874633]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:42:28,853 : INFO : Removed 0 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:28,854 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:28,855 : INFO : built Dictionary(188 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 1129 corpus positions)\n",
      "2020-12-23 02:42:28,974 : INFO : token count processed\n",
      "2020-12-23 02:42:28,977 : INFO : frequencies processed\n",
      "2020-12-23 02:42:29,104 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:29,105 : INFO : entropies processed\n",
      "2020-12-23 02:42:29,106 : INFO : extropies processed\n",
      "2020-12-23 02:42:29,108 : INFO : token count processed\n",
      "2020-12-23 02:42:29,109 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:29,109 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:29,110 : INFO : vocab #2480\n",
      "2020-12-23 02:42:29,111 : INFO : diff #set()\n",
      "2020-12-23 02:42:29,367 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:29,496 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.1338301007487248, 0.4686408724148736], [0.7816629558801651, 0.21833704], [3.170950594454669, 1.3519896400533202], [4.770972031480867, 7.2441902753576075, 7.341830319292438, 4.673331987546037, 2.5708582878115704, 0.09764004393483017]]\n",
      "2020-12-23 02:42:29,498 : INFO : Removed 0 and 136 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:29,499 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:29,501 : INFO : built Dictionary(153 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 736 corpus positions)\n",
      "2020-12-23 02:42:29,586 : INFO : token count processed\n",
      "2020-12-23 02:42:29,588 : INFO : frequencies processed\n",
      "2020-12-23 02:42:29,718 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:29,719 : INFO : entropies processed\n",
      "2020-12-23 02:42:29,719 : INFO : extropies processed\n",
      "2020-12-23 02:42:29,721 : INFO : token count processed\n",
      "2020-12-23 02:42:29,722 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:29,723 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:29,724 : INFO : vocab #2480\n",
      "2020-12-23 02:42:29,725 : INFO : diff #set()\n",
      "2020-12-23 02:42:29,989 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:30,121 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.1281065746996901, 0.46990127838927237], [0.7296589910984039, 0.270341], [2.498929563690935, 1.2766598002239642], [4.770972031480867, 6.2567074920449475, 6.458958328283989, 4.568721195241826, 1.687986296803122, 0.20225083623904183]]\n",
      "2020-12-23 02:42:30,124 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:30,125 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:30,126 : INFO : built Dictionary(102 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 246 corpus positions)\n",
      "2020-12-23 02:42:30,180 : INFO : token count processed\n",
      "2020-12-23 02:42:30,182 : INFO : frequencies processed\n",
      "2020-12-23 02:42:30,309 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:30,310 : INFO : entropies processed\n",
      "2020-12-23 02:42:30,311 : INFO : extropies processed\n",
      "2020-12-23 02:42:30,312 : INFO : token count processed\n",
      "2020-12-23 02:42:30,313 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:30,314 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:30,315 : INFO : vocab #2480\n",
      "2020-12-23 02:42:30,316 : INFO : diff #set()\n",
      "2020-12-23 02:42:30,574 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:30,703 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.1566877949964722, 0.4636739737295336], [0.7759425193071365, 0.22405748], [1.3709505944546687, 1.0438561897747245], [4.770972031480867, 5.7680018917339435, 6.268044297598268, 4.270929625616543, 1.4970722661174012, 0.5000424058643249]]\n",
      "2020-12-23 02:42:30,706 : INFO : Removed 0 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:30,707 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:30,709 : INFO : built Dictionary(192 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 787 corpus positions)\n",
      "2020-12-23 02:42:30,834 : INFO : token count processed\n",
      "2020-12-23 02:42:30,837 : INFO : frequencies processed\n",
      "2020-12-23 02:42:30,963 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:30,963 : INFO : entropies processed\n",
      "2020-12-23 02:42:30,964 : INFO : extropies processed\n",
      "2020-12-23 02:42:30,965 : INFO : token count processed\n",
      "2020-12-23 02:42:30,966 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:30,967 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:30,967 : INFO : vocab #2480\n",
      "2020-12-23 02:42:30,968 : INFO : diff #set()\n",
      "2020-12-23 02:42:31,241 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:31,370 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.092447259606892, 0.4779092975504052], [0.7149750590324402, 0.28502494], [3.327819531114783, 1.3601165249282492], [4.770972031480867, 6.846479111193757, 6.997393199394763, 4.620057943279862, 2.2264211679138954, 0.15091408820100582]]\n",
      "2020-12-23 02:42:31,373 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:31,374 : INFO : built Dictionary(53 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 90 corpus positions)\n",
      "2020-12-23 02:42:31,405 : INFO : token count processed\n",
      "2020-12-23 02:42:31,407 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:42:31,408 : INFO : frequencies processed\n",
      "2020-12-23 02:42:31,409 : INFO : token count processed\n",
      "2020-12-23 02:42:31,410 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:31,411 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:31,412 : INFO : vocab #2480\n",
      "2020-12-23 02:42:31,414 : INFO : diff #set()\n",
      "2020-12-23 02:42:31,674 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:31,801 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2055887128656548, 0.45339368766569815], [0.8820773139595985, 0.117922686], [nan, nan], [4.770972031480867, 4.165013816065912, 5.49686559608929, 3.4391202514574886, 0.7258935646084224, 1.3318517800233778]]\n",
      "2020-12-23 02:42:31,803 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:31,804 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:31,805 : INFO : built Dictionary(79 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 197 corpus positions)\n",
      "2020-12-23 02:42:31,839 : INFO : token count processed\n",
      "2020-12-23 02:42:31,844 : INFO : frequencies processed\n",
      "2020-12-23 02:42:31,983 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:31,984 : INFO : entropies processed\n",
      "2020-12-23 02:42:31,985 : INFO : extropies processed\n",
      "2020-12-23 02:42:31,987 : INFO : token count processed\n",
      "2020-12-23 02:42:31,988 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:31,990 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:31,991 : INFO : vocab #2480\n",
      "2020-12-23 02:42:31,992 : INFO : diff #set()\n",
      "2020-12-23 02:42:32,256 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:32,384 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.1633854716567027, 0.462238474419544], [0.7855261266231537, 0.21447387], [1.0, 1.0], [4.770972031480867, 5.449968864419248, 6.073073211506829, 4.147867684393287, 1.3021011800259616, 0.6231043470875806]]\n",
      "2020-12-23 02:42:32,387 : INFO : Removed 0 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:32,387 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:32,390 : INFO : built Dictionary(169 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 634 corpus positions)\n",
      "2020-12-23 02:42:32,507 : INFO : token count processed\n",
      "2020-12-23 02:42:32,510 : INFO : frequencies processed\n",
      "2020-12-23 02:42:32,637 : INFO : scalar_distribution processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:42:32,638 : INFO : entropies processed\n",
      "2020-12-23 02:42:32,641 : INFO : extropies processed\n",
      "2020-12-23 02:42:32,642 : INFO : token count processed\n",
      "2020-12-23 02:42:32,643 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:32,644 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:32,644 : INFO : vocab #2480\n",
      "2020-12-23 02:42:32,645 : INFO : diff #set()\n",
      "2020-12-23 02:42:32,902 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:33,030 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.1699041354746123, 0.4608498521439405], [0.8258924782276154, 0.17410752], [2.197159723424149, 1.2560617931988725], [4.770972031480867, 6.530294129310484, 6.748887275889292, 4.552378884902058, 1.9779152444084245, 0.21859314657880802]]\n",
      "2020-12-23 02:42:33,032 : INFO : Removed 0 and 142 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:33,033 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:33,035 : INFO : built Dictionary(140 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 650 corpus positions)\n",
      "2020-12-23 02:42:33,123 : INFO : token count processed\n",
      "2020-12-23 02:42:33,129 : INFO : frequencies processed\n",
      "2020-12-23 02:42:33,257 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:33,258 : INFO : entropies processed\n",
      "2020-12-23 02:42:33,258 : INFO : extropies processed\n",
      "2020-12-23 02:42:33,259 : INFO : token count processed\n",
      "2020-12-23 02:42:33,260 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:33,261 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:33,262 : INFO : vocab #2480\n",
      "2020-12-23 02:42:33,263 : INFO : diff #set()\n",
      "2020-12-23 02:42:33,520 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:33,656 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.1507976819022916, 0.4649437780291549], [0.8361218869686127, 0.16387811], [2.663532754804255, 1.307883000782319], [4.770972031480867, 6.470272233491701, 6.679388012565145, 4.5618562524074235, 1.9084159810842776, 0.2091157790734437]]\n",
      "2020-12-23 02:42:33,659 : INFO : Removed 0 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:33,659 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:33,661 : INFO : built Dictionary(143 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 439 corpus positions)\n",
      "2020-12-23 02:42:33,744 : INFO : token count processed\n",
      "2020-12-23 02:42:33,746 : INFO : frequencies processed\n",
      "2020-12-23 02:42:33,874 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:33,875 : INFO : entropies processed\n",
      "2020-12-23 02:42:33,876 : INFO : extropies processed\n",
      "2020-12-23 02:42:33,877 : INFO : token count processed\n",
      "2020-12-23 02:42:33,878 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:33,879 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:33,880 : INFO : vocab #2480\n",
      "2020-12-23 02:42:33,881 : INFO : diff #set()\n",
      "2020-12-23 02:42:34,139 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:34,266 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.1543388679531217, 0.4641795285205613], [0.7684769779443741, 0.23152302], [2.321928094887362, 1.2877123795494492], [4.770972031480867, 6.550038223589686, 6.8284422509251295, 4.492568004145425, 2.0574702194442622, 0.27840402733544334]]\n",
      "2020-12-23 02:42:34,269 : INFO : Removed 0 and 70 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:34,270 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:34,271 : INFO : built Dictionary(98 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 355 corpus positions)\n",
      "2020-12-23 02:42:34,317 : INFO : token count processed\n",
      "2020-12-23 02:42:34,319 : INFO : frequencies processed\n",
      "2020-12-23 02:42:34,446 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:34,447 : INFO : entropies processed\n",
      "2020-12-23 02:42:34,448 : INFO : extropies processed\n",
      "2020-12-23 02:42:34,449 : INFO : token count processed\n",
      "2020-12-23 02:42:34,450 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:34,451 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:34,452 : INFO : vocab #2480\n",
      "2020-12-23 02:42:34,454 : INFO : diff #set()\n",
      "2020-12-23 02:42:34,720 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:34,850 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.1344253782650298, 0.4685101714883334], [0.825536921620369, 0.17446308], [2.197159723424149, 1.2560617931988722], [4.770972031480867, 5.860525481261383, 6.224811683239339, 4.406685829502911, 1.4538396517584715, 0.364286201977956]]\n",
      "2020-12-23 02:42:34,855 : INFO : Removed 0 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:34,856 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:34,858 : INFO : built Dictionary(74 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 393 corpus positions)\n",
      "2020-12-23 02:42:34,891 : INFO : token count processed\n",
      "2020-12-23 02:42:34,895 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:42:34,896 : INFO : frequencies processed\n",
      "2020-12-23 02:42:34,898 : INFO : token count processed\n",
      "2020-12-23 02:42:34,899 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:34,901 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:34,902 : INFO : vocab #2480\n",
      "2020-12-23 02:42:34,903 : INFO : diff #set()\n",
      "2020-12-23 02:42:35,155 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:35,283 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.2679976130535053, 0.440917571625508], [0.912705160677433, 0.08729484], [nan, nan], [4.770972031480867, 5.945464049777852, 6.300191139986865, 4.416244941271856, 1.5292191085059974, 0.3547270902090123]]\n",
      "2020-12-23 02:42:35,285 : INFO : Removed 0 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:35,286 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:35,288 : INFO : built Dictionary(214 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 940 corpus positions)\n",
      "2020-12-23 02:42:35,432 : INFO : token count processed\n",
      "2020-12-23 02:42:35,434 : INFO : frequencies processed\n",
      "2020-12-23 02:42:35,560 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:35,561 : INFO : entropies processed\n",
      "2020-12-23 02:42:35,561 : INFO : extropies processed\n",
      "2020-12-23 02:42:35,562 : INFO : token count processed\n",
      "2020-12-23 02:42:35,563 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:35,564 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:35,564 : INFO : vocab #2480\n",
      "2020-12-23 02:42:35,565 : INFO : diff #set()\n",
      "2020-12-23 02:42:35,833 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:35,961 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.126804015565955, 0.470189068988519], [0.7223344445228577, 0.27766556], [2.7806390622295662, 1.3157882921475101], [4.770972031480867, 6.811563897304216, 6.9560690472078095, 4.626466881577275, 2.1850970157269423, 0.1445051499035932]]\n",
      "2020-12-23 02:42:35,964 : INFO : Removed 0 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:35,964 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:35,967 : INFO : built Dictionary(231 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 1035 corpus positions)\n",
      "2020-12-23 02:42:36,143 : INFO : token count processed\n",
      "2020-12-23 02:42:36,150 : INFO : frequencies processed\n",
      "2020-12-23 02:42:36,280 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:36,281 : INFO : entropies processed\n",
      "2020-12-23 02:42:36,281 : INFO : extropies processed\n",
      "2020-12-23 02:42:36,283 : INFO : token count processed\n",
      "2020-12-23 02:42:36,284 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:36,284 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:36,285 : INFO : vocab #2480\n",
      "2020-12-23 02:42:36,286 : INFO : diff #set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:42:36,544 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:36,671 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.1009213115594625, 0.4759816536192517], [0.7109296917915344, 0.2890703], [3.4464393446710155, 1.3676053818146474], [4.770972031480867, 7.502034948968415, 7.5883400559766985, 4.684666924472583, 2.8173680244958312, 0.08630510700828342]]\n",
      "2020-12-23 02:42:36,674 : INFO : Removed 0 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:36,675 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:36,677 : INFO : built Dictionary(275 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 1608 corpus positions)\n",
      "2020-12-23 02:42:36,900 : INFO : token count processed\n",
      "2020-12-23 02:42:36,906 : INFO : frequencies processed\n",
      "2020-12-23 02:42:37,031 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:37,032 : INFO : entropies processed\n",
      "2020-12-23 02:42:37,033 : INFO : extropies processed\n",
      "2020-12-23 02:42:37,034 : INFO : token count processed\n",
      "2020-12-23 02:42:37,035 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:37,036 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:37,036 : INFO : vocab #2480\n",
      "2020-12-23 02:42:37,037 : INFO : diff #set()\n",
      "2020-12-23 02:42:37,297 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:37,426 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.1285131051552926, 0.4698115306774406], [0.7566205710172653, 0.24337943], [3.4613201402110083, 1.368539624592205], [4.770972031480867, 7.39180093901977, 7.475524998429788, 4.687247972070849, 2.7045529669489206, 0.08372405941001748]]\n",
      "2020-12-23 02:42:37,428 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:37,429 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:37,430 : INFO : built Dictionary(70 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 163 corpus positions)\n",
      "2020-12-23 02:42:37,456 : INFO : token count processed\n",
      "2020-12-23 02:42:37,458 : INFO : frequencies processed\n",
      "2020-12-23 02:42:37,586 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:37,586 : INFO : entropies processed\n",
      "2020-12-23 02:42:37,587 : INFO : extropies processed\n",
      "2020-12-23 02:42:37,588 : INFO : token count processed\n",
      "2020-12-23 02:42:37,589 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:37,590 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:37,591 : INFO : vocab #2480\n",
      "2020-12-23 02:42:37,592 : INFO : diff #set()\n",
      "2020-12-23 02:42:37,849 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:37,976 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.1762021190862135, 0.4595161410925836], [0.8104742020368576, 0.1895258], [0.0, 0.0], [4.770972031480867, 4.927561309677364, 5.779772587995087, 3.918760753163145, 1.0088005565142195, 0.8522112783177231]]\n",
      "2020-12-23 02:42:37,979 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:37,980 : INFO : built Dictionary(38 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 64 corpus positions)\n",
      "2020-12-23 02:42:37,986 : INFO : token count processed\n",
      "2020-12-23 02:42:37,989 : INFO : frequencies processed\n",
      "2020-12-23 02:42:38,117 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:38,117 : INFO : entropies processed\n",
      "2020-12-23 02:42:38,118 : INFO : extropies processed\n",
      "2020-12-23 02:42:38,119 : INFO : token count processed\n",
      "2020-12-23 02:42:38,120 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:38,121 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:38,122 : INFO : vocab #2480\n",
      "2020-12-23 02:42:38,123 : INFO : diff #set()\n",
      "2020-12-23 02:42:38,381 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:38,509 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2036830486505452, 0.4537857658851455], [0.8150227963924408, 0.1849772], [0.0, 0.0], [4.770972031480867, 2.5216406363433186, 4.97993417945087, 2.312678488373315, 0.208962147970003, 2.4582935431075517]]\n",
      "2020-12-23 02:42:38,512 : INFO : Removed 0 and 530 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:38,513 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:38,515 : INFO : built Dictionary(340 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 2936 corpus positions)\n",
      "2020-12-23 02:42:38,841 : INFO : token count processed\n",
      "2020-12-23 02:42:38,844 : INFO : frequencies processed\n",
      "2020-12-23 02:42:38,972 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:38,973 : INFO : entropies processed\n",
      "2020-12-23 02:42:38,973 : INFO : extropies processed\n",
      "2020-12-23 02:42:38,975 : INFO : token count processed\n",
      "2020-12-23 02:42:38,976 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:38,977 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:38,977 : INFO : vocab #2480\n",
      "2020-12-23 02:42:38,978 : INFO : diff #set()\n",
      "2020-12-23 02:42:39,248 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:39,377 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.0465006595555377, 0.4886389825142698], [0.6499083638191223, 0.35009164], [4.120635070586275, 1.3895985064921268], [4.770972031480867, 7.480007711014331, 7.513049719344535, 4.737930023150662, 2.7420776878636675, 0.0330420083302041]]\n",
      "2020-12-23 02:42:39,380 : INFO : Removed 0 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:39,381 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:39,384 : INFO : built Dictionary(221 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 1085 corpus positions)\n",
      "2020-12-23 02:42:39,548 : INFO : token count processed\n",
      "2020-12-23 02:42:39,551 : INFO : frequencies processed\n",
      "2020-12-23 02:42:39,679 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:39,680 : INFO : entropies processed\n",
      "2020-12-23 02:42:39,680 : INFO : extropies processed\n",
      "2020-12-23 02:42:39,681 : INFO : token count processed\n",
      "2020-12-23 02:42:39,682 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:39,683 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:39,684 : INFO : vocab #2480\n",
      "2020-12-23 02:42:39,686 : INFO : diff #set()\n",
      "2020-12-23 02:42:39,946 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:40,073 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.0047984530563259, 0.49880325799109365], [0.6202639639377594, 0.37973604], [3.8062389286533898, 1.3755406966835422], [4.770972031480867, 7.131331012509435, 7.201928197777697, 4.700374846212604, 2.4309561662968298, 0.07059718526826231]]\n",
      "2020-12-23 02:42:40,076 : INFO : Removed 0 and 166 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:40,077 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:40,079 : INFO : built Dictionary(216 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 904 corpus positions)\n",
      "2020-12-23 02:42:40,231 : INFO : token count processed\n",
      "2020-12-23 02:42:40,237 : INFO : frequencies processed\n",
      "2020-12-23 02:42:40,364 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:40,365 : INFO : entropies processed\n",
      "2020-12-23 02:42:40,365 : INFO : extropies processed\n",
      "2020-12-23 02:42:40,366 : INFO : token count processed\n",
      "2020-12-23 02:42:40,367 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:40,368 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:40,368 : INFO : vocab #2480\n",
      "2020-12-23 02:42:40,370 : INFO : diff #set()\n",
      "2020-12-23 02:42:40,625 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:40,752 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.1339001595571254, 0.46862548630557405], [0.7271790504455566, 0.27282095], [3.108694969562842, 1.3252061213745068], [4.770972031480867, 7.203742744794778, 7.324635045045344, 4.650079731230301, 2.553663013564477, 0.12089230025056619]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:42:40,754 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:40,755 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:40,756 : INFO : built Dictionary(79 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 236 corpus positions)\n",
      "2020-12-23 02:42:40,790 : INFO : token count processed\n",
      "2020-12-23 02:42:40,795 : INFO : frequencies processed\n",
      "2020-12-23 02:42:40,931 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:40,931 : INFO : entropies processed\n",
      "2020-12-23 02:42:40,932 : INFO : extropies processed\n",
      "2020-12-23 02:42:40,933 : INFO : token count processed\n",
      "2020-12-23 02:42:40,934 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:40,935 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:40,936 : INFO : vocab #2480\n",
      "2020-12-23 02:42:40,938 : INFO : diff #set()\n",
      "2020-12-23 02:42:41,199 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:41,327 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.0594297612740147, 0.48557130658409786], [0.7093963623046875, 0.29060364], [2.4193819456463714, 1.2761517340193214], [4.770972031480867, 5.195502554608948, 5.773235706911292, 4.193238879178525, 1.0022636754304246, 0.5777331523023435]]\n",
      "2020-12-23 02:42:41,330 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:41,331 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:41,332 : INFO : built Dictionary(85 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 297 corpus positions)\n",
      "2020-12-23 02:42:41,368 : INFO : token count processed\n",
      "2020-12-23 02:42:41,370 : INFO : frequencies processed\n",
      "2020-12-23 02:42:41,499 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:41,499 : INFO : entropies processed\n",
      "2020-12-23 02:42:41,500 : INFO : extropies processed\n",
      "2020-12-23 02:42:41,501 : INFO : token count processed\n",
      "2020-12-23 02:42:41,502 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:41,503 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:41,504 : INFO : vocab #2480\n",
      "2020-12-23 02:42:41,505 : INFO : diff #set()\n",
      "2020-12-23 02:42:41,762 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:41,889 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.106626303121259, 0.4746926393724228], [0.7256211936473846, 0.2743788], [2.2221915755066783, 1.2138650192953737], [4.770972031480867, 5.32027245610305, 5.818552900286186, 4.272691587297731, 1.0475808688053192, 0.4982804441831368]]\n",
      "2020-12-23 02:42:41,892 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:41,893 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:41,895 : INFO : built Dictionary(179 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 537 corpus positions)\n",
      "2020-12-23 02:42:42,022 : INFO : token count processed\n",
      "2020-12-23 02:42:42,025 : INFO : frequencies processed\n",
      "2020-12-23 02:42:42,158 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:42,158 : INFO : entropies processed\n",
      "2020-12-23 02:42:42,159 : INFO : extropies processed\n",
      "2020-12-23 02:42:42,161 : INFO : token count processed\n",
      "2020-12-23 02:42:42,162 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:42,163 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:42,164 : INFO : vocab #2480\n",
      "2020-12-23 02:42:42,165 : INFO : diff #set()\n",
      "2020-12-23 02:42:42,426 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:42,554 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.0742937105193577, 0.48209180547995867], [0.6581195890903473, 0.3418804], [3.4464393446710155, 1.3676053818146474], [4.770972031480867, 6.898202761357263, 7.054549259386984, 4.6146255334511475, 2.2835772279061164, 0.15634649802972067]]\n",
      "2020-12-23 02:42:42,556 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:42,557 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:42,559 : INFO : built Dictionary(147 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 559 corpus positions)\n",
      "2020-12-23 02:42:42,657 : INFO : token count processed\n",
      "2020-12-23 02:42:42,659 : INFO : frequencies processed\n",
      "2020-12-23 02:42:42,789 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:42,790 : INFO : entropies processed\n",
      "2020-12-23 02:42:42,791 : INFO : extropies processed\n",
      "2020-12-23 02:42:42,792 : INFO : token count processed\n",
      "2020-12-23 02:42:42,793 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:42,793 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:42,794 : INFO : vocab #2480\n",
      "2020-12-23 02:42:42,795 : INFO : diff #set()\n",
      "2020-12-23 02:42:43,052 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:43,180 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.0276786265410418, 0.4931747994532402], [0.6634157001972198, 0.3365843], [2.6601297526332566, 1.2849790309834181], [4.770972031480867, 6.388500481644799, 6.590550324452359, 4.568922188673309, 1.8195782929714914, 0.20204984280755944]]\n",
      "2020-12-23 02:42:43,182 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:43,183 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:43,185 : INFO : built Dictionary(72 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 223 corpus positions)\n",
      "2020-12-23 02:42:43,218 : INFO : token count processed\n",
      "2020-12-23 02:42:43,220 : INFO : frequencies processed\n",
      "2020-12-23 02:42:43,352 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:43,353 : INFO : entropies processed\n",
      "2020-12-23 02:42:43,353 : INFO : extropies processed\n",
      "2020-12-23 02:42:43,355 : INFO : token count processed\n",
      "2020-12-23 02:42:43,356 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:43,357 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:43,357 : INFO : vocab #2480\n",
      "2020-12-23 02:42:43,358 : INFO : diff #set()\n",
      "2020-12-23 02:42:43,616 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:43,743 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.080890241449319, 0.4805635492353072], [0.7398797571659088, 0.26012024], [1.8423709931771084, 1.1893232685884285], [4.770972031480867, 4.8191513650620195, 5.527755729795517, 4.0623676667473685, 0.7567836983146501, 0.7086043647334979]]\n",
      "2020-12-23 02:42:43,746 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:43,747 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:43,748 : INFO : built Dictionary(77 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 278 corpus positions)\n",
      "2020-12-23 02:42:43,784 : INFO : token count processed\n",
      "2020-12-23 02:42:43,786 : INFO : frequencies processed\n",
      "2020-12-23 02:42:43,918 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:43,919 : INFO : entropies processed\n",
      "2020-12-23 02:42:43,920 : INFO : extropies processed\n",
      "2020-12-23 02:42:43,921 : INFO : token count processed\n",
      "2020-12-23 02:42:43,922 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:43,923 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:43,924 : INFO : vocab #2480\n",
      "2020-12-23 02:42:43,925 : INFO : diff #set()\n",
      "2020-12-23 02:42:44,184 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:44,312 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.119412774057064, 0.4718288066584407], [0.737208902835846, 0.2627911], [1.6577427265048887, 1.104054509393245], [4.770972031480867, 5.062480936779194, 5.645474121641842, 4.18797884661822, 0.8745020901609744, 0.582993184862648]]\n",
      "2020-12-23 02:42:44,315 : INFO : Removed 0 and 223 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:44,316 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:44,318 : INFO : built Dictionary(262 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 1832 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:42:44,529 : INFO : token count processed\n",
      "2020-12-23 02:42:44,531 : INFO : frequencies processed\n",
      "2020-12-23 02:42:44,661 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:44,662 : INFO : entropies processed\n",
      "2020-12-23 02:42:44,665 : INFO : extropies processed\n",
      "2020-12-23 02:42:44,667 : INFO : token count processed\n",
      "2020-12-23 02:42:44,667 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:44,668 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:44,669 : INFO : vocab #2480\n",
      "2020-12-23 02:42:44,670 : INFO : diff #set()\n",
      "2020-12-23 02:42:44,938 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:45,066 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.1626793625687186, 0.46238939405805013], [0.7426838874816895, 0.2573161], [3.202819531114783, 1.354100834893818], [4.770972031480867, 7.185085743102134, 7.276642827204739, 4.6794149473782625, 2.505670795723872, 0.09155708410260477]]\n",
      "2020-12-23 02:42:45,069 : INFO : Removed 0 and 148 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:45,070 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:45,071 : INFO : built Dictionary(179 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 807 corpus positions)\n",
      "2020-12-23 02:42:45,187 : INFO : token count processed\n",
      "2020-12-23 02:42:45,191 : INFO : frequencies processed\n",
      "2020-12-23 02:42:45,319 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:45,320 : INFO : entropies processed\n",
      "2020-12-23 02:42:45,320 : INFO : extropies processed\n",
      "2020-12-23 02:42:45,322 : INFO : token count processed\n",
      "2020-12-23 02:42:45,323 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:45,324 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:45,324 : INFO : vocab #2480\n",
      "2020-12-23 02:42:45,325 : INFO : diff #set()\n",
      "2020-12-23 02:42:45,586 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:45,713 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.1029944250423054, 0.47551243507451685], [0.7278936505317688, 0.27210635], [2.9329993066372895, 1.3053812878934759], [4.770972031480867, 6.591225336124281, 6.7497177530019705, 4.612479614603178, 1.9787457215211033, 0.15849241687768956]]\n",
      "2020-12-23 02:42:45,716 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:45,716 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:45,717 : INFO : built Dictionary(64 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 145 corpus positions)\n",
      "2020-12-23 02:42:45,740 : INFO : token count processed\n",
      "2020-12-23 02:42:45,742 : INFO : frequencies processed\n",
      "2020-12-23 02:42:45,881 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:45,882 : INFO : entropies processed\n",
      "2020-12-23 02:42:45,883 : INFO : extropies processed\n",
      "2020-12-23 02:42:45,885 : INFO : token count processed\n",
      "2020-12-23 02:42:45,887 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:45,888 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:45,889 : INFO : vocab #2480\n",
      "2020-12-23 02:42:45,890 : INFO : diff #set()\n",
      "2020-12-23 02:42:46,150 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:46,278 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.1533181121428602, 0.46439956751436823], [0.8199726045131683, 0.1800274], [1.5, 1.1225562489182657], [4.770972031480867, 4.7032114441396695, 5.631407474370456, 3.8427760012500807, 0.8604354428895888, 0.9281960302307866]]\n",
      "2020-12-23 02:42:46,280 : INFO : Removed 0 and 9 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:46,281 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:46,283 : INFO : built Dictionary(137 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 385 corpus positions)\n",
      "2020-12-23 02:42:46,363 : INFO : token count processed\n",
      "2020-12-23 02:42:46,368 : INFO : frequencies processed\n",
      "2020-12-23 02:42:46,499 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:46,500 : INFO : entropies processed\n",
      "2020-12-23 02:42:46,500 : INFO : extropies processed\n",
      "2020-12-23 02:42:46,502 : INFO : token count processed\n",
      "2020-12-23 02:42:46,502 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:46,503 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:46,504 : INFO : vocab #2480\n",
      "2020-12-23 02:42:46,505 : INFO : diff #set()\n",
      "2020-12-23 02:42:46,770 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:46,899 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.1326698136175575, 0.4688958382656256], [0.7365910112857819, 0.263409], [2.8464393446710154, 1.3178207096846455], [4.770972031480867, 6.14228447828618, 6.481238673297339, 4.432017836469708, 1.7102666418164718, 0.3389541950111594]]\n",
      "2020-12-23 02:42:46,901 : INFO : Removed 0 and 184 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:46,902 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:46,904 : INFO : built Dictionary(267 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 1175 corpus positions)\n",
      "2020-12-23 02:42:47,119 : INFO : token count processed\n",
      "2020-12-23 02:42:47,121 : INFO : frequencies processed\n",
      "2020-12-23 02:42:47,247 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:47,248 : INFO : entropies processed\n",
      "2020-12-23 02:42:47,248 : INFO : extropies processed\n",
      "2020-12-23 02:42:47,249 : INFO : token count processed\n",
      "2020-12-23 02:42:47,250 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:47,251 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:47,252 : INFO : vocab #2480\n",
      "2020-12-23 02:42:47,253 : INFO : diff #set()\n",
      "2020-12-23 02:42:47,512 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:47,639 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.1057998706749672, 0.47487893504308765], [0.6729117929935455, 0.3270882], [3.882045108136862, 1.3879142369542155], [4.770972031480867, 7.450178124335845, 7.5373848748635295, 4.683765280953183, 2.7664128433826622, 0.08720675052768456]]\n",
      "2020-12-23 02:42:47,642 : INFO : Removed 0 and 16 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:47,643 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:47,644 : INFO : built Dictionary(79 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 223 corpus positions)\n",
      "2020-12-23 02:42:47,676 : INFO : token count processed\n",
      "2020-12-23 02:42:47,678 : INFO : frequencies processed\n",
      "2020-12-23 02:42:47,806 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:47,806 : INFO : entropies processed\n",
      "2020-12-23 02:42:47,807 : INFO : extropies processed\n",
      "2020-12-23 02:42:47,808 : INFO : token count processed\n",
      "2020-12-23 02:42:47,809 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:47,810 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:47,811 : INFO : vocab #2480\n",
      "2020-12-23 02:42:47,812 : INFO : diff #set()\n",
      "2020-12-23 02:42:48,069 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:48,197 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.048702198644846, 0.48811389017958273], [0.711372435092926, 0.28862756], [2.4193819456463714, 1.2761517340193214], [4.770972031480867, 5.20665021947654, 5.78019536650538, 4.197426884452026, 1.0092233350245126, 0.57354514702884]]\n",
      "2020-12-23 02:42:48,199 : INFO : Removed 0 and 89 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:48,200 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:48,201 : INFO : built Dictionary(149 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 561 corpus positions)\n",
      "2020-12-23 02:42:48,290 : INFO : token count processed\n",
      "2020-12-23 02:42:48,292 : INFO : frequencies processed\n",
      "2020-12-23 02:42:48,426 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:48,426 : INFO : entropies processed\n",
      "2020-12-23 02:42:48,427 : INFO : extropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:42:48,428 : INFO : token count processed\n",
      "2020-12-23 02:42:48,429 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:48,430 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:48,430 : INFO : vocab #2480\n",
      "2020-12-23 02:42:48,431 : INFO : diff #set()\n",
      "2020-12-23 02:42:48,686 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:48,814 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.1607497114001477, 0.4628023295451505], [0.7675171792507172, 0.23248282], [2.058813890331201, 1.2062416803425784], [4.770972031480867, 6.524718477352, 6.764333377067496, 4.53135713176537, 1.9933613455866288, 0.23961489971549632]]\n",
      "2020-12-23 02:42:48,816 : INFO : Removed 0 and 8 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:48,817 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:48,819 : INFO : built Dictionary(86 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 268 corpus positions)\n",
      "2020-12-23 02:42:48,862 : INFO : token count processed\n",
      "2020-12-23 02:42:48,865 : INFO : frequencies processed\n",
      "2020-12-23 02:42:48,992 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:48,993 : INFO : entropies processed\n",
      "2020-12-23 02:42:48,994 : INFO : extropies processed\n",
      "2020-12-23 02:42:48,996 : INFO : token count processed\n",
      "2020-12-23 02:42:48,998 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:48,999 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:49,000 : INFO : vocab #2480\n",
      "2020-12-23 02:42:49,002 : INFO : diff #set()\n",
      "2020-12-23 02:42:49,261 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:49,388 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.092340270317555, 0.4779337348643726], [0.7465183436870575, 0.25348166], [2.1556390622295662, 1.2407663947533205], [4.770972031480867, 5.321859380715434, 5.850332710720232, 4.242498701476069, 1.0793606792393646, 0.5284733300047977]]\n",
      "2020-12-23 02:42:49,391 : INFO : Removed 0 and 119 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:49,392 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:49,394 : INFO : built Dictionary(157 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 886 corpus positions)\n",
      "2020-12-23 02:42:49,493 : INFO : token count processed\n",
      "2020-12-23 02:42:49,501 : INFO : frequencies processed\n",
      "2020-12-23 02:42:49,629 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:49,630 : INFO : entropies processed\n",
      "2020-12-23 02:42:49,631 : INFO : extropies processed\n",
      "2020-12-23 02:42:49,632 : INFO : token count processed\n",
      "2020-12-23 02:42:49,633 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:49,634 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:49,634 : INFO : vocab #2480\n",
      "2020-12-23 02:42:49,635 : INFO : diff #set()\n",
      "2020-12-23 02:42:49,894 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:50,022 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.0095436078885274, 0.49762542901506], [0.6209501326084137, 0.37904987], [3.4072682296173897, 1.3520177890881737], [4.770972031480867, 6.500767808767801, 6.628724374641165, 4.643015465607502, 1.857752343160298, 0.12795656587336435]]\n",
      "2020-12-23 02:42:50,024 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:50,025 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:50,027 : INFO : built Dictionary(62 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 108 corpus positions)\n",
      "2020-12-23 02:42:50,055 : INFO : token count processed\n",
      "2020-12-23 02:42:50,057 : INFO : frequencies processed\n",
      "2020-12-23 02:42:50,187 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:50,188 : INFO : entropies processed\n",
      "2020-12-23 02:42:50,189 : INFO : extropies processed\n",
      "2020-12-23 02:42:50,190 : INFO : token count processed\n",
      "2020-12-23 02:42:50,191 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:50,192 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:50,193 : INFO : vocab #2480\n",
      "2020-12-23 02:42:50,194 : INFO : diff #set()\n",
      "2020-12-23 02:42:50,459 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:50,587 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.1773441458484342, 0.45927512281726834], [0.8480230569839478, 0.15197694], [0.0, 0.0], [4.770972031480867, 4.736228843383063, 5.727604710074523, 3.7795961647894067, 0.9566326785936559, 0.9913758666914605]]\n",
      "2020-12-23 02:42:50,589 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:50,590 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:50,591 : INFO : built Dictionary(119 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 449 corpus positions)\n",
      "2020-12-23 02:42:50,651 : INFO : token count processed\n",
      "2020-12-23 02:42:50,653 : INFO : frequencies processed\n",
      "2020-12-23 02:42:50,783 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:50,783 : INFO : entropies processed\n",
      "2020-12-23 02:42:50,784 : INFO : extropies processed\n",
      "2020-12-23 02:42:50,785 : INFO : token count processed\n",
      "2020-12-23 02:42:50,786 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:50,787 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:50,787 : INFO : vocab #2480\n",
      "2020-12-23 02:42:50,788 : INFO : diff #set()\n",
      "2020-12-23 02:42:51,045 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:51,172 : INFO : Computed distances or similarities ('287', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.1262225930769147, 0.4703176437199233], [0.7595125138759613, 0.24048749], [2.4464393446710155, 1.2856945251022456], [4.770972031480867, 5.788442787590127, 6.142550515706748, 4.416864303364246, 1.3715784842258811, 0.35410772811662117]]\n",
      "2020-12-23 02:42:51,175 : INFO : Removed 0 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:51,176 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:51,177 : INFO : built Dictionary(80 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 135 corpus positions)\n",
      "2020-12-23 02:42:51,209 : INFO : token count processed\n",
      "2020-12-23 02:42:51,212 : INFO : frequencies processed\n",
      "2020-12-23 02:42:51,339 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:51,340 : INFO : entropies processed\n",
      "2020-12-23 02:42:51,341 : INFO : extropies processed\n",
      "2020-12-23 02:42:51,342 : INFO : token count processed\n",
      "2020-12-23 02:42:51,343 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:51,344 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:51,345 : INFO : vocab #2480\n",
      "2020-12-23 02:42:51,346 : INFO : diff #set()\n",
      "2020-12-23 02:42:51,603 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:51,731 : INFO : Computed distances or similarities ('287', 'sacp-python-common/setup.py')[[1.1792339798952018, 0.4588768389377305], [0.8456666022539139, 0.1543334], [1.0, 1.0], [4.770972031480867, 5.370004292053436, 6.062151331134715, 4.078824992399588, 1.291179299653848, 0.692147039081279]]\n",
      "2020-12-23 02:42:51,733 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:51,734 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:51,735 : INFO : built Dictionary(105 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 380 corpus positions)\n",
      "2020-12-23 02:42:51,791 : INFO : token count processed\n",
      "2020-12-23 02:42:51,794 : INFO : frequencies processed\n",
      "2020-12-23 02:42:51,921 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:51,921 : INFO : entropies processed\n",
      "2020-12-23 02:42:51,922 : INFO : extropies processed\n",
      "2020-12-23 02:42:51,923 : INFO : token count processed\n",
      "2020-12-23 02:42:51,924 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:51,924 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:51,925 : INFO : vocab #2480\n",
      "2020-12-23 02:42:51,926 : INFO : diff #set()\n",
      "2020-12-23 02:42:52,190 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:52,318 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.178203500726766, 0.45909392748030486], [0.8375726938247681, 0.1624273], [1.584962500721156, 1.1699250014423124], [4.770972031480867, 5.695663584743922, 6.127765982087018, 4.338869634137771, 1.3567939506061508, 0.43210239734309575]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:42:52,321 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:52,322 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:52,323 : INFO : built Dictionary(66 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 155 corpus positions)\n",
      "2020-12-23 02:42:52,354 : INFO : token count processed\n",
      "2020-12-23 02:42:52,356 : INFO : frequencies processed\n",
      "2020-12-23 02:42:52,486 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:52,487 : INFO : entropies processed\n",
      "2020-12-23 02:42:52,488 : INFO : extropies processed\n",
      "2020-12-23 02:42:52,489 : INFO : token count processed\n",
      "2020-12-23 02:42:52,490 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:52,491 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:52,492 : INFO : vocab #2480\n",
      "2020-12-23 02:42:52,493 : INFO : diff #set()\n",
      "2020-12-23 02:42:52,759 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:52,888 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.1724240347101718, 0.4603152902114767], [0.8657518625259399, 0.13424814], [1.584962500721156, 1.1699250014423124], [4.770972031480867, 4.9004417692112465, 5.745136732425699, 3.9262770682664154, 0.974164700944832, 0.8446949632144527]]\n",
      "2020-12-23 02:42:52,890 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:52,891 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:52,892 : INFO : built Dictionary(64 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 162 corpus positions)\n",
      "2020-12-23 02:42:52,914 : INFO : token count processed\n",
      "2020-12-23 02:42:52,916 : INFO : frequencies processed\n",
      "2020-12-23 02:42:53,043 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:53,044 : INFO : entropies processed\n",
      "2020-12-23 02:42:53,045 : INFO : extropies processed\n",
      "2020-12-23 02:42:53,046 : INFO : token count processed\n",
      "2020-12-23 02:42:53,047 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:53,048 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:53,048 : INFO : vocab #2480\n",
      "2020-12-23 02:42:53,050 : INFO : diff #set()\n",
      "2020-12-23 02:42:53,305 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:53,433 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.1712338293387459, 0.46056762127023054], [0.8683467507362366, 0.13165325], [1.5, 1.1225562489182657], [4.770972031480867, 4.778624108914332, 5.6476833538923366, 3.901912786502862, 0.8767113224114693, 0.8690592449780041]]\n",
      "2020-12-23 02:42:53,438 : INFO : Removed 0 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:53,440 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:53,442 : INFO : built Dictionary(66 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 193 corpus positions)\n",
      "2020-12-23 02:42:53,469 : INFO : token count processed\n",
      "2020-12-23 02:42:53,472 : INFO : frequencies processed\n",
      "2020-12-23 02:42:53,603 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:53,604 : INFO : entropies processed\n",
      "2020-12-23 02:42:53,604 : INFO : extropies processed\n",
      "2020-12-23 02:42:53,606 : INFO : token count processed\n",
      "2020-12-23 02:42:53,607 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:53,608 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:53,609 : INFO : vocab #2480\n",
      "2020-12-23 02:42:53,610 : INFO : diff #set()\n",
      "2020-12-23 02:42:53,875 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:54,003 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.200638484149807, 0.45441357460688925], [0.9091172143816948, 0.090882786], [1.0, 1.0], [4.770972031480867, 4.773880192225086, 5.607677021332574, 3.9371752023733784, 0.8367049898517065, 0.833796829107488]]\n",
      "2020-12-23 02:42:54,006 : INFO : Removed 0 and 346 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:54,007 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:54,008 : INFO : built Dictionary(173 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 2018 corpus positions)\n",
      "2020-12-23 02:42:54,110 : INFO : token count processed\n",
      "2020-12-23 02:42:54,113 : INFO : frequencies processed\n",
      "2020-12-23 02:42:54,239 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:54,240 : INFO : entropies processed\n",
      "2020-12-23 02:42:54,241 : INFO : extropies processed\n",
      "2020-12-23 02:42:54,242 : INFO : token count processed\n",
      "2020-12-23 02:42:54,243 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:54,244 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:54,245 : INFO : vocab #2480\n",
      "2020-12-23 02:42:54,246 : INFO : diff #set()\n",
      "2020-12-23 02:42:54,507 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:54,635 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.185759614747798, 0.4575068517383071], [0.8714377880096436, 0.12856221], [2.4056390622295662, 1.2666171566933806], [4.770972031480867, 6.620773041953877, 6.724881296148618, 4.666863777286126, 1.9539092646677512, 0.10410825419474179]]\n",
      "2020-12-23 02:42:54,638 : INFO : Removed 0 and 132 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:54,639 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:54,640 : INFO : built Dictionary(104 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 581 corpus positions)\n",
      "2020-12-23 02:42:54,700 : INFO : token count processed\n",
      "2020-12-23 02:42:54,702 : INFO : frequencies processed\n",
      "2020-12-23 02:42:54,835 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:54,836 : INFO : entropies processed\n",
      "2020-12-23 02:42:54,837 : INFO : extropies processed\n",
      "2020-12-23 02:42:54,838 : INFO : token count processed\n",
      "2020-12-23 02:42:54,840 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:54,841 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:54,841 : INFO : vocab #2480\n",
      "2020-12-23 02:42:54,842 : INFO : diff #set()\n",
      "2020-12-23 02:42:55,104 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:55,231 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.1505397307708998, 0.464999546714504], [0.7950132042169571, 0.2049868], [2.321928094887362, 1.2877123795494492], [4.770972031480867, 5.828370634755606, 6.120111846085505, 4.479230820150969, 1.349139814604638, 0.2917412113298994]]\n",
      "2020-12-23 02:42:55,234 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:55,235 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:55,236 : INFO : built Dictionary(103 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 344 corpus positions)\n",
      "2020-12-23 02:42:55,286 : INFO : token count processed\n",
      "2020-12-23 02:42:55,291 : INFO : frequencies processed\n",
      "2020-12-23 02:42:55,418 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:55,418 : INFO : entropies processed\n",
      "2020-12-23 02:42:55,419 : INFO : extropies processed\n",
      "2020-12-23 02:42:55,420 : INFO : token count processed\n",
      "2020-12-23 02:42:55,421 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:55,421 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:55,422 : INFO : vocab #2480\n",
      "2020-12-23 02:42:55,423 : INFO : diff #set()\n",
      "2020-12-23 02:42:55,690 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:55,819 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.1133305210389546, 0.47318674956172047], [0.7748239636421204, 0.22517604], [2.725480556997868, 1.3192201298976014], [4.770972031480867, 5.774409284925443, 6.163016335444958, 4.382364980961352, 1.3920443039640906, 0.38860705051951516]]\n",
      "2020-12-23 02:42:55,821 : INFO : Removed 0 and 51 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:55,822 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:55,823 : INFO : built Dictionary(115 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 393 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:42:55,881 : INFO : token count processed\n",
      "2020-12-23 02:42:55,883 : INFO : frequencies processed\n",
      "2020-12-23 02:42:56,011 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:56,012 : INFO : entropies processed\n",
      "2020-12-23 02:42:56,013 : INFO : extropies processed\n",
      "2020-12-23 02:42:56,014 : INFO : token count processed\n",
      "2020-12-23 02:42:56,015 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:56,016 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:56,017 : INFO : vocab #2480\n",
      "2020-12-23 02:42:56,018 : INFO : diff #set()\n",
      "2020-12-23 02:42:56,277 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:56,405 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.1451029588385695, 0.4661780899045673], [0.7763667702674866, 0.22363323], [2.4056390622295662, 1.2666171566933806], [4.770972031480867, 5.977819040873918, 6.3266564552069955, 4.422134617147789, 1.5556844237261283, 0.3488374143330777]]\n",
      "2020-12-23 02:42:56,408 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:56,409 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:56,413 : INFO : built Dictionary(99 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 340 corpus positions)\n",
      "2020-12-23 02:42:56,464 : INFO : token count processed\n",
      "2020-12-23 02:42:56,472 : INFO : frequencies processed\n",
      "2020-12-23 02:42:56,602 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:56,603 : INFO : entropies processed\n",
      "2020-12-23 02:42:56,603 : INFO : extropies processed\n",
      "2020-12-23 02:42:56,604 : INFO : token count processed\n",
      "2020-12-23 02:42:56,605 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:56,606 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:56,606 : INFO : vocab #2480\n",
      "2020-12-23 02:42:56,607 : INFO : diff #set()\n",
      "2020-12-23 02:42:56,876 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:57,005 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.1645449774746686, 0.4619908620086426], [0.7573389261960983, 0.24266107], [1.584962500721156, 1.1699250014423124], [4.770972031480867, 5.901812829596593, 6.314422877175717, 4.358361983901743, 1.5434508456948501, 0.41261004757912456]]\n",
      "2020-12-23 02:42:57,008 : INFO : Removed 0 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:57,009 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:57,010 : INFO : built Dictionary(100 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 340 corpus positions)\n",
      "2020-12-23 02:42:57,065 : INFO : token count processed\n",
      "2020-12-23 02:42:57,067 : INFO : frequencies processed\n",
      "2020-12-23 02:42:57,195 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:57,196 : INFO : entropies processed\n",
      "2020-12-23 02:42:57,197 : INFO : extropies processed\n",
      "2020-12-23 02:42:57,198 : INFO : token count processed\n",
      "2020-12-23 02:42:57,199 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:57,200 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:57,201 : INFO : vocab #2480\n",
      "2020-12-23 02:42:57,203 : INFO : diff #set()\n",
      "2020-12-23 02:42:57,462 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:57,591 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.1487198678523067, 0.46539337908180756], [0.8277363479137421, 0.17226365], [2.0, 1.2451124978365313], [4.770972031480867, 5.643202320803383, 6.099282963015337, 4.314891389268912, 1.32831093153447, 0.45608064221195477]]\n",
      "2020-12-23 02:42:57,594 : INFO : Removed 0 and 30 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:57,594 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:57,595 : INFO : built Dictionary(113 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 374 corpus positions)\n",
      "2020-12-23 02:42:57,651 : INFO : token count processed\n",
      "2020-12-23 02:42:57,654 : INFO : frequencies processed\n",
      "2020-12-23 02:42:57,781 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:57,781 : INFO : entropies processed\n",
      "2020-12-23 02:42:57,782 : INFO : extropies processed\n",
      "2020-12-23 02:42:57,783 : INFO : token count processed\n",
      "2020-12-23 02:42:57,784 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:57,785 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:57,785 : INFO : vocab #2480\n",
      "2020-12-23 02:42:57,786 : INFO : diff #set()\n",
      "2020-12-23 02:42:58,054 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:58,185 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.157399099813756, 0.4635210981993679], [0.8168505430221558, 0.18314946], [1.7924812503605778, 1.1575860145844845], [4.770972031480867, 5.925214310725336, 6.309843334147969, 4.386343008058235, 1.5388713026671015, 0.38462902342263305]]\n",
      "2020-12-23 02:42:58,188 : INFO : Removed 0 and 293 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:58,189 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:58,190 : INFO : built Dictionary(183 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 1769 corpus positions)\n",
      "2020-12-23 02:42:58,323 : INFO : token count processed\n",
      "2020-12-23 02:42:58,325 : INFO : frequencies processed\n",
      "2020-12-23 02:42:58,456 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:58,457 : INFO : entropies processed\n",
      "2020-12-23 02:42:58,458 : INFO : extropies processed\n",
      "2020-12-23 02:42:58,460 : INFO : token count processed\n",
      "2020-12-23 02:42:58,461 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:58,462 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:58,462 : INFO : vocab #2480\n",
      "2020-12-23 02:42:58,463 : INFO : diff #set()\n",
      "2020-12-23 02:42:58,722 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:58,850 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.1898939255656473, 0.4566431224479063], [0.8813396692276001, 0.11866033], [2.6416041678685938, 1.2962416748397703], [4.770972031480867, 6.551685682764175, 6.667052629119729, 4.655605085125314, 1.8960805976388615, 0.11536694635555378]]\n",
      "2020-12-23 02:42:58,853 : INFO : Removed 0 and 88 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:58,854 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:58,855 : INFO : built Dictionary(160 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 610 corpus positions)\n",
      "2020-12-23 02:42:58,958 : INFO : token count processed\n",
      "2020-12-23 02:42:58,960 : INFO : frequencies processed\n",
      "2020-12-23 02:42:59,089 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:59,090 : INFO : entropies processed\n",
      "2020-12-23 02:42:59,090 : INFO : extropies processed\n",
      "2020-12-23 02:42:59,092 : INFO : token count processed\n",
      "2020-12-23 02:42:59,093 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:59,094 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:59,094 : INFO : vocab #2480\n",
      "2020-12-23 02:42:59,095 : INFO : diff #set()\n",
      "2020-12-23 02:42:59,355 : INFO : alphabet #2480\n",
      "2020-12-23 02:42:59,482 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.1477898329702136, 0.4655949034906659], [0.7518896907567978, 0.24811031], [2.584962500721156, 1.315172029168969], [4.770972031480867, 6.642985062562557, 6.867306982560171, 4.546650111483254, 2.096334951079304, 0.22432191999761386]]\n",
      "2020-12-23 02:42:59,485 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:42:59,486 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:42:59,487 : INFO : built Dictionary(78 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 205 corpus positions)\n",
      "2020-12-23 02:42:59,516 : INFO : token count processed\n",
      "2020-12-23 02:42:59,519 : INFO : frequencies processed\n",
      "2020-12-23 02:42:59,647 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:42:59,648 : INFO : entropies processed\n",
      "2020-12-23 02:42:59,649 : INFO : extropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:42:59,650 : INFO : token count processed\n",
      "2020-12-23 02:42:59,651 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:42:59,652 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:42:59,653 : INFO : vocab #2480\n",
      "2020-12-23 02:42:59,654 : INFO : diff #set()\n",
      "2020-12-23 02:42:59,912 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:00,039 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.1795181962827024, 0.4588169998789454], [0.7916687577962875, 0.20833124], [0.0, 0.0], [4.770972031480867, 5.2461980344571995, 5.92898674493814, 4.088183320999928, 1.1580147134572725, 0.6827887104809403]]\n",
      "2020-12-23 02:43:00,042 : INFO : Removed 0 and 23 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:00,043 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:00,044 : INFO : built Dictionary(97 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 265 corpus positions)\n",
      "2020-12-23 02:43:00,090 : INFO : token count processed\n",
      "2020-12-23 02:43:00,092 : INFO : frequencies processed\n",
      "2020-12-23 02:43:00,223 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:00,224 : INFO : entropies processed\n",
      "2020-12-23 02:43:00,224 : INFO : extropies processed\n",
      "2020-12-23 02:43:00,226 : INFO : token count processed\n",
      "2020-12-23 02:43:00,226 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:00,227 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:00,228 : INFO : vocab #2480\n",
      "2020-12-23 02:43:00,229 : INFO : diff #set()\n",
      "2020-12-23 02:43:00,485 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:00,614 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/test_auth_utility.py')[[1.1653722844156582, 0.461814352754523], [0.7909450829029083, 0.20905492], [2.0, 1.2451124978365313], [4.770972031480867, 5.903090303960449, 6.356019338738368, 4.318042996702949, 1.5850473072575006, 0.45292903477791846]]\n",
      "2020-12-23 02:43:00,617 : INFO : Removed 0 and 442 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:00,618 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:00,619 : INFO : built Dictionary(129 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 1264 corpus positions)\n",
      "2020-12-23 02:43:00,691 : INFO : token count processed\n",
      "2020-12-23 02:43:00,694 : INFO : frequencies processed\n",
      "2020-12-23 02:43:00,822 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:00,822 : INFO : entropies processed\n",
      "2020-12-23 02:43:00,823 : INFO : extropies processed\n",
      "2020-12-23 02:43:00,825 : INFO : token count processed\n",
      "2020-12-23 02:43:00,826 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:00,827 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:00,828 : INFO : vocab #2480\n",
      "2020-12-23 02:43:00,829 : INFO : diff #set()\n",
      "2020-12-23 02:43:01,098 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:01,228 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.12216777141187, 0.4712162786897399], [0.8055569380521774, 0.19444306], [2.4464393446710155, 1.2856945251022456], [4.770972031480867, 6.16659449033757, 6.30726278785197, 4.630303733966466, 1.5362907563711028, 0.14066829751440046]]\n",
      "2020-12-23 02:43:01,231 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:01,232 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:01,233 : INFO : built Dictionary(90 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 299 corpus positions)\n",
      "2020-12-23 02:43:01,278 : INFO : token count processed\n",
      "2020-12-23 02:43:01,280 : INFO : frequencies processed\n",
      "2020-12-23 02:43:01,413 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:01,414 : INFO : entropies processed\n",
      "2020-12-23 02:43:01,415 : INFO : extropies processed\n",
      "2020-12-23 02:43:01,416 : INFO : token count processed\n",
      "2020-12-23 02:43:01,417 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:01,418 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:01,419 : INFO : vocab #2480\n",
      "2020-12-23 02:43:01,420 : INFO : diff #set()\n",
      "2020-12-23 02:43:01,681 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:01,809 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.0750826874962958, 0.481908507080533], [0.7186073362827301, 0.28139266], [2.197159723424149, 1.2560617931988725], [4.770972031480867, 5.906856253399655, 6.277685473279182, 4.400142811601341, 1.5067134417983148, 0.37082921987952666]]\n",
      "2020-12-23 02:43:01,812 : INFO : Removed 0 and 68 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:01,813 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:01,814 : INFO : built Dictionary(106 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 298 corpus positions)\n",
      "2020-12-23 02:43:01,876 : INFO : token count processed\n",
      "2020-12-23 02:43:01,878 : INFO : frequencies processed\n",
      "2020-12-23 02:43:02,008 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:02,008 : INFO : entropies processed\n",
      "2020-12-23 02:43:02,009 : INFO : extropies processed\n",
      "2020-12-23 02:43:02,011 : INFO : token count processed\n",
      "2020-12-23 02:43:02,012 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:02,012 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:02,013 : INFO : vocab #2480\n",
      "2020-12-23 02:43:02,014 : INFO : diff #set()\n",
      "2020-12-23 02:43:02,269 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:02,397 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.182715250620105, 0.4581449640377516], [0.8307541161775589, 0.16924588], [1.0, 1.0], [4.770972031480867, 5.965115449163356, 6.390165372857488, 4.345922107786735, 1.6191933413766204, 0.4250499236941314]]\n",
      "2020-12-23 02:43:02,400 : INFO : Removed 0 and 13 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:02,401 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:02,402 : INFO : built Dictionary(108 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 412 corpus positions)\n",
      "2020-12-23 02:43:02,455 : INFO : token count processed\n",
      "2020-12-23 02:43:02,457 : INFO : frequencies processed\n",
      "2020-12-23 02:43:02,586 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:02,587 : INFO : entropies processed\n",
      "2020-12-23 02:43:02,587 : INFO : extropies processed\n",
      "2020-12-23 02:43:02,588 : INFO : token count processed\n",
      "2020-12-23 02:43:02,589 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:02,590 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:02,591 : INFO : vocab #2480\n",
      "2020-12-23 02:43:02,592 : INFO : diff #set()\n",
      "2020-12-23 02:43:02,852 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:02,978 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.1027757612159979, 0.4755618827476486], [0.7552276700735092, 0.24477233], [2.6635327548042547, 1.307883000782319], [4.770972031480867, 5.791362404253194, 6.145412780665122, 4.41692165506894, 1.374440749184255, 0.35405037641192827]]\n",
      "2020-12-23 02:43:02,981 : INFO : Removed 0 and 41 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:02,981 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:02,983 : INFO : built Dictionary(98 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 424 corpus positions)\n",
      "2020-12-23 02:43:03,032 : INFO : token count processed\n",
      "2020-12-23 02:43:03,037 : INFO : frequencies processed\n",
      "2020-12-23 02:43:03,164 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:03,165 : INFO : entropies processed\n",
      "2020-12-23 02:43:03,165 : INFO : extropies processed\n",
      "2020-12-23 02:43:03,167 : INFO : token count processed\n",
      "2020-12-23 02:43:03,168 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:03,169 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:03,169 : INFO : vocab #2480\n",
      "2020-12-23 02:43:03,170 : INFO : diff #set()\n",
      "2020-12-23 02:43:03,434 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:03,560 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.0982410310165112, 0.47658966973662764], [0.7708364427089691, 0.22916356], [2.6464393446710153, 1.3017576173934458], [4.770972031480867, 5.651670454631116, 6.009739208620092, 4.412903277491891, 1.2387671771392244, 0.35806875398897553]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:43:03,563 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:03,564 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:03,565 : INFO : built Dictionary(65 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 134 corpus positions)\n",
      "2020-12-23 02:43:03,593 : INFO : token count processed\n",
      "2020-12-23 02:43:03,595 : INFO : frequencies processed\n",
      "2020-12-23 02:43:03,723 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:03,724 : INFO : entropies processed\n",
      "2020-12-23 02:43:03,725 : INFO : extropies processed\n",
      "2020-12-23 02:43:03,726 : INFO : token count processed\n",
      "2020-12-23 02:43:03,727 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:03,728 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:03,729 : INFO : vocab #2480\n",
      "2020-12-23 02:43:03,730 : INFO : diff #set()\n",
      "2020-12-23 02:43:03,988 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:04,116 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.1156556710845444, 0.47266670737935906], [0.7361631691455841, 0.26383683], [1.3709505944546687, 1.0438561897747245], [4.770972031480867, 4.8226207261920235, 5.6827853901195065, 3.9108073675533843, 0.9118133586386392, 0.8601646639274829]]\n",
      "2020-12-23 02:43:04,118 : INFO : Removed 0 and 50 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:04,119 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:04,120 : INFO : built Dictionary(106 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 304 corpus positions)\n",
      "2020-12-23 02:43:04,177 : INFO : token count processed\n",
      "2020-12-23 02:43:04,179 : INFO : frequencies processed\n",
      "2020-12-23 02:43:04,306 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:04,307 : INFO : entropies processed\n",
      "2020-12-23 02:43:04,308 : INFO : extropies processed\n",
      "2020-12-23 02:43:04,309 : INFO : token count processed\n",
      "2020-12-23 02:43:04,309 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:04,310 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:04,311 : INFO : vocab #2480\n",
      "2020-12-23 02:43:04,311 : INFO : diff #set()\n",
      "2020-12-23 02:43:04,570 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:04,700 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.0700904685016832, 0.48307067503373075], [0.705897867679596, 0.29410213], [2.5654483718208256, 1.2887495548865375], [4.770972031480867, 6.24862851613934, 6.541223471117173, 4.478377076503034, 1.770251439636306, 0.29259495497783305]]\n",
      "2020-12-23 02:43:04,702 : INFO : Removed 0 and 32 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:04,703 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:04,704 : INFO : built Dictionary(107 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 420 corpus positions)\n",
      "2020-12-23 02:43:04,763 : INFO : token count processed\n",
      "2020-12-23 02:43:04,766 : INFO : frequencies processed\n",
      "2020-12-23 02:43:04,893 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:04,894 : INFO : entropies processed\n",
      "2020-12-23 02:43:04,894 : INFO : extropies processed\n",
      "2020-12-23 02:43:04,896 : INFO : token count processed\n",
      "2020-12-23 02:43:04,896 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:04,897 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:04,898 : INFO : vocab #2480\n",
      "2020-12-23 02:43:04,899 : INFO : diff #set()\n",
      "2020-12-23 02:43:05,167 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:05,299 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.1183317619225348, 0.4720695870095578], [0.7831552177667618, 0.21684478], [2.6635327548042547, 1.307883000782319], [4.770972031480867, 5.850156917433494, 6.190480543030834, 4.430648405883527, 1.4195085115499664, 0.34032362559733986]]\n",
      "2020-12-23 02:43:05,302 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:05,303 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:05,304 : INFO : built Dictionary(102 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 418 corpus positions)\n",
      "2020-12-23 02:43:05,360 : INFO : token count processed\n",
      "2020-12-23 02:43:05,362 : INFO : frequencies processed\n",
      "2020-12-23 02:43:05,489 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:05,490 : INFO : entropies processed\n",
      "2020-12-23 02:43:05,491 : INFO : extropies processed\n",
      "2020-12-23 02:43:05,493 : INFO : token count processed\n",
      "2020-12-23 02:43:05,494 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:05,495 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:05,496 : INFO : vocab #2480\n",
      "2020-12-23 02:43:05,497 : INFO : diff #set()\n",
      "2020-12-23 02:43:05,758 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:05,886 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.1057563819175271, 0.47488874239544643], [0.7736679166555405, 0.22633208], [2.6464393446710153, 1.3017576173934458], [4.770972031480867, 5.6831976040360095, 6.041700207960369, 4.412469427556507, 1.2707281764795022, 0.35850260392435995]]\n",
      "2020-12-23 02:43:05,888 : INFO : Removed 0 and 52 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:05,889 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:05,890 : INFO : built Dictionary(92 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 396 corpus positions)\n",
      "2020-12-23 02:43:05,937 : INFO : token count processed\n",
      "2020-12-23 02:43:05,939 : INFO : frequencies processed\n",
      "2020-12-23 02:43:06,067 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:06,067 : INFO : entropies processed\n",
      "2020-12-23 02:43:06,068 : INFO : extropies processed\n",
      "2020-12-23 02:43:06,069 : INFO : token count processed\n",
      "2020-12-23 02:43:06,069 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:06,070 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:06,071 : INFO : vocab #2480\n",
      "2020-12-23 02:43:06,072 : INFO : diff #set()\n",
      "2020-12-23 02:43:06,331 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:06,460 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.1479030171074447, 0.46557036888317616], [0.81308813393116, 0.18691187], [1.9219280948873623, 1.2148067842293933], [4.770972031480867, 5.749308601266266, 6.138268199467205, 4.38201243327993, 1.3672961679863374, 0.3889595982009384]]\n",
      "2020-12-23 02:43:06,462 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:06,463 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:06,464 : INFO : built Dictionary(84 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 277 corpus positions)\n",
      "2020-12-23 02:43:06,506 : INFO : token count processed\n",
      "2020-12-23 02:43:06,509 : INFO : frequencies processed\n",
      "2020-12-23 02:43:06,642 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:06,642 : INFO : entropies processed\n",
      "2020-12-23 02:43:06,643 : INFO : extropies processed\n",
      "2020-12-23 02:43:06,645 : INFO : token count processed\n",
      "2020-12-23 02:43:06,646 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:06,647 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:06,647 : INFO : vocab #2480\n",
      "2020-12-23 02:43:06,648 : INFO : diff #set()\n",
      "2020-12-23 02:43:06,913 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:07,041 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.1314216571834894, 0.4691704227691031], [0.7973621040582657, 0.2026379], [2.1280852788913944, 1.2238339714721664], [4.770972031480867, 5.015422548793484, 5.6295736668340925, 4.156820913440258, 0.8586016353532253, 0.614151118040609]]\n",
      "2020-12-23 02:43:07,044 : INFO : Removed 0 and 81 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:07,045 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:07,046 : INFO : built Dictionary(111 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 403 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:43:07,107 : INFO : token count processed\n",
      "2020-12-23 02:43:07,111 : INFO : frequencies processed\n",
      "2020-12-23 02:43:07,238 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:07,239 : INFO : entropies processed\n",
      "2020-12-23 02:43:07,239 : INFO : extropies processed\n",
      "2020-12-23 02:43:07,240 : INFO : token count processed\n",
      "2020-12-23 02:43:07,241 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:07,242 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:07,243 : INFO : vocab #2480\n",
      "2020-12-23 02:43:07,244 : INFO : diff #set()\n",
      "2020-12-23 02:43:07,506 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:07,632 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.0634864942376254, 0.4846166925698535], [0.7423499822616577, 0.25765002], [2.6635327548042547, 1.307883000782319], [4.770972031480867, 6.030001281822029, 6.331406933779129, 4.469566379523768, 1.5604349022982618, 0.3014056519570998]]\n",
      "2020-12-23 02:43:07,634 : INFO : Removed 0 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:07,635 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:07,636 : INFO : built Dictionary(96 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 366 corpus positions)\n",
      "2020-12-23 02:43:07,682 : INFO : token count processed\n",
      "2020-12-23 02:43:07,685 : INFO : frequencies processed\n",
      "2020-12-23 02:43:07,813 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:07,814 : INFO : entropies processed\n",
      "2020-12-23 02:43:07,815 : INFO : extropies processed\n",
      "2020-12-23 02:43:07,816 : INFO : token count processed\n",
      "2020-12-23 02:43:07,818 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:07,819 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:07,820 : INFO : vocab #2480\n",
      "2020-12-23 02:43:07,821 : INFO : diff #set()\n",
      "2020-12-23 02:43:08,091 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:08,222 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.0390497592626338, 0.4904245202734153], [0.6550485193729401, 0.34495148], [2.8954238702803154, 1.295593999852071], [4.770972031480867, 5.9537092545441395, 6.241128491101064, 4.483552794923943, 1.4701564596201964, 0.28741923655692414]]\n",
      "2020-12-23 02:43:08,224 : INFO : Removed 0 and 90 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:08,225 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:08,226 : INFO : built Dictionary(114 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 367 corpus positions)\n",
      "2020-12-23 02:43:08,282 : INFO : token count processed\n",
      "2020-12-23 02:43:08,284 : INFO : frequencies processed\n",
      "2020-12-23 02:43:08,412 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:08,413 : INFO : entropies processed\n",
      "2020-12-23 02:43:08,416 : INFO : extropies processed\n",
      "2020-12-23 02:43:08,417 : INFO : token count processed\n",
      "2020-12-23 02:43:08,418 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:08,419 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:08,419 : INFO : vocab #2480\n",
      "2020-12-23 02:43:08,420 : INFO : diff #set()\n",
      "2020-12-23 02:43:08,679 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:08,807 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.164025952510584, 0.46210166696007277], [0.7802624851465225, 0.21973751], [1.1488348542809166, 0.8976312714503689], [4.770972031480867, 6.184756445474906, 6.518537105475673, 4.4371913714801, 1.7475650739948057, 0.3337806600007669]]\n",
      "2020-12-23 02:43:08,810 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:08,810 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:08,812 : INFO : built Dictionary(119 unique tokens: ['allow', 'append', 'artifact', 'artifactori', 'caus']...) from 2 documents (total 480 corpus positions)\n",
      "2020-12-23 02:43:08,883 : INFO : token count processed\n",
      "2020-12-23 02:43:08,888 : INFO : frequencies processed\n",
      "2020-12-23 02:43:09,016 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:09,017 : INFO : entropies processed\n",
      "2020-12-23 02:43:09,017 : INFO : extropies processed\n",
      "2020-12-23 02:43:09,018 : INFO : token count processed\n",
      "2020-12-23 02:43:09,019 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:09,020 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:09,020 : INFO : vocab #2480\n",
      "2020-12-23 02:43:09,021 : INFO : diff #set()\n",
      "2020-12-23 02:43:09,278 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:09,405 : INFO : Computed distances or similarities ('287', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.0962465649752888, 0.477043119215219], [0.7452505230903625, 0.25474948], [2.6635327548042547, 1.307883000782319], [4.770972031480867, 6.212221456585881, 6.477358477276743, 4.505835010790005, 1.706386445795876, 0.26513702069086253]]\n",
      "2020-12-23 02:43:09,408 : INFO : Removed 1 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:09,408 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:09,409 : INFO : built Dictionary(118 unique tokens: ['docker', 'move', 'slave', 'test', '\"])']...) from 2 documents (total 411 corpus positions)\n",
      "2020-12-23 02:43:09,429 : INFO : token count processed\n",
      "2020-12-23 02:43:09,432 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:43:09,435 : INFO : frequencies processed\n",
      "2020-12-23 02:43:09,436 : INFO : token count processed\n",
      "2020-12-23 02:43:09,439 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:09,439 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:09,442 : INFO : vocab #2480\n",
      "2020-12-23 02:43:09,443 : INFO : diff #set()\n",
      "2020-12-23 02:43:09,694 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:09,822 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.3122485724413075, 0.4324794539473689], [1.0, 0.0], [nan, nan], [2.321928094887362, 6.301552355933639, 6.34622451945189, 2.277255931369111, 4.024296424564527, 0.04467216351825076]]\n",
      "2020-12-23 02:43:09,825 : INFO : Removed 1 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:09,825 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:09,827 : INFO : built Dictionary(155 unique tokens: ['docker', 'move', 'slave', 'test', '\")).']...) from 2 documents (total 647 corpus positions)\n",
      "2020-12-23 02:43:09,850 : INFO : token count processed\n",
      "2020-12-23 02:43:09,855 : INFO : frequencies processed\n",
      "2020-12-23 02:43:09,985 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:09,986 : INFO : entropies processed\n",
      "2020-12-23 02:43:09,986 : INFO : extropies processed\n",
      "2020-12-23 02:43:09,987 : INFO : token count processed\n",
      "2020-12-23 02:43:09,988 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:09,991 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:09,991 : INFO : vocab #2480\n",
      "2020-12-23 02:43:09,992 : INFO : diff #set()\n",
      "2020-12-23 02:43:10,251 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:10,378 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.30361942343836, 0.434099482677312], [0.9959621820598841, 0.004037818], [0.0, 0.0], [2.321928094887362, 6.739005504021667, 6.765719072405089, 2.2952145265039405, 4.443790977517727, 0.026713568383422093]]\n",
      "2020-12-23 02:43:10,381 : INFO : Removed 1 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:10,382 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:10,384 : INFO : built Dictionary(99 unique tokens: ['docker', 'move', 'slave', 'test', '\".\"']...) from 2 documents (total 498 corpus positions)\n",
      "2020-12-23 02:43:10,403 : INFO : token count processed\n",
      "2020-12-23 02:43:10,412 : INFO : frequencies processed\n",
      "2020-12-23 02:43:10,541 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:10,542 : INFO : entropies processed\n",
      "2020-12-23 02:43:10,542 : INFO : extropies processed\n",
      "2020-12-23 02:43:10,543 : INFO : token count processed\n",
      "2020-12-23 02:43:10,544 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:10,545 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:10,545 : INFO : vocab #2480\n",
      "2020-12-23 02:43:10,546 : INFO : diff #set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:43:10,802 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:10,930 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2882956227870164, 0.43700647330787434], [0.9597422033548355, 0.040257797], [0.0, 0.0], [2.321928094887362, 5.870833373337847, 5.902022479870351, 2.2907389883548577, 3.580094384982989, 0.031189106532504063]]\n",
      "2020-12-23 02:43:10,933 : INFO : Removed 1 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:10,934 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:10,935 : INFO : built Dictionary(59 unique tokens: ['docker', 'move', 'slave', 'test', '\"].']...) from 2 documents (total 181 corpus positions)\n",
      "2020-12-23 02:43:10,947 : INFO : token count processed\n",
      "2020-12-23 02:43:10,950 : INFO : frequencies processed\n",
      "2020-12-23 02:43:11,077 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:11,078 : INFO : entropies processed\n",
      "2020-12-23 02:43:11,079 : INFO : extropies processed\n",
      "2020-12-23 02:43:11,080 : INFO : token count processed\n",
      "2020-12-23 02:43:11,081 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:11,082 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:11,083 : INFO : vocab #2480\n",
      "2020-12-23 02:43:11,084 : INFO : diff #set()\n",
      "2020-12-23 02:43:11,342 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:11,472 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.2512426494810538, 0.44419911830939923], [0.9193860292434692, 0.08061397], [0.0, 0.0], [2.321928094887362, 5.371881234145534, 5.443239915652821, 2.2505694133800755, 3.121311820765459, 0.07135868150728708]]\n",
      "2020-12-23 02:43:11,474 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:11,475 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:11,476 : INFO : built Dictionary(50 unique tokens: ['docker', 'move', 'slave', 'test', 'add']...) from 2 documents (total 133 corpus positions)\n",
      "2020-12-23 02:43:11,482 : INFO : token count processed\n",
      "2020-12-23 02:43:11,484 : INFO : frequencies processed\n",
      "2020-12-23 02:43:11,613 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:11,613 : INFO : entropies processed\n",
      "2020-12-23 02:43:11,614 : INFO : extropies processed\n",
      "2020-12-23 02:43:11,615 : INFO : token count processed\n",
      "2020-12-23 02:43:11,616 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:11,617 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:11,618 : INFO : vocab #2480\n",
      "2020-12-23 02:43:11,619 : INFO : diff #set()\n",
      "2020-12-23 02:43:11,877 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:12,006 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.2337079066438619, 0.4476861083875986], [0.888375997543335, 0.111624], [0.0, 0.0], [2.321928094887362, 4.85108279267097, 4.952397984758582, 2.2206129027997505, 2.6304698898712195, 0.10131519208761208]]\n",
      "2020-12-23 02:43:12,008 : INFO : Removed 1 and 127 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:12,009 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:12,010 : INFO : built Dictionary(90 unique tokens: ['docker', 'move', 'slave', 'test', '\":\")']...) from 2 documents (total 406 corpus positions)\n",
      "2020-12-23 02:43:12,022 : INFO : token count processed\n",
      "2020-12-23 02:43:12,024 : INFO : frequencies processed\n",
      "2020-12-23 02:43:12,151 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:12,151 : INFO : entropies processed\n",
      "2020-12-23 02:43:12,152 : INFO : extropies processed\n",
      "2020-12-23 02:43:12,154 : INFO : token count processed\n",
      "2020-12-23 02:43:12,155 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:12,156 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:12,157 : INFO : vocab #2480\n",
      "2020-12-23 02:43:12,158 : INFO : diff #set()\n",
      "2020-12-23 02:43:12,547 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:12,675 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.3101547327666634, 0.43287143749128465], [0.9896185146644711, 0.010381485], [0.0, 0.0], [2.321928094887362, 6.139571208108155, 6.1752085385765625, 2.286290764418954, 3.8532804436892003, 0.03563733046840767]]\n",
      "2020-12-23 02:43:12,678 : INFO : Removed 1 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:12,679 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:12,680 : INFO : built Dictionary(77 unique tokens: ['docker', 'move', 'slave', 'test', '\"),']...) from 2 documents (total 416 corpus positions)\n",
      "2020-12-23 02:43:12,705 : INFO : token count processed\n",
      "2020-12-23 02:43:12,709 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:43:12,710 : INFO : frequencies processed\n",
      "2020-12-23 02:43:12,715 : INFO : token count processed\n",
      "2020-12-23 02:43:12,715 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:12,718 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:12,719 : INFO : vocab #2480\n",
      "2020-12-23 02:43:12,720 : INFO : diff #set()\n",
      "2020-12-23 02:43:12,977 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:13,103 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.3077743756779574, 0.43331792333738384], [1.0, 0.0], [nan, nan], [2.321928094887362, 5.609710627339259, 5.6608617378443, 2.270776984382321, 3.338933642956938, 0.05115111050504062]]\n",
      "2020-12-23 02:43:13,106 : INFO : Removed 1 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:13,107 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:13,110 : INFO : built Dictionary(168 unique tokens: ['docker', 'move', 'slave', 'test', '\"\"\"']...) from 2 documents (total 1076 corpus positions)\n",
      "2020-12-23 02:43:13,139 : INFO : token count processed\n",
      "2020-12-23 02:43:13,141 : INFO : frequencies processed\n",
      "2020-12-23 02:43:13,272 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:13,273 : INFO : entropies processed\n",
      "2020-12-23 02:43:13,274 : INFO : extropies processed\n",
      "2020-12-23 02:43:13,276 : INFO : token count processed\n",
      "2020-12-23 02:43:13,277 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:13,278 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:13,278 : INFO : vocab #2480\n",
      "2020-12-23 02:43:13,280 : INFO : diff #set()\n",
      "2020-12-23 02:43:13,538 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:13,665 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.2902813641024236, 0.4366275758401879], [0.9464175850152969, 0.053582415], [1.0, 1.0], [2.321928094887362, 7.2441902753576075, 7.2548714190447, 2.3112469512002702, 4.932943324157337, 0.010681143687092387]]\n",
      "2020-12-23 02:43:13,668 : INFO : Removed 1 and 136 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:13,669 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:13,670 : INFO : built Dictionary(130 unique tokens: ['docker', 'move', 'slave', 'test', '\"--']...) from 2 documents (total 683 corpus positions)\n",
      "2020-12-23 02:43:13,691 : INFO : token count processed\n",
      "2020-12-23 02:43:13,693 : INFO : frequencies processed\n",
      "2020-12-23 02:43:13,821 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:13,822 : INFO : entropies processed\n",
      "2020-12-23 02:43:13,822 : INFO : extropies processed\n",
      "2020-12-23 02:43:13,824 : INFO : token count processed\n",
      "2020-12-23 02:43:13,825 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:13,826 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:13,826 : INFO : vocab #2480\n",
      "2020-12-23 02:43:13,827 : INFO : diff #set()\n",
      "2020-12-23 02:43:14,087 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:14,214 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.3029488082289131, 0.43422589179003585], [0.9963582761120051, 0.003641724], [0.0, 0.0], [2.321928094887362, 6.2567074920449475, 6.283909078755572, 2.294726508176737, 3.96198098386821, 0.02720158671062478]]\n",
      "2020-12-23 02:43:14,216 : INFO : Removed 1 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:14,217 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:14,218 : INFO : built Dictionary(76 unique tokens: ['docker', 'move', 'slave', 'test', '__doc__']...) from 2 documents (total 193 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:43:14,227 : INFO : token count processed\n",
      "2020-12-23 02:43:14,229 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:43:14,230 : INFO : frequencies processed\n",
      "2020-12-23 02:43:14,231 : INFO : token count processed\n",
      "2020-12-23 02:43:14,232 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:14,233 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:14,233 : INFO : vocab #2480\n",
      "2020-12-23 02:43:14,235 : INFO : diff #set()\n",
      "2020-12-23 02:43:14,493 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:14,621 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.30408077009107, 0.43401256283236744], [1.0, 0.0], [nan, nan], [2.321928094887362, 5.7680018917339435, 5.848062606230643, 2.241867380390662, 3.526134511343281, 0.0800607144966996]]\n",
      "2020-12-23 02:43:14,624 : INFO : Removed 1 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:14,624 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:14,626 : INFO : built Dictionary(173 unique tokens: ['docker', 'move', 'slave', 'test', '\"\"\"']...) from 2 documents (total 734 corpus positions)\n",
      "2020-12-23 02:43:14,646 : INFO : token count processed\n",
      "2020-12-23 02:43:14,649 : INFO : frequencies processed\n",
      "2020-12-23 02:43:14,776 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:14,777 : INFO : entropies processed\n",
      "2020-12-23 02:43:14,778 : INFO : extropies processed\n",
      "2020-12-23 02:43:14,779 : INFO : token count processed\n",
      "2020-12-23 02:43:14,780 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:14,781 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:14,782 : INFO : vocab #2480\n",
      "2020-12-23 02:43:14,783 : INFO : diff #set()\n",
      "2020-12-23 02:43:15,040 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:15,167 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.3074016692080166, 0.43338791565633045], [0.9932590611279011, 0.006740939], [0.0, 0.0], [2.321928094887362, 6.846479111193757, 6.868701382841499, 2.29970582323962, 4.546773287954137, 0.02222227164774182]]\n",
      "2020-12-23 02:43:15,169 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:15,170 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:15,171 : INFO : built Dictionary(24 unique tokens: ['docker', 'move', 'slave', 'test', 'buddi']...) from 2 documents (total 37 corpus positions)\n",
      "2020-12-23 02:43:15,175 : INFO : token count processed\n",
      "2020-12-23 02:43:15,177 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:43:15,177 : INFO : frequencies processed\n",
      "2020-12-23 02:43:15,178 : INFO : token count processed\n",
      "2020-12-23 02:43:15,179 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:15,180 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:15,181 : INFO : vocab #2480\n",
      "2020-12-23 02:43:15,182 : INFO : diff #set()\n",
      "2020-12-23 02:43:15,442 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:15,572 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/fireException.py')[[1.3046527067925902, 0.4339048556221344], [1.0, 0.0], [nan, nan], [2.321928094887362, 4.165013816065912, 4.484255144794799, 2.002686766158474, 2.162327049907437, 0.31924132872888755]]\n",
      "2020-12-23 02:43:15,574 : INFO : Removed 1 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:15,575 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:15,576 : INFO : built Dictionary(52 unique tokens: ['docker', 'move', 'slave', 'test', 'autoescap']...) from 2 documents (total 144 corpus positions)\n",
      "2020-12-23 02:43:15,589 : INFO : token count processed\n",
      "2020-12-23 02:43:15,592 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:43:15,593 : INFO : frequencies processed\n",
      "2020-12-23 02:43:15,594 : INFO : token count processed\n",
      "2020-12-23 02:43:15,595 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:15,596 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:15,596 : INFO : vocab #2480\n",
      "2020-12-23 02:43:15,598 : INFO : diff #set()\n",
      "2020-12-23 02:43:15,867 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:15,996 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.3061119565307107, 0.43363029152513], [1.0, 0.0], [nan, nan], [2.321928094887362, 5.449968864419248, 5.552131818550282, 2.219765140756329, 3.2302037236629197, 0.1021629541310336]]\n",
      "2020-12-23 02:43:15,999 : INFO : Removed 1 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:15,999 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:16,002 : INFO : built Dictionary(144 unique tokens: ['docker', 'move', 'slave', 'test', '\"\"\"']...) from 2 documents (total 581 corpus positions)\n",
      "2020-12-23 02:43:16,026 : INFO : token count processed\n",
      "2020-12-23 02:43:16,029 : INFO : frequencies processed\n",
      "2020-12-23 02:43:16,157 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:16,158 : INFO : entropies processed\n",
      "2020-12-23 02:43:16,161 : INFO : extropies processed\n",
      "2020-12-23 02:43:16,162 : INFO : token count processed\n",
      "2020-12-23 02:43:16,162 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:16,163 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:16,164 : INFO : vocab #2480\n",
      "2020-12-23 02:43:16,165 : INFO : diff #set()\n",
      "2020-12-23 02:43:16,427 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:16,555 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.2920963398224223, 0.4362818362501612], [0.9602387212216854, 0.03976128], [0.0, 0.0], [2.321928094887362, 6.530294129310484, 6.553958900782921, 2.2982633234149246, 4.232030805895558, 0.023664771472437174]]\n",
      "2020-12-23 02:43:16,557 : INFO : Removed 1 and 142 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:16,558 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:16,560 : INFO : built Dictionary(116 unique tokens: ['docker', 'move', 'slave', 'test', '\"):']...) from 2 documents (total 597 corpus positions)\n",
      "2020-12-23 02:43:16,584 : INFO : token count processed\n",
      "2020-12-23 02:43:16,587 : INFO : frequencies processed\n",
      "2020-12-23 02:43:16,715 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:16,716 : INFO : entropies processed\n",
      "2020-12-23 02:43:16,716 : INFO : extropies processed\n",
      "2020-12-23 02:43:16,718 : INFO : token count processed\n",
      "2020-12-23 02:43:16,719 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:16,720 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:16,721 : INFO : vocab #2480\n",
      "2020-12-23 02:43:16,722 : INFO : diff #set()\n",
      "2020-12-23 02:43:16,983 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:17,111 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.1984323091616427, 0.4548695885848509], [0.7836261689662933, 0.21637383], [1.0, 1.0], [2.321928094887362, 6.470272233491701, 6.485638532028689, 2.3065617963503744, 4.163710437141328, 0.015366298536988232]]\n",
      "2020-12-23 02:43:17,113 : INFO : Removed 1 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:17,114 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:17,115 : INFO : built Dictionary(119 unique tokens: ['docker', 'move', 'slave', 'test', '\"\":']...) from 2 documents (total 386 corpus positions)\n",
      "2020-12-23 02:43:17,132 : INFO : token count processed\n",
      "2020-12-23 02:43:17,136 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:43:17,138 : INFO : frequencies processed\n",
      "2020-12-23 02:43:17,140 : INFO : token count processed\n",
      "2020-12-23 02:43:17,142 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:17,144 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:17,145 : INFO : vocab #2480\n",
      "2020-12-23 02:43:17,147 : INFO : diff #set()\n",
      "2020-12-23 02:43:17,403 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:17,530 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.305891562730226, 0.4336717372849824], [1.0, 0.0], [nan, nan], [2.321928094887362, 6.550038223589686, 6.591694907368556, 2.280271411108492, 4.269766812481194, 0.04165668377886966]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:43:17,533 : INFO : Removed 1 and 70 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:17,534 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:17,534 : INFO : built Dictionary(72 unique tokens: ['docker', 'move', 'slave', 'test', '\")).']...) from 2 documents (total 302 corpus positions)\n",
      "2020-12-23 02:43:17,543 : INFO : token count processed\n",
      "2020-12-23 02:43:17,545 : INFO : frequencies processed\n",
      "2020-12-23 02:43:17,673 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:17,674 : INFO : entropies processed\n",
      "2020-12-23 02:43:17,675 : INFO : extropies processed\n",
      "2020-12-23 02:43:17,676 : INFO : token count processed\n",
      "2020-12-23 02:43:17,677 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:17,678 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:17,679 : INFO : vocab #2480\n",
      "2020-12-23 02:43:17,680 : INFO : diff #set()\n",
      "2020-12-23 02:43:17,937 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:18,065 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.2711756908630458, 0.44030059146150885], [0.9443015195429325, 0.05569848], [1.0, 1.0], [2.321928094887362, 5.860525481261383, 5.896338244722621, 2.2861153314261244, 3.5744101498352587, 0.035812763461238184]]\n",
      "2020-12-23 02:43:18,068 : INFO : Removed 1 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:18,069 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:18,069 : INFO : built Dictionary(45 unique tokens: ['docker', 'move', 'slave', 'test', '\"]),']...) from 2 documents (total 340 corpus positions)\n",
      "2020-12-23 02:43:18,082 : INFO : token count processed\n",
      "2020-12-23 02:43:18,084 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:43:18,087 : INFO : frequencies processed\n",
      "2020-12-23 02:43:18,088 : INFO : token count processed\n",
      "2020-12-23 02:43:18,090 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:18,091 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:18,091 : INFO : vocab #2480\n",
      "2020-12-23 02:43:18,092 : INFO : diff #set()\n",
      "2020-12-23 02:43:18,350 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:18,479 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.333602644457016, 0.4285219689715773], [1.0, 0.0], [nan, nan], [2.321928094887362, 5.945464049777852, 5.989488053627914, 2.2779040910373, 3.667559958740552, 0.04402400385006189]]\n",
      "2020-12-23 02:43:18,482 : INFO : Removed 1 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:18,483 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:18,485 : INFO : built Dictionary(192 unique tokens: ['docker', 'move', 'slave', 'test', '\"\"\"']...) from 2 documents (total 887 corpus positions)\n",
      "2020-12-23 02:43:18,514 : INFO : token count processed\n",
      "2020-12-23 02:43:18,518 : INFO : frequencies processed\n",
      "2020-12-23 02:43:18,646 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:18,646 : INFO : entropies processed\n",
      "2020-12-23 02:43:18,647 : INFO : extropies processed\n",
      "2020-12-23 02:43:18,648 : INFO : token count processed\n",
      "2020-12-23 02:43:18,649 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:18,649 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:18,650 : INFO : vocab #2480\n",
      "2020-12-23 02:43:18,651 : INFO : diff #set()\n",
      "2020-12-23 02:43:18,907 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:19,036 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.3040399331367476, 0.43402025529938965], [0.9967997903004289, 0.0032002097], [0.0, 0.0], [2.321928094887362, 6.811563897304216, 6.831893606823884, 2.3015983853676945, 4.509965511936521, 0.020329709519667283]]\n",
      "2020-12-23 02:43:19,039 : INFO : Removed 1 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:19,040 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:19,041 : INFO : built Dictionary(213 unique tokens: ['docker', 'move', 'slave', 'test', '\"\"\"']...) from 2 documents (total 982 corpus positions)\n",
      "2020-12-23 02:43:19,080 : INFO : token count processed\n",
      "2020-12-23 02:43:19,082 : INFO : frequencies processed\n",
      "2020-12-23 02:43:19,210 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:19,210 : INFO : entropies processed\n",
      "2020-12-23 02:43:19,211 : INFO : extropies processed\n",
      "2020-12-23 02:43:19,213 : INFO : token count processed\n",
      "2020-12-23 02:43:19,213 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:19,214 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:19,215 : INFO : vocab #2480\n",
      "2020-12-23 02:43:19,215 : INFO : diff #set()\n",
      "2020-12-23 02:43:19,473 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:19,601 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.283380900742708, 0.43794708087237355], [0.9449571780860424, 0.055042822], [1.0, 1.0], [2.321928094887362, 7.502034948968415, 7.512369369622162, 2.311593674233615, 5.1904412747348, 0.01033442065374679]]\n",
      "2020-12-23 02:43:19,604 : INFO : Removed 1 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:19,605 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:19,608 : INFO : built Dictionary(256 unique tokens: ['docker', 'move', 'slave', 'test', '\"\"\"']...) from 2 documents (total 1555 corpus positions)\n",
      "2020-12-23 02:43:19,652 : INFO : token count processed\n",
      "2020-12-23 02:43:19,659 : INFO : frequencies processed\n",
      "2020-12-23 02:43:19,789 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:19,789 : INFO : entropies processed\n",
      "2020-12-23 02:43:19,790 : INFO : extropies processed\n",
      "2020-12-23 02:43:19,791 : INFO : token count processed\n",
      "2020-12-23 02:43:19,792 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:19,793 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:19,793 : INFO : vocab #2480\n",
      "2020-12-23 02:43:19,794 : INFO : diff #set()\n",
      "2020-12-23 02:43:20,065 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:20,193 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.2935638718694493, 0.43600268222960586], [0.9779164921492338, 0.022083508], [1.0, 1.0], [2.321928094887362, 7.39180093901977, 7.400803632671102, 2.31292540123603, 5.078875537783739, 0.009002693651331661]]\n",
      "2020-12-23 02:43:20,195 : INFO : Removed 1 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:20,196 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:20,197 : INFO : built Dictionary(42 unique tokens: ['docker', 'move', 'slave', 'test', 'aggreg']...) from 2 documents (total 110 corpus positions)\n",
      "2020-12-23 02:43:20,202 : INFO : token count processed\n",
      "2020-12-23 02:43:20,204 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:43:20,205 : INFO : frequencies processed\n",
      "2020-12-23 02:43:20,206 : INFO : token count processed\n",
      "2020-12-23 02:43:20,207 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:20,208 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:20,209 : INFO : vocab #2480\n",
      "2020-12-23 02:43:20,209 : INFO : diff #set()\n",
      "2020-12-23 02:43:20,468 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:20,597 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.2969764231673082, 0.43535492568142986], [1.0, 0.0], [nan, nan], [2.321928094887362, 4.927561309677364, 5.073707911189529, 2.1757814933751964, 2.7517798163021667, 0.1461466015121653]]\n",
      "2020-12-23 02:43:20,599 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:20,600 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:20,600 : INFO : built Dictionary(10 unique tokens: ['docker', 'move', 'slave', 'test', 'dirnam']...) from 2 documents (total 11 corpus positions)\n",
      "2020-12-23 02:43:20,603 : INFO : token count processed\n",
      "2020-12-23 02:43:20,605 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:43:20,606 : INFO : frequencies processed\n",
      "2020-12-23 02:43:20,607 : INFO : token count processed\n",
      "2020-12-23 02:43:20,608 : INFO : alphabet_source #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:43:20,608 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:20,609 : INFO : vocab #2480\n",
      "2020-12-23 02:43:20,611 : INFO : diff #set()\n",
      "2020-12-23 02:43:20,882 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:21,011 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2908758802522882, 0.43651426453094117], [1.0, 0.0], [nan, nan], [2.321928094887362, 2.5216406363433186, 3.4182958340544896, 1.4252728971761912, 1.0963677391671274, 0.896655197711171]]\n",
      "2020-12-23 02:43:21,014 : INFO : Removed 1 and 530 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:21,015 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:21,019 : INFO : built Dictionary(331 unique tokens: ['docker', 'move', 'slave', 'test', '\"\"\"']...) from 2 documents (total 2883 corpus positions)\n",
      "2020-12-23 02:43:21,089 : INFO : token count processed\n",
      "2020-12-23 02:43:21,093 : INFO : frequencies processed\n",
      "2020-12-23 02:43:21,223 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:21,223 : INFO : entropies processed\n",
      "2020-12-23 02:43:21,224 : INFO : extropies processed\n",
      "2020-12-23 02:43:21,226 : INFO : token count processed\n",
      "2020-12-23 02:43:21,227 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:21,228 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:21,228 : INFO : vocab #2480\n",
      "2020-12-23 02:43:21,229 : INFO : diff #set()\n",
      "2020-12-23 02:43:21,491 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:21,618 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.3100405106405608, 0.4328928412267133], [0.9972906392067671, 0.0027093608], [0.0, 0.0], [2.321928094887362, 7.480007711014331, 7.487768223172665, 2.314167582729027, 5.1658401282853035, 0.00776051215833462]]\n",
      "2020-12-23 02:43:21,621 : INFO : Removed 1 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:21,622 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:21,623 : INFO : built Dictionary(209 unique tokens: ['docker', 'move', 'slave', 'test', '\")).']...) from 2 documents (total 1032 corpus positions)\n",
      "2020-12-23 02:43:21,652 : INFO : token count processed\n",
      "2020-12-23 02:43:21,657 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:43:21,658 : INFO : frequencies processed\n",
      "2020-12-23 02:43:21,660 : INFO : token count processed\n",
      "2020-12-23 02:43:21,661 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:21,663 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:21,663 : INFO : vocab #2480\n",
      "2020-12-23 02:43:21,665 : INFO : diff #set()\n",
      "2020-12-23 02:43:21,921 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:22,048 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.3050579667670934, 0.4338285693537361], [0.9994860390434042, 0.00051396096], [nan, nan], [2.321928094887362, 7.131331012509435, 7.1501717397784965, 2.3030873676183, 4.828243644891135, 0.01884072726906183]]\n",
      "2020-12-23 02:43:22,051 : INFO : Removed 1 and 166 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:22,052 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:22,053 : INFO : built Dictionary(198 unique tokens: ['docker', 'move', 'slave', 'test', '\")).']...) from 2 documents (total 851 corpus positions)\n",
      "2020-12-23 02:43:22,085 : INFO : token count processed\n",
      "2020-12-23 02:43:22,088 : INFO : frequencies processed\n",
      "2020-12-23 02:43:22,224 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:22,225 : INFO : entropies processed\n",
      "2020-12-23 02:43:22,226 : INFO : extropies processed\n",
      "2020-12-23 02:43:22,228 : INFO : token count processed\n",
      "2020-12-23 02:43:22,229 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:22,230 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:22,230 : INFO : vocab #2480\n",
      "2020-12-23 02:43:22,231 : INFO : diff #set()\n",
      "2020-12-23 02:43:22,500 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:22,629 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.3090498893983495, 0.43307855953712715], [0.9274283200502396, 0.07257168], [0.0, 0.0], [2.321928094887362, 7.203742744794778, 7.218553473761841, 2.3071173659202993, 4.896625378874479, 0.014810728967063369]]\n",
      "2020-12-23 02:43:22,631 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:22,632 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:22,633 : INFO : built Dictionary(56 unique tokens: ['docker', 'move', 'slave', 'test', 'argv']...) from 2 documents (total 183 corpus positions)\n",
      "2020-12-23 02:43:22,639 : INFO : token count processed\n",
      "2020-12-23 02:43:22,641 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:43:22,642 : INFO : frequencies processed\n",
      "2020-12-23 02:43:22,644 : INFO : token count processed\n",
      "2020-12-23 02:43:22,645 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:22,647 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:22,648 : INFO : vocab #2480\n",
      "2020-12-23 02:43:22,649 : INFO : diff #set()\n",
      "2020-12-23 02:43:22,911 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:23,038 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.3034204042496411, 0.4341369895634656], [1.0, 0.0], [nan, nan], [2.321928094887362, 5.195502554608948, 5.297094446950201, 2.2203362025461093, 2.9751663520628386, 0.10159189234125243]]\n",
      "2020-12-23 02:43:23,041 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:23,042 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:23,043 : INFO : built Dictionary(62 unique tokens: ['docker', 'move', 'slave', 'test', ')).']...) from 2 documents (total 244 corpus positions)\n",
      "2020-12-23 02:43:23,050 : INFO : token count processed\n",
      "2020-12-23 02:43:23,052 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:43:23,053 : INFO : frequencies processed\n",
      "2020-12-23 02:43:23,054 : INFO : token count processed\n",
      "2020-12-23 02:43:23,055 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:23,056 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:23,057 : INFO : vocab #2480\n",
      "2020-12-23 02:43:23,058 : INFO : diff #set()\n",
      "2020-12-23 02:43:23,317 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:23,445 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.3031589177525529, 0.43418627880693994], [1.0, 0.0], [nan, nan], [2.321928094887362, 5.32027245610305, 5.402593090255557, 2.239607460734854, 3.080664995368195, 0.08232063415250757]]\n",
      "2020-12-23 02:43:23,447 : INFO : Removed 1 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:23,448 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:23,449 : INFO : built Dictionary(162 unique tokens: ['docker', 'move', 'slave', 'test', '\"):']...) from 2 documents (total 484 corpus positions)\n",
      "2020-12-23 02:43:23,476 : INFO : token count processed\n",
      "2020-12-23 02:43:23,479 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:43:23,480 : INFO : frequencies processed\n",
      "2020-12-23 02:43:23,482 : INFO : token count processed\n",
      "2020-12-23 02:43:23,484 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:23,485 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:23,486 : INFO : vocab #2480\n",
      "2020-12-23 02:43:23,488 : INFO : diff #set()\n",
      "2020-12-23 02:43:23,748 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:23,875 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.3065010942927107, 0.4335571322616911], [0.9994489757809788, 0.0005510242], [nan, nan], [2.321928094887362, 6.898202761357263, 6.931671354155984, 2.2884595020886405, 4.6097432592686225, 0.033468592798721275]]\n",
      "2020-12-23 02:43:23,878 : INFO : Removed 1 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:23,879 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:23,881 : INFO : built Dictionary(126 unique tokens: ['docker', 'move', 'slave', 'test', '\"\"\"']...) from 2 documents (total 506 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:43:23,903 : INFO : token count processed\n",
      "2020-12-23 02:43:23,905 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:43:23,906 : INFO : frequencies processed\n",
      "2020-12-23 02:43:23,907 : INFO : token count processed\n",
      "2020-12-23 02:43:23,908 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:23,909 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:23,910 : INFO : vocab #2480\n",
      "2020-12-23 02:43:23,911 : INFO : diff #set()\n",
      "2020-12-23 02:43:24,176 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:24,304 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.3089839122791982, 0.43309093436380847], [1.0, 0.0], [nan, nan], [2.321928094887362, 6.388500481644799, 6.427059467294418, 2.283369109237743, 4.105131372407056, 0.038558985649618904]]\n",
      "2020-12-23 02:43:24,306 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:24,307 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:24,308 : INFO : built Dictionary(47 unique tokens: ['docker', 'move', 'slave', 'test', 'asset']...) from 2 documents (total 170 corpus positions)\n",
      "2020-12-23 02:43:24,314 : INFO : token count processed\n",
      "2020-12-23 02:43:24,316 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:43:24,316 : INFO : frequencies processed\n",
      "2020-12-23 02:43:24,317 : INFO : token count processed\n",
      "2020-12-23 02:43:24,318 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:24,319 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:24,320 : INFO : vocab #2480\n",
      "2020-12-23 02:43:24,321 : INFO : diff #set()\n",
      "2020-12-23 02:43:24,577 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:24,705 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.303913611091165, 0.4340440523403073], [1.0, 0.0], [nan, nan], [2.321928094887362, 4.8191513650620195, 4.936262784482276, 2.2048166754671055, 2.6143346895949136, 0.11711141942025627]]\n",
      "2020-12-23 02:43:24,708 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:24,708 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:24,709 : INFO : built Dictionary(52 unique tokens: ['docker', 'move', 'slave', 'test', '\"),']...) from 2 documents (total 225 corpus positions)\n",
      "2020-12-23 02:43:24,715 : INFO : token count processed\n",
      "2020-12-23 02:43:24,718 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:43:24,718 : INFO : frequencies processed\n",
      "2020-12-23 02:43:24,720 : INFO : token count processed\n",
      "2020-12-23 02:43:24,721 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:24,721 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:24,722 : INFO : vocab #2480\n",
      "2020-12-23 02:43:24,723 : INFO : diff #set()\n",
      "2020-12-23 02:43:24,983 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:25,112 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.3029740775489944, 0.43422112725831385], [1.0, 0.0], [nan, nan], [2.321928094887362, 5.062480936779194, 5.154788335904384, 2.229620695762171, 2.832860241017022, 0.09230739912519059]]\n",
      "2020-12-23 02:43:25,115 : INFO : Removed 1 and 223 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:25,116 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:25,117 : INFO : built Dictionary(243 unique tokens: ['docker', 'move', 'slave', 'test', '\"\"\"']...) from 2 documents (total 1779 corpus positions)\n",
      "2020-12-23 02:43:25,154 : INFO : token count processed\n",
      "2020-12-23 02:43:25,157 : INFO : frequencies processed\n",
      "2020-12-23 02:43:25,284 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:25,285 : INFO : entropies processed\n",
      "2020-12-23 02:43:25,286 : INFO : extropies processed\n",
      "2020-12-23 02:43:25,288 : INFO : token count processed\n",
      "2020-12-23 02:43:25,289 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:25,290 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:25,290 : INFO : vocab #2480\n",
      "2020-12-23 02:43:25,291 : INFO : diff #set()\n",
      "2020-12-23 02:43:25,563 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:25,693 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.3143966963999598, 0.4320780450281053], [0.9984982464229688, 0.0015017536], [0.0, 0.0], [2.321928094887362, 7.185085743102134, 7.197127065707273, 2.3098867722822245, 4.875198970819911, 0.012041322605138127]]\n",
      "2020-12-23 02:43:25,696 : INFO : Removed 1 and 148 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:25,697 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:25,699 : INFO : built Dictionary(160 unique tokens: ['docker', 'move', 'slave', 'test', '\"\",']...) from 2 documents (total 754 corpus positions)\n",
      "2020-12-23 02:43:25,726 : INFO : token count processed\n",
      "2020-12-23 02:43:25,729 : INFO : frequencies processed\n",
      "2020-12-23 02:43:25,856 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:25,857 : INFO : entropies processed\n",
      "2020-12-23 02:43:25,858 : INFO : extropies processed\n",
      "2020-12-23 02:43:25,859 : INFO : token count processed\n",
      "2020-12-23 02:43:25,860 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:25,862 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:25,863 : INFO : vocab #2480\n",
      "2020-12-23 02:43:25,864 : INFO : diff #set()\n",
      "2020-12-23 02:43:26,134 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:26,264 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.3034445194653326, 0.43413244449756333], [0.9943267111666501, 0.005673289], [0.0, 0.0], [2.321928094887362, 6.591225336124281, 6.614010899894913, 2.299142531116731, 4.292082805007551, 0.02278556377063179]]\n",
      "2020-12-23 02:43:26,266 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:26,267 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:26,268 : INFO : built Dictionary(38 unique tokens: ['docker', 'move', 'slave', 'test', 'autoescap']...) from 2 documents (total 92 corpus positions)\n",
      "2020-12-23 02:43:26,273 : INFO : token count processed\n",
      "2020-12-23 02:43:26,275 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:43:26,276 : INFO : frequencies processed\n",
      "2020-12-23 02:43:26,277 : INFO : token count processed\n",
      "2020-12-23 02:43:26,278 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:26,279 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:26,279 : INFO : vocab #2480\n",
      "2020-12-23 02:43:26,281 : INFO : diff #set()\n",
      "2020-12-23 02:43:26,540 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:26,669 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.2962490792497965, 0.43549282568540354], [1.0, 0.0], [nan, nan], [2.321928094887362, 4.7032114441396695, 4.876349949256587, 2.1487895897704448, 2.554421854369225, 0.17313850511691786]]\n",
      "2020-12-23 02:43:26,671 : INFO : Removed 1 and 9 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:26,672 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:26,673 : INFO : built Dictionary(116 unique tokens: ['docker', 'move', 'slave', 'test', '\"\"\"']...) from 2 documents (total 332 corpus positions)\n",
      "2020-12-23 02:43:26,693 : INFO : token count processed\n",
      "2020-12-23 02:43:26,695 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:43:26,696 : INFO : frequencies processed\n",
      "2020-12-23 02:43:26,697 : INFO : token count processed\n",
      "2020-12-23 02:43:26,698 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:26,699 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:26,699 : INFO : vocab #2480\n",
      "2020-12-23 02:43:26,700 : INFO : diff #set()\n",
      "2020-12-23 02:43:26,963 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:27,091 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.3087308276474239, 0.43313841008438], [1.0, 0.0], [nan, nan], [2.321928094887362, 6.14228447828618, 6.19649011115536, 2.267722462018182, 3.874562016267998, 0.054205632869180675]]\n",
      "2020-12-23 02:43:27,094 : INFO : Removed 1 and 184 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:43:27,095 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:27,096 : INFO : built Dictionary(253 unique tokens: ['docker', 'move', 'slave', 'test', '\"\"\"']...) from 2 documents (total 1122 corpus positions)\n",
      "2020-12-23 02:43:27,136 : INFO : token count processed\n",
      "2020-12-23 02:43:27,138 : INFO : frequencies processed\n",
      "2020-12-23 02:43:27,264 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:27,265 : INFO : entropies processed\n",
      "2020-12-23 02:43:27,268 : INFO : extropies processed\n",
      "2020-12-23 02:43:27,270 : INFO : token count processed\n",
      "2020-12-23 02:43:27,270 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:27,271 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:27,272 : INFO : vocab #2480\n",
      "2020-12-23 02:43:27,273 : INFO : diff #set()\n",
      "2020-12-23 02:43:27,533 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:27,660 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.3086404249197066, 0.4331553711032239], [0.9974372528959066, 0.002562747], [0.0, 0.0], [2.321928094887362, 7.450178124335845, 7.4652576854003385, 2.306848533822869, 5.143329590512977, 0.015079561064493596]]\n",
      "2020-12-23 02:43:27,662 : INFO : Removed 1 and 16 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:27,663 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:27,664 : INFO : built Dictionary(56 unique tokens: ['docker', 'move', 'slave', 'test', 'argv']...) from 2 documents (total 170 corpus positions)\n",
      "2020-12-23 02:43:27,676 : INFO : token count processed\n",
      "2020-12-23 02:43:27,678 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:43:27,679 : INFO : frequencies processed\n",
      "2020-12-23 02:43:27,680 : INFO : token count processed\n",
      "2020-12-23 02:43:27,681 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:27,682 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:27,683 : INFO : vocab #2480\n",
      "2020-12-23 02:43:27,684 : INFO : diff #set()\n",
      "2020-12-23 02:43:27,943 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:28,071 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.3007927157153842, 0.4346328085835714], [1.0, 0.0], [nan, nan], [2.321928094887362, 5.20665021947654, 5.307277964853237, 2.2213003495106642, 2.985349869965875, 0.1006277453766975]]\n",
      "2020-12-23 02:43:28,074 : INFO : Removed 1 and 89 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:28,075 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:28,076 : INFO : built Dictionary(125 unique tokens: ['docker', 'move', 'slave', 'test', '\"--']...) from 2 documents (total 508 corpus positions)\n",
      "2020-12-23 02:43:28,094 : INFO : token count processed\n",
      "2020-12-23 02:43:28,096 : INFO : frequencies processed\n",
      "2020-12-23 02:43:28,225 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:28,225 : INFO : entropies processed\n",
      "2020-12-23 02:43:28,226 : INFO : extropies processed\n",
      "2020-12-23 02:43:28,228 : INFO : token count processed\n",
      "2020-12-23 02:43:28,229 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:28,230 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:28,230 : INFO : vocab #2480\n",
      "2020-12-23 02:43:28,232 : INFO : diff #set()\n",
      "2020-12-23 02:43:28,504 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:28,634 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.304948611178081, 0.4338491518424311], [0.9128220826387405, 0.08717792], [0.0, 0.0], [2.321928094887362, 6.524718477352, 6.549600661528121, 2.2970459107112404, 4.227672566640759, 0.024882184176121314]]\n",
      "2020-12-23 02:43:28,636 : INFO : Removed 1 and 8 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:28,637 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:28,638 : INFO : built Dictionary(62 unique tokens: ['docker', 'move', 'slave', 'test', '(),']...) from 2 documents (total 215 corpus positions)\n",
      "2020-12-23 02:43:28,645 : INFO : token count processed\n",
      "2020-12-23 02:43:28,647 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:43:28,648 : INFO : frequencies processed\n",
      "2020-12-23 02:43:28,649 : INFO : token count processed\n",
      "2020-12-23 02:43:28,650 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:28,651 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:28,652 : INFO : vocab #2480\n",
      "2020-12-23 02:43:28,653 : INFO : diff #set()\n",
      "2020-12-23 02:43:28,911 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:29,040 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.3017534957940846, 0.43445138753010076], [1.0, 0.0], [nan, nan], [2.321928094887362, 5.321859380715434, 5.409180093146993, 2.2346073824558026, 3.087251998259631, 0.08732071243155914]]\n",
      "2020-12-23 02:43:29,043 : INFO : Removed 1 and 119 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:29,044 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:29,045 : INFO : built Dictionary(141 unique tokens: ['docker', 'move', 'slave', 'test', '\"--']...) from 2 documents (total 833 corpus positions)\n",
      "2020-12-23 02:43:29,068 : INFO : token count processed\n",
      "2020-12-23 02:43:29,076 : INFO : frequencies processed\n",
      "2020-12-23 02:43:29,207 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:29,207 : INFO : entropies processed\n",
      "2020-12-23 02:43:29,208 : INFO : extropies processed\n",
      "2020-12-23 02:43:29,209 : INFO : token count processed\n",
      "2020-12-23 02:43:29,210 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:29,211 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:29,211 : INFO : vocab #2480\n",
      "2020-12-23 02:43:29,212 : INFO : diff #set()\n",
      "2020-12-23 02:43:29,472 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:29,600 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.305099159721363, 0.43382081668056244], [0.9889000784605742, 0.011099922], [0.0, 0.0], [2.321928094887362, 6.500767808767801, 6.522344620365228, 2.3003512832899347, 4.200416525477866, 0.02157681159742708]]\n",
      "2020-12-23 02:43:29,602 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:29,603 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:29,604 : INFO : built Dictionary(34 unique tokens: ['docker', 'move', 'slave', 'test', 'accept']...) from 2 documents (total 55 corpus positions)\n",
      "2020-12-23 02:43:29,608 : INFO : token count processed\n",
      "2020-12-23 02:43:29,610 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:43:29,611 : INFO : frequencies processed\n",
      "2020-12-23 02:43:29,612 : INFO : token count processed\n",
      "2020-12-23 02:43:29,613 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:29,614 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:29,615 : INFO : vocab #2480\n",
      "2020-12-23 02:43:29,616 : INFO : diff #set()\n",
      "2020-12-23 02:43:29,876 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:30,003 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.3176549272685605, 0.43147061636933837], [1.0, 0.0], [nan, nan], [2.321928094887362, 4.736228843383063, 4.9532590407012735, 2.1048978975691517, 2.6313309458139114, 0.21703019731821094]]\n",
      "2020-12-23 02:43:30,005 : INFO : Removed 1 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:30,006 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:30,008 : INFO : built Dictionary(96 unique tokens: ['docker', 'move', 'slave', 'test', '(\",\")']...) from 2 documents (total 396 corpus positions)\n",
      "2020-12-23 02:43:30,029 : INFO : token count processed\n",
      "2020-12-23 02:43:30,031 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:43:30,034 : INFO : frequencies processed\n",
      "2020-12-23 02:43:30,035 : INFO : token count processed\n",
      "2020-12-23 02:43:30,038 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:30,038 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:30,039 : INFO : vocab #2480\n",
      "2020-12-23 02:43:30,040 : INFO : diff #set()\n",
      "2020-12-23 02:43:30,298 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:30,428 : INFO : Computed distances or similarities ('274', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.3162461760447683, 0.43173303871681035], [1.0, 0.0], [nan, nan], [2.321928094887362, 5.788442787590127, 5.841100153960053, 2.269270728517437, 3.5191720590726905, 0.05265736636992546]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:43:30,431 : INFO : Removed 1 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:30,432 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:30,433 : INFO : built Dictionary(53 unique tokens: ['docker', 'move', 'slave', 'test', 'author']...) from 2 documents (total 82 corpus positions)\n",
      "2020-12-23 02:43:30,438 : INFO : token count processed\n",
      "2020-12-23 02:43:30,441 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:43:30,441 : INFO : frequencies processed\n",
      "2020-12-23 02:43:30,443 : INFO : token count processed\n",
      "2020-12-23 02:43:30,443 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:30,444 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:30,445 : INFO : vocab #2480\n",
      "2020-12-23 02:43:30,446 : INFO : diff #set()\n",
      "2020-12-23 02:43:30,705 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:30,834 : INFO : Computed distances or similarities ('274', 'sacp-python-common/setup.py')[[1.2998007190544845, 0.43482028321616023], [0.9971496993675828, 0.0028503006], [nan, nan], [2.321928094887362, 5.370004292053436, 5.510853622191934, 2.1810787647488645, 3.1889255273045722, 0.1408493301384981]]\n",
      "2020-12-23 02:43:30,836 : INFO : Removed 1 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:30,837 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:30,838 : INFO : built Dictionary(78 unique tokens: ['docker', 'move', 'slave', 'test', '\"--']...) from 2 documents (total 327 corpus positions)\n",
      "2020-12-23 02:43:30,853 : INFO : token count processed\n",
      "2020-12-23 02:43:30,859 : INFO : frequencies processed\n",
      "2020-12-23 02:43:30,985 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:30,986 : INFO : entropies processed\n",
      "2020-12-23 02:43:30,986 : INFO : extropies processed\n",
      "2020-12-23 02:43:30,987 : INFO : token count processed\n",
      "2020-12-23 02:43:30,988 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:30,988 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:30,989 : INFO : vocab #2480\n",
      "2020-12-23 02:43:30,991 : INFO : diff #set()\n",
      "2020-12-23 02:43:31,244 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:31,371 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.2335959470540105, 0.4477085487726393], [0.8984171748161316, 0.101582825], [0.0, 0.0], [2.321928094887362, 5.695663584743922, 5.739351086909425, 2.2782405927218594, 3.4174229920220625, 0.04368750216550232]]\n",
      "2020-12-23 02:43:31,374 : INFO : Removed 1 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:31,374 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:31,375 : INFO : built Dictionary(39 unique tokens: ['docker', 'move', 'slave', 'test', 'actual']...) from 2 documents (total 102 corpus positions)\n",
      "2020-12-23 02:43:31,380 : INFO : token count processed\n",
      "2020-12-23 02:43:31,383 : INFO : frequencies processed\n",
      "2020-12-23 02:43:31,510 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:31,511 : INFO : entropies processed\n",
      "2020-12-23 02:43:31,512 : INFO : extropies processed\n",
      "2020-12-23 02:43:31,513 : INFO : token count processed\n",
      "2020-12-23 02:43:31,514 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:31,515 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:31,516 : INFO : vocab #2480\n",
      "2020-12-23 02:43:31,517 : INFO : diff #set()\n",
      "2020-12-23 02:43:31,775 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:31,904 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.2307056481109515, 0.4482886394477187], [0.8885307982563972, 0.1114692], [0.0, 0.0], [2.321928094887362, 4.9004417692112465, 5.0158365289110955, 2.2065333351875127, 2.6939084340237334, 0.11539475969984903]]\n",
      "2020-12-23 02:43:31,906 : INFO : Removed 1 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:31,907 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:31,908 : INFO : built Dictionary(37 unique tokens: ['docker', 'move', 'slave', 'test', 'actual']...) from 2 documents (total 109 corpus positions)\n",
      "2020-12-23 02:43:31,921 : INFO : token count processed\n",
      "2020-12-23 02:43:31,925 : INFO : frequencies processed\n",
      "2020-12-23 02:43:32,055 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:32,056 : INFO : entropies processed\n",
      "2020-12-23 02:43:32,057 : INFO : extropies processed\n",
      "2020-12-23 02:43:32,058 : INFO : token count processed\n",
      "2020-12-23 02:43:32,059 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:32,060 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:32,061 : INFO : vocab #2480\n",
      "2020-12-23 02:43:32,062 : INFO : diff #set()\n",
      "2020-12-23 02:43:32,320 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:32,448 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.2372160486226995, 0.44698409910640113], [0.8968362361192703, 0.103163764], [0.0, 0.0], [2.321928094887362, 4.778624108914332, 4.896846387186896, 2.2037058166147983, 2.5749182922995337, 0.11822227827256349]]\n",
      "2020-12-23 02:43:32,451 : INFO : Removed 1 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:32,451 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:32,452 : INFO : built Dictionary(38 unique tokens: ['docker', 'move', 'slave', 'test', 'actual']...) from 2 documents (total 140 corpus positions)\n",
      "2020-12-23 02:43:32,457 : INFO : token count processed\n",
      "2020-12-23 02:43:32,459 : INFO : frequencies processed\n",
      "2020-12-23 02:43:32,588 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:32,589 : INFO : entropies processed\n",
      "2020-12-23 02:43:32,590 : INFO : extropies processed\n",
      "2020-12-23 02:43:32,591 : INFO : token count processed\n",
      "2020-12-23 02:43:32,592 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:32,593 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:32,593 : INFO : vocab #2480\n",
      "2020-12-23 02:43:32,594 : INFO : diff #set()\n",
      "2020-12-23 02:43:32,851 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:32,980 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.2242956369793336, 0.44958052489732553], [0.8817654475569725, 0.11823455], [0.0, 0.0], [2.321928094887362, 4.773880192225086, 4.873466482628252, 2.222341804484196, 2.5515383877408895, 0.09958629040316591]]\n",
      "2020-12-23 02:43:32,983 : INFO : Removed 1 and 346 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:32,984 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:32,985 : INFO : built Dictionary(149 unique tokens: ['docker', 'move', 'slave', 'test', '\"))']...) from 2 documents (total 1965 corpus positions)\n",
      "2020-12-23 02:43:33,010 : INFO : token count processed\n",
      "2020-12-23 02:43:33,012 : INFO : frequencies processed\n",
      "2020-12-23 02:43:33,145 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:33,145 : INFO : entropies processed\n",
      "2020-12-23 02:43:33,146 : INFO : extropies processed\n",
      "2020-12-23 02:43:33,148 : INFO : token count processed\n",
      "2020-12-23 02:43:33,149 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:33,150 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:33,151 : INFO : vocab #2480\n",
      "2020-12-23 02:43:33,152 : INFO : diff #set()\n",
      "2020-12-23 02:43:33,423 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:33,553 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.2630919174980844, 0.4418733469321608], [0.9005369395017624, 0.09946306], [0.0, 0.0], [2.321928094887362, 6.620773041953877, 6.630450324878247, 2.3122508119629916, 4.308522229990885, 0.00967728292437009]]\n",
      "2020-12-23 02:43:33,556 : INFO : Removed 1 and 132 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:33,557 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:33,558 : INFO : built Dictionary(79 unique tokens: ['docker', 'move', 'slave', 'test', '\"))']...) from 2 documents (total 528 corpus positions)\n",
      "2020-12-23 02:43:33,569 : INFO : token count processed\n",
      "2020-12-23 02:43:33,572 : INFO : frequencies processed\n",
      "2020-12-23 02:43:33,703 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:33,703 : INFO : entropies processed\n",
      "2020-12-23 02:43:33,704 : INFO : extropies processed\n",
      "2020-12-23 02:43:33,706 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:43:33,707 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:33,708 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:33,709 : INFO : vocab #2480\n",
      "2020-12-23 02:43:33,710 : INFO : diff #set()\n",
      "2020-12-23 02:43:33,976 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:34,104 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.2183415680528813, 0.45078720716473625], [0.8773526772856712, 0.12264732], [0.0, 0.0], [2.321928094887362, 5.828370634755606, 5.856109869041205, 2.2941888606017624, 3.534181774153843, 0.027739234285599323]]\n",
      "2020-12-23 02:43:34,107 : INFO : Removed 1 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:34,107 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:34,109 : INFO : built Dictionary(80 unique tokens: ['docker', 'move', 'slave', 'test', '\"):']...) from 2 documents (total 291 corpus positions)\n",
      "2020-12-23 02:43:34,125 : INFO : token count processed\n",
      "2020-12-23 02:43:34,128 : INFO : frequencies processed\n",
      "2020-12-23 02:43:34,258 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:34,259 : INFO : entropies processed\n",
      "2020-12-23 02:43:34,260 : INFO : extropies processed\n",
      "2020-12-23 02:43:34,261 : INFO : token count processed\n",
      "2020-12-23 02:43:34,262 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:34,263 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:34,264 : INFO : vocab #2480\n",
      "2020-12-23 02:43:34,265 : INFO : diff #set()\n",
      "2020-12-23 02:43:34,530 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:34,658 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.2524304659983458, 0.4439648704346438], [0.9137010648846626, 0.086298935], [0.0, 0.0], [2.321928094887362, 5.774409284925443, 5.8209865444706885, 2.275350835342117, 3.4990584495833263, 0.04657725954524583]]\n",
      "2020-12-23 02:43:34,660 : INFO : Removed 1 and 51 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:34,661 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:34,662 : INFO : built Dictionary(91 unique tokens: ['docker', 'move', 'slave', 'test', '\"])']...) from 2 documents (total 340 corpus positions)\n",
      "2020-12-23 02:43:34,673 : INFO : token count processed\n",
      "2020-12-23 02:43:34,675 : INFO : frequencies processed\n",
      "2020-12-23 02:43:34,803 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:34,804 : INFO : entropies processed\n",
      "2020-12-23 02:43:34,806 : INFO : extropies processed\n",
      "2020-12-23 02:43:34,807 : INFO : token count processed\n",
      "2020-12-23 02:43:34,808 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:34,808 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:34,809 : INFO : vocab #2480\n",
      "2020-12-23 02:43:34,810 : INFO : diff #set()\n",
      "2020-12-23 02:43:35,073 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:35,201 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.2390666893950488, 0.44661465633709196], [0.8968772888183594, 0.10312271], [0.0, 0.0], [2.321928094887362, 5.977819040873918, 6.015969773004106, 2.2837773627571734, 3.694041678116744, 0.03815073213018838]]\n",
      "2020-12-23 02:43:35,204 : INFO : Removed 1 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:35,205 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:35,206 : INFO : built Dictionary(72 unique tokens: ['docker', 'move', 'slave', 'test', '\").']...) from 2 documents (total 287 corpus positions)\n",
      "2020-12-23 02:43:35,214 : INFO : token count processed\n",
      "2020-12-23 02:43:35,217 : INFO : frequencies processed\n",
      "2020-12-23 02:43:35,345 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:35,346 : INFO : entropies processed\n",
      "2020-12-23 02:43:35,347 : INFO : extropies processed\n",
      "2020-12-23 02:43:35,348 : INFO : token count processed\n",
      "2020-12-23 02:43:35,349 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:35,350 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:35,350 : INFO : vocab #2480\n",
      "2020-12-23 02:43:35,351 : INFO : diff #set()\n",
      "2020-12-23 02:43:35,609 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:35,738 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.2448616411356948, 0.4454617521523916], [0.8954311087727547, 0.10456889], [0.0, 0.0], [2.321928094887362, 5.901812829596593, 5.945032879907477, 2.2787080445764785, 3.623104785020115, 0.04322005031088416]]\n",
      "2020-12-23 02:43:35,741 : INFO : Removed 1 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:35,742 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:35,743 : INFO : built Dictionary(74 unique tokens: ['docker', 'move', 'slave', 'test', '\"],']...) from 2 documents (total 287 corpus positions)\n",
      "2020-12-23 02:43:35,757 : INFO : token count processed\n",
      "2020-12-23 02:43:35,760 : INFO : frequencies processed\n",
      "2020-12-23 02:43:35,892 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:35,893 : INFO : entropies processed\n",
      "2020-12-23 02:43:35,894 : INFO : extropies processed\n",
      "2020-12-23 02:43:35,896 : INFO : token count processed\n",
      "2020-12-23 02:43:35,897 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:35,899 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:35,900 : INFO : vocab #2480\n",
      "2020-12-23 02:43:35,902 : INFO : diff #set()\n",
      "2020-12-23 02:43:36,175 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:36,304 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.2631989585320045, 0.44185244793884026], [0.9354797899723053, 0.06452021], [0.0, 0.0], [2.321928094887362, 5.643202320803383, 5.694227482352264, 2.2709029333384807, 3.3722993874649014, 0.05102516154888104]]\n",
      "2020-12-23 02:43:36,306 : INFO : Removed 1 and 30 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:36,307 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:36,309 : INFO : built Dictionary(87 unique tokens: ['docker', 'move', 'slave', 'test', '\"))']...) from 2 documents (total 321 corpus positions)\n",
      "2020-12-23 02:43:36,327 : INFO : token count processed\n",
      "2020-12-23 02:43:36,330 : INFO : frequencies processed\n",
      "2020-12-23 02:43:36,460 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:36,461 : INFO : entropies processed\n",
      "2020-12-23 02:43:36,464 : INFO : extropies processed\n",
      "2020-12-23 02:43:36,465 : INFO : token count processed\n",
      "2020-12-23 02:43:36,465 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:36,466 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:36,467 : INFO : vocab #2480\n",
      "2020-12-23 02:43:36,467 : INFO : diff #set()\n",
      "2020-12-23 02:43:36,728 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:36,856 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.2719986931555853, 0.44014109823764785], [0.9526021666824818, 0.047397833], [0.0, 0.0], [2.321928094887362, 5.925214310725336, 5.9697920832025195, 2.277350322410178, 3.6478639883151573, 0.044577772477183863]]\n",
      "2020-12-23 02:43:36,859 : INFO : Removed 1 and 293 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:36,860 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:36,861 : INFO : built Dictionary(160 unique tokens: ['docker', 'move', 'slave', 'test', '\"))']...) from 2 documents (total 1716 corpus positions)\n",
      "2020-12-23 02:43:36,885 : INFO : token count processed\n",
      "2020-12-23 02:43:36,888 : INFO : frequencies processed\n",
      "2020-12-23 02:43:37,016 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:37,017 : INFO : entropies processed\n",
      "2020-12-23 02:43:37,018 : INFO : extropies processed\n",
      "2020-12-23 02:43:37,020 : INFO : token count processed\n",
      "2020-12-23 02:43:37,021 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:37,023 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:37,023 : INFO : vocab #2480\n",
      "2020-12-23 02:43:37,025 : INFO : diff #set()\n",
      "2020-12-23 02:43:37,296 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:37,425 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.2585311175919336, 0.4427656507412699], [0.8965277299284935, 0.10347227], [0.0, 0.0], [2.321928094887362, 6.551685682764175, 6.562516653896252, 2.3110971237552844, 4.24058855900889, 0.010830971132077316]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:43:37,427 : INFO : Removed 1 and 88 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:37,428 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:37,430 : INFO : built Dictionary(136 unique tokens: ['docker', 'move', 'slave', 'test', '\")}']...) from 2 documents (total 557 corpus positions)\n",
      "2020-12-23 02:43:37,452 : INFO : token count processed\n",
      "2020-12-23 02:43:37,455 : INFO : frequencies processed\n",
      "2020-12-23 02:43:37,582 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:37,582 : INFO : entropies processed\n",
      "2020-12-23 02:43:37,583 : INFO : extropies processed\n",
      "2020-12-23 02:43:37,585 : INFO : token count processed\n",
      "2020-12-23 02:43:37,586 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:37,587 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:37,588 : INFO : vocab #2480\n",
      "2020-12-23 02:43:37,589 : INFO : diff #set()\n",
      "2020-12-23 02:43:37,846 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:37,972 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.2505023132684483, 0.4443452442169146], [0.9139620140194893, 0.086037986], [0.0, 0.0], [2.321928094887362, 6.642985062562557, 6.66608037880796, 2.298832778641959, 4.344152283920598, 0.02309531624540284]]\n",
      "2020-12-23 02:43:37,975 : INFO : Removed 1 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:37,976 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:37,977 : INFO : built Dictionary(49 unique tokens: ['docker', 'move', 'slave', 'test', 'assert']...) from 2 documents (total 152 corpus positions)\n",
      "2020-12-23 02:43:37,988 : INFO : token count processed\n",
      "2020-12-23 02:43:37,990 : INFO : frequencies processed\n",
      "2020-12-23 02:43:38,120 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:38,121 : INFO : entropies processed\n",
      "2020-12-23 02:43:38,122 : INFO : extropies processed\n",
      "2020-12-23 02:43:38,123 : INFO : token count processed\n",
      "2020-12-23 02:43:38,124 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:38,125 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:38,126 : INFO : vocab #2480\n",
      "2020-12-23 02:43:38,127 : INFO : diff #set()\n",
      "2020-12-23 02:43:38,392 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:38,519 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.2385731693828494, 0.4467131178364338], [0.9043099507689476, 0.09569005], [0.0, 0.0], [2.321928094887362, 5.2461980344571995, 5.326697499475013, 2.2414286298695485, 3.0047694045876505, 0.08049946501781324]]\n",
      "2020-12-23 02:43:38,522 : INFO : Removed 1 and 23 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:38,523 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:38,524 : INFO : built Dictionary(71 unique tokens: ['docker', 'move', 'slave', 'test', '__main__']...) from 2 documents (total 212 corpus positions)\n",
      "2020-12-23 02:43:38,532 : INFO : token count processed\n",
      "2020-12-23 02:43:38,534 : INFO : frequencies processed\n",
      "2020-12-23 02:43:38,663 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:38,664 : INFO : entropies processed\n",
      "2020-12-23 02:43:38,665 : INFO : extropies processed\n",
      "2020-12-23 02:43:38,666 : INFO : token count processed\n",
      "2020-12-23 02:43:38,667 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:38,668 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:38,668 : INFO : vocab #2480\n",
      "2020-12-23 02:43:38,669 : INFO : diff #set()\n",
      "2020-12-23 02:43:38,931 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:39,059 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/test_auth_utility.py')[[1.279898257574129, 0.4386160639747258], [0.9548496678471565, 0.045150332], [0.0, 0.0], [2.321928094887362, 5.903090303960449, 5.9599743678928485, 2.2650440309549635, 3.6380462730054863, 0.05688406393239909]]\n",
      "2020-12-23 02:43:39,062 : INFO : Removed 1 and 442 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:39,063 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:39,065 : INFO : built Dictionary(105 unique tokens: ['docker', 'move', 'slave', 'test', '\":\"']...) from 2 documents (total 1211 corpus positions)\n",
      "2020-12-23 02:43:39,082 : INFO : token count processed\n",
      "2020-12-23 02:43:39,085 : INFO : frequencies processed\n",
      "2020-12-23 02:43:39,214 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:39,215 : INFO : entropies processed\n",
      "2020-12-23 02:43:39,216 : INFO : extropies processed\n",
      "2020-12-23 02:43:39,219 : INFO : token count processed\n",
      "2020-12-23 02:43:39,220 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:39,222 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:39,222 : INFO : vocab #2480\n",
      "2020-12-23 02:43:39,223 : INFO : diff #set()\n",
      "2020-12-23 02:43:39,489 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:39,617 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.2846603538979102, 0.4377018221959678], [0.9680331498384476, 0.03196685], [0.0, 0.0], [2.321928094887362, 6.16659449033757, 6.18116544990643, 2.307357135318501, 3.859237355019068, 0.014570959568860786]]\n",
      "2020-12-23 02:43:39,619 : INFO : Removed 1 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:39,620 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:39,621 : INFO : built Dictionary(65 unique tokens: ['docker', 'move', 'slave', 'test', '\":\"']...) from 2 documents (total 246 corpus positions)\n",
      "2020-12-23 02:43:39,629 : INFO : token count processed\n",
      "2020-12-23 02:43:39,631 : INFO : frequencies processed\n",
      "2020-12-23 02:43:39,757 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:39,758 : INFO : entropies processed\n",
      "2020-12-23 02:43:39,759 : INFO : extropies processed\n",
      "2020-12-23 02:43:39,760 : INFO : token count processed\n",
      "2020-12-23 02:43:39,761 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:39,762 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:39,763 : INFO : vocab #2480\n",
      "2020-12-23 02:43:39,764 : INFO : diff #set()\n",
      "2020-12-23 02:43:40,022 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:40,150 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.267265601187175, 0.44105992675775824], [0.9350425526499748, 0.06495745], [0.0, 0.0], [2.321928094887362, 5.906856253399655, 5.954667148059671, 2.274117200227347, 3.6327390531723087, 0.047810894660015535]]\n",
      "2020-12-23 02:43:40,152 : INFO : Removed 1 and 68 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:40,153 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:40,155 : INFO : built Dictionary(78 unique tokens: ['docker', 'move', 'slave', 'test', '())']...) from 2 documents (total 245 corpus positions)\n",
      "2020-12-23 02:43:40,173 : INFO : token count processed\n",
      "2020-12-23 02:43:40,176 : INFO : frequencies processed\n",
      "2020-12-23 02:43:40,303 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:40,303 : INFO : entropies processed\n",
      "2020-12-23 02:43:40,304 : INFO : extropies processed\n",
      "2020-12-23 02:43:40,305 : INFO : token count processed\n",
      "2020-12-23 02:43:40,305 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:40,306 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:40,306 : INFO : vocab #2480\n",
      "2020-12-23 02:43:40,308 : INFO : diff #set()\n",
      "2020-12-23 02:43:40,561 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:40,688 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.2007697778104127, 0.45438646517352616], [0.6360602974891663, 0.3639397], [1.0, 1.0], [2.321928094887362, 5.965115449163356, 5.987523925682258, 2.2995196183684596, 3.665595830794896, 0.0224084765189021]]\n",
      "2020-12-23 02:43:40,691 : INFO : Removed 1 and 13 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:40,691 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:40,692 : INFO : built Dictionary(85 unique tokens: ['docker', 'move', 'slave', 'test', '\"))']...) from 2 documents (total 359 corpus positions)\n",
      "2020-12-23 02:43:40,704 : INFO : token count processed\n",
      "2020-12-23 02:43:40,709 : INFO : frequencies processed\n",
      "2020-12-23 02:43:40,837 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:40,838 : INFO : entropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:43:40,839 : INFO : extropies processed\n",
      "2020-12-23 02:43:40,840 : INFO : token count processed\n",
      "2020-12-23 02:43:40,841 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:40,841 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:40,842 : INFO : vocab #2480\n",
      "2020-12-23 02:43:40,843 : INFO : diff #set()\n",
      "2020-12-23 02:43:41,109 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:41,237 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.2615474906789452, 0.4421751053743237], [0.9225063920021057, 0.07749361], [0.0, 0.0], [2.321928094887362, 5.791362404253194, 5.833821092665465, 2.2794694064750907, 3.511892997778103, 0.04245868841227107]]\n",
      "2020-12-23 02:43:41,240 : INFO : Removed 1 and 41 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:41,241 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:41,242 : INFO : built Dictionary(75 unique tokens: ['docker', 'move', 'slave', 'test', '\"))']...) from 2 documents (total 371 corpus positions)\n",
      "2020-12-23 02:43:41,257 : INFO : token count processed\n",
      "2020-12-23 02:43:41,260 : INFO : frequencies processed\n",
      "2020-12-23 02:43:41,394 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:41,395 : INFO : entropies processed\n",
      "2020-12-23 02:43:41,396 : INFO : extropies processed\n",
      "2020-12-23 02:43:41,397 : INFO : token count processed\n",
      "2020-12-23 02:43:41,398 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:41,399 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:41,399 : INFO : vocab #2480\n",
      "2020-12-23 02:43:41,400 : INFO : diff #set()\n",
      "2020-12-23 02:43:41,658 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:41,786 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.181221185049653, 0.4584587784375642], [0.8010196536779404, 0.19898035], [0.0, 0.0], [2.321928094887362, 5.651670454631116, 5.689951571842116, 2.283646977676362, 3.3680234769547535, 0.03828111721099958]]\n",
      "2020-12-23 02:43:41,788 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:41,789 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:41,790 : INFO : built Dictionary(38 unique tokens: ['docker', 'move', 'slave', 'test', 'arg']...) from 2 documents (total 81 corpus positions)\n",
      "2020-12-23 02:43:41,795 : INFO : token count processed\n",
      "2020-12-23 02:43:41,797 : INFO : frequencies processed\n",
      "2020-12-23 02:43:41,924 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:41,925 : INFO : entropies processed\n",
      "2020-12-23 02:43:41,926 : INFO : extropies processed\n",
      "2020-12-23 02:43:41,927 : INFO : token count processed\n",
      "2020-12-23 02:43:41,928 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:41,929 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:41,930 : INFO : vocab #2480\n",
      "2020-12-23 02:43:41,931 : INFO : diff #set()\n",
      "2020-12-23 02:43:42,187 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:42,315 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.2320963781781837, 0.44800932870837357], [0.887858122587204, 0.11214188], [0.0, 0.0], [2.321928094887362, 4.8226207261920235, 4.956887661434702, 2.1876611596446844, 2.6349595665473395, 0.1342669352426782]]\n",
      "2020-12-23 02:43:42,318 : INFO : Removed 1 and 50 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:42,319 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:42,320 : INFO : built Dictionary(83 unique tokens: ['docker', 'move', 'slave', 'test', '\")):']...) from 2 documents (total 251 corpus positions)\n",
      "2020-12-23 02:43:42,336 : INFO : token count processed\n",
      "2020-12-23 02:43:42,338 : INFO : frequencies processed\n",
      "2020-12-23 02:43:42,465 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:42,466 : INFO : entropies processed\n",
      "2020-12-23 02:43:42,466 : INFO : extropies processed\n",
      "2020-12-23 02:43:42,467 : INFO : token count processed\n",
      "2020-12-23 02:43:42,468 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:42,469 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:42,470 : INFO : vocab #2480\n",
      "2020-12-23 02:43:42,471 : INFO : diff #set()\n",
      "2020-12-23 02:43:42,729 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:42,857 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.2700898500685234, 0.44051119825491253], [0.9329186826944351, 0.06708132], [0.0, 0.0], [2.321928094887362, 6.24862851613934, 6.291544411396206, 2.279012199630497, 3.969616316508844, 0.04291589525686579]]\n",
      "2020-12-23 02:43:42,859 : INFO : Removed 1 and 32 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:42,860 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:42,861 : INFO : built Dictionary(84 unique tokens: ['docker', 'move', 'slave', 'test', '\"))']...) from 2 documents (total 367 corpus positions)\n",
      "2020-12-23 02:43:42,874 : INFO : token count processed\n",
      "2020-12-23 02:43:42,876 : INFO : frequencies processed\n",
      "2020-12-23 02:43:43,003 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:43,004 : INFO : entropies processed\n",
      "2020-12-23 02:43:43,005 : INFO : extropies processed\n",
      "2020-12-23 02:43:43,006 : INFO : token count processed\n",
      "2020-12-23 02:43:43,007 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:43,008 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:43,009 : INFO : vocab #2480\n",
      "2020-12-23 02:43:43,010 : INFO : diff #set()\n",
      "2020-12-23 02:43:43,280 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:43,408 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.2676391802521823, 0.44098726495314433], [0.9255789071321487, 0.07442109], [0.0, 0.0], [2.321928094887362, 5.850156917433494, 5.890283247396188, 2.2818017649246682, 3.568355152508826, 0.040126329962694385]]\n",
      "2020-12-23 02:43:43,411 : INFO : Removed 1 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:43,412 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:43,413 : INFO : built Dictionary(79 unique tokens: ['docker', 'move', 'slave', 'test', '\"))']...) from 2 documents (total 365 corpus positions)\n",
      "2020-12-23 02:43:43,422 : INFO : token count processed\n",
      "2020-12-23 02:43:43,424 : INFO : frequencies processed\n",
      "2020-12-23 02:43:43,553 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:43,554 : INFO : entropies processed\n",
      "2020-12-23 02:43:43,554 : INFO : extropies processed\n",
      "2020-12-23 02:43:43,556 : INFO : token count processed\n",
      "2020-12-23 02:43:43,557 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:43,557 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:43,558 : INFO : vocab #2480\n",
      "2020-12-23 02:43:43,559 : INFO : diff #set()\n",
      "2020-12-23 02:43:43,818 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:43,957 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.1850949504988155, 0.4576460166052368], [0.8036606907844543, 0.19633931], [0.0, 0.0], [2.321928094887362, 5.6831976040360095, 5.721398750142581, 2.2837269487807914, 3.3994706552552185, 0.038201146106571215]]\n",
      "2020-12-23 02:43:43,960 : INFO : Removed 1 and 52 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:43,960 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:43,961 : INFO : built Dictionary(66 unique tokens: ['docker', 'move', 'slave', 'test', '\"\",']...) from 2 documents (total 343 corpus positions)\n",
      "2020-12-23 02:43:43,969 : INFO : token count processed\n",
      "2020-12-23 02:43:43,972 : INFO : frequencies processed\n",
      "2020-12-23 02:43:44,103 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:44,103 : INFO : entropies processed\n",
      "2020-12-23 02:43:44,104 : INFO : extropies processed\n",
      "2020-12-23 02:43:44,106 : INFO : token count processed\n",
      "2020-12-23 02:43:44,107 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:44,108 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:44,108 : INFO : vocab #2480\n",
      "2020-12-23 02:43:44,109 : INFO : diff #set()\n",
      "2020-12-23 02:43:44,378 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:44,506 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.221001256766756, 0.45024738142461007], [0.8628476858139038, 0.13715231], [0.0, 0.0], [2.321928094887362, 5.749308601266266, 5.7889529271913975, 2.2822837689622304, 3.4670248323040354, 0.03964432592513134]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:43:44,508 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:44,509 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:44,510 : INFO : built Dictionary(59 unique tokens: ['docker', 'move', 'slave', 'test', 'assert']...) from 2 documents (total 224 corpus positions)\n",
      "2020-12-23 02:43:44,516 : INFO : token count processed\n",
      "2020-12-23 02:43:44,519 : INFO : frequencies processed\n",
      "2020-12-23 02:43:44,647 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:44,647 : INFO : entropies processed\n",
      "2020-12-23 02:43:44,648 : INFO : extropies processed\n",
      "2020-12-23 02:43:44,649 : INFO : token count processed\n",
      "2020-12-23 02:43:44,650 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:44,651 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:44,652 : INFO : vocab #2480\n",
      "2020-12-23 02:43:44,653 : INFO : diff #set()\n",
      "2020-12-23 02:43:44,909 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:45,037 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.2117588688533145, 0.4521288527797109], [0.8809311091899872, 0.11906889], [0.0, 0.0], [2.321928094887362, 5.015422548793484, 5.085583951939668, 2.251766691741178, 2.763655857052306, 0.07016140314618458]]\n",
      "2020-12-23 02:43:45,039 : INFO : Removed 1 and 81 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:45,040 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:45,041 : INFO : built Dictionary(88 unique tokens: ['docker', 'move', 'slave', 'test', '\"))']...) from 2 documents (total 350 corpus positions)\n",
      "2020-12-23 02:43:45,057 : INFO : token count processed\n",
      "2020-12-23 02:43:45,062 : INFO : frequencies processed\n",
      "2020-12-23 02:43:45,197 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:45,198 : INFO : entropies processed\n",
      "2020-12-23 02:43:45,199 : INFO : extropies processed\n",
      "2020-12-23 02:43:45,200 : INFO : token count processed\n",
      "2020-12-23 02:43:45,201 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:45,202 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:45,203 : INFO : vocab #2480\n",
      "2020-12-23 02:43:45,204 : INFO : diff #set()\n",
      "2020-12-23 02:43:45,473 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:45,602 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.263300292584578, 0.4418326650141723], [0.9301521182060242, 0.06984788], [0.0, 0.0], [2.321928094887362, 6.030001281822029, 6.066645257432945, 2.2852841192764464, 3.7447171625455824, 0.03664397561091537]]\n",
      "2020-12-23 02:43:45,604 : INFO : Removed 1 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:45,605 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:45,605 : INFO : built Dictionary(76 unique tokens: ['docker', 'move', 'slave', 'test', '__main__']...) from 2 documents (total 313 corpus positions)\n",
      "2020-12-23 02:43:45,614 : INFO : token count processed\n",
      "2020-12-23 02:43:45,617 : INFO : frequencies processed\n",
      "2020-12-23 02:43:45,743 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:45,744 : INFO : entropies processed\n",
      "2020-12-23 02:43:45,745 : INFO : extropies processed\n",
      "2020-12-23 02:43:45,746 : INFO : token count processed\n",
      "2020-12-23 02:43:45,747 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:45,748 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:45,749 : INFO : vocab #2480\n",
      "2020-12-23 02:43:45,750 : INFO : diff #set()\n",
      "2020-12-23 02:43:46,008 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:46,136 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.244896211036159, 0.4454548923392935], [0.901011660695076, 0.09898834], [0.0, 0.0], [2.321928094887362, 5.9537092545441395, 5.9947886272828415, 2.2808487221486606, 3.6728605323954793, 0.04107937273870199]]\n",
      "2020-12-23 02:43:46,139 : INFO : Removed 1 and 90 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:46,140 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:46,141 : INFO : built Dictionary(87 unique tokens: ['docker', 'move', 'slave', 'test', '())']...) from 2 documents (total 314 corpus positions)\n",
      "2020-12-23 02:43:46,163 : INFO : token count processed\n",
      "2020-12-23 02:43:46,168 : INFO : frequencies processed\n",
      "2020-12-23 02:43:46,296 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:46,297 : INFO : entropies processed\n",
      "2020-12-23 02:43:46,297 : INFO : extropies processed\n",
      "2020-12-23 02:43:46,298 : INFO : token count processed\n",
      "2020-12-23 02:43:46,299 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:46,300 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:46,300 : INFO : vocab #2480\n",
      "2020-12-23 02:43:46,301 : INFO : diff #set()\n",
      "2020-12-23 02:43:46,568 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:46,697 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.2068148093428275, 0.45314178415260514], [0.7473326027393341, 0.2526674], [1.0, 1.0], [2.321928094887362, 6.184756445474906, 6.20522163229676, 2.301462908065509, 3.8832935374093975, 0.02046518682185372]]\n",
      "2020-12-23 02:43:46,700 : INFO : Removed 1 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:46,700 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:46,701 : INFO : built Dictionary(96 unique tokens: ['docker', 'move', 'slave', 'test', '\"))']...) from 2 documents (total 427 corpus positions)\n",
      "2020-12-23 02:43:46,718 : INFO : token count processed\n",
      "2020-12-23 02:43:46,721 : INFO : frequencies processed\n",
      "2020-12-23 02:43:46,849 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:46,850 : INFO : entropies processed\n",
      "2020-12-23 02:43:46,850 : INFO : extropies processed\n",
      "2020-12-23 02:43:46,851 : INFO : token count processed\n",
      "2020-12-23 02:43:46,852 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:46,853 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:46,854 : INFO : vocab #2480\n",
      "2020-12-23 02:43:46,855 : INFO : diff #set()\n",
      "2020-12-23 02:43:47,113 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:47,241 : INFO : Computed distances or similarities ('274', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.2643563773103432, 0.4416265964228758], [0.9274494126439095, 0.07255059], [0.0, 0.0], [2.321928094887362, 6.212221456585881, 6.244500648989113, 2.2896489024841307, 3.9225725541017504, 0.03227919240323196]]\n",
      "2020-12-23 02:43:47,243 : INFO : Removed 9 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:47,244 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:47,246 : INFO : built Dictionary(148 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 467 corpus positions)\n",
      "2020-12-23 02:43:47,347 : INFO : token count processed\n",
      "2020-12-23 02:43:47,355 : INFO : frequencies processed\n",
      "2020-12-23 02:43:47,486 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:47,487 : INFO : entropies processed\n",
      "2020-12-23 02:43:47,488 : INFO : extropies processed\n",
      "2020-12-23 02:43:47,489 : INFO : token count processed\n",
      "2020-12-23 02:43:47,490 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:47,491 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:47,491 : INFO : vocab #2480\n",
      "2020-12-23 02:43:47,493 : INFO : diff #set()\n",
      "2020-12-23 02:43:47,755 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:47,882 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.1744438866669735, 0.4598877010033208], [0.8759371563792229, 0.12406284], [2.7219280948873625, 1.3198385641318495], [5.327737457054585, 6.301552355933639, 6.693091071401522, 4.936198741586702, 1.3653536143469367, 0.3915387154678829]]\n",
      "2020-12-23 02:43:47,884 : INFO : Removed 9 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:47,885 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:47,887 : INFO : built Dictionary(180 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 703 corpus positions)\n",
      "2020-12-23 02:43:48,015 : INFO : token count processed\n",
      "2020-12-23 02:43:48,021 : INFO : frequencies processed\n",
      "2020-12-23 02:43:48,146 : INFO : scalar_distribution processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:43:48,146 : INFO : entropies processed\n",
      "2020-12-23 02:43:48,147 : INFO : extropies processed\n",
      "2020-12-23 02:43:48,148 : INFO : token count processed\n",
      "2020-12-23 02:43:48,149 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:48,150 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:48,150 : INFO : vocab #2480\n",
      "2020-12-23 02:43:48,151 : INFO : diff #set()\n",
      "2020-12-23 02:43:48,409 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:48,537 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.151425950533673, 0.4648080031534177], [0.8496904671192169, 0.15030953], [3.368042422572715, 1.3419860123862013], [5.327737457054585, 6.739005504021667, 6.97391462076642, 5.092828340309831, 1.646177163711835, 0.23490911674475345]]\n",
      "2020-12-23 02:43:48,540 : INFO : Removed 9 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:48,540 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:48,541 : INFO : built Dictionary(125 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 554 corpus positions)\n",
      "2020-12-23 02:43:48,616 : INFO : token count processed\n",
      "2020-12-23 02:43:48,619 : INFO : frequencies processed\n",
      "2020-12-23 02:43:48,748 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:48,749 : INFO : entropies processed\n",
      "2020-12-23 02:43:48,750 : INFO : extropies processed\n",
      "2020-12-23 02:43:48,751 : INFO : token count processed\n",
      "2020-12-23 02:43:48,752 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:48,753 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:48,754 : INFO : vocab #2480\n",
      "2020-12-23 02:43:48,755 : INFO : diff #set()\n",
      "2020-12-23 02:43:49,014 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:49,142 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.1167000655257, 0.472433490359269], [0.8818822354078293, 0.118117765], [3.27280432733462, 1.3385495915153633], [5.327737457054585, 5.870833373337847, 6.197739099775607, 5.0008317306168255, 0.8700016427210215, 0.3269057264377597]]\n",
      "2020-12-23 02:43:49,144 : INFO : Removed 9 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:49,145 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:49,146 : INFO : built Dictionary(88 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 237 corpus positions)\n",
      "2020-12-23 02:43:49,190 : INFO : token count processed\n",
      "2020-12-23 02:43:49,200 : INFO : frequencies processed\n",
      "2020-12-23 02:43:49,332 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:49,332 : INFO : entropies processed\n",
      "2020-12-23 02:43:49,333 : INFO : extropies processed\n",
      "2020-12-23 02:43:49,334 : INFO : token count processed\n",
      "2020-12-23 02:43:49,335 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:49,336 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:49,337 : INFO : vocab #2480\n",
      "2020-12-23 02:43:49,338 : INFO : diff #set()\n",
      "2020-12-23 02:43:49,597 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:49,724 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.0892372678983033, 0.47864357742668623], [0.8153772205114365, 0.18462278], [2.94770277922009, 1.3393100707180505], [5.327737457054585, 5.371881234145534, 6.0647168164208, 4.6349018747793185, 0.7369793593662148, 0.6928355822752659]]\n",
      "2020-12-23 02:43:49,727 : INFO : Removed 9 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:49,728 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:49,729 : INFO : built Dictionary(80 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 189 corpus positions)\n",
      "2020-12-23 02:43:49,771 : INFO : token count processed\n",
      "2020-12-23 02:43:49,773 : INFO : frequencies processed\n",
      "2020-12-23 02:43:49,905 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:49,906 : INFO : entropies processed\n",
      "2020-12-23 02:43:49,907 : INFO : extropies processed\n",
      "2020-12-23 02:43:49,908 : INFO : token count processed\n",
      "2020-12-23 02:43:49,909 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:49,910 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:49,910 : INFO : vocab #2480\n",
      "2020-12-23 02:43:49,912 : INFO : diff #set()\n",
      "2020-12-23 02:43:50,179 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:50,307 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.1017469250449532, 0.4757946773152108], [0.8284679502248764, 0.17153205], [2.75, 1.3226647836567116], [5.327737457054585, 4.85108279267097, 5.806403930425148, 4.372416319300406, 0.4786664733705628, 0.9553211377541784]]\n",
      "2020-12-23 02:43:50,310 : INFO : Removed 9 and 127 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:50,310 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:50,312 : INFO : built Dictionary(122 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 462 corpus positions)\n",
      "2020-12-23 02:43:50,387 : INFO : token count processed\n",
      "2020-12-23 02:43:50,390 : INFO : frequencies processed\n",
      "2020-12-23 02:43:50,516 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:50,517 : INFO : entropies processed\n",
      "2020-12-23 02:43:50,517 : INFO : extropies processed\n",
      "2020-12-23 02:43:50,518 : INFO : token count processed\n",
      "2020-12-23 02:43:50,519 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:50,520 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:50,521 : INFO : vocab #2480\n",
      "2020-12-23 02:43:50,522 : INFO : diff #set()\n",
      "2020-12-23 02:43:50,790 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:50,919 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.1780739039700312, 0.45912124385553416], [0.9346959963440895, 0.065304], [2.521640636343318, 1.2998438251349493], [5.327737457054585, 6.139571208108155, 6.523005228671293, 4.944303436491447, 1.1952677716167077, 0.38343402056313813]]\n",
      "2020-12-23 02:43:50,921 : INFO : Removed 9 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:50,922 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:50,923 : INFO : built Dictionary(109 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 472 corpus positions)\n",
      "2020-12-23 02:43:50,991 : INFO : token count processed\n",
      "2020-12-23 02:43:50,996 : INFO : frequencies processed\n",
      "2020-12-23 02:43:51,123 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:51,124 : INFO : entropies processed\n",
      "2020-12-23 02:43:51,125 : INFO : extropies processed\n",
      "2020-12-23 02:43:51,126 : INFO : token count processed\n",
      "2020-12-23 02:43:51,127 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:51,129 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:51,129 : INFO : vocab #2480\n",
      "2020-12-23 02:43:51,130 : INFO : diff #set()\n",
      "2020-12-23 02:43:51,390 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:51,516 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.1865496261538344, 0.45734155220570566], [0.9017480909824371, 0.09825191], [2.321928094887362, 1.2877123795494492], [5.327737457054585, 5.609710627339259, 6.101059549927308, 4.836388534466536, 0.7733220928727231, 0.49134892258804896]]\n",
      "2020-12-23 02:43:51,519 : INFO : Removed 9 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:51,520 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:51,521 : INFO : built Dictionary(189 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 1132 corpus positions)\n",
      "2020-12-23 02:43:51,664 : INFO : token count processed\n",
      "2020-12-23 02:43:51,666 : INFO : frequencies processed\n",
      "2020-12-23 02:43:51,794 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:51,794 : INFO : entropies processed\n",
      "2020-12-23 02:43:51,795 : INFO : extropies processed\n",
      "2020-12-23 02:43:51,797 : INFO : token count processed\n",
      "2020-12-23 02:43:51,798 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:51,798 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:51,799 : INFO : vocab #2480\n",
      "2020-12-23 02:43:51,800 : INFO : diff #set()\n",
      "2020-12-23 02:43:52,059 : INFO : alphabet #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:43:52,187 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.1173501461503557, 0.47228844120002655], [0.8582263290882111, 0.14177367], [3.8512274809529554, 1.3769777429328072], [5.327737457054585, 7.2441902753576075, 7.371354069851418, 5.200573662560774, 2.043616612796833, 0.12716379449381066]]\n",
      "2020-12-23 02:43:52,189 : INFO : Removed 9 and 136 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:52,190 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:52,192 : INFO : built Dictionary(156 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 739 corpus positions)\n",
      "2020-12-23 02:43:52,307 : INFO : token count processed\n",
      "2020-12-23 02:43:52,310 : INFO : frequencies processed\n",
      "2020-12-23 02:43:52,436 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:52,436 : INFO : entropies processed\n",
      "2020-12-23 02:43:52,437 : INFO : extropies processed\n",
      "2020-12-23 02:43:52,438 : INFO : token count processed\n",
      "2020-12-23 02:43:52,439 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:52,439 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:52,440 : INFO : vocab #2480\n",
      "2020-12-23 02:43:52,441 : INFO : diff #set()\n",
      "2020-12-23 02:43:52,702 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:52,831 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.169543900438442, 0.4609263724960396], [0.9016943871974945, 0.09830561], [3.5068905956085183, 1.3728719392429896], [5.327737457054585, 6.2567074920449475, 6.52521395729763, 5.059230991801902, 1.1974765002430452, 0.26850646525268296]]\n",
      "2020-12-23 02:43:52,833 : INFO : Removed 9 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:52,834 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:52,836 : INFO : built Dictionary(110 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 249 corpus positions)\n",
      "2020-12-23 02:43:52,902 : INFO : token count processed\n",
      "2020-12-23 02:43:52,907 : INFO : frequencies processed\n",
      "2020-12-23 02:43:53,036 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:53,037 : INFO : entropies processed\n",
      "2020-12-23 02:43:53,038 : INFO : extropies processed\n",
      "2020-12-23 02:43:53,039 : INFO : token count processed\n",
      "2020-12-23 02:43:53,039 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:53,040 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:53,041 : INFO : vocab #2480\n",
      "2020-12-23 02:43:53,041 : INFO : diff #set()\n",
      "2020-12-23 02:43:53,299 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:53,426 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.2148334842170498, 0.45150121087026224], [0.9235582128167152, 0.07644179], [1.5, 1.1225562489182657], [5.327737457054585, 5.7680018917339435, 6.438415410646055, 4.657323938142474, 1.1106779535914697, 0.6704135189121114]]\n",
      "2020-12-23 02:43:53,429 : INFO : Removed 9 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:53,430 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:53,431 : INFO : built Dictionary(196 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 790 corpus positions)\n",
      "2020-12-23 02:43:53,587 : INFO : token count processed\n",
      "2020-12-23 02:43:53,590 : INFO : frequencies processed\n",
      "2020-12-23 02:43:53,716 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:53,717 : INFO : entropies processed\n",
      "2020-12-23 02:43:53,717 : INFO : extropies processed\n",
      "2020-12-23 02:43:53,718 : INFO : token count processed\n",
      "2020-12-23 02:43:53,719 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:53,720 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:53,720 : INFO : vocab #2480\n",
      "2020-12-23 02:43:53,721 : INFO : diff #set()\n",
      "2020-12-23 02:43:53,978 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:54,105 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.1529421135773252, 0.46448067214329397], [0.8825432509183884, 0.11745675], [3.821928094887362, 1.3870194442186057], [5.327737457054585, 6.846479111193757, 7.057171495445778, 5.117045072802564, 1.7294340383911928, 0.2106923842520212]]\n",
      "2020-12-23 02:43:54,108 : INFO : Removed 9 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:54,109 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:54,109 : INFO : built Dictionary(59 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 93 corpus positions)\n",
      "2020-12-23 02:43:54,127 : INFO : token count processed\n",
      "2020-12-23 02:43:54,129 : INFO : frequencies processed\n",
      "2020-12-23 02:43:54,257 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:54,257 : INFO : entropies processed\n",
      "2020-12-23 02:43:54,258 : INFO : extropies processed\n",
      "2020-12-23 02:43:54,259 : INFO : token count processed\n",
      "2020-12-23 02:43:54,260 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:54,261 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:54,262 : INFO : vocab #2480\n",
      "2020-12-23 02:43:54,263 : INFO : diff #set()\n",
      "2020-12-23 02:43:54,520 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:54,648 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2354160997462793, 0.44734400906994465], [0.9562062881886959, 0.04379371], [0.0, 0.0], [5.327737457054585, 4.165013816065912, 5.832731807738705, 3.660019465381792, 0.5049943506841199, 1.6677179916727933]]\n",
      "2020-12-23 02:43:54,650 : INFO : Removed 9 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:54,651 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:54,652 : INFO : built Dictionary(85 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 200 corpus positions)\n",
      "2020-12-23 02:43:54,697 : INFO : token count processed\n",
      "2020-12-23 02:43:54,700 : INFO : frequencies processed\n",
      "2020-12-23 02:43:54,832 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:54,833 : INFO : entropies processed\n",
      "2020-12-23 02:43:54,834 : INFO : extropies processed\n",
      "2020-12-23 02:43:54,835 : INFO : token count processed\n",
      "2020-12-23 02:43:54,836 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:54,837 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:54,838 : INFO : vocab #2480\n",
      "2020-12-23 02:43:54,839 : INFO : diff #set()\n",
      "2020-12-23 02:43:55,106 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:55,233 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.1709435195918663, 0.46062921074427504], [0.8615809679031372, 0.13841903], [2.0, 1.2451124978365313], [5.327737457054585, 5.449968864419248, 6.240205496922308, 4.5375008245515245, 0.912468039867723, 0.7902366325030599]]\n",
      "2020-12-23 02:43:55,236 : INFO : Removed 9 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:55,237 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:55,238 : INFO : built Dictionary(169 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 637 corpus positions)\n",
      "2020-12-23 02:43:55,365 : INFO : token count processed\n",
      "2020-12-23 02:43:55,367 : INFO : frequencies processed\n",
      "2020-12-23 02:43:55,493 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:55,494 : INFO : entropies processed\n",
      "2020-12-23 02:43:55,494 : INFO : extropies processed\n",
      "2020-12-23 02:43:55,496 : INFO : token count processed\n",
      "2020-12-23 02:43:55,496 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:55,497 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:55,498 : INFO : vocab #2480\n",
      "2020-12-23 02:43:55,499 : INFO : diff #set()\n",
      "2020-12-23 02:43:55,757 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:55,885 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.1323074832353033, 0.4689755149584346], [0.8812829852104187, 0.118717015], [3.5766176449086653, 1.3742526132853383], [5.327737457054585, 6.530294129310484, 6.780947706549108, 5.0770838798159605, 1.4532102494945232, 0.2506535772386247]]\n",
      "2020-12-23 02:43:55,887 : INFO : Removed 9 and 142 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:43:55,888 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:55,889 : INFO : built Dictionary(148 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 653 corpus positions)\n",
      "2020-12-23 02:43:55,992 : INFO : token count processed\n",
      "2020-12-23 02:43:55,995 : INFO : frequencies processed\n",
      "2020-12-23 02:43:56,122 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:56,122 : INFO : entropies processed\n",
      "2020-12-23 02:43:56,123 : INFO : extropies processed\n",
      "2020-12-23 02:43:56,125 : INFO : token count processed\n",
      "2020-12-23 02:43:56,126 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:56,127 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:56,127 : INFO : vocab #2480\n",
      "2020-12-23 02:43:56,129 : INFO : diff #set()\n",
      "2020-12-23 02:43:56,400 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:56,528 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.1671044228072087, 0.46144523054621744], [0.8923965990543365, 0.1076034], [2.807354922057604, 1.3343545280186873], [5.327737457054585, 6.470272233491701, 6.760089672890638, 5.037920017655649, 1.4323522158360529, 0.289817439398937]]\n",
      "2020-12-23 02:43:56,531 : INFO : Removed 9 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:56,532 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:56,533 : INFO : built Dictionary(148 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 442 corpus positions)\n",
      "2020-12-23 02:43:56,627 : INFO : token count processed\n",
      "2020-12-23 02:43:56,629 : INFO : frequencies processed\n",
      "2020-12-23 02:43:56,756 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:56,757 : INFO : entropies processed\n",
      "2020-12-23 02:43:56,760 : INFO : extropies processed\n",
      "2020-12-23 02:43:56,761 : INFO : token count processed\n",
      "2020-12-23 02:43:56,762 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:56,763 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:56,764 : INFO : vocab #2480\n",
      "2020-12-23 02:43:56,765 : INFO : diff #set()\n",
      "2020-12-23 02:43:57,032 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:57,162 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.176037840626138, 0.45955083194337176], [0.8894861489534378, 0.11051385], [2.9139770731827523, 1.3356231683419404], [5.327737457054585, 6.550038223589686, 6.897698769960154, 4.980076910684117, 1.5699613129055692, 0.3476605463704683]]\n",
      "2020-12-23 02:43:57,166 : INFO : Removed 9 and 70 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:57,167 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:57,167 : INFO : built Dictionary(104 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 358 corpus positions)\n",
      "2020-12-23 02:43:57,227 : INFO : token count processed\n",
      "2020-12-23 02:43:57,230 : INFO : frequencies processed\n",
      "2020-12-23 02:43:57,360 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:57,361 : INFO : entropies processed\n",
      "2020-12-23 02:43:57,362 : INFO : extropies processed\n",
      "2020-12-23 02:43:57,363 : INFO : token count processed\n",
      "2020-12-23 02:43:57,364 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:57,365 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:57,365 : INFO : vocab #2480\n",
      "2020-12-23 02:43:57,366 : INFO : diff #set()\n",
      "2020-12-23 02:43:57,740 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:57,868 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.138784712595771, 0.4675552401841949], [0.8983700051903725, 0.101629995], [2.75, 1.3226647836567116], [5.327737457054585, 5.860525481261383, 6.3431842709459465, 4.845078667370022, 1.0154468138913613, 0.48265878968456377]]\n",
      "2020-12-23 02:43:57,870 : INFO : Removed 9 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:57,871 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:57,872 : INFO : built Dictionary(77 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 396 corpus positions)\n",
      "2020-12-23 02:43:57,904 : INFO : token count processed\n",
      "2020-12-23 02:43:57,907 : INFO : frequencies processed\n",
      "2020-12-23 02:43:58,033 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:58,033 : INFO : entropies processed\n",
      "2020-12-23 02:43:58,036 : INFO : extropies processed\n",
      "2020-12-23 02:43:58,037 : INFO : token count processed\n",
      "2020-12-23 02:43:58,038 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:58,039 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:58,040 : INFO : vocab #2480\n",
      "2020-12-23 02:43:58,041 : INFO : diff #set()\n",
      "2020-12-23 02:43:58,304 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:58,432 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.175382987522416, 0.4596891700154917], [0.8052908778190613, 0.19470912], [2.0, 1.2451124978365313], [5.327737457054585, 5.945464049777852, 6.343910957766836, 4.929290549065602, 1.0161735007122505, 0.3984469079889834]]\n",
      "2020-12-23 02:43:58,435 : INFO : Removed 9 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:58,436 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:58,437 : INFO : built Dictionary(212 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 943 corpus positions)\n",
      "2020-12-23 02:43:58,610 : INFO : token count processed\n",
      "2020-12-23 02:43:58,615 : INFO : frequencies processed\n",
      "2020-12-23 02:43:58,745 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:58,745 : INFO : entropies processed\n",
      "2020-12-23 02:43:58,746 : INFO : extropies processed\n",
      "2020-12-23 02:43:58,747 : INFO : token count processed\n",
      "2020-12-23 02:43:58,748 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:58,748 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:58,749 : INFO : vocab #2480\n",
      "2020-12-23 02:43:58,750 : INFO : diff #set()\n",
      "2020-12-23 02:43:59,007 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:59,135 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.1628122884821872, 0.4623609757191538], [0.8918228968977928, 0.1081771], [4.1757358691004915, 1.39945878886432], [5.327737457054585, 6.811563897304216, 6.9920505840305, 5.147250770328301, 1.664313126975915, 0.18048668672628398]]\n",
      "2020-12-23 02:43:59,138 : INFO : Removed 9 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:59,139 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:59,141 : INFO : built Dictionary(230 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 1038 corpus positions)\n",
      "2020-12-23 02:43:59,347 : INFO : token count processed\n",
      "2020-12-23 02:43:59,352 : INFO : frequencies processed\n",
      "2020-12-23 02:43:59,477 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:43:59,478 : INFO : entropies processed\n",
      "2020-12-23 02:43:59,479 : INFO : extropies processed\n",
      "2020-12-23 02:43:59,480 : INFO : token count processed\n",
      "2020-12-23 02:43:59,481 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:43:59,482 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:43:59,482 : INFO : vocab #2480\n",
      "2020-12-23 02:43:59,483 : INFO : diff #set()\n",
      "2020-12-23 02:43:59,741 : INFO : alphabet #2480\n",
      "2020-12-23 02:43:59,867 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.0751395384012346, 0.4818953046263277], [0.8094586133956909, 0.19054139], [4.257111133792139, 1.395343071116144], [5.327737457054585, 7.502034948968415, 7.604528567883799, 5.2252438381392015, 2.2767911108292136, 0.10249361891538378]]\n",
      "2020-12-23 02:43:59,870 : INFO : Removed 9 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:43:59,871 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:43:59,872 : INFO : built Dictionary(270 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 1611 corpus positions)\n",
      "2020-12-23 02:44:00,127 : INFO : token count processed\n",
      "2020-12-23 02:44:00,130 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:44:00,257 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:00,258 : INFO : entropies processed\n",
      "2020-12-23 02:44:00,258 : INFO : extropies processed\n",
      "2020-12-23 02:44:00,260 : INFO : token count processed\n",
      "2020-12-23 02:44:00,261 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:00,262 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:00,263 : INFO : vocab #2480\n",
      "2020-12-23 02:44:00,264 : INFO : diff #set()\n",
      "2020-12-23 02:44:00,527 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:00,655 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.0663338424844797, 0.4839489047895759], [0.707764595746994, 0.2922354], [4.481254029767085, 1.4023026269184644], [5.327737457054585, 7.39180093901977, 7.4626324090608325, 5.256905987013523, 2.1348949520062472, 0.07083147004106216]]\n",
      "2020-12-23 02:44:00,658 : INFO : Removed 9 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:00,659 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:00,660 : INFO : built Dictionary(73 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 166 corpus positions)\n",
      "2020-12-23 02:44:00,696 : INFO : token count processed\n",
      "2020-12-23 02:44:00,699 : INFO : frequencies processed\n",
      "2020-12-23 02:44:00,830 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:00,831 : INFO : entropies processed\n",
      "2020-12-23 02:44:00,832 : INFO : extropies processed\n",
      "2020-12-23 02:44:00,833 : INFO : token count processed\n",
      "2020-12-23 02:44:00,834 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:00,835 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:00,836 : INFO : vocab #2480\n",
      "2020-12-23 02:44:00,837 : INFO : diff #set()\n",
      "2020-12-23 02:44:01,101 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:01,228 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.1351428345333614, 0.46835274147762174], [0.8154482692480087, 0.18455173], [2.521640636343318, 1.2998438251349491], [5.327737457054585, 4.927561309677364, 5.910666816066144, 4.344631950665804, 0.5829293590115592, 0.9831055063887808]]\n",
      "2020-12-23 02:44:01,231 : INFO : Removed 9 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:01,231 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:01,232 : INFO : built Dictionary(45 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 67 corpus positions)\n",
      "2020-12-23 02:44:01,240 : INFO : token count processed\n",
      "2020-12-23 02:44:01,242 : INFO : frequencies processed\n",
      "2020-12-23 02:44:01,369 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:01,370 : INFO : entropies processed\n",
      "2020-12-23 02:44:01,371 : INFO : extropies processed\n",
      "2020-12-23 02:44:01,372 : INFO : token count processed\n",
      "2020-12-23 02:44:01,373 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:01,374 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:01,375 : INFO : vocab #2480\n",
      "2020-12-23 02:44:01,376 : INFO : diff #set()\n",
      "2020-12-23 02:44:01,633 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:01,760 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2437371385504605, 0.44568500597446903], [0.9418484307825565, 0.05815157], [0.0, 0.0], [5.327737457054585, 2.5216406363433186, 5.486423526852438, 2.362954566545466, 0.15868606979785316, 2.96478289050912]]\n",
      "2020-12-23 02:44:01,763 : INFO : Removed 9 and 530 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:01,764 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:01,768 : INFO : built Dictionary(350 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 2939 corpus positions)\n",
      "2020-12-23 02:44:02,133 : INFO : token count processed\n",
      "2020-12-23 02:44:02,139 : INFO : frequencies processed\n",
      "2020-12-23 02:44:02,265 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:02,265 : INFO : entropies processed\n",
      "2020-12-23 02:44:02,266 : INFO : extropies processed\n",
      "2020-12-23 02:44:02,268 : INFO : token count processed\n",
      "2020-12-23 02:44:02,269 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:02,269 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:02,270 : INFO : vocab #2480\n",
      "2020-12-23 02:44:02,271 : INFO : diff #set()\n",
      "2020-12-23 02:44:02,532 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:02,661 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.145181609795209, 0.4661609979471461], [0.8820416107773781, 0.11795839], [4.1757358691004915, 1.39945878886432], [5.327737457054585, 7.480007711014331, 7.553016346714228, 5.254728821354687, 2.225278889659643, 0.0730086356998978]]\n",
      "2020-12-23 02:44:02,664 : INFO : Removed 9 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:02,665 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:02,667 : INFO : built Dictionary(229 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 1088 corpus positions)\n",
      "2020-12-23 02:44:02,868 : INFO : token count processed\n",
      "2020-12-23 02:44:02,870 : INFO : frequencies processed\n",
      "2020-12-23 02:44:02,999 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:02,999 : INFO : entropies processed\n",
      "2020-12-23 02:44:03,000 : INFO : extropies processed\n",
      "2020-12-23 02:44:03,001 : INFO : token count processed\n",
      "2020-12-23 02:44:03,002 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:03,003 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:03,003 : INFO : vocab #2480\n",
      "2020-12-23 02:44:03,004 : INFO : diff #set()\n",
      "2020-12-23 02:44:03,267 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:03,394 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.147172198011873, 0.46572883205451715], [0.8829287886619568, 0.11707121], [4.004886164091843, 1.3937786241520365], [5.327737457054585, 7.131331012509435, 7.285902220932224, 5.173166248631796, 1.9581647638776385, 0.15457120842278904]]\n",
      "2020-12-23 02:44:03,397 : INFO : Removed 9 and 166 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:03,398 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:03,399 : INFO : built Dictionary(224 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 907 corpus positions)\n",
      "2020-12-23 02:44:03,582 : INFO : token count processed\n",
      "2020-12-23 02:44:03,585 : INFO : frequencies processed\n",
      "2020-12-23 02:44:03,712 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:03,713 : INFO : entropies processed\n",
      "2020-12-23 02:44:03,713 : INFO : extropies processed\n",
      "2020-12-23 02:44:03,715 : INFO : token count processed\n",
      "2020-12-23 02:44:03,716 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:03,717 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:03,718 : INFO : vocab #2480\n",
      "2020-12-23 02:44:03,719 : INFO : diff #set()\n",
      "2020-12-23 02:44:03,978 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:04,106 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.1772093739288425, 0.4593035525083509], [0.8934332132339478, 0.10656679], [3.378783493486176, 1.3660934553878117], [5.327737457054585, 7.203742744794778, 7.3849869893571, 5.146493212492262, 2.0572495323025146, 0.1812442445623219]]\n",
      "2020-12-23 02:44:04,108 : INFO : Removed 9 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:04,109 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:04,111 : INFO : built Dictionary(85 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 239 corpus positions)\n",
      "2020-12-23 02:44:04,155 : INFO : token count processed\n",
      "2020-12-23 02:44:04,158 : INFO : frequencies processed\n",
      "2020-12-23 02:44:04,295 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:04,295 : INFO : entropies processed\n",
      "2020-12-23 02:44:04,296 : INFO : extropies processed\n",
      "2020-12-23 02:44:04,297 : INFO : token count processed\n",
      "2020-12-23 02:44:04,298 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:04,299 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:04,300 : INFO : vocab #2480\n",
      "2020-12-23 02:44:04,301 : INFO : diff #set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:44:04,560 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:04,688 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.1476760099330194, 0.46561957919862756], [0.8715332299470901, 0.12846677], [3.0, 1.3485155455967714], [5.327737457054585, 5.195502554608948, 5.984394822318195, 4.538845189345339, 0.6566573652636096, 0.7888922677092465]]\n",
      "2020-12-23 02:44:04,691 : INFO : Removed 9 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:04,692 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:04,693 : INFO : built Dictionary(93 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 300 corpus positions)\n",
      "2020-12-23 02:44:04,742 : INFO : token count processed\n",
      "2020-12-23 02:44:04,747 : INFO : frequencies processed\n",
      "2020-12-23 02:44:04,875 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:04,875 : INFO : entropies processed\n",
      "2020-12-23 02:44:04,876 : INFO : extropies processed\n",
      "2020-12-23 02:44:04,877 : INFO : token count processed\n",
      "2020-12-23 02:44:04,878 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:04,878 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:04,879 : INFO : vocab #2480\n",
      "2020-12-23 02:44:04,880 : INFO : diff #set()\n",
      "2020-12-23 02:44:05,142 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:05,270 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.1385613041471583, 0.46760408413860843], [0.8795302659273148, 0.120469734], [2.584962500721156, 1.315172029168969], [5.327737457054585, 5.32027245610305, 6.014796537070208, 4.633213376087427, 0.6870590800156231, 0.6945240809671587]]\n",
      "2020-12-23 02:44:05,272 : INFO : Removed 9 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:05,273 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:05,274 : INFO : built Dictionary(185 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 540 corpus positions)\n",
      "2020-12-23 02:44:05,411 : INFO : token count processed\n",
      "2020-12-23 02:44:05,414 : INFO : frequencies processed\n",
      "2020-12-23 02:44:05,540 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:05,540 : INFO : entropies processed\n",
      "2020-12-23 02:44:05,541 : INFO : extropies processed\n",
      "2020-12-23 02:44:05,542 : INFO : token count processed\n",
      "2020-12-23 02:44:05,543 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:05,543 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:05,544 : INFO : vocab #2480\n",
      "2020-12-23 02:44:05,545 : INFO : diff #set()\n",
      "2020-12-23 02:44:05,802 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:05,928 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.167131711398295, 0.4614394200132725], [0.879248596727848, 0.1207514], [3.7345216647797517, 1.383483006702452], [5.327737457054585, 6.898202761357263, 7.155423810218831, 5.070516408193018, 1.8276863531642453, 0.2572210488615676]]\n",
      "2020-12-23 02:44:05,931 : INFO : Removed 9 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:05,932 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:05,933 : INFO : built Dictionary(159 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 562 corpus positions)\n",
      "2020-12-23 02:44:06,040 : INFO : token count processed\n",
      "2020-12-23 02:44:06,042 : INFO : frequencies processed\n",
      "2020-12-23 02:44:06,168 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:06,169 : INFO : entropies processed\n",
      "2020-12-23 02:44:06,169 : INFO : extropies processed\n",
      "2020-12-23 02:44:06,170 : INFO : token count processed\n",
      "2020-12-23 02:44:06,171 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:06,172 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:06,173 : INFO : vocab #2480\n",
      "2020-12-23 02:44:06,174 : INFO : diff #set()\n",
      "2020-12-23 02:44:06,438 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:06,567 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.1909081857354629, 0.4564317238443798], [0.9240416809916496, 0.07595832], [2.0, 1.2451124978365313], [5.327737457054585, 6.388500481644799, 6.754845710112468, 4.9613922285869165, 1.4271082530578827, 0.3663452284676687]]\n",
      "2020-12-23 02:44:06,569 : INFO : Removed 9 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:06,570 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:06,571 : INFO : built Dictionary(76 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 226 corpus positions)\n",
      "2020-12-23 02:44:06,610 : INFO : token count processed\n",
      "2020-12-23 02:44:06,612 : INFO : frequencies processed\n",
      "2020-12-23 02:44:06,748 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:06,749 : INFO : entropies processed\n",
      "2020-12-23 02:44:06,750 : INFO : extropies processed\n",
      "2020-12-23 02:44:06,751 : INFO : token count processed\n",
      "2020-12-23 02:44:06,752 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:06,753 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:06,754 : INFO : vocab #2480\n",
      "2020-12-23 02:44:06,755 : INFO : diff #set()\n",
      "2020-12-23 02:44:07,014 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:07,142 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.1498166973486517, 0.4651559368914059], [0.8675175309181213, 0.13248247], [3.0, 1.3485155455967714], [5.327737457054585, 4.8191513650620195, 5.735774150342154, 4.411114671774451, 0.40803669328756875, 0.9166227852801345]]\n",
      "2020-12-23 02:44:07,144 : INFO : Removed 9 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:07,145 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:07,146 : INFO : built Dictionary(83 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 281 corpus positions)\n",
      "2020-12-23 02:44:07,183 : INFO : token count processed\n",
      "2020-12-23 02:44:07,185 : INFO : frequencies processed\n",
      "2020-12-23 02:44:07,312 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:07,313 : INFO : entropies processed\n",
      "2020-12-23 02:44:07,314 : INFO : extropies processed\n",
      "2020-12-23 02:44:07,315 : INFO : token count processed\n",
      "2020-12-23 02:44:07,316 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:07,317 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:07,317 : INFO : vocab #2480\n",
      "2020-12-23 02:44:07,318 : INFO : diff #set()\n",
      "2020-12-23 02:44:07,576 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:07,704 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.1425306994171094, 0.4667377696254514], [0.8738647848367691, 0.12613522], [2.584962500721156, 1.315172029168969], [5.327737457054585, 5.062480936779194, 5.840268839191353, 4.549949554642426, 0.5125313821367676, 0.7777879024121592]]\n",
      "2020-12-23 02:44:07,707 : INFO : Removed 9 and 223 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:07,708 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:07,710 : INFO : built Dictionary(270 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 1835 corpus positions)\n",
      "2020-12-23 02:44:07,951 : INFO : token count processed\n",
      "2020-12-23 02:44:07,953 : INFO : frequencies processed\n",
      "2020-12-23 02:44:08,084 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:08,084 : INFO : entropies processed\n",
      "2020-12-23 02:44:08,085 : INFO : extropies processed\n",
      "2020-12-23 02:44:08,087 : INFO : token count processed\n",
      "2020-12-23 02:44:08,088 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:08,089 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:08,090 : INFO : vocab #2480\n",
      "2020-12-23 02:44:08,091 : INFO : diff #set()\n",
      "2020-12-23 02:44:08,348 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:08,476 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.2007117804932417, 0.45439844002465046], [0.907730370759964, 0.09226963], [3.238901256602631, 1.3579502728384498], [5.327737457054585, 7.185085743102134, 7.316326294543317, 5.196496905613404, 1.9885888374887317, 0.13124055144118252]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:44:08,479 : INFO : Removed 9 and 148 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:08,480 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:08,482 : INFO : built Dictionary(184 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 810 corpus positions)\n",
      "2020-12-23 02:44:08,635 : INFO : token count processed\n",
      "2020-12-23 02:44:08,640 : INFO : frequencies processed\n",
      "2020-12-23 02:44:08,771 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:08,771 : INFO : entropies processed\n",
      "2020-12-23 02:44:08,772 : INFO : extropies processed\n",
      "2020-12-23 02:44:08,773 : INFO : token count processed\n",
      "2020-12-23 02:44:08,774 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:08,775 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:08,775 : INFO : vocab #2480\n",
      "2020-12-23 02:44:08,776 : INFO : diff #set()\n",
      "2020-12-23 02:44:09,046 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:09,175 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.1703728312603545, 0.4607503308172593], [0.9015109613537788, 0.09848904], [3.625, 1.3785939957689286], [5.327737457054585, 6.591225336124281, 6.822842341973596, 5.096120451205271, 1.4951048849190105, 0.23161700584931477]]\n",
      "2020-12-23 02:44:09,177 : INFO : Removed 9 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:09,178 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:09,179 : INFO : built Dictionary(70 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 148 corpus positions)\n",
      "2020-12-23 02:44:09,211 : INFO : token count processed\n",
      "2020-12-23 02:44:09,214 : INFO : frequencies processed\n",
      "2020-12-23 02:44:09,341 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:09,342 : INFO : entropies processed\n",
      "2020-12-23 02:44:09,343 : INFO : extropies processed\n",
      "2020-12-23 02:44:09,344 : INFO : token count processed\n",
      "2020-12-23 02:44:09,345 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:09,346 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:09,346 : INFO : vocab #2480\n",
      "2020-12-23 02:44:09,347 : INFO : diff #set()\n",
      "2020-12-23 02:44:09,604 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:09,734 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.1584644590011914, 0.46329231682727845], [0.8661577105522156, 0.13384229], [2.321928094887362, 1.2877123795494492], [5.327737457054585, 4.7032114441396695, 5.85861753687454, 4.172331364319714, 0.5308800798199549, 1.1554060927348706]]\n",
      "2020-12-23 02:44:09,736 : INFO : Removed 9 and 9 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:09,737 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:09,738 : INFO : built Dictionary(147 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 388 corpus positions)\n",
      "2020-12-23 02:44:09,833 : INFO : token count processed\n",
      "2020-12-23 02:44:09,841 : INFO : frequencies processed\n",
      "2020-12-23 02:44:09,969 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:09,970 : INFO : entropies processed\n",
      "2020-12-23 02:44:09,971 : INFO : extropies processed\n",
      "2020-12-23 02:44:09,972 : INFO : token count processed\n",
      "2020-12-23 02:44:09,973 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:09,974 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:09,975 : INFO : vocab #2480\n",
      "2020-12-23 02:44:09,976 : INFO : diff #set()\n",
      "2020-12-23 02:44:10,241 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:10,369 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.2012191801518037, 0.4542936973368715], [0.918783538043499, 0.08121646], [2.321928094887362, 1.2877123795494492], [5.327737457054585, 6.14228447828618, 6.629129856752827, 4.840892078587937, 1.3013923996982415, 0.4868453784666471]]\n",
      "2020-12-23 02:44:10,371 : INFO : Removed 9 and 184 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:10,372 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:10,374 : INFO : built Dictionary(272 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 1178 corpus positions)\n",
      "2020-12-23 02:44:10,638 : INFO : token count processed\n",
      "2020-12-23 02:44:10,641 : INFO : frequencies processed\n",
      "2020-12-23 02:44:10,767 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:10,768 : INFO : entropies processed\n",
      "2020-12-23 02:44:10,769 : INFO : extropies processed\n",
      "2020-12-23 02:44:10,771 : INFO : token count processed\n",
      "2020-12-23 02:44:10,772 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:10,773 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:10,774 : INFO : vocab #2480\n",
      "2020-12-23 02:44:10,775 : INFO : diff #set()\n",
      "2020-12-23 02:44:11,042 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:11,169 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.1397486945108701, 0.4673446010577388], [0.8589858561754227, 0.14101414], [4.168295834054489, 1.3991405171282758], [5.327737457054585, 7.450178124335845, 7.57866243447204, 5.199253146918391, 2.250924977417455, 0.1284843101361952]]\n",
      "2020-12-23 02:44:11,171 : INFO : Removed 9 and 16 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:11,172 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:11,173 : INFO : built Dictionary(85 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 226 corpus positions)\n",
      "2020-12-23 02:44:11,212 : INFO : token count processed\n",
      "2020-12-23 02:44:11,215 : INFO : frequencies processed\n",
      "2020-12-23 02:44:11,342 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:11,343 : INFO : entropies processed\n",
      "2020-12-23 02:44:11,344 : INFO : extropies processed\n",
      "2020-12-23 02:44:11,345 : INFO : token count processed\n",
      "2020-12-23 02:44:11,346 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:11,346 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:11,347 : INFO : vocab #2480\n",
      "2020-12-23 02:44:11,348 : INFO : diff #set()\n",
      "2020-12-23 02:44:11,606 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:11,733 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.1430633566408888, 0.46662176220839047], [0.8739205151796341, 0.12607948], [3.0, 1.3485155455967714], [5.327737457054585, 5.20665021947654, 5.990480420607426, 4.543907255923699, 0.6627429635528408, 0.7838302011308862]]\n",
      "2020-12-23 02:44:11,736 : INFO : Removed 9 and 89 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:11,737 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:11,738 : INFO : built Dictionary(152 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 564 corpus positions)\n",
      "2020-12-23 02:44:11,843 : INFO : token count processed\n",
      "2020-12-23 02:44:11,847 : INFO : frequencies processed\n",
      "2020-12-23 02:44:11,975 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:11,975 : INFO : entropies processed\n",
      "2020-12-23 02:44:11,976 : INFO : extropies processed\n",
      "2020-12-23 02:44:11,977 : INFO : token count processed\n",
      "2020-12-23 02:44:11,978 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:11,979 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:11,979 : INFO : vocab #2480\n",
      "2020-12-23 02:44:11,980 : INFO : diff #set()\n",
      "2020-12-23 02:44:12,247 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:12,376 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.1753740401429793, 0.45969106073099675], [0.900564081966877, 0.09943592], [3.238901256602631, 1.3579502728384498], [5.327737457054585, 6.524718477352, 6.825962017029308, 5.026493917377277, 1.498224559974723, 0.30124353967730855]]\n",
      "2020-12-23 02:44:12,379 : INFO : Removed 9 and 8 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:12,380 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:12,381 : INFO : built Dictionary(90 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 271 corpus positions)\n",
      "2020-12-23 02:44:12,431 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:44:12,433 : INFO : frequencies processed\n",
      "2020-12-23 02:44:12,559 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:12,560 : INFO : entropies processed\n",
      "2020-12-23 02:44:12,561 : INFO : extropies processed\n",
      "2020-12-23 02:44:12,562 : INFO : token count processed\n",
      "2020-12-23 02:44:12,564 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:12,565 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:12,566 : INFO : vocab #2480\n",
      "2020-12-23 02:44:12,567 : INFO : diff #set()\n",
      "2020-12-23 02:44:12,834 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:12,962 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.142565233067528, 0.4667302467931358], [0.8796107545495033, 0.120389245], [3.0, 1.3485155455967714], [5.327737457054585, 5.321859380715434, 6.029431578793783, 4.620165258976236, 0.7016941217391981, 0.7075721980783491]]\n",
      "2020-12-23 02:44:12,964 : INFO : Removed 9 and 119 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:12,965 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:12,966 : INFO : built Dictionary(166 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 889 corpus positions)\n",
      "2020-12-23 02:44:13,087 : INFO : token count processed\n",
      "2020-12-23 02:44:13,096 : INFO : frequencies processed\n",
      "2020-12-23 02:44:13,227 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:13,228 : INFO : entropies processed\n",
      "2020-12-23 02:44:13,228 : INFO : extropies processed\n",
      "2020-12-23 02:44:13,229 : INFO : token count processed\n",
      "2020-12-23 02:44:13,230 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:13,231 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:13,231 : INFO : vocab #2480\n",
      "2020-12-23 02:44:13,232 : INFO : diff #set()\n",
      "2020-12-23 02:44:13,494 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:13,622 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.1673578095532342, 0.4613912827832216], [0.8916635438799858, 0.108336456], [3.506890595608519, 1.3728719392429896], [5.327737457054585, 6.500767808767801, 6.733092702587443, 5.095412563234943, 1.4053552455328582, 0.2323248938196425]]\n",
      "2020-12-23 02:44:13,624 : INFO : Removed 9 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:13,625 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:13,627 : INFO : built Dictionary(68 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 111 corpus positions)\n",
      "2020-12-23 02:44:13,657 : INFO : token count processed\n",
      "2020-12-23 02:44:13,660 : INFO : frequencies processed\n",
      "2020-12-23 02:44:13,787 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:13,788 : INFO : entropies processed\n",
      "2020-12-23 02:44:13,789 : INFO : extropies processed\n",
      "2020-12-23 02:44:13,790 : INFO : token count processed\n",
      "2020-12-23 02:44:13,791 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:13,792 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:13,792 : INFO : vocab #2480\n",
      "2020-12-23 02:44:13,794 : INFO : diff #set()\n",
      "2020-12-23 02:44:14,053 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:14,182 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.1985890753468245, 0.4548371549796095], [0.924955889582634, 0.07504411], [1.0, 1.0], [5.327737457054585, 4.736228843383063, 6.013714117665739, 4.05025218277191, 0.6859766606111535, 1.277485274282676]]\n",
      "2020-12-23 02:44:14,184 : INFO : Removed 9 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:14,185 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:14,186 : INFO : built Dictionary(130 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 452 corpus positions)\n",
      "2020-12-23 02:44:14,268 : INFO : token count processed\n",
      "2020-12-23 02:44:14,273 : INFO : frequencies processed\n",
      "2020-12-23 02:44:14,400 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:14,401 : INFO : entropies processed\n",
      "2020-12-23 02:44:14,402 : INFO : extropies processed\n",
      "2020-12-23 02:44:14,403 : INFO : token count processed\n",
      "2020-12-23 02:44:14,404 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:14,405 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:14,405 : INFO : vocab #2480\n",
      "2020-12-23 02:44:14,406 : INFO : diff #set()\n",
      "2020-12-23 02:44:14,677 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:14,814 : INFO : Computed distances or similarities ('289', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.2322185218595636, 0.44798481430345977], [0.9371666237711906, 0.062833376], [1.5, 1.1225562489182657], [5.327737457054585, 5.788442787590127, 6.300946227221088, 4.8152340174236254, 0.9732087701665026, 0.5125034396309607]]\n",
      "2020-12-23 02:44:14,816 : INFO : Removed 9 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:14,817 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:14,818 : INFO : built Dictionary(81 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 138 corpus positions)\n",
      "2020-12-23 02:44:14,858 : INFO : token count processed\n",
      "2020-12-23 02:44:14,863 : INFO : frequencies processed\n",
      "2020-12-23 02:44:14,996 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:14,997 : INFO : entropies processed\n",
      "2020-12-23 02:44:14,998 : INFO : extropies processed\n",
      "2020-12-23 02:44:15,000 : INFO : token count processed\n",
      "2020-12-23 02:44:15,002 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:15,003 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:15,004 : INFO : vocab #2480\n",
      "2020-12-23 02:44:15,006 : INFO : diff #set()\n",
      "2020-12-23 02:44:15,267 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:15,395 : INFO : Computed distances or similarities ('289', 'sacp-python-common/setup.py')[[1.0869116159445082, 0.47917697729015385], [0.7880552113056183, 0.21194479], [3.2516291673878226, 1.3589504783379556], [5.327737457054585, 5.370004292053436, 6.162465989930092, 4.535275759177931, 0.8347285328755065, 0.7924616978766554]]\n",
      "2020-12-23 02:44:15,397 : INFO : Removed 9 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:15,398 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:15,400 : INFO : built Dictionary(111 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 383 corpus positions)\n",
      "2020-12-23 02:44:15,470 : INFO : token count processed\n",
      "2020-12-23 02:44:15,473 : INFO : frequencies processed\n",
      "2020-12-23 02:44:15,600 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:15,601 : INFO : entropies processed\n",
      "2020-12-23 02:44:15,601 : INFO : extropies processed\n",
      "2020-12-23 02:44:15,602 : INFO : token count processed\n",
      "2020-12-23 02:44:15,603 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:15,604 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:15,605 : INFO : vocab #2480\n",
      "2020-12-23 02:44:15,606 : INFO : diff #set()\n",
      "2020-12-23 02:44:15,875 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:16,004 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.1803485454531488, 0.45864226712072165], [0.9143412783741951, 0.08565872], [2.251629167387823, 1.2667563532600834], [5.327737457054585, 5.695663584743922, 6.230091656115317, 4.793309385683191, 0.9023541990607313, 0.5344280713713943]]\n",
      "2020-12-23 02:44:16,007 : INFO : Removed 9 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:16,008 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:16,009 : INFO : built Dictionary(72 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 158 corpus positions)\n",
      "2020-12-23 02:44:16,044 : INFO : token count processed\n",
      "2020-12-23 02:44:16,046 : INFO : frequencies processed\n",
      "2020-12-23 02:44:16,177 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:16,178 : INFO : entropies processed\n",
      "2020-12-23 02:44:16,179 : INFO : extropies processed\n",
      "2020-12-23 02:44:16,181 : INFO : token count processed\n",
      "2020-12-23 02:44:16,182 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:16,184 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:16,185 : INFO : vocab #2480\n",
      "2020-12-23 02:44:16,186 : INFO : diff #set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:44:16,446 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:16,575 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.1589008870080477, 0.4631986609565335], [0.9102148041129112, 0.089785196], [2.2516291673878226, 1.2667563532600834], [5.327737457054585, 4.9004417692112465, 5.950710468634652, 4.27746875763118, 0.6229730115800667, 1.0502686994234054]]\n",
      "2020-12-23 02:44:16,577 : INFO : Removed 9 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:16,578 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:16,579 : INFO : built Dictionary(70 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 165 corpus positions)\n",
      "2020-12-23 02:44:16,611 : INFO : token count processed\n",
      "2020-12-23 02:44:16,614 : INFO : frequencies processed\n",
      "2020-12-23 02:44:16,750 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:16,751 : INFO : entropies processed\n",
      "2020-12-23 02:44:16,752 : INFO : extropies processed\n",
      "2020-12-23 02:44:16,758 : INFO : token count processed\n",
      "2020-12-23 02:44:16,760 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:16,761 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:16,761 : INFO : vocab #2480\n",
      "2020-12-23 02:44:16,763 : INFO : diff #set()\n",
      "2020-12-23 02:44:17,029 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:17,156 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.1533957999196802, 0.4643828134323003], [0.9108171463012695, 0.08918285], [2.2516291673878226, 1.2667563532600834], [5.327737457054585, 4.778624108914332, 5.86065546572934, 4.245706100239578, 0.5329180086747547, 1.0820313568150075]]\n",
      "2020-12-23 02:44:17,158 : INFO : Removed 9 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:17,159 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:17,161 : INFO : built Dictionary(70 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 196 corpus positions)\n",
      "2020-12-23 02:44:17,194 : INFO : token count processed\n",
      "2020-12-23 02:44:17,199 : INFO : frequencies processed\n",
      "2020-12-23 02:44:17,340 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:17,341 : INFO : entropies processed\n",
      "2020-12-23 02:44:17,342 : INFO : extropies processed\n",
      "2020-12-23 02:44:17,343 : INFO : token count processed\n",
      "2020-12-23 02:44:17,345 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:17,346 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:17,347 : INFO : vocab #2480\n",
      "2020-12-23 02:44:17,349 : INFO : diff #set()\n",
      "2020-12-23 02:44:17,609 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:17,736 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.1377539845808924, 0.46778067411533814], [0.887055866420269, 0.11294413], [2.521640636343318, 1.2998438251349491], [5.327737457054585, 4.773880192225086, 5.753782236445671, 4.347835412834, 0.426044779391086, 0.9799020442205855]]\n",
      "2020-12-23 02:44:17,739 : INFO : Removed 9 and 346 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:17,740 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:17,742 : INFO : built Dictionary(171 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 2021 corpus positions)\n",
      "2020-12-23 02:44:17,868 : INFO : token count processed\n",
      "2020-12-23 02:44:17,874 : INFO : frequencies processed\n",
      "2020-12-23 02:44:18,002 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:18,002 : INFO : entropies processed\n",
      "2020-12-23 02:44:18,003 : INFO : extropies processed\n",
      "2020-12-23 02:44:18,004 : INFO : token count processed\n",
      "2020-12-23 02:44:18,005 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:18,006 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:18,008 : INFO : vocab #2480\n",
      "2020-12-23 02:44:18,009 : INFO : diff #set()\n",
      "2020-12-23 02:44:18,275 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:18,404 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.1089415022736242, 0.4741715210791348], [0.8587143570184708, 0.14128564], [3.7821222241453056, 1.3706381016974654], [5.327737457054585, 6.620773041953877, 6.73165443088273, 5.216856068125732, 1.403916973828145, 0.11088138892885357]]\n",
      "2020-12-23 02:44:18,407 : INFO : Removed 9 and 132 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:18,408 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:18,409 : INFO : built Dictionary(110 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 584 corpus positions)\n",
      "2020-12-23 02:44:18,471 : INFO : token count processed\n",
      "2020-12-23 02:44:18,476 : INFO : frequencies processed\n",
      "2020-12-23 02:44:18,602 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:18,603 : INFO : entropies processed\n",
      "2020-12-23 02:44:18,603 : INFO : extropies processed\n",
      "2020-12-23 02:44:18,604 : INFO : token count processed\n",
      "2020-12-23 02:44:18,605 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:18,606 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:18,606 : INFO : vocab #2480\n",
      "2020-12-23 02:44:18,607 : INFO : diff #set()\n",
      "2020-12-23 02:44:18,873 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:19,003 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.1415074557515659, 0.4669607837760472], [0.8676403760910034, 0.13235962], [2.75, 1.3226647836567116], [5.327737457054585, 5.828370634755606, 6.188361973034027, 4.967746118776164, 0.8606245159794419, 0.3599913382784212]]\n",
      "2020-12-23 02:44:19,005 : INFO : Removed 9 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:19,006 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:19,007 : INFO : built Dictionary(110 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 347 corpus positions)\n",
      "2020-12-23 02:44:19,073 : INFO : token count processed\n",
      "2020-12-23 02:44:19,075 : INFO : frequencies processed\n",
      "2020-12-23 02:44:19,201 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:19,201 : INFO : entropies processed\n",
      "2020-12-23 02:44:19,202 : INFO : extropies processed\n",
      "2020-12-23 02:44:19,203 : INFO : token count processed\n",
      "2020-12-23 02:44:19,204 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:19,205 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:19,206 : INFO : vocab #2480\n",
      "2020-12-23 02:44:19,207 : INFO : diff #set()\n",
      "2020-12-23 02:44:19,466 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:19,594 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.1723991933183087, 0.46032055391832216], [0.9044699370861053, 0.09553006], [3.0, 1.3485155455967714], [5.327737457054585, 5.774409284925443, 6.310058149787911, 4.792088592192118, 0.9823206927333255, 0.535648864862468]]\n",
      "2020-12-23 02:44:19,596 : INFO : Removed 9 and 51 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:19,597 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:19,598 : INFO : built Dictionary(123 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 396 corpus positions)\n",
      "2020-12-23 02:44:19,671 : INFO : token count processed\n",
      "2020-12-23 02:44:19,675 : INFO : frequencies processed\n",
      "2020-12-23 02:44:19,804 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:19,805 : INFO : entropies processed\n",
      "2020-12-23 02:44:19,805 : INFO : extropies processed\n",
      "2020-12-23 02:44:19,806 : INFO : token count processed\n",
      "2020-12-23 02:44:19,807 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:19,808 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:19,809 : INFO : vocab #2480\n",
      "2020-12-23 02:44:19,809 : INFO : diff #set()\n",
      "2020-12-23 02:44:20,077 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:20,205 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.1353866853649184, 0.4682992578597581], [0.8472770005464554, 0.152723], [2.5, 1.2968140217166515], [5.327737457054585, 5.977819040873918, 6.424191831074955, 4.881364666853548, 1.0964543740203698, 0.44637279020103726]]\n",
      "2020-12-23 02:44:20,207 : INFO : Removed 9 and 43 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:44:20,208 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:20,209 : INFO : built Dictionary(103 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 343 corpus positions)\n",
      "2020-12-23 02:44:20,268 : INFO : token count processed\n",
      "2020-12-23 02:44:20,270 : INFO : frequencies processed\n",
      "2020-12-23 02:44:20,405 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:20,406 : INFO : entropies processed\n",
      "2020-12-23 02:44:20,407 : INFO : extropies processed\n",
      "2020-12-23 02:44:20,409 : INFO : token count processed\n",
      "2020-12-23 02:44:20,410 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:20,411 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:20,412 : INFO : vocab #2480\n",
      "2020-12-23 02:44:20,414 : INFO : diff #set()\n",
      "2020-12-23 02:44:20,671 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:20,797 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.1477682793302213, 0.46559957590576234], [0.85158970952034, 0.14841029], [2.725480556997868, 1.3192201298976014], [5.327737457054585, 5.901812829596593, 6.397843551425365, 4.831706735225814, 1.0701060943707796, 0.49603072182877206]]\n",
      "2020-12-23 02:44:20,800 : INFO : Removed 9 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:20,801 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:20,802 : INFO : built Dictionary(105 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 343 corpus positions)\n",
      "2020-12-23 02:44:20,863 : INFO : token count processed\n",
      "2020-12-23 02:44:20,866 : INFO : frequencies processed\n",
      "2020-12-23 02:44:20,994 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:20,995 : INFO : entropies processed\n",
      "2020-12-23 02:44:20,998 : INFO : extropies processed\n",
      "2020-12-23 02:44:20,999 : INFO : token count processed\n",
      "2020-12-23 02:44:21,000 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:21,000 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:21,001 : INFO : vocab #2480\n",
      "2020-12-23 02:44:21,002 : INFO : diff #set()\n",
      "2020-12-23 02:44:21,262 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:21,391 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.1591729008536633, 0.4631403069224489], [0.8838420957326889, 0.116157904], [2.725480556997868, 1.3192201298976014], [5.327737457054585, 5.643202320803383, 6.202314819779066, 4.7686249580789015, 0.8745773627244811, 0.5591124989756837]]\n",
      "2020-12-23 02:44:21,393 : INFO : Removed 9 and 30 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:21,394 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:21,396 : INFO : built Dictionary(117 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 377 corpus positions)\n",
      "2020-12-23 02:44:21,474 : INFO : token count processed\n",
      "2020-12-23 02:44:21,478 : INFO : frequencies processed\n",
      "2020-12-23 02:44:21,605 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:21,605 : INFO : entropies processed\n",
      "2020-12-23 02:44:21,606 : INFO : extropies processed\n",
      "2020-12-23 02:44:21,607 : INFO : token count processed\n",
      "2020-12-23 02:44:21,608 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:21,609 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:21,611 : INFO : vocab #2480\n",
      "2020-12-23 02:44:21,612 : INFO : diff #set()\n",
      "2020-12-23 02:44:21,868 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:21,995 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.116792940745579, 0.47241276213240724], [0.8528915643692017, 0.14710844], [2.9219280948873623, 1.3359016564230495], [5.327737457054585, 5.925214310725336, 6.387958684414189, 4.864993083365732, 1.0602212273596034, 0.462744373688853]]\n",
      "2020-12-23 02:44:21,998 : INFO : Removed 9 and 293 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:21,999 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:22,000 : INFO : built Dictionary(180 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 1772 corpus positions)\n",
      "2020-12-23 02:44:22,136 : INFO : token count processed\n",
      "2020-12-23 02:44:22,141 : INFO : frequencies processed\n",
      "2020-12-23 02:44:22,269 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:22,269 : INFO : entropies processed\n",
      "2020-12-23 02:44:22,270 : INFO : extropies processed\n",
      "2020-12-23 02:44:22,271 : INFO : token count processed\n",
      "2020-12-23 02:44:22,272 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:22,272 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:22,273 : INFO : vocab #2480\n",
      "2020-12-23 02:44:22,274 : INFO : diff #set()\n",
      "2020-12-23 02:44:22,535 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:22,662 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.106394344618137, 0.47474491305724026], [0.8543653041124344, 0.1456347], [3.9523029776623857, 1.3787484370272818], [5.327737457054585, 6.551685682764175, 6.671487300314873, 5.207935839503887, 1.3437498432602881, 0.1198016175506984]]\n",
      "2020-12-23 02:44:22,665 : INFO : Removed 9 and 88 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:22,666 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:22,667 : INFO : built Dictionary(165 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 613 corpus positions)\n",
      "2020-12-23 02:44:22,785 : INFO : token count processed\n",
      "2020-12-23 02:44:22,787 : INFO : frequencies processed\n",
      "2020-12-23 02:44:22,914 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:22,915 : INFO : entropies processed\n",
      "2020-12-23 02:44:22,916 : INFO : extropies processed\n",
      "2020-12-23 02:44:22,917 : INFO : token count processed\n",
      "2020-12-23 02:44:22,919 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:22,920 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:22,921 : INFO : vocab #2480\n",
      "2020-12-23 02:44:22,922 : INFO : diff #set()\n",
      "2020-12-23 02:44:23,180 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:23,309 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.1567352528357597, 0.46366377082451865], [0.8582466393709183, 0.14175336], [2.9735572622751856, 1.3294086744318045], [5.327737457054585, 6.642985062562557, 6.910328821388277, 5.060393698228865, 1.582591364333692, 0.2673437588257199]]\n",
      "2020-12-23 02:44:23,312 : INFO : Removed 9 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:23,312 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:23,313 : INFO : built Dictionary(81 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 208 corpus positions)\n",
      "2020-12-23 02:44:23,349 : INFO : token count processed\n",
      "2020-12-23 02:44:23,351 : INFO : frequencies processed\n",
      "2020-12-23 02:44:23,478 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:23,479 : INFO : entropies processed\n",
      "2020-12-23 02:44:23,480 : INFO : extropies processed\n",
      "2020-12-23 02:44:23,481 : INFO : token count processed\n",
      "2020-12-23 02:44:23,482 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:23,483 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:23,484 : INFO : vocab #2480\n",
      "2020-12-23 02:44:23,485 : INFO : diff #set()\n",
      "2020-12-23 02:44:23,758 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:23,890 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.1357695677677537, 0.4682153051956686], [0.8130283504724503, 0.18697165], [2.5, 1.2968140217166515], [5.327737457054585, 5.2461980344571995, 6.029694676146326, 4.544240815365459, 0.7019572190917405, 0.7834966416891262]]\n",
      "2020-12-23 02:44:23,892 : INFO : Removed 9 and 23 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:23,893 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:23,894 : INFO : built Dictionary(103 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 268 corpus positions)\n",
      "2020-12-23 02:44:23,947 : INFO : token count processed\n",
      "2020-12-23 02:44:23,950 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:44:24,080 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:24,081 : INFO : entropies processed\n",
      "2020-12-23 02:44:24,082 : INFO : extropies processed\n",
      "2020-12-23 02:44:24,084 : INFO : token count processed\n",
      "2020-12-23 02:44:24,085 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:24,086 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:24,087 : INFO : vocab #2480\n",
      "2020-12-23 02:44:24,088 : INFO : diff #set()\n",
      "2020-12-23 02:44:24,358 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:24,487 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/test_auth_utility.py')[[1.1812538877643994, 0.458451904938455], [0.8999584764242172, 0.10004152], [2.5219280948873624, 1.3037754718406493], [5.327737457054585, 5.903090303960449, 6.476738317803849, 4.754089443211186, 1.1490008607492639, 0.5736480138433997]]\n",
      "2020-12-23 02:44:24,490 : INFO : Removed 9 and 442 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:24,491 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:24,494 : INFO : built Dictionary(138 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 1267 corpus positions)\n",
      "2020-12-23 02:44:24,586 : INFO : token count processed\n",
      "2020-12-23 02:44:24,588 : INFO : frequencies processed\n",
      "2020-12-23 02:44:24,716 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:24,717 : INFO : entropies processed\n",
      "2020-12-23 02:44:24,718 : INFO : extropies processed\n",
      "2020-12-23 02:44:24,720 : INFO : token count processed\n",
      "2020-12-23 02:44:24,721 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:24,722 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:24,723 : INFO : vocab #2480\n",
      "2020-12-23 02:44:24,724 : INFO : diff #set()\n",
      "2020-12-23 02:44:24,985 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:25,114 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.2017906693855267, 0.4541757824230761], [0.9446996226906776, 0.055300377], [2.251629167387823, 1.2667563532600834], [5.327737457054585, 6.16659449033757, 6.362010883721611, 5.132321063670544, 1.034273426667026, 0.1954163933840416]]\n",
      "2020-12-23 02:44:25,116 : INFO : Removed 9 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:25,117 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:25,119 : INFO : built Dictionary(97 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 302 corpus positions)\n",
      "2020-12-23 02:44:25,172 : INFO : token count processed\n",
      "2020-12-23 02:44:25,174 : INFO : frequencies processed\n",
      "2020-12-23 02:44:25,304 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:25,304 : INFO : entropies processed\n",
      "2020-12-23 02:44:25,305 : INFO : extropies processed\n",
      "2020-12-23 02:44:25,307 : INFO : token count processed\n",
      "2020-12-23 02:44:25,308 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:25,310 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:25,311 : INFO : vocab #2480\n",
      "2020-12-23 02:44:25,312 : INFO : diff #set()\n",
      "2020-12-23 02:44:25,575 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:25,703 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.15901437929175, 0.46317431212664883], [0.8987506106495857, 0.10124939], [2.5, 1.2968140217166515], [5.327737457054585, 5.906856253399655, 6.432201360218469, 4.802392350235772, 1.104463903163884, 0.525345106818814]]\n",
      "2020-12-23 02:44:25,706 : INFO : Removed 9 and 68 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:25,707 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:25,708 : INFO : built Dictionary(110 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 301 corpus positions)\n",
      "2020-12-23 02:44:25,766 : INFO : token count processed\n",
      "2020-12-23 02:44:25,768 : INFO : frequencies processed\n",
      "2020-12-23 02:44:25,897 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:25,898 : INFO : entropies processed\n",
      "2020-12-23 02:44:25,899 : INFO : extropies processed\n",
      "2020-12-23 02:44:25,900 : INFO : token count processed\n",
      "2020-12-23 02:44:25,902 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:25,903 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:25,904 : INFO : vocab #2480\n",
      "2020-12-23 02:44:25,905 : INFO : diff #set()\n",
      "2020-12-23 02:44:26,175 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:26,305 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.1608205760425476, 0.4627871518288937], [0.881727546453476, 0.11827245], [2.584962500721156, 1.315172029168969], [5.327737457054585, 5.965115449163356, 6.478761622380811, 4.81409128383713, 1.151024165326226, 0.5136461732174551]]\n",
      "2020-12-23 02:44:26,308 : INFO : Removed 9 and 13 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:26,309 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:26,310 : INFO : built Dictionary(113 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 415 corpus positions)\n",
      "2020-12-23 02:44:26,387 : INFO : token count processed\n",
      "2020-12-23 02:44:26,391 : INFO : frequencies processed\n",
      "2020-12-23 02:44:26,522 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:26,522 : INFO : entropies processed\n",
      "2020-12-23 02:44:26,523 : INFO : extropies processed\n",
      "2020-12-23 02:44:26,524 : INFO : token count processed\n",
      "2020-12-23 02:44:26,525 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:26,526 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:26,527 : INFO : vocab #2480\n",
      "2020-12-23 02:44:26,528 : INFO : diff #set()\n",
      "2020-12-23 02:44:26,788 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:26,916 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.105845193128262, 0.47486871459648283], [0.8240353018045425, 0.1759647], [3.238901256602631, 1.3579502728384498], [5.327737457054585, 5.791362404253194, 6.246807139538138, 4.872292721769641, 0.9190696824835527, 0.455444735284944]]\n",
      "2020-12-23 02:44:26,919 : INFO : Removed 9 and 41 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:26,920 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:26,921 : INFO : built Dictionary(103 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 427 corpus positions)\n",
      "2020-12-23 02:44:26,989 : INFO : token count processed\n",
      "2020-12-23 02:44:26,991 : INFO : frequencies processed\n",
      "2020-12-23 02:44:27,117 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:27,118 : INFO : entropies processed\n",
      "2020-12-23 02:44:27,119 : INFO : extropies processed\n",
      "2020-12-23 02:44:27,121 : INFO : token count processed\n",
      "2020-12-23 02:44:27,122 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:27,123 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:27,124 : INFO : vocab #2480\n",
      "2020-12-23 02:44:27,125 : INFO : diff #set()\n",
      "2020-12-23 02:44:27,512 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:27,641 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.1200264741888062, 0.47169222279765816], [0.8456197381019592, 0.15438026], [3.321928094887362, 1.3680278410054498], [5.327737457054585, 5.651670454631116, 6.119460003745019, 4.859947907940683, 0.7917225466904334, 0.4677895491139026]]\n",
      "2020-12-23 02:44:27,643 : INFO : Removed 9 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:27,644 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:27,645 : INFO : built Dictionary(73 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 137 corpus positions)\n",
      "2020-12-23 02:44:27,673 : INFO : token count processed\n",
      "2020-12-23 02:44:27,675 : INFO : frequencies processed\n",
      "2020-12-23 02:44:27,803 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:27,804 : INFO : entropies processed\n",
      "2020-12-23 02:44:27,805 : INFO : extropies processed\n",
      "2020-12-23 02:44:27,806 : INFO : token count processed\n",
      "2020-12-23 02:44:27,807 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:27,808 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:27,808 : INFO : vocab #2480\n",
      "2020-12-23 02:44:27,810 : INFO : diff #set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:44:28,066 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:28,193 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.2070349790338484, 0.45309657957381355], [0.9132286384701729, 0.08677136], [1.584962500721156, 1.1699250014423124], [5.327737457054585, 4.8226207261920235, 6.00010916534109, 4.150249017905519, 0.672371708286505, 1.1774884391490668]]\n",
      "2020-12-23 02:44:28,196 : INFO : Removed 9 and 50 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:28,197 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:28,199 : INFO : built Dictionary(114 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 307 corpus positions)\n",
      "2020-12-23 02:44:28,269 : INFO : token count processed\n",
      "2020-12-23 02:44:28,271 : INFO : frequencies processed\n",
      "2020-12-23 02:44:28,399 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:28,400 : INFO : entropies processed\n",
      "2020-12-23 02:44:28,400 : INFO : extropies processed\n",
      "2020-12-23 02:44:28,402 : INFO : token count processed\n",
      "2020-12-23 02:44:28,403 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:28,404 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:28,405 : INFO : vocab #2480\n",
      "2020-12-23 02:44:28,406 : INFO : diff #set()\n",
      "2020-12-23 02:44:28,673 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:28,801 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.1687746444207532, 0.4610898613060302], [0.9015566408634186, 0.09844336], [2.94770277922009, 1.3393100707180505], [5.327737457054585, 6.24862851613934, 6.702310698949686, 4.87405527424424, 1.374573241895101, 0.4536821828103461]]\n",
      "2020-12-23 02:44:28,803 : INFO : Removed 9 and 32 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:28,804 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:28,806 : INFO : built Dictionary(112 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 423 corpus positions)\n",
      "2020-12-23 02:44:28,883 : INFO : token count processed\n",
      "2020-12-23 02:44:28,888 : INFO : frequencies processed\n",
      "2020-12-23 02:44:29,014 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:29,015 : INFO : entropies processed\n",
      "2020-12-23 02:44:29,015 : INFO : extropies processed\n",
      "2020-12-23 02:44:29,016 : INFO : token count processed\n",
      "2020-12-23 02:44:29,017 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:29,018 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:29,019 : INFO : vocab #2480\n",
      "2020-12-23 02:44:29,020 : INFO : diff #set()\n",
      "2020-12-23 02:44:29,281 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:29,408 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.1100424396173802, 0.4739241169866388], [0.8371433466672897, 0.16285665], [3.238901256602631, 1.3579502728384498], [5.327737457054585, 5.850156917433494, 6.284317732726661, 4.893576641761419, 0.956580275672076, 0.4341608152931675]]\n",
      "2020-12-23 02:44:29,411 : INFO : Removed 9 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:29,412 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:29,413 : INFO : built Dictionary(107 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 421 corpus positions)\n",
      "2020-12-23 02:44:29,476 : INFO : token count processed\n",
      "2020-12-23 02:44:29,478 : INFO : frequencies processed\n",
      "2020-12-23 02:44:29,612 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:29,613 : INFO : entropies processed\n",
      "2020-12-23 02:44:29,614 : INFO : extropies processed\n",
      "2020-12-23 02:44:29,615 : INFO : token count processed\n",
      "2020-12-23 02:44:29,616 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:29,617 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:29,618 : INFO : vocab #2480\n",
      "2020-12-23 02:44:29,620 : INFO : diff #set()\n",
      "2020-12-23 02:44:29,890 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:30,020 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.1198797838991972, 0.47172486269983277], [0.843749389052391, 0.15625061], [3.321928094887362, 1.3680278410054498], [5.327737457054585, 5.6831976040360095, 6.148297053207875, 4.862638007882721, 0.8205595961532897, 0.4650994491718654]]\n",
      "2020-12-23 02:44:30,023 : INFO : Removed 9 and 52 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:30,024 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:30,025 : INFO : built Dictionary(99 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 399 corpus positions)\n",
      "2020-12-23 02:44:30,083 : INFO : token count processed\n",
      "2020-12-23 02:44:30,088 : INFO : frequencies processed\n",
      "2020-12-23 02:44:30,218 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:30,218 : INFO : entropies processed\n",
      "2020-12-23 02:44:30,219 : INFO : extropies processed\n",
      "2020-12-23 02:44:30,220 : INFO : token count processed\n",
      "2020-12-23 02:44:30,220 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:30,221 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:30,221 : INFO : vocab #2480\n",
      "2020-12-23 02:44:30,222 : INFO : diff #set()\n",
      "2020-12-23 02:44:30,478 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:30,606 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.1678441148112266, 0.46128778041177504], [0.9047186598181725, 0.09528134], [2.2516291673878226, 1.2667563532600834], [5.327737457054585, 5.749308601266266, 6.247705981702818, 4.829340076618033, 0.9199685246482332, 0.49839738043655224]]\n",
      "2020-12-23 02:44:30,609 : INFO : Removed 9 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:30,609 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:30,610 : INFO : built Dictionary(91 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 280 corpus positions)\n",
      "2020-12-23 02:44:30,660 : INFO : token count processed\n",
      "2020-12-23 02:44:30,662 : INFO : frequencies processed\n",
      "2020-12-23 02:44:30,795 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:30,796 : INFO : entropies processed\n",
      "2020-12-23 02:44:30,797 : INFO : extropies processed\n",
      "2020-12-23 02:44:30,798 : INFO : token count processed\n",
      "2020-12-23 02:44:30,800 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:30,801 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:30,802 : INFO : vocab #2480\n",
      "2020-12-23 02:44:30,803 : INFO : diff #set()\n",
      "2020-12-23 02:44:31,064 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:31,192 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.1369400869330395, 0.4679588380202139], [0.8385035693645477, 0.16149643], [2.584962500721156, 1.315172029168969], [5.327737457054585, 5.015422548793484, 5.7898395697762925, 4.553320436071777, 0.4621021127217073, 0.774417020982809]]\n",
      "2020-12-23 02:44:31,194 : INFO : Removed 9 and 81 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:31,195 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:31,196 : INFO : built Dictionary(116 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 406 corpus positions)\n",
      "2020-12-23 02:44:31,266 : INFO : token count processed\n",
      "2020-12-23 02:44:31,272 : INFO : frequencies processed\n",
      "2020-12-23 02:44:31,400 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:31,401 : INFO : entropies processed\n",
      "2020-12-23 02:44:31,402 : INFO : extropies processed\n",
      "2020-12-23 02:44:31,403 : INFO : token count processed\n",
      "2020-12-23 02:44:31,404 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:31,405 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:31,405 : INFO : vocab #2480\n",
      "2020-12-23 02:44:31,406 : INFO : diff #set()\n",
      "2020-12-23 02:44:31,663 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:31,791 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.092433375727562, 0.4779124686119524], [0.8386747390031815, 0.16132526], [3.238901256602631, 1.3579502728384498], [5.327737457054585, 6.030001281822029, 6.42505065975678, 4.932688079119835, 1.0973132027021943, 0.39504937793475037]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:44:31,794 : INFO : Removed 9 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:31,795 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:31,796 : INFO : built Dictionary(108 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 369 corpus positions)\n",
      "2020-12-23 02:44:31,853 : INFO : token count processed\n",
      "2020-12-23 02:44:31,856 : INFO : frequencies processed\n",
      "2020-12-23 02:44:31,983 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:31,984 : INFO : entropies processed\n",
      "2020-12-23 02:44:31,984 : INFO : extropies processed\n",
      "2020-12-23 02:44:31,986 : INFO : token count processed\n",
      "2020-12-23 02:44:31,987 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:31,987 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:31,988 : INFO : vocab #2480\n",
      "2020-12-23 02:44:31,989 : INFO : diff #set()\n",
      "2020-12-23 02:44:32,245 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:32,372 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.1759848027712267, 0.45956203312010696], [0.8865209594368935, 0.11347904], [2.521640636343318, 1.2998438251349493], [5.327737457054585, 5.9537092545441395, 6.440807545233402, 4.840639166365324, 1.1130700881788167, 0.48709829068926247]]\n",
      "2020-12-23 02:44:32,375 : INFO : Removed 9 and 90 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:32,375 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:32,377 : INFO : built Dictionary(119 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 370 corpus positions)\n",
      "2020-12-23 02:44:32,450 : INFO : token count processed\n",
      "2020-12-23 02:44:32,452 : INFO : frequencies processed\n",
      "2020-12-23 02:44:32,585 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:32,586 : INFO : entropies processed\n",
      "2020-12-23 02:44:32,587 : INFO : extropies processed\n",
      "2020-12-23 02:44:32,588 : INFO : token count processed\n",
      "2020-12-23 02:44:32,588 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:32,589 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:32,590 : INFO : vocab #2480\n",
      "2020-12-23 02:44:32,591 : INFO : diff #set()\n",
      "2020-12-23 02:44:32,848 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:32,976 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.1361571568495752, 0.4681303511745407], [0.868320420384407, 0.13167958], [2.5216406363433186, 1.2998438251349493], [5.327737457054585, 6.184756445474906, 6.609837117828792, 4.9026567847007, 1.2820996607742066, 0.4250806723538858]]\n",
      "2020-12-23 02:44:32,979 : INFO : Removed 9 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:32,980 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:32,981 : INFO : built Dictionary(123 unique tokens: ['asset', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 483 corpus positions)\n",
      "2020-12-23 02:44:33,067 : INFO : token count processed\n",
      "2020-12-23 02:44:33,071 : INFO : frequencies processed\n",
      "2020-12-23 02:44:33,202 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:33,203 : INFO : entropies processed\n",
      "2020-12-23 02:44:33,204 : INFO : extropies processed\n",
      "2020-12-23 02:44:33,205 : INFO : token count processed\n",
      "2020-12-23 02:44:33,206 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:33,207 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:33,208 : INFO : vocab #2480\n",
      "2020-12-23 02:44:33,209 : INFO : diff #set()\n",
      "2020-12-23 02:44:33,470 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:33,597 : INFO : Computed distances or similarities ('289', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.083626365362853, 0.4799324949153516], [0.8173661977052689, 0.1826338], [3.378783493486176, 1.3660934553878117], [5.327737457054585, 6.212221456585881, 6.555560652275, 4.984398261365465, 1.2278231952204148, 0.3433391956891194]]\n",
      "2020-12-23 02:44:33,600 : INFO : Removed 3 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:33,601 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:33,602 : INFO : built Dictionary(142 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 460 corpus positions)\n",
      "2020-12-23 02:44:33,683 : INFO : token count processed\n",
      "2020-12-23 02:44:33,687 : INFO : frequencies processed\n",
      "2020-12-23 02:44:33,818 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:33,818 : INFO : entropies processed\n",
      "2020-12-23 02:44:33,819 : INFO : extropies processed\n",
      "2020-12-23 02:44:33,820 : INFO : token count processed\n",
      "2020-12-23 02:44:33,821 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:33,822 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:33,823 : INFO : vocab #2480\n",
      "2020-12-23 02:44:33,824 : INFO : diff #set()\n",
      "2020-12-23 02:44:34,081 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:34,209 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.1794382408417279, 0.4588338321593305], [0.8867874071002007, 0.11321259], [2.321928094887362, 1.2877123795494492], [4.938976520573842, 6.301552355933639, 6.631003233640433, 4.609525642867048, 1.692026713066591, 0.32945087770679393]]\n",
      "2020-12-23 02:44:34,211 : INFO : Removed 3 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:34,212 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:34,214 : INFO : built Dictionary(176 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 696 corpus positions)\n",
      "2020-12-23 02:44:34,322 : INFO : token count processed\n",
      "2020-12-23 02:44:34,325 : INFO : frequencies processed\n",
      "2020-12-23 02:44:34,450 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:34,451 : INFO : entropies processed\n",
      "2020-12-23 02:44:34,452 : INFO : extropies processed\n",
      "2020-12-23 02:44:34,453 : INFO : token count processed\n",
      "2020-12-23 02:44:34,454 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:34,455 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:34,456 : INFO : vocab #2480\n",
      "2020-12-23 02:44:34,457 : INFO : diff #set()\n",
      "2020-12-23 02:44:34,715 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:34,843 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.1367771475694857, 0.46799452209486014], [0.8099397867918015, 0.19006021], [2.8895494308620875, 1.3156299779017808], [4.938976520573842, 6.739005504021667, 6.92330272019322, 4.754679304402288, 1.9843261996193782, 0.18429721617155348]]\n",
      "2020-12-23 02:44:34,845 : INFO : Removed 3 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:34,846 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:34,848 : INFO : built Dictionary(121 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 547 corpus positions)\n",
      "2020-12-23 02:44:34,918 : INFO : token count processed\n",
      "2020-12-23 02:44:34,921 : INFO : frequencies processed\n",
      "2020-12-23 02:44:35,049 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:35,050 : INFO : entropies processed\n",
      "2020-12-23 02:44:35,050 : INFO : extropies processed\n",
      "2020-12-23 02:44:35,051 : INFO : token count processed\n",
      "2020-12-23 02:44:35,052 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:35,053 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:35,053 : INFO : vocab #2480\n",
      "2020-12-23 02:44:35,054 : INFO : diff #set()\n",
      "2020-12-23 02:44:35,310 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:35,438 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.1601046065216012, 0.46294054324077005], [0.8691684752702713, 0.13083152], [2.6384600665861555, 1.2788151091892093], [4.938976520573842, 5.870833373337847, 6.145490683867758, 4.6643192100439315, 1.2065141632939156, 0.2746573105299106]]\n",
      "2020-12-23 02:44:35,441 : INFO : Removed 3 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:35,441 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:35,442 : INFO : built Dictionary(86 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 230 corpus positions)\n",
      "2020-12-23 02:44:35,482 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:44:35,487 : INFO : frequencies processed\n",
      "2020-12-23 02:44:35,616 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:35,616 : INFO : entropies processed\n",
      "2020-12-23 02:44:35,617 : INFO : extropies processed\n",
      "2020-12-23 02:44:35,618 : INFO : token count processed\n",
      "2020-12-23 02:44:35,618 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:35,619 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:35,620 : INFO : vocab #2480\n",
      "2020-12-23 02:44:35,620 : INFO : diff #set()\n",
      "2020-12-23 02:44:35,874 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:36,001 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.202633963037337, 0.4540018980825317], [0.921373762190342, 0.07862624], [1.584962500721156, 1.1699250014423124], [4.938976520573842, 5.371881234145534, 6.018635780498505, 4.292221974220871, 1.0796592599246626, 0.6467545463529705]]\n",
      "2020-12-23 02:44:36,004 : INFO : Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:36,005 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:36,006 : INFO : built Dictionary(76 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 182 corpus positions)\n",
      "2020-12-23 02:44:36,036 : INFO : token count processed\n",
      "2020-12-23 02:44:36,038 : INFO : frequencies processed\n",
      "2020-12-23 02:44:36,165 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:36,166 : INFO : entropies processed\n",
      "2020-12-23 02:44:36,166 : INFO : extropies processed\n",
      "2020-12-23 02:44:36,168 : INFO : token count processed\n",
      "2020-12-23 02:44:36,169 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:36,169 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:36,170 : INFO : vocab #2480\n",
      "2020-12-23 02:44:36,171 : INFO : diff #set()\n",
      "2020-12-23 02:44:36,436 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:36,565 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.201896763400546, 0.4541538988665521], [0.9367152824997902, 0.06328472], [2.0, 1.2451124978365313], [4.938976520573842, 4.85108279267097, 5.713077463625628, 4.076981849619183, 0.7741009430517858, 0.8619946709546582]]\n",
      "2020-12-23 02:44:36,567 : INFO : Removed 3 and 127 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:36,568 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:36,569 : INFO : built Dictionary(116 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 455 corpus positions)\n",
      "2020-12-23 02:44:36,625 : INFO : token count processed\n",
      "2020-12-23 02:44:36,628 : INFO : frequencies processed\n",
      "2020-12-23 02:44:36,757 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:36,757 : INFO : entropies processed\n",
      "2020-12-23 02:44:36,758 : INFO : extropies processed\n",
      "2020-12-23 02:44:36,760 : INFO : token count processed\n",
      "2020-12-23 02:44:36,762 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:36,763 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:36,763 : INFO : vocab #2480\n",
      "2020-12-23 02:44:36,764 : INFO : diff #set()\n",
      "2020-12-23 02:44:37,033 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:37,165 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.178203440375544, 0.4590939402003653], [0.9151960834860802, 0.08480392], [2.0, 1.2451124978365313], [4.938976520573842, 6.139571208108155, 6.450988520736745, 4.627559207945252, 1.512012000162903, 0.3114173126285902]]\n",
      "2020-12-23 02:44:37,167 : INFO : Removed 3 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:37,168 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:37,169 : INFO : built Dictionary(102 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 465 corpus positions)\n",
      "2020-12-23 02:44:37,220 : INFO : token count processed\n",
      "2020-12-23 02:44:37,225 : INFO : frequencies processed\n",
      "2020-12-23 02:44:37,356 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:37,357 : INFO : entropies processed\n",
      "2020-12-23 02:44:37,358 : INFO : extropies processed\n",
      "2020-12-23 02:44:37,359 : INFO : token count processed\n",
      "2020-12-23 02:44:37,360 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:37,361 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:37,362 : INFO : vocab #2480\n",
      "2020-12-23 02:44:37,363 : INFO : diff #set()\n",
      "2020-12-23 02:44:37,626 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:37,754 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.1915474359836038, 0.45629858773793003], [0.8780824765563011, 0.12191752], [1.7924812503605778, 1.1575860145844845], [4.938976520573842, 5.609710627339259, 6.004880259131463, 4.5438068887816385, 1.0659037385576209, 0.39516963179220355]]\n",
      "2020-12-23 02:44:37,757 : INFO : Removed 3 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:37,758 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:37,761 : INFO : built Dictionary(188 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 1125 corpus positions)\n",
      "2020-12-23 02:44:37,883 : INFO : token count processed\n",
      "2020-12-23 02:44:37,888 : INFO : frequencies processed\n",
      "2020-12-23 02:44:38,021 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:38,022 : INFO : entropies processed\n",
      "2020-12-23 02:44:38,022 : INFO : extropies processed\n",
      "2020-12-23 02:44:38,024 : INFO : token count processed\n",
      "2020-12-23 02:44:38,025 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:38,025 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:38,026 : INFO : vocab #2480\n",
      "2020-12-23 02:44:38,027 : INFO : diff #set()\n",
      "2020-12-23 02:44:38,286 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:38,415 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.1479736528909283, 0.46555505867314234], [0.8509984463453293, 0.14900155], [3.189898095464288, 1.351714529210319], [4.938976520573842, 7.2441902753576075, 7.351900939634435, 4.831265856297014, 2.4129244190605927, 0.1077106642768273]]\n",
      "2020-12-23 02:44:38,418 : INFO : Removed 3 and 136 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:38,419 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:38,421 : INFO : built Dictionary(150 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 732 corpus positions)\n",
      "2020-12-23 02:44:38,512 : INFO : token count processed\n",
      "2020-12-23 02:44:38,514 : INFO : frequencies processed\n",
      "2020-12-23 02:44:38,642 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:38,643 : INFO : entropies processed\n",
      "2020-12-23 02:44:38,646 : INFO : extropies processed\n",
      "2020-12-23 02:44:38,647 : INFO : token count processed\n",
      "2020-12-23 02:44:38,647 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:38,648 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:38,648 : INFO : vocab #2480\n",
      "2020-12-23 02:44:38,649 : INFO : diff #set()\n",
      "2020-12-23 02:44:38,921 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:39,048 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.1799651033891982, 0.45872293939260633], [0.8791141360998154, 0.120885864], [3.1820058147602133, 1.3495612861500508], [4.938976520573842, 6.2567074920449475, 6.470258902510464, 4.7254251101083256, 1.531282381936622, 0.2135514104655165]]\n",
      "2020-12-23 02:44:39,050 : INFO : Removed 3 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:39,051 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:39,052 : INFO : built Dictionary(103 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 242 corpus positions)\n",
      "2020-12-23 02:44:39,106 : INFO : token count processed\n",
      "2020-12-23 02:44:39,108 : INFO : frequencies processed\n",
      "2020-12-23 02:44:39,236 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:39,237 : INFO : entropies processed\n",
      "2020-12-23 02:44:39,237 : INFO : extropies processed\n",
      "2020-12-23 02:44:39,239 : INFO : token count processed\n",
      "2020-12-23 02:44:39,240 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:39,241 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:39,242 : INFO : vocab #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:44:39,243 : INFO : diff #set()\n",
      "2020-12-23 02:44:39,507 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:39,635 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.1903877687785185, 0.4565401680258904], [0.8754660561680794, 0.124533944], [1.0, 1.0], [4.938976520573842, 5.7680018917339435, 6.319358032516543, 4.387620379791243, 1.3803815119427005, 0.5513561407825991]]\n",
      "2020-12-23 02:44:39,637 : INFO : Removed 3 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:39,638 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:39,639 : INFO : built Dictionary(192 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 783 corpus positions)\n",
      "2020-12-23 02:44:39,766 : INFO : token count processed\n",
      "2020-12-23 02:44:39,769 : INFO : frequencies processed\n",
      "2020-12-23 02:44:39,895 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:39,895 : INFO : entropies processed\n",
      "2020-12-23 02:44:39,896 : INFO : extropies processed\n",
      "2020-12-23 02:44:39,897 : INFO : token count processed\n",
      "2020-12-23 02:44:39,898 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:39,899 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:39,899 : INFO : vocab #2480\n",
      "2020-12-23 02:44:39,900 : INFO : diff #set()\n",
      "2020-12-23 02:44:40,160 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:40,288 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.158089154451674, 0.4633728861188218], [0.8419721722602844, 0.15802783], [3.323231428797621, 1.358590359366923], [4.938976520573842, 6.846479111193757, 7.018871551579982, 4.7665840801876165, 2.0798950310061404, 0.1723924403862256]]\n",
      "2020-12-23 02:44:40,291 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:40,292 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:40,293 : INFO : built Dictionary(52 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 86 corpus positions)\n",
      "2020-12-23 02:44:40,314 : INFO : token count processed\n",
      "2020-12-23 02:44:40,317 : INFO : frequencies processed\n",
      "2020-12-23 02:44:40,450 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:40,451 : INFO : entropies processed\n",
      "2020-12-23 02:44:40,451 : INFO : extropies processed\n",
      "2020-12-23 02:44:40,453 : INFO : token count processed\n",
      "2020-12-23 02:44:40,454 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:40,455 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:40,455 : INFO : vocab #2480\n",
      "2020-12-23 02:44:40,456 : INFO : diff #set()\n",
      "2020-12-23 02:44:40,715 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:40,842 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2256343712383113, 0.44931009914427866], [0.9274336472153664, 0.07256635], [0.0, 0.0], [4.938976520573842, 4.165013816065912, 5.580803762519831, 3.523186574119923, 0.6418272419459887, 1.415789946453919]]\n",
      "2020-12-23 02:44:40,845 : INFO : Removed 3 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:40,846 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:40,847 : INFO : built Dictionary(77 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 193 corpus positions)\n",
      "2020-12-23 02:44:40,878 : INFO : token count processed\n",
      "2020-12-23 02:44:40,880 : INFO : frequencies processed\n",
      "2020-12-23 02:44:41,007 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:41,008 : INFO : entropies processed\n",
      "2020-12-23 02:44:41,009 : INFO : extropies processed\n",
      "2020-12-23 02:44:41,011 : INFO : token count processed\n",
      "2020-12-23 02:44:41,013 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:41,014 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:41,014 : INFO : vocab #2480\n",
      "2020-12-23 02:44:41,016 : INFO : diff #set()\n",
      "2020-12-23 02:44:41,282 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:41,411 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.1363176152431438, 0.46809518999644895], [0.824657067656517, 0.17534293], [1.7924812503605778, 1.1575860145844845], [4.938976520573842, 5.449968864419248, 6.075157278239496, 4.313788106753593, 1.1361807576656542, 0.6251884138202479]]\n",
      "2020-12-23 02:44:41,413 : INFO : Removed 3 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:41,414 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:41,417 : INFO : built Dictionary(166 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 630 corpus positions)\n",
      "2020-12-23 02:44:41,527 : INFO : token count processed\n",
      "2020-12-23 02:44:41,532 : INFO : frequencies processed\n",
      "2020-12-23 02:44:41,659 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:41,660 : INFO : entropies processed\n",
      "2020-12-23 02:44:41,660 : INFO : extropies processed\n",
      "2020-12-23 02:44:41,662 : INFO : token count processed\n",
      "2020-12-23 02:44:41,662 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:41,663 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:41,664 : INFO : vocab #2480\n",
      "2020-12-23 02:44:41,664 : INFO : diff #set()\n",
      "2020-12-23 02:44:41,923 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:42,051 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.141962088684872, 0.46686167102704546], [0.8521642833948135, 0.14783572], [2.8464393446710154, 1.3178207096846455], [4.938976520573842, 6.530294129310484, 6.741843834008304, 4.727426815876022, 1.8028673134344615, 0.21154970469781986]]\n",
      "2020-12-23 02:44:42,053 : INFO : Removed 3 and 142 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:42,054 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:42,055 : INFO : built Dictionary(141 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 646 corpus positions)\n",
      "2020-12-23 02:44:42,131 : INFO : token count processed\n",
      "2020-12-23 02:44:42,134 : INFO : frequencies processed\n",
      "2020-12-23 02:44:42,263 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:42,263 : INFO : entropies processed\n",
      "2020-12-23 02:44:42,264 : INFO : extropies processed\n",
      "2020-12-23 02:44:42,265 : INFO : token count processed\n",
      "2020-12-23 02:44:42,266 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:42,267 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:42,268 : INFO : vocab #2480\n",
      "2020-12-23 02:44:42,269 : INFO : diff #set()\n",
      "2020-12-23 02:44:42,527 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:42,655 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.180496312229118, 0.4586111860825399], [0.8973865956068039, 0.102613404], [2.521640636343318, 1.2998438251349493], [4.938976520573842, 6.470272233491701, 6.705111511525733, 4.704137242539811, 1.7661349909518913, 0.23483927803403226]]\n",
      "2020-12-23 02:44:42,657 : INFO : Removed 3 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:42,658 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:42,659 : INFO : built Dictionary(141 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 435 corpus positions)\n",
      "2020-12-23 02:44:42,738 : INFO : token count processed\n",
      "2020-12-23 02:44:42,740 : INFO : frequencies processed\n",
      "2020-12-23 02:44:42,870 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:42,871 : INFO : entropies processed\n",
      "2020-12-23 02:44:42,872 : INFO : extropies processed\n",
      "2020-12-23 02:44:42,873 : INFO : token count processed\n",
      "2020-12-23 02:44:42,875 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:42,876 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:42,876 : INFO : vocab #2480\n",
      "2020-12-23 02:44:42,878 : INFO : diff #set()\n",
      "2020-12-23 02:44:43,146 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:43,272 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.1747231153898856, 0.45982865263319717], [0.8681932538747787, 0.13180675], [2.75, 1.3226647836567116], [4.938976520573842, 6.550038223589686, 6.834301524566804, 4.654713219596724, 1.895325003992962, 0.28426330097711805]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:44:43,274 : INFO : Removed 3 and 70 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:43,275 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:43,276 : INFO : built Dictionary(97 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 351 corpus positions)\n",
      "2020-12-23 02:44:43,324 : INFO : token count processed\n",
      "2020-12-23 02:44:43,329 : INFO : frequencies processed\n",
      "2020-12-23 02:44:43,458 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:43,459 : INFO : entropies processed\n",
      "2020-12-23 02:44:43,460 : INFO : extropies processed\n",
      "2020-12-23 02:44:43,461 : INFO : token count processed\n",
      "2020-12-23 02:44:43,462 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:43,463 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:43,464 : INFO : vocab #2480\n",
      "2020-12-23 02:44:43,465 : INFO : diff #set()\n",
      "2020-12-23 02:44:43,726 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:43,853 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.143569033012805, 0.46651168429807516], [0.8710135668516159, 0.12898643], [2.4056390622295662, 1.2666171566933806], [4.938976520573842, 5.860525481261383, 6.247660664974676, 4.55184133686055, 1.308684144400834, 0.38713518371329325]]\n",
      "2020-12-23 02:44:43,856 : INFO : Removed 3 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:43,857 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:43,858 : INFO : built Dictionary(73 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 389 corpus positions)\n",
      "2020-12-23 02:44:43,888 : INFO : token count processed\n",
      "2020-12-23 02:44:43,892 : INFO : frequencies processed\n",
      "2020-12-23 02:44:44,020 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:44,021 : INFO : entropies processed\n",
      "2020-12-23 02:44:44,022 : INFO : extropies processed\n",
      "2020-12-23 02:44:44,023 : INFO : token count processed\n",
      "2020-12-23 02:44:44,024 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:44,025 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:44,026 : INFO : vocab #2480\n",
      "2020-12-23 02:44:44,027 : INFO : diff #set()\n",
      "2020-12-23 02:44:44,286 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:44,414 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.2497823508651111, 0.444487441025337], [0.9126525595784187, 0.08734744], [0.0, 0.0], [4.938976520573842, 5.945464049777852, 6.300623044638429, 4.583817525713266, 1.3616465240645867, 0.35515899486057645]]\n",
      "2020-12-23 02:44:44,417 : INFO : Removed 3 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:44,418 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:44,419 : INFO : built Dictionary(210 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 936 corpus positions)\n",
      "2020-12-23 02:44:44,567 : INFO : token count processed\n",
      "2020-12-23 02:44:44,572 : INFO : frequencies processed\n",
      "2020-12-23 02:44:44,703 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:44,704 : INFO : entropies processed\n",
      "2020-12-23 02:44:44,704 : INFO : extropies processed\n",
      "2020-12-23 02:44:44,706 : INFO : token count processed\n",
      "2020-12-23 02:44:44,707 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:44,708 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:44,708 : INFO : vocab #2480\n",
      "2020-12-23 02:44:44,709 : INFO : diff #set()\n",
      "2020-12-23 02:44:44,980 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:45,108 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.1755381067100932, 0.45965639347601533], [0.8734364062547684, 0.1265636], [3.521640636343319, 1.3740281872300928], [4.938976520573842, 6.811563897304216, 6.96645608248109, 4.784084335396968, 2.0274795619072483, 0.1548921851768741]]\n",
      "2020-12-23 02:44:45,111 : INFO : Removed 3 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:45,112 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:45,114 : INFO : built Dictionary(228 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 1031 corpus positions)\n",
      "2020-12-23 02:44:45,275 : INFO : token count processed\n",
      "2020-12-23 02:44:45,277 : INFO : frequencies processed\n",
      "2020-12-23 02:44:45,404 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:45,405 : INFO : entropies processed\n",
      "2020-12-23 02:44:45,408 : INFO : extropies processed\n",
      "2020-12-23 02:44:45,409 : INFO : token count processed\n",
      "2020-12-23 02:44:45,410 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:45,410 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:45,411 : INFO : vocab #2480\n",
      "2020-12-23 02:44:45,412 : INFO : diff #set()\n",
      "2020-12-23 02:44:45,669 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:45,796 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.1391696876738395, 0.467471096735394], [0.8361742794513702, 0.16382572], [3.794653473544342, 1.3826918618338855], [4.938976520573842, 7.502034948968415, 7.600918668348733, 4.840092801193524, 2.661942147774891, 0.09888371938031781]]\n",
      "2020-12-23 02:44:45,799 : INFO : Removed 3 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:45,800 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:45,803 : INFO : built Dictionary(268 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 1604 corpus positions)\n",
      "2020-12-23 02:44:46,023 : INFO : token count processed\n",
      "2020-12-23 02:44:46,031 : INFO : frequencies processed\n",
      "2020-12-23 02:44:46,160 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:46,161 : INFO : entropies processed\n",
      "2020-12-23 02:44:46,161 : INFO : extropies processed\n",
      "2020-12-23 02:44:46,163 : INFO : token count processed\n",
      "2020-12-23 02:44:46,163 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:46,164 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:46,165 : INFO : vocab #2480\n",
      "2020-12-23 02:44:46,166 : INFO : diff #set()\n",
      "2020-12-23 02:44:46,428 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:46,556 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.0690803479086899, 0.483306509102338], [0.6899689733982086, 0.31003103], [3.9900172174899025, 1.3872964783133246], [4.938976520573842, 7.39180093901977, 7.445709984162994, 4.885067475430619, 2.5067334635891516, 0.053909045143223366]]\n",
      "2020-12-23 02:44:46,559 : INFO : Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:46,560 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:46,561 : INFO : built Dictionary(67 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 159 corpus positions)\n",
      "2020-12-23 02:44:46,587 : INFO : token count processed\n",
      "2020-12-23 02:44:46,592 : INFO : frequencies processed\n",
      "2020-12-23 02:44:46,727 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:46,728 : INFO : entropies processed\n",
      "2020-12-23 02:44:46,729 : INFO : extropies processed\n",
      "2020-12-23 02:44:46,730 : INFO : token count processed\n",
      "2020-12-23 02:44:46,731 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:46,732 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:46,732 : INFO : vocab #2480\n",
      "2020-12-23 02:44:46,733 : INFO : diff #set()\n",
      "2020-12-23 02:44:46,991 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:47,119 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.1056098309313787, 0.47492179477413815], [0.7802820652723312, 0.21971793], [1.8423709931771084, 1.1893232685884285], [4.938976520573842, 4.927561309677364, 5.728639329433205, 4.137898500818, 0.7896628088593634, 0.8010780197558418]]\n",
      "2020-12-23 02:44:47,121 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:47,122 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:47,123 : INFO : built Dictionary(39 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 60 corpus positions)\n",
      "2020-12-23 02:44:47,129 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:44:47,131 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:44:47,132 : INFO : frequencies processed\n",
      "2020-12-23 02:44:47,133 : INFO : token count processed\n",
      "2020-12-23 02:44:47,134 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:47,135 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:47,135 : INFO : vocab #2480\n",
      "2020-12-23 02:44:47,136 : INFO : diff #set()\n",
      "2020-12-23 02:44:47,393 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:47,521 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2537547467720171, 0.44370400170305524], [0.9619524776935577, 0.038047522], [nan, nan], [4.938976520573842, 2.5216406363433186, 5.1736419793238735, 2.2869751775932876, 0.23466545875003142, 2.652001342980555]]\n",
      "2020-12-23 02:44:47,525 : INFO : Removed 3 and 530 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:47,526 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:47,530 : INFO : built Dictionary(348 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 2932 corpus positions)\n",
      "2020-12-23 02:44:47,850 : INFO : token count processed\n",
      "2020-12-23 02:44:47,853 : INFO : frequencies processed\n",
      "2020-12-23 02:44:48,091 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:48,091 : INFO : entropies processed\n",
      "2020-12-23 02:44:48,092 : INFO : extropies processed\n",
      "2020-12-23 02:44:48,094 : INFO : token count processed\n",
      "2020-12-23 02:44:48,095 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:48,096 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:48,096 : INFO : vocab #2480\n",
      "2020-12-23 02:44:48,097 : INFO : diff #set()\n",
      "2020-12-23 02:44:48,356 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:48,485 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.1748116734184624, 0.4598099284744766], [0.8591766506433487, 0.14082335], [3.6402239289418516, 1.3797477693995936], [4.938976520573842, 7.480007711014331, 7.542024824024335, 4.876959407563838, 2.6030483034504925, 0.06201711301000401]]\n",
      "2020-12-23 02:44:48,488 : INFO : Removed 3 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:48,488 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:48,490 : INFO : built Dictionary(227 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 1081 corpus positions)\n",
      "2020-12-23 02:44:48,659 : INFO : token count processed\n",
      "2020-12-23 02:44:48,665 : INFO : frequencies processed\n",
      "2020-12-23 02:44:48,792 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:48,792 : INFO : entropies processed\n",
      "2020-12-23 02:44:48,793 : INFO : extropies processed\n",
      "2020-12-23 02:44:48,794 : INFO : token count processed\n",
      "2020-12-23 02:44:48,795 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:48,795 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:48,796 : INFO : vocab #2480\n",
      "2020-12-23 02:44:48,797 : INFO : diff #set()\n",
      "2020-12-23 02:44:49,061 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:49,189 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.1742230234091542, 0.459934417598068], [0.8752855062484741, 0.124714494], [3.323231428797621, 1.358590359366923], [4.938976520573842, 7.131331012509435, 7.263588005655482, 4.806719527427795, 2.3246114850816397, 0.1322569931460471]]\n",
      "2020-12-23 02:44:49,192 : INFO : Removed 3 and 166 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:49,193 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:49,195 : INFO : built Dictionary(218 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 900 corpus positions)\n",
      "2020-12-23 02:44:49,354 : INFO : token count processed\n",
      "2020-12-23 02:44:49,361 : INFO : frequencies processed\n",
      "2020-12-23 02:44:49,491 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:49,491 : INFO : entropies processed\n",
      "2020-12-23 02:44:49,492 : INFO : extropies processed\n",
      "2020-12-23 02:44:49,493 : INFO : token count processed\n",
      "2020-12-23 02:44:49,494 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:49,495 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:49,495 : INFO : vocab #2480\n",
      "2020-12-23 02:44:49,496 : INFO : diff #set()\n",
      "2020-12-23 02:44:49,761 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:49,888 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.1748868293383228, 0.45979403917041295], [0.872345045208931, 0.12765495], [3.0220552088742, 1.3359632893587228], [4.938976520573842, 7.203742744794778, 7.349352143477217, 4.793367121891402, 2.4103756229033753, 0.14560939868243938]]\n",
      "2020-12-23 02:44:49,890 : INFO : Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:49,891 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:49,893 : INFO : built Dictionary(80 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 232 corpus positions)\n",
      "2020-12-23 02:44:49,930 : INFO : token count processed\n",
      "2020-12-23 02:44:49,933 : INFO : frequencies processed\n",
      "2020-12-23 02:44:50,070 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:50,070 : INFO : entropies processed\n",
      "2020-12-23 02:44:50,071 : INFO : extropies processed\n",
      "2020-12-23 02:44:50,073 : INFO : token count processed\n",
      "2020-12-23 02:44:50,074 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:50,075 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:50,076 : INFO : vocab #2480\n",
      "2020-12-23 02:44:50,078 : INFO : diff #set()\n",
      "2020-12-23 02:44:50,340 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:50,467 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.1661137979744816, 0.46165626244341057], [0.8602861911058426, 0.13971381], [2.1556390622295662, 1.2407663947533205], [4.938976520573842, 5.195502554608948, 5.853714909679237, 4.2807641655035535, 0.9147383891053948, 0.6582123550702885]]\n",
      "2020-12-23 02:44:50,470 : INFO : Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:50,471 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:50,472 : INFO : built Dictionary(87 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 293 corpus positions)\n",
      "2020-12-23 02:44:50,510 : INFO : token count processed\n",
      "2020-12-23 02:44:50,512 : INFO : frequencies processed\n",
      "2020-12-23 02:44:50,639 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:50,640 : INFO : entropies processed\n",
      "2020-12-23 02:44:50,641 : INFO : extropies processed\n",
      "2020-12-23 02:44:50,642 : INFO : token count processed\n",
      "2020-12-23 02:44:50,643 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:50,644 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:50,645 : INFO : vocab #2480\n",
      "2020-12-23 02:44:50,646 : INFO : diff #set()\n",
      "2020-12-23 02:44:50,903 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:51,030 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.1535317685682094, 0.4643534934545484], [0.8647149950265884, 0.135285], [1.8423709931771084, 1.1893232685884285], [4.938976520573842, 5.32027245610305, 5.882718376580786, 4.376530600096106, 0.9437418560069437, 0.5624459204777361]]\n",
      "2020-12-23 02:44:51,033 : INFO : Removed 3 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:51,034 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:51,035 : INFO : built Dictionary(182 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 533 corpus positions)\n",
      "2020-12-23 02:44:51,151 : INFO : token count processed\n",
      "2020-12-23 02:44:51,153 : INFO : frequencies processed\n",
      "2020-12-23 02:44:51,280 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:51,281 : INFO : entropies processed\n",
      "2020-12-23 02:44:51,282 : INFO : extropies processed\n",
      "2020-12-23 02:44:51,284 : INFO : token count processed\n",
      "2020-12-23 02:44:51,285 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:51,286 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:51,287 : INFO : vocab #2480\n",
      "2020-12-23 02:44:51,290 : INFO : diff #set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:44:51,552 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:51,679 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.1634958460552238, 0.46221489254224096], [0.855490505695343, 0.1445095], [3.095795255000934, 1.3487605247277434], [4.938976520573842, 6.898202761357263, 7.11592125237784, 4.721258029553265, 2.1769447318039976, 0.21771849102057672]]\n",
      "2020-12-23 02:44:51,682 : INFO : Removed 3 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:51,683 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:51,684 : INFO : built Dictionary(151 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 555 corpus positions)\n",
      "2020-12-23 02:44:51,770 : INFO : token count processed\n",
      "2020-12-23 02:44:51,772 : INFO : frequencies processed\n",
      "2020-12-23 02:44:51,902 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:51,902 : INFO : entropies processed\n",
      "2020-12-23 02:44:51,903 : INFO : extropies processed\n",
      "2020-12-23 02:44:51,905 : INFO : token count processed\n",
      "2020-12-23 02:44:51,906 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:51,906 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:51,907 : INFO : vocab #2480\n",
      "2020-12-23 02:44:51,908 : INFO : diff #set()\n",
      "2020-12-23 02:44:52,167 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:52,295 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.177674785726239, 0.4592053903340333], [0.9057475849986076, 0.094252415], [2.0, 1.2451124978365313], [4.938976520573842, 6.388500481644799, 6.6823209172358435, 4.645156084982798, 1.7433443966620015, 0.2938204355910443]]\n",
      "2020-12-23 02:44:52,297 : INFO : Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:52,298 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:52,299 : INFO : built Dictionary(72 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 219 corpus positions)\n",
      "2020-12-23 02:44:52,327 : INFO : token count processed\n",
      "2020-12-23 02:44:52,329 : INFO : frequencies processed\n",
      "2020-12-23 02:44:52,458 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:52,459 : INFO : entropies processed\n",
      "2020-12-23 02:44:52,460 : INFO : extropies processed\n",
      "2020-12-23 02:44:52,461 : INFO : token count processed\n",
      "2020-12-23 02:44:52,462 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:52,463 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:52,463 : INFO : vocab #2480\n",
      "2020-12-23 02:44:52,464 : INFO : diff #set()\n",
      "2020-12-23 02:44:52,721 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:52,849 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.1773098472333545, 0.4592823576629074], [0.8771595507860184, 0.12284045], [1.7924812503605778, 1.1575860145844845], [4.938976520573842, 4.8191513650620195, 5.602897021630005, 4.155230864005857, 0.6639205010561628, 0.7837456565679854]]\n",
      "2020-12-23 02:44:52,852 : INFO : Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:52,853 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:52,854 : INFO : built Dictionary(77 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 274 corpus positions)\n",
      "2020-12-23 02:44:52,892 : INFO : token count processed\n",
      "2020-12-23 02:44:52,894 : INFO : frequencies processed\n",
      "2020-12-23 02:44:53,035 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:53,036 : INFO : entropies processed\n",
      "2020-12-23 02:44:53,037 : INFO : extropies processed\n",
      "2020-12-23 02:44:53,039 : INFO : token count processed\n",
      "2020-12-23 02:44:53,040 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:53,041 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:53,042 : INFO : vocab #2480\n",
      "2020-12-23 02:44:53,043 : INFO : diff #set()\n",
      "2020-12-23 02:44:53,310 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:53,437 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.1600854225844595, 0.46294465466256346], [0.8727131187915802, 0.12728688], [1.7924812503605778, 1.1575860145844845], [4.938976520573842, 5.062480936779194, 5.702431075202319, 4.299026382150717, 0.7634545546284768, 0.6399501384231252]]\n",
      "2020-12-23 02:44:53,440 : INFO : Removed 3 and 223 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:53,441 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:53,443 : INFO : built Dictionary(258 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 1828 corpus positions)\n",
      "2020-12-23 02:44:53,659 : INFO : token count processed\n",
      "2020-12-23 02:44:53,663 : INFO : frequencies processed\n",
      "2020-12-23 02:44:53,791 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:53,791 : INFO : entropies processed\n",
      "2020-12-23 02:44:53,792 : INFO : extropies processed\n",
      "2020-12-23 02:44:53,793 : INFO : token count processed\n",
      "2020-12-23 02:44:53,794 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:53,795 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:53,796 : INFO : vocab #2480\n",
      "2020-12-23 02:44:53,796 : INFO : diff #set()\n",
      "2020-12-23 02:44:54,061 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:54,188 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.183490456976812, 0.45798230846612753], [0.8775632604956627, 0.12243674], [3.75, 1.3846096858033596], [4.938976520573842, 7.185085743102134, 7.281113188653995, 4.842949075021982, 2.342136668080153, 0.0960274455518606]]\n",
      "2020-12-23 02:44:54,191 : INFO : Removed 3 and 148 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:54,192 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:54,193 : INFO : built Dictionary(181 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 803 corpus positions)\n",
      "2020-12-23 02:44:54,312 : INFO : token count processed\n",
      "2020-12-23 02:44:54,315 : INFO : frequencies processed\n",
      "2020-12-23 02:44:54,443 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:54,444 : INFO : entropies processed\n",
      "2020-12-23 02:44:54,444 : INFO : extropies processed\n",
      "2020-12-23 02:44:54,445 : INFO : token count processed\n",
      "2020-12-23 02:44:54,446 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:54,447 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:54,447 : INFO : vocab #2480\n",
      "2020-12-23 02:44:54,448 : INFO : diff #set()\n",
      "2020-12-23 02:44:54,709 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:54,837 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.185777329495425, 0.457503143849902], [0.8987851217389107, 0.10121488], [2.9219280948873623, 1.3359016564230495], [4.938976520573842, 6.591225336124281, 6.785298606869532, 4.744903249828592, 1.8463220862956895, 0.1940732707452506]]\n",
      "2020-12-23 02:44:54,840 : INFO : Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:54,841 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:54,842 : INFO : built Dictionary(65 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 141 corpus positions)\n",
      "2020-12-23 02:44:54,872 : INFO : token count processed\n",
      "2020-12-23 02:44:54,874 : INFO : frequencies processed\n",
      "2020-12-23 02:44:55,006 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:55,007 : INFO : entropies processed\n",
      "2020-12-23 02:44:55,008 : INFO : extropies processed\n",
      "2020-12-23 02:44:55,010 : INFO : token count processed\n",
      "2020-12-23 02:44:55,011 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:55,013 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:55,014 : INFO : vocab #2480\n",
      "2020-12-23 02:44:55,015 : INFO : diff #set()\n",
      "2020-12-23 02:44:55,285 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:55,415 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.1681668189851855, 0.4612191235672778], [0.854884147644043, 0.14511585], [0.8112781244591328, 0.8112781244591328], [4.938976520573842, 4.7032114441396695, 5.692814194783858, 3.949373769929653, 0.7538376742100157, 0.9896027506441882]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:44:55,417 : INFO : Removed 3 and 9 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:55,418 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:55,419 : INFO : built Dictionary(138 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 381 corpus positions)\n",
      "2020-12-23 02:44:55,505 : INFO : token count processed\n",
      "2020-12-23 02:44:55,508 : INFO : frequencies processed\n",
      "2020-12-23 02:44:55,634 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:55,634 : INFO : entropies processed\n",
      "2020-12-23 02:44:55,635 : INFO : extropies processed\n",
      "2020-12-23 02:44:55,636 : INFO : token count processed\n",
      "2020-12-23 02:44:55,637 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:55,637 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:55,638 : INFO : vocab #2480\n",
      "2020-12-23 02:44:55,639 : INFO : diff #set()\n",
      "2020-12-23 02:44:55,896 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:56,024 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.1472570850493375, 0.46571042050003203], [0.8580498099327087, 0.14195019], [2.725480556997868, 1.3192201298976014], [4.938976520573842, 6.14228447828618, 6.503292042942643, 4.5779689559173775, 1.5643155223688012, 0.36100756465646366]]\n",
      "2020-12-23 02:44:56,027 : INFO : Removed 3 and 184 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:56,028 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:56,029 : INFO : built Dictionary(269 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 1171 corpus positions)\n",
      "2020-12-23 02:44:56,245 : INFO : token count processed\n",
      "2020-12-23 02:44:56,248 : INFO : frequencies processed\n",
      "2020-12-23 02:44:56,373 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:56,374 : INFO : entropies processed\n",
      "2020-12-23 02:44:56,374 : INFO : extropies processed\n",
      "2020-12-23 02:44:56,375 : INFO : token count processed\n",
      "2020-12-23 02:44:56,376 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:56,377 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:56,377 : INFO : vocab #2480\n",
      "2020-12-23 02:44:56,378 : INFO : diff #set()\n",
      "2020-12-23 02:44:56,647 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:56,775 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.1725170473042115, 0.4602955826012319], [0.8538682758808136, 0.14613172], [3.7345216647797517, 1.3834830067024517], [4.938976520573842, 7.450178124335845, 7.559896844706161, 4.829257800203527, 2.620920324132319, 0.10971872037031627]]\n",
      "2020-12-23 02:44:56,777 : INFO : Removed 3 and 16 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:56,778 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:56,780 : INFO : built Dictionary(80 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 219 corpus positions)\n",
      "2020-12-23 02:44:56,819 : INFO : token count processed\n",
      "2020-12-23 02:44:56,821 : INFO : frequencies processed\n",
      "2020-12-23 02:44:56,950 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:56,950 : INFO : entropies processed\n",
      "2020-12-23 02:44:56,951 : INFO : extropies processed\n",
      "2020-12-23 02:44:56,952 : INFO : token count processed\n",
      "2020-12-23 02:44:56,953 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:56,954 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:56,955 : INFO : vocab #2480\n",
      "2020-12-23 02:44:56,956 : INFO : diff #set()\n",
      "2020-12-23 02:44:57,218 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:57,344 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.1598675750351637, 0.46299134796896957], [0.8625006228685379, 0.13749938], [2.1556390622295662, 1.2407663947533205], [4.938976520573842, 5.20665021947654, 5.859976469077705, 4.285650270972677, 0.9209999485038631, 0.6533262496011654]]\n",
      "2020-12-23 02:44:57,347 : INFO : Removed 3 and 89 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:57,348 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:57,349 : INFO : built Dictionary(147 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 557 corpus positions)\n",
      "2020-12-23 02:44:57,433 : INFO : token count processed\n",
      "2020-12-23 02:44:57,436 : INFO : frequencies processed\n",
      "2020-12-23 02:44:57,563 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:57,564 : INFO : entropies processed\n",
      "2020-12-23 02:44:57,565 : INFO : extropies processed\n",
      "2020-12-23 02:44:57,566 : INFO : token count processed\n",
      "2020-12-23 02:44:57,567 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:57,568 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:57,569 : INFO : vocab #2480\n",
      "2020-12-23 02:44:57,570 : INFO : diff #set()\n",
      "2020-12-23 02:44:57,828 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:57,956 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.1788016033051019, 0.4589679016589048], [0.8761073052883148, 0.123892695], [2.6464393446710153, 1.3017576173934455], [4.938976520573842, 6.524718477352, 6.773520496223467, 4.690174501702375, 1.8345439756496251, 0.24880201887146747]]\n",
      "2020-12-23 02:44:57,958 : INFO : Removed 3 and 8 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:57,959 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:57,961 : INFO : built Dictionary(85 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 264 corpus positions)\n",
      "2020-12-23 02:44:58,004 : INFO : token count processed\n",
      "2020-12-23 02:44:58,007 : INFO : frequencies processed\n",
      "2020-12-23 02:44:58,143 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:58,144 : INFO : entropies processed\n",
      "2020-12-23 02:44:58,145 : INFO : extropies processed\n",
      "2020-12-23 02:44:58,146 : INFO : token count processed\n",
      "2020-12-23 02:44:58,147 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:58,148 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:58,149 : INFO : vocab #2480\n",
      "2020-12-23 02:44:58,150 : INFO : diff #set()\n",
      "2020-12-23 02:44:58,420 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:58,549 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.1614860837706065, 0.46264466262745907], [0.8745690137147903, 0.12543099], [2.4056390622295662, 1.2666171566933806], [4.938976520573842, 5.321859380715434, 5.903049755114191, 4.357786146175085, 0.9640732345403489, 0.5811903743987568]]\n",
      "2020-12-23 02:44:58,551 : INFO : Removed 3 and 119 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:58,552 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:58,554 : INFO : built Dictionary(162 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 882 corpus positions)\n",
      "2020-12-23 02:44:58,655 : INFO : token count processed\n",
      "2020-12-23 02:44:58,661 : INFO : frequencies processed\n",
      "2020-12-23 02:44:58,789 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:58,789 : INFO : entropies processed\n",
      "2020-12-23 02:44:58,790 : INFO : extropies processed\n",
      "2020-12-23 02:44:58,791 : INFO : token count processed\n",
      "2020-12-23 02:44:58,792 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:58,793 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:58,793 : INFO : vocab #2480\n",
      "2020-12-23 02:44:58,794 : INFO : diff #set()\n",
      "2020-12-23 02:44:59,062 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:59,195 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.1884239515709696, 0.4569498516419296], [0.8906670585274696, 0.10933294], [2.9219280948873623, 1.3359016564230495], [4.938976520573842, 6.500767808767801, 6.695916557798286, 4.743827771543357, 1.7569400372244441, 0.1951487490304853]]\n",
      "2020-12-23 02:44:59,198 : INFO : Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:59,198 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:59,199 : INFO : built Dictionary(61 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 104 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:44:59,221 : INFO : token count processed\n",
      "2020-12-23 02:44:59,224 : INFO : frequencies processed\n",
      "2020-12-23 02:44:59,358 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:59,358 : INFO : entropies processed\n",
      "2020-12-23 02:44:59,359 : INFO : extropies processed\n",
      "2020-12-23 02:44:59,360 : INFO : token count processed\n",
      "2020-12-23 02:44:59,361 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:59,362 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:59,363 : INFO : vocab #2480\n",
      "2020-12-23 02:44:59,364 : INFO : diff #set()\n",
      "2020-12-23 02:44:59,623 : INFO : alphabet #2480\n",
      "2020-12-23 02:44:59,752 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.1708735036920854, 0.46064406714590356], [0.89930859208107, 0.10069141], [1.0, 1.0], [4.938976520573842, 4.736228843383063, 5.803330502436539, 3.871874861520366, 0.8643539818626973, 1.0671016590534768]]\n",
      "2020-12-23 02:44:59,754 : INFO : Removed 3 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:44:59,755 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:44:59,756 : INFO : built Dictionary(122 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 445 corpus positions)\n",
      "2020-12-23 02:44:59,822 : INFO : token count processed\n",
      "2020-12-23 02:44:59,824 : INFO : frequencies processed\n",
      "2020-12-23 02:44:59,956 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:44:59,956 : INFO : entropies processed\n",
      "2020-12-23 02:44:59,957 : INFO : extropies processed\n",
      "2020-12-23 02:44:59,958 : INFO : token count processed\n",
      "2020-12-23 02:44:59,959 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:44:59,960 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:44:59,960 : INFO : vocab #2480\n",
      "2020-12-23 02:44:59,961 : INFO : diff #set()\n",
      "2020-12-23 02:45:00,231 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:00,358 : INFO : Computed distances or similarities ('291', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.2000759193970025, 0.4545297692609082], [0.8874410092830658, 0.11255899], [2.0, 1.2451124978365313], [4.938976520573842, 5.788442787590127, 6.189859326856734, 4.537559981307236, 1.2508828062828918, 0.4014165392666067]]\n",
      "2020-12-23 02:45:00,361 : INFO : Removed 3 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:00,362 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:00,363 : INFO : built Dictionary(76 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 131 corpus positions)\n",
      "2020-12-23 02:45:00,402 : INFO : token count processed\n",
      "2020-12-23 02:45:00,404 : INFO : frequencies processed\n",
      "2020-12-23 02:45:00,536 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:00,537 : INFO : entropies processed\n",
      "2020-12-23 02:45:00,538 : INFO : extropies processed\n",
      "2020-12-23 02:45:00,539 : INFO : token count processed\n",
      "2020-12-23 02:45:00,540 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:00,541 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:00,542 : INFO : vocab #2480\n",
      "2020-12-23 02:45:00,543 : INFO : diff #set()\n",
      "2020-12-23 02:45:00,812 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:00,940 : INFO : Computed distances or similarities ('291', 'sacp-python-common/setup.py')[[1.1197403487831674, 0.47175589244883126], [0.8695205599069595, 0.13047944], [2.584962500721156, 1.315172029168969], [4.938976520573842, 5.370004292053436, 6.072045385028218, 4.236935427599061, 1.1330688644543763, 0.702041092974782]]\n",
      "2020-12-23 02:45:00,942 : INFO : Removed 3 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:00,943 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:00,944 : INFO : built Dictionary(102 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 376 corpus positions)\n",
      "2020-12-23 02:45:00,994 : INFO : token count processed\n",
      "2020-12-23 02:45:00,996 : INFO : frequencies processed\n",
      "2020-12-23 02:45:01,125 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:01,126 : INFO : entropies processed\n",
      "2020-12-23 02:45:01,127 : INFO : extropies processed\n",
      "2020-12-23 02:45:01,129 : INFO : token count processed\n",
      "2020-12-23 02:45:01,130 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:01,131 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:01,132 : INFO : vocab #2480\n",
      "2020-12-23 02:45:01,133 : INFO : diff #set()\n",
      "2020-12-23 02:45:01,401 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:01,532 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.1770203266357704, 0.4593434373418721], [0.8978532403707504, 0.10214676], [2.4056390622295662, 1.2666171566933806], [4.938976520573842, 5.695663584743922, 6.118587686556712, 4.516052418761053, 1.1796111659828696, 0.4229241018127894]]\n",
      "2020-12-23 02:45:01,535 : INFO : Removed 3 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:01,536 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:01,537 : INFO : built Dictionary(66 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 151 corpus positions)\n",
      "2020-12-23 02:45:01,562 : INFO : token count processed\n",
      "2020-12-23 02:45:01,567 : INFO : frequencies processed\n",
      "2020-12-23 02:45:01,702 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:01,703 : INFO : entropies processed\n",
      "2020-12-23 02:45:01,704 : INFO : extropies processed\n",
      "2020-12-23 02:45:01,706 : INFO : token count processed\n",
      "2020-12-23 02:45:01,707 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:01,708 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:01,708 : INFO : vocab #2480\n",
      "2020-12-23 02:45:01,710 : INFO : diff #set()\n",
      "2020-12-23 02:45:01,970 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:02,099 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.1711399817445451, 0.4605875293201888], [0.9027931988239288, 0.0972068], [1.584962500721156, 1.1699250014423124], [4.938976520573842, 4.9004417692112465, 5.809309846868246, 4.030108442916843, 0.8703333262944035, 0.9088680776569991]]\n",
      "2020-12-23 02:45:02,101 : INFO : Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:02,102 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:02,103 : INFO : built Dictionary(65 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 158 corpus positions)\n",
      "2020-12-23 02:45:02,125 : INFO : token count processed\n",
      "2020-12-23 02:45:02,127 : INFO : frequencies processed\n",
      "2020-12-23 02:45:02,255 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:02,256 : INFO : entropies processed\n",
      "2020-12-23 02:45:02,256 : INFO : extropies processed\n",
      "2020-12-23 02:45:02,258 : INFO : token count processed\n",
      "2020-12-23 02:45:02,260 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:02,261 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:02,262 : INFO : vocab #2480\n",
      "2020-12-23 02:45:02,264 : INFO : diff #set()\n",
      "2020-12-23 02:45:02,524 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:02,651 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.2048341432805914, 0.4535488544785013], [0.9299115091562271, 0.07008849], [1.0, 1.0], [4.938976520573842, 4.778624108914332, 5.735200559776637, 3.982400069711537, 0.7962240392027953, 0.9565764508623049]]\n",
      "2020-12-23 02:45:02,653 : INFO : Removed 3 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:02,654 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:02,655 : INFO : built Dictionary(65 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 189 corpus positions)\n",
      "2020-12-23 02:45:02,678 : INFO : token count processed\n",
      "2020-12-23 02:45:02,680 : INFO : frequencies processed\n",
      "2020-12-23 02:45:02,808 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:02,809 : INFO : entropies processed\n",
      "2020-12-23 02:45:02,810 : INFO : extropies processed\n",
      "2020-12-23 02:45:02,811 : INFO : token count processed\n",
      "2020-12-23 02:45:02,812 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:02,813 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:02,814 : INFO : vocab #2480\n",
      "2020-12-23 02:45:02,815 : INFO : diff #set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:45:03,073 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:03,201 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.1918084901673758, 0.4562442405374731], [0.9330286160111427, 0.066971384], [1.584962500721156, 1.1699250014423124], [4.938976520573842, 4.773880192225086, 5.643756220260349, 4.0691004925385785, 0.7047796996865072, 0.8698760280352635]]\n",
      "2020-12-23 02:45:03,204 : INFO : Removed 3 and 346 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:03,205 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:03,207 : INFO : built Dictionary(170 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 2014 corpus positions)\n",
      "2020-12-23 02:45:03,315 : INFO : token count processed\n",
      "2020-12-23 02:45:03,318 : INFO : frequencies processed\n",
      "2020-12-23 02:45:03,445 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:03,446 : INFO : entropies processed\n",
      "2020-12-23 02:45:03,447 : INFO : extropies processed\n",
      "2020-12-23 02:45:03,448 : INFO : token count processed\n",
      "2020-12-23 02:45:03,449 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:03,450 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:03,451 : INFO : vocab #2480\n",
      "2020-12-23 02:45:03,452 : INFO : diff #set()\n",
      "2020-12-23 02:45:03,720 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:03,849 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.1294220278057558, 0.46961099629012537], [0.833503320813179, 0.16649668], [2.774397470347699, 1.293949352965088], [4.938976520573842, 6.620773041953877, 6.716453473561985, 4.843296088965734, 1.7774769529881427, 0.09568043160810813]]\n",
      "2020-12-23 02:45:03,851 : INFO : Removed 3 and 132 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:03,852 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:03,853 : INFO : built Dictionary(103 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 577 corpus positions)\n",
      "2020-12-23 02:45:03,911 : INFO : token count processed\n",
      "2020-12-23 02:45:03,914 : INFO : frequencies processed\n",
      "2020-12-23 02:45:04,046 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:04,047 : INFO : entropies processed\n",
      "2020-12-23 02:45:04,048 : INFO : extropies processed\n",
      "2020-12-23 02:45:04,049 : INFO : token count processed\n",
      "2020-12-23 02:45:04,050 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:04,051 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:04,052 : INFO : vocab #2480\n",
      "2020-12-23 02:45:04,054 : INFO : diff #set()\n",
      "2020-12-23 02:45:04,313 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:04,441 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.148833482072235, 0.4653687725656836], [0.8399135023355484, 0.1600865], [2.413088436425758, 1.2801428332226974], [4.938976520573842, 5.828370634755606, 6.112605372466394, 4.654741782863054, 1.173628851892552, 0.2842347377107881]]\n",
      "2020-12-23 02:45:04,444 : INFO : Removed 3 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:04,444 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:04,445 : INFO : built Dictionary(106 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 340 corpus positions)\n",
      "2020-12-23 02:45:04,501 : INFO : token count processed\n",
      "2020-12-23 02:45:04,504 : INFO : frequencies processed\n",
      "2020-12-23 02:45:04,637 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:04,638 : INFO : entropies processed\n",
      "2020-12-23 02:45:04,639 : INFO : extropies processed\n",
      "2020-12-23 02:45:04,640 : INFO : token count processed\n",
      "2020-12-23 02:45:04,641 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:04,642 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:04,643 : INFO : vocab #2480\n",
      "2020-12-23 02:45:04,644 : INFO : diff #set()\n",
      "2020-12-23 02:45:04,911 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:05,040 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.1818884727581478, 0.45831856783032066], [0.8823094367980957, 0.11769056], [2.0, 1.2451124978365313], [4.938976520573842, 5.774409284925443, 6.230704403240418, 4.4826814022588675, 1.2917278826665761, 0.4562951183149755]]\n",
      "2020-12-23 02:45:05,043 : INFO : Removed 3 and 51 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:05,044 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:05,045 : INFO : built Dictionary(115 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 389 corpus positions)\n",
      "2020-12-23 02:45:05,100 : INFO : token count processed\n",
      "2020-12-23 02:45:05,103 : INFO : frequencies processed\n",
      "2020-12-23 02:45:05,233 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:05,234 : INFO : entropies processed\n",
      "2020-12-23 02:45:05,234 : INFO : extropies processed\n",
      "2020-12-23 02:45:05,236 : INFO : token count processed\n",
      "2020-12-23 02:45:05,237 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:05,238 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:05,239 : INFO : vocab #2480\n",
      "2020-12-23 02:45:05,241 : INFO : diff #set()\n",
      "2020-12-23 02:45:05,508 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:05,638 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.1271371479080252, 0.47011543237043724], [0.8349159955978394, 0.165084], [2.4056390622295662, 1.2666171566933806], [4.938976520573842, 5.977819040873918, 6.338739876448637, 4.578055684999123, 1.3997633558747946, 0.3609208355747189]]\n",
      "2020-12-23 02:45:05,641 : INFO : Removed 3 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:05,641 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:05,642 : INFO : built Dictionary(95 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 336 corpus positions)\n",
      "2020-12-23 02:45:05,688 : INFO : token count processed\n",
      "2020-12-23 02:45:05,693 : INFO : frequencies processed\n",
      "2020-12-23 02:45:05,818 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:05,819 : INFO : entropies processed\n",
      "2020-12-23 02:45:05,820 : INFO : extropies processed\n",
      "2020-12-23 02:45:05,821 : INFO : token count processed\n",
      "2020-12-23 02:45:05,822 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:05,823 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:05,823 : INFO : vocab #2480\n",
      "2020-12-23 02:45:05,824 : INFO : diff #set()\n",
      "2020-12-23 02:45:06,089 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:06,217 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.1367824697573796, 0.46799335643817064], [0.8298932909965515, 0.17010671], [2.6464393446710153, 1.3017576173934455], [4.938976520573842, 5.901812829596593, 6.293767780345246, 4.54702156982519, 1.354791259771404, 0.3919549507486533]]\n",
      "2020-12-23 02:45:06,219 : INFO : Removed 3 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:06,220 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:06,221 : INFO : built Dictionary(98 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 336 corpus positions)\n",
      "2020-12-23 02:45:06,266 : INFO : token count processed\n",
      "2020-12-23 02:45:06,270 : INFO : frequencies processed\n",
      "2020-12-23 02:45:06,400 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:06,401 : INFO : entropies processed\n",
      "2020-12-23 02:45:06,402 : INFO : extropies processed\n",
      "2020-12-23 02:45:06,403 : INFO : token count processed\n",
      "2020-12-23 02:45:06,404 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:06,405 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:06,406 : INFO : vocab #2480\n",
      "2020-12-23 02:45:06,407 : INFO : diff #set()\n",
      "2020-12-23 02:45:06,664 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:06,791 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.188786926397901, 0.4568740739171472], [0.8974904343485832, 0.102509566], [2.584962500721156, 1.315172029168969], [4.938976520573842, 5.643202320803383, 6.114878758708706, 4.467300082668519, 1.1759022381348636, 0.4716764379053231]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:45:06,794 : INFO : Removed 3 and 30 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:06,795 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:06,796 : INFO : built Dictionary(113 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 370 corpus positions)\n",
      "2020-12-23 02:45:06,847 : INFO : token count processed\n",
      "2020-12-23 02:45:06,850 : INFO : frequencies processed\n",
      "2020-12-23 02:45:06,979 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:06,980 : INFO : entropies processed\n",
      "2020-12-23 02:45:06,981 : INFO : extropies processed\n",
      "2020-12-23 02:45:06,982 : INFO : token count processed\n",
      "2020-12-23 02:45:06,983 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:06,984 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:06,985 : INFO : vocab #2480\n",
      "2020-12-23 02:45:06,986 : INFO : diff #set()\n",
      "2020-12-23 02:45:07,245 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:07,372 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.1557991666552128, 0.46386510184598045], [0.8555148541927338, 0.14448515], [2.0, 1.2451124978365313], [4.938976520573842, 5.925214310725336, 6.3295516026022804, 4.534639228696897, 1.3905750820284384, 0.4043372918769448]]\n",
      "2020-12-23 02:45:07,375 : INFO : Removed 3 and 293 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:07,376 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:07,378 : INFO : built Dictionary(179 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 1765 corpus positions)\n",
      "2020-12-23 02:45:07,492 : INFO : token count processed\n",
      "2020-12-23 02:45:07,494 : INFO : frequencies processed\n",
      "2020-12-23 02:45:07,627 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:07,628 : INFO : entropies processed\n",
      "2020-12-23 02:45:07,629 : INFO : extropies processed\n",
      "2020-12-23 02:45:07,630 : INFO : token count processed\n",
      "2020-12-23 02:45:07,632 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:07,633 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:07,634 : INFO : vocab #2480\n",
      "2020-12-23 02:45:07,635 : INFO : diff #set()\n",
      "2020-12-23 02:45:07,896 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:08,022 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.1266301364728595, 0.47022751293206], [0.8277656435966492, 0.17223436], [3.080500530640267, 1.3220000168707582], [4.938976520573842, 6.551685682764175, 6.654323947594882, 4.836338255743135, 1.7153474270210403, 0.10263826483070737]]\n",
      "2020-12-23 02:45:08,025 : INFO : Removed 3 and 88 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:08,026 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:08,027 : INFO : built Dictionary(154 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 606 corpus positions)\n",
      "2020-12-23 02:45:08,119 : INFO : token count processed\n",
      "2020-12-23 02:45:08,122 : INFO : frequencies processed\n",
      "2020-12-23 02:45:08,249 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:08,250 : INFO : entropies processed\n",
      "2020-12-23 02:45:08,251 : INFO : extropies processed\n",
      "2020-12-23 02:45:08,253 : INFO : token count processed\n",
      "2020-12-23 02:45:08,254 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:08,255 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:08,256 : INFO : vocab #2480\n",
      "2020-12-23 02:45:08,257 : INFO : diff #set()\n",
      "2020-12-23 02:45:08,516 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:08,644 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.1356005883093159, 0.4682523527452606], [0.8429205566644669, 0.15707944], [3.3502090290998976, 1.3531670961856495], [4.938976520573842, 6.642985062562557, 6.836791198180533, 4.745170384955866, 1.897814677606691, 0.19380613561797588]]\n",
      "2020-12-23 02:45:08,647 : INFO : Removed 3 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:08,648 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:08,649 : INFO : built Dictionary(73 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 201 corpus positions)\n",
      "2020-12-23 02:45:08,678 : INFO : token count processed\n",
      "2020-12-23 02:45:08,680 : INFO : frequencies processed\n",
      "2020-12-23 02:45:08,808 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:08,809 : INFO : entropies processed\n",
      "2020-12-23 02:45:08,810 : INFO : extropies processed\n",
      "2020-12-23 02:45:08,811 : INFO : token count processed\n",
      "2020-12-23 02:45:08,812 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:08,813 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:08,814 : INFO : vocab #2480\n",
      "2020-12-23 02:45:08,815 : INFO : diff #set()\n",
      "2020-12-23 02:45:09,073 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:09,200 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.1158109728181211, 0.47263201337313504], [0.7897769212722778, 0.21022308], [2.4130884364257574, 1.2801428332226972], [4.938976520573842, 5.2461980344571995, 5.8675013395706115, 4.31767321546043, 0.9285248189967694, 0.621303305113412]]\n",
      "2020-12-23 02:45:09,202 : INFO : Removed 3 and 23 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:09,203 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:09,204 : INFO : built Dictionary(95 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 261 corpus positions)\n",
      "2020-12-23 02:45:09,246 : INFO : token count processed\n",
      "2020-12-23 02:45:09,249 : INFO : frequencies processed\n",
      "2020-12-23 02:45:09,376 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:09,377 : INFO : entropies processed\n",
      "2020-12-23 02:45:09,378 : INFO : extropies processed\n",
      "2020-12-23 02:45:09,379 : INFO : token count processed\n",
      "2020-12-23 02:45:09,380 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:09,381 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:09,381 : INFO : vocab #2480\n",
      "2020-12-23 02:45:09,382 : INFO : diff #set()\n",
      "2020-12-23 02:45:09,642 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:09,770 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/test_auth_utility.py')[[1.1457064572890607, 0.46604697329541767], [0.8596034198999405, 0.14039658], [2.584962500721156, 1.315172029168969], [4.938976520573842, 5.903090303960449, 6.369093165984306, 4.472973658549987, 1.4301166454104637, 0.4660028620238563]]\n",
      "2020-12-23 02:45:09,773 : INFO : Removed 3 and 442 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:09,774 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:09,775 : INFO : built Dictionary(131 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 1260 corpus positions)\n",
      "2020-12-23 02:45:09,842 : INFO : token count processed\n",
      "2020-12-23 02:45:09,844 : INFO : frequencies processed\n",
      "2020-12-23 02:45:09,973 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:09,974 : INFO : entropies processed\n",
      "2020-12-23 02:45:09,975 : INFO : extropies processed\n",
      "2020-12-23 02:45:09,976 : INFO : token count processed\n",
      "2020-12-23 02:45:09,978 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:09,979 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:09,980 : INFO : vocab #2480\n",
      "2020-12-23 02:45:09,981 : INFO : diff #set()\n",
      "2020-12-23 02:45:10,251 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:10,383 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.1871631316650295, 0.4572132665928428], [0.8992780223488808, 0.10072198], [1.9219280948873623, 1.2148067842293933], [4.938976520573842, 6.16659449033757, 6.322061522417236, 4.783509488494175, 1.3830850018433942, 0.15546703207966672]]\n",
      "2020-12-23 02:45:10,385 : INFO : Removed 3 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:10,386 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:10,388 : INFO : built Dictionary(91 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 295 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:45:10,438 : INFO : token count processed\n",
      "2020-12-23 02:45:10,447 : INFO : frequencies processed\n",
      "2020-12-23 02:45:10,577 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:10,577 : INFO : entropies processed\n",
      "2020-12-23 02:45:10,578 : INFO : extropies processed\n",
      "2020-12-23 02:45:10,579 : INFO : token count processed\n",
      "2020-12-23 02:45:10,580 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:10,580 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:10,581 : INFO : vocab #2480\n",
      "2020-12-23 02:45:10,582 : INFO : diff #set()\n",
      "2020-12-23 02:45:10,841 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:10,969 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.177608127833806, 0.4592194468867814], [0.8679043799638748, 0.13209562], [2.0, 1.2451124978365313], [4.938976520573842, 5.906856253399655, 6.348841459890396, 4.496991314083102, 1.409864939316554, 0.4419852064907408]]\n",
      "2020-12-23 02:45:10,972 : INFO : Removed 3 and 68 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:10,973 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:10,974 : INFO : built Dictionary(104 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 294 corpus positions)\n",
      "2020-12-23 02:45:11,034 : INFO : token count processed\n",
      "2020-12-23 02:45:11,036 : INFO : frequencies processed\n",
      "2020-12-23 02:45:11,168 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:11,169 : INFO : entropies processed\n",
      "2020-12-23 02:45:11,170 : INFO : extropies processed\n",
      "2020-12-23 02:45:11,172 : INFO : token count processed\n",
      "2020-12-23 02:45:11,173 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:11,174 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:11,174 : INFO : vocab #2480\n",
      "2020-12-23 02:45:11,175 : INFO : diff #set()\n",
      "2020-12-23 02:45:11,433 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:11,561 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.181255770689496, 0.45845150918908495], [0.8907420188188553, 0.10925798], [2.0, 1.2451124978365313], [4.938976520573842, 5.965115449163356, 6.402039010435245, 4.502052959301953, 1.4630624898614029, 0.4369235612718887]]\n",
      "2020-12-23 02:45:11,564 : INFO : Removed 3 and 13 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:11,564 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:11,565 : INFO : built Dictionary(109 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 408 corpus positions)\n",
      "2020-12-23 02:45:11,624 : INFO : token count processed\n",
      "2020-12-23 02:45:11,626 : INFO : frequencies processed\n",
      "2020-12-23 02:45:11,758 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:11,759 : INFO : entropies processed\n",
      "2020-12-23 02:45:11,760 : INFO : extropies processed\n",
      "2020-12-23 02:45:11,761 : INFO : token count processed\n",
      "2020-12-23 02:45:11,762 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:11,763 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:11,764 : INFO : vocab #2480\n",
      "2020-12-23 02:45:11,765 : INFO : diff #set()\n",
      "2020-12-23 02:45:12,037 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:12,168 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.1200290300720206, 0.47169165413080616], [0.806922197341919, 0.1930778], [2.3709505944546683, 1.2676135783638416], [4.938976520573842, 5.791362404253194, 6.167191180135442, 4.563147744691594, 1.2282146595616004, 0.3758287758822485]]\n",
      "2020-12-23 02:45:12,170 : INFO : Removed 3 and 41 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:12,171 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:12,172 : INFO : built Dictionary(98 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 420 corpus positions)\n",
      "2020-12-23 02:45:12,220 : INFO : token count processed\n",
      "2020-12-23 02:45:12,225 : INFO : frequencies processed\n",
      "2020-12-23 02:45:12,354 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:12,355 : INFO : entropies processed\n",
      "2020-12-23 02:45:12,358 : INFO : extropies processed\n",
      "2020-12-23 02:45:12,359 : INFO : token count processed\n",
      "2020-12-23 02:45:12,360 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:12,361 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:12,361 : INFO : vocab #2480\n",
      "2020-12-23 02:45:12,362 : INFO : diff #set()\n",
      "2020-12-23 02:45:12,625 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:12,763 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.1139220267129193, 0.4730543451287878], [0.824832558631897, 0.17516744], [2.5949066182439395, 1.2932801896085004], [4.938976520573842, 5.651670454631116, 6.025520792753086, 4.5651261824518725, 1.0865442721792435, 0.37385033812196955]]\n",
      "2020-12-23 02:45:12,765 : INFO : Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:12,766 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:12,767 : INFO : built Dictionary(66 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 130 corpus positions)\n",
      "2020-12-23 02:45:12,790 : INFO : token count processed\n",
      "2020-12-23 02:45:12,793 : INFO : frequencies processed\n",
      "2020-12-23 02:45:12,919 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:12,920 : INFO : entropies processed\n",
      "2020-12-23 02:45:12,921 : INFO : extropies processed\n",
      "2020-12-23 02:45:12,922 : INFO : token count processed\n",
      "2020-12-23 02:45:12,923 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:12,924 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:12,925 : INFO : vocab #2480\n",
      "2020-12-23 02:45:12,926 : INFO : diff #set()\n",
      "2020-12-23 02:45:13,185 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:13,312 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.1971237069314984, 0.45514050794918565], [0.9044099673628807, 0.09559003], [1.0, 1.0], [4.938976520573842, 4.8226207261920235, 5.821864213507262, 3.9397330332586042, 0.8828876929334202, 0.9992434873152387]]\n",
      "2020-12-23 02:45:13,314 : INFO : Removed 3 and 50 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:13,315 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:13,317 : INFO : built Dictionary(109 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 300 corpus positions)\n",
      "2020-12-23 02:45:13,376 : INFO : token count processed\n",
      "2020-12-23 02:45:13,378 : INFO : frequencies processed\n",
      "2020-12-23 02:45:13,508 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:13,509 : INFO : entropies processed\n",
      "2020-12-23 02:45:13,510 : INFO : extropies processed\n",
      "2020-12-23 02:45:13,511 : INFO : token count processed\n",
      "2020-12-23 02:45:13,513 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:13,514 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:13,515 : INFO : vocab #2480\n",
      "2020-12-23 02:45:13,516 : INFO : diff #set()\n",
      "2020-12-23 02:45:13,776 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:13,904 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.2029900107329314, 0.4539285222030133], [0.9059624969959259, 0.0940375], [2.0, 1.2451124978365313], [4.938976520573842, 6.24862851613934, 6.649248735686099, 4.538356301027084, 1.7102722151122567, 0.4006202195467585]]\n",
      "2020-12-23 02:45:13,907 : INFO : Removed 3 and 32 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:13,908 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:13,910 : INFO : built Dictionary(108 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 416 corpus positions)\n",
      "2020-12-23 02:45:13,967 : INFO : token count processed\n",
      "2020-12-23 02:45:13,970 : INFO : frequencies processed\n",
      "2020-12-23 02:45:14,100 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:14,101 : INFO : entropies processed\n",
      "2020-12-23 02:45:14,102 : INFO : extropies processed\n",
      "2020-12-23 02:45:14,103 : INFO : token count processed\n",
      "2020-12-23 02:45:14,104 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:14,105 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:14,106 : INFO : vocab #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:45:14,107 : INFO : diff #set()\n",
      "2020-12-23 02:45:14,365 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:14,493 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.119613576085338, 0.4717841078593559], [0.8173128515481949, 0.18268715], [2.3709505944546683, 1.2676135783638416], [4.938976520573842, 5.850156917433494, 6.206381266295388, 4.582752171711949, 1.2674047457215458, 0.35622434886189414]]\n",
      "2020-12-23 02:45:14,495 : INFO : Removed 3 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:14,496 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:14,498 : INFO : built Dictionary(102 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 414 corpus positions)\n",
      "2020-12-23 02:45:14,560 : INFO : token count processed\n",
      "2020-12-23 02:45:14,562 : INFO : frequencies processed\n",
      "2020-12-23 02:45:14,697 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:14,698 : INFO : entropies processed\n",
      "2020-12-23 02:45:14,699 : INFO : extropies processed\n",
      "2020-12-23 02:45:14,700 : INFO : token count processed\n",
      "2020-12-23 02:45:14,701 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:14,702 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:14,703 : INFO : vocab #2480\n",
      "2020-12-23 02:45:14,704 : INFO : diff #set()\n",
      "2020-12-23 02:45:14,974 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:15,103 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.1139741647046126, 0.47304267795521093], [0.8248317688703537, 0.17516823], [2.5949066182439395, 1.2932801896085004], [4.938976520573842, 5.6831976040360095, 6.055905438178309, 4.566268686431544, 1.1169289176044668, 0.37270783414229935]]\n",
      "2020-12-23 02:45:15,106 : INFO : Removed 3 and 52 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:15,106 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:15,107 : INFO : built Dictionary(91 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 392 corpus positions)\n",
      "2020-12-23 02:45:15,150 : INFO : token count processed\n",
      "2020-12-23 02:45:15,159 : INFO : frequencies processed\n",
      "2020-12-23 02:45:15,290 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:15,290 : INFO : entropies processed\n",
      "2020-12-23 02:45:15,291 : INFO : extropies processed\n",
      "2020-12-23 02:45:15,292 : INFO : token count processed\n",
      "2020-12-23 02:45:15,292 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:15,293 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:15,294 : INFO : vocab #2480\n",
      "2020-12-23 02:45:15,294 : INFO : diff #set()\n",
      "2020-12-23 02:45:15,555 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:15,687 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.1725127054753406, 0.4602965025151383], [0.8560580760240555, 0.14394192], [2.1280852788913944, 1.2238339714721667], [4.938976520573842, 5.749308601266266, 6.1490918474450895, 4.539193274395019, 1.2101153268712475, 0.3997832461788233]]\n",
      "2020-12-23 02:45:15,692 : INFO : Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:15,693 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:15,694 : INFO : built Dictionary(85 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 273 corpus positions)\n",
      "2020-12-23 02:45:15,730 : INFO : token count processed\n",
      "2020-12-23 02:45:15,732 : INFO : frequencies processed\n",
      "2020-12-23 02:45:15,860 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:15,860 : INFO : entropies processed\n",
      "2020-12-23 02:45:15,861 : INFO : extropies processed\n",
      "2020-12-23 02:45:15,862 : INFO : token count processed\n",
      "2020-12-23 02:45:15,863 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:15,864 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:15,865 : INFO : vocab #2480\n",
      "2020-12-23 02:45:15,866 : INFO : diff #set()\n",
      "2020-12-23 02:45:16,123 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:16,250 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.0971201152027872, 0.47684440807688405], [0.8029755055904388, 0.1970245], [1.811278124459133, 1.18471876778999], [4.938976520573842, 5.015422548793484, 5.635721383598205, 4.318677685769122, 0.6967448630243629, 0.6202988348047214]]\n",
      "2020-12-23 02:45:16,252 : INFO : Removed 3 and 81 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:16,253 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:16,255 : INFO : built Dictionary(112 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 399 corpus positions)\n",
      "2020-12-23 02:45:16,316 : INFO : token count processed\n",
      "2020-12-23 02:45:16,324 : INFO : frequencies processed\n",
      "2020-12-23 02:45:16,453 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:16,454 : INFO : entropies processed\n",
      "2020-12-23 02:45:16,454 : INFO : extropies processed\n",
      "2020-12-23 02:45:16,456 : INFO : token count processed\n",
      "2020-12-23 02:45:16,457 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:16,458 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:16,458 : INFO : vocab #2480\n",
      "2020-12-23 02:45:16,459 : INFO : diff #set()\n",
      "2020-12-23 02:45:16,725 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:16,854 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.1140392569997557, 0.4730281127414823], [0.8182705491781235, 0.18172945], [2.3709505944546683, 1.2676135783638416], [4.938976520573842, 6.030001281822029, 6.356272553086487, 4.612705249309384, 1.4172960325126454, 0.3262712712644582]]\n",
      "2020-12-23 02:45:16,857 : INFO : Removed 3 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:16,857 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:16,858 : INFO : built Dictionary(102 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 362 corpus positions)\n",
      "2020-12-23 02:45:16,911 : INFO : token count processed\n",
      "2020-12-23 02:45:16,913 : INFO : frequencies processed\n",
      "2020-12-23 02:45:17,047 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:17,048 : INFO : entropies processed\n",
      "2020-12-23 02:45:17,049 : INFO : extropies processed\n",
      "2020-12-23 02:45:17,050 : INFO : token count processed\n",
      "2020-12-23 02:45:17,051 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:17,052 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:17,053 : INFO : vocab #2480\n",
      "2020-12-23 02:45:17,054 : INFO : diff #set()\n",
      "2020-12-23 02:45:17,315 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:17,443 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.1731733226165593, 0.4601565782134547], [0.8760633170604706, 0.12393668], [1.9219280948873623, 1.2148067842293933], [4.938976520573842, 5.9537092545441395, 6.356899648167903, 4.53578612695008, 1.4179231275940607, 0.4031903936237633]]\n",
      "2020-12-23 02:45:17,445 : INFO : Removed 3 and 90 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:17,446 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:17,447 : INFO : built Dictionary(112 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 363 corpus positions)\n",
      "2020-12-23 02:45:17,507 : INFO : token count processed\n",
      "2020-12-23 02:45:17,512 : INFO : frequencies processed\n",
      "2020-12-23 02:45:17,640 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:17,640 : INFO : entropies processed\n",
      "2020-12-23 02:45:17,641 : INFO : extropies processed\n",
      "2020-12-23 02:45:17,642 : INFO : token count processed\n",
      "2020-12-23 02:45:17,642 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:17,643 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:17,643 : INFO : vocab #2480\n",
      "2020-12-23 02:45:17,644 : INFO : diff #set()\n",
      "2020-12-23 02:45:17,901 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:18,029 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.1527996618557896, 0.46451140703820276], [0.8575169593095779, 0.14248304], [2.2516291673878226, 1.2667563532600834], [4.938976520573842, 6.184756445474906, 6.534365060759168, 4.589367905289581, 1.5953885401853256, 0.3496086152842617]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:45:18,031 : INFO : Removed 3 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:18,032 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:18,033 : INFO : built Dictionary(119 unique tokens: ['autom', 'back', 'bug', 'call', 'case']...) from 2 documents (total 476 corpus positions)\n",
      "2020-12-23 02:45:18,102 : INFO : token count processed\n",
      "2020-12-23 02:45:18,104 : INFO : frequencies processed\n",
      "2020-12-23 02:45:18,231 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:18,232 : INFO : entropies processed\n",
      "2020-12-23 02:45:18,232 : INFO : extropies processed\n",
      "2020-12-23 02:45:18,234 : INFO : token count processed\n",
      "2020-12-23 02:45:18,235 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:18,236 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:18,237 : INFO : vocab #2480\n",
      "2020-12-23 02:45:18,239 : INFO : diff #set()\n",
      "2020-12-23 02:45:18,500 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:18,628 : INFO : Computed distances or similarities ('291', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.1166614332832803, 0.4724421129782859], [0.8085899204015732, 0.19141008], [2.5949066182439395, 1.2932801896085004], [4.938976520573842, 6.212221456585881, 6.495644028677981, 4.6555539484817405, 1.5566675081041392, 0.28342257209210064]]\n",
      "2020-12-23 02:45:18,631 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:18,632 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:18,634 : INFO : built Dictionary(123 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 416 corpus positions)\n",
      "2020-12-23 02:45:18,663 : INFO : token count processed\n",
      "2020-12-23 02:45:18,665 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:18,668 : INFO : frequencies processed\n",
      "2020-12-23 02:45:18,669 : INFO : token count processed\n",
      "2020-12-23 02:45:18,671 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:18,672 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:18,673 : INFO : vocab #2480\n",
      "2020-12-23 02:45:18,674 : INFO : diff #set()\n",
      "2020-12-23 02:45:18,929 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:19,056 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.2802326802372794, 0.43855173582370577], [0.9808014985173941, 0.019198501], [nan, nan], [3.169925001442312, 6.301552355933639, 6.381367763536463, 3.090109593839488, 3.2114427620941512, 0.07981540760282435]]\n",
      "2020-12-23 02:45:19,059 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:19,060 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:19,061 : INFO : built Dictionary(158 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 652 corpus positions)\n",
      "2020-12-23 02:45:19,095 : INFO : token count processed\n",
      "2020-12-23 02:45:19,097 : INFO : frequencies processed\n",
      "2020-12-23 02:45:19,225 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:19,225 : INFO : entropies processed\n",
      "2020-12-23 02:45:19,229 : INFO : extropies processed\n",
      "2020-12-23 02:45:19,230 : INFO : token count processed\n",
      "2020-12-23 02:45:19,230 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:19,231 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:19,231 : INFO : vocab #2480\n",
      "2020-12-23 02:45:19,232 : INFO : diff #set()\n",
      "2020-12-23 02:45:19,489 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:19,618 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.2588395002765147, 0.44270520321500734], [0.9438470862805843, 0.056152914], [1.584962500721156, 1.1699250014423124], [3.169925001442312, 6.739005504021667, 6.776637526530262, 3.132292978933716, 3.60671252508795, 0.03763202250859532]]\n",
      "2020-12-23 02:45:19,620 : INFO : Removed 0 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:19,621 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:19,623 : INFO : built Dictionary(102 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 503 corpus positions)\n",
      "2020-12-23 02:45:19,652 : INFO : token count processed\n",
      "2020-12-23 02:45:19,655 : INFO : frequencies processed\n",
      "2020-12-23 02:45:19,782 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:19,782 : INFO : entropies processed\n",
      "2020-12-23 02:45:19,783 : INFO : extropies processed\n",
      "2020-12-23 02:45:19,784 : INFO : token count processed\n",
      "2020-12-23 02:45:19,785 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:19,786 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:19,786 : INFO : vocab #2480\n",
      "2020-12-23 02:45:19,787 : INFO : diff #set()\n",
      "2020-12-23 02:45:20,044 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:20,172 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2560467382085512, 0.44325322834138864], [0.9476778693497181, 0.05232213], [1.584962500721156, 1.1699250014423124], [3.169925001442312, 5.870833373337847, 5.924155606380073, 3.116602768400086, 2.754230604937761, 0.05332223304222605]]\n",
      "2020-12-23 02:45:20,175 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:20,175 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:20,176 : INFO : built Dictionary(65 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 186 corpus positions)\n",
      "2020-12-23 02:45:20,188 : INFO : token count processed\n",
      "2020-12-23 02:45:20,190 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:20,191 : INFO : frequencies processed\n",
      "2020-12-23 02:45:20,192 : INFO : token count processed\n",
      "2020-12-23 02:45:20,193 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:20,194 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:20,194 : INFO : vocab #2480\n",
      "2020-12-23 02:45:20,195 : INFO : diff #set()\n",
      "2020-12-23 02:45:20,453 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:20,580 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.2791377785451095, 0.4387624168286795], [0.9835623130202293, 0.016437687], [nan, nan], [3.169925001442312, 5.371881234145534, 5.53755823760957, 3.0042479979782755, 2.367633236167258, 0.1656770034640358]]\n",
      "2020-12-23 02:45:20,583 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:20,584 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:20,585 : INFO : built Dictionary(55 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 138 corpus positions)\n",
      "2020-12-23 02:45:20,600 : INFO : token count processed\n",
      "2020-12-23 02:45:20,602 : INFO : frequencies processed\n",
      "2020-12-23 02:45:20,734 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:20,734 : INFO : entropies processed\n",
      "2020-12-23 02:45:20,735 : INFO : extropies processed\n",
      "2020-12-23 02:45:20,737 : INFO : token count processed\n",
      "2020-12-23 02:45:20,739 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:20,740 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:20,741 : INFO : vocab #2480\n",
      "2020-12-23 02:45:20,742 : INFO : diff #set()\n",
      "2020-12-23 02:45:21,007 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:21,137 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.2778834311338836, 0.4390040273053923], [0.9801619686186314, 0.019838031], [0.0, 0.0], [3.169925001442312, 4.85108279267097, 5.073854363036651, 2.9471534310766296, 1.9039293615943391, 0.22277157036568163]]\n",
      "2020-12-23 02:45:21,140 : INFO : Removed 0 and 127 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:21,141 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:21,143 : INFO : built Dictionary(96 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 411 corpus positions)\n",
      "2020-12-23 02:45:21,171 : INFO : token count processed\n",
      "2020-12-23 02:45:21,176 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:21,177 : INFO : frequencies processed\n",
      "2020-12-23 02:45:21,180 : INFO : token count processed\n",
      "2020-12-23 02:45:21,182 : INFO : alphabet_source #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:45:21,183 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:21,184 : INFO : vocab #2480\n",
      "2020-12-23 02:45:21,184 : INFO : diff #set()\n",
      "2020-12-23 02:45:21,436 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:21,563 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.2887706339161602, 0.43691577704707263], [0.9938864484429359, 0.0061135516], [nan, nan], [3.169925001442312, 6.139571208108155, 6.2125490160527095, 3.0969471934977575, 3.0426240146103973, 0.07297780794455466]]\n",
      "2020-12-23 02:45:21,566 : INFO : Removed 0 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:21,566 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:21,567 : INFO : built Dictionary(80 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 421 corpus positions)\n",
      "2020-12-23 02:45:21,586 : INFO : token count processed\n",
      "2020-12-23 02:45:21,589 : INFO : frequencies processed\n",
      "2020-12-23 02:45:21,716 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:21,717 : INFO : entropies processed\n",
      "2020-12-23 02:45:21,718 : INFO : extropies processed\n",
      "2020-12-23 02:45:21,719 : INFO : token count processed\n",
      "2020-12-23 02:45:21,720 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:21,721 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:21,721 : INFO : vocab #2480\n",
      "2020-12-23 02:45:21,723 : INFO : diff #set()\n",
      "2020-12-23 02:45:21,992 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:22,121 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.2752530424512702, 0.43951155381057677], [0.9769578203558922, 0.02304218], [1.0, 1.0], [3.169925001442312, 5.609710627339259, 5.68967743220937, 3.0899581965722014, 2.519752430767058, 0.07996680487011076]]\n",
      "2020-12-23 02:45:22,124 : INFO : Removed 0 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:22,124 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:22,126 : INFO : built Dictionary(171 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 1081 corpus positions)\n",
      "2020-12-23 02:45:22,168 : INFO : token count processed\n",
      "2020-12-23 02:45:22,170 : INFO : frequencies processed\n",
      "2020-12-23 02:45:22,298 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:22,299 : INFO : entropies processed\n",
      "2020-12-23 02:45:22,300 : INFO : extropies processed\n",
      "2020-12-23 02:45:22,301 : INFO : token count processed\n",
      "2020-12-23 02:45:22,303 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:22,304 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:22,305 : INFO : vocab #2480\n",
      "2020-12-23 02:45:22,306 : INFO : diff #set()\n",
      "2020-12-23 02:45:22,572 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:22,702 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.281482661136697, 0.4383114616798239], [0.9782559219747782, 0.021744078], [1.584962500721156, 1.1699250014423124], [3.169925001442312, 7.2441902753576075, 7.267762203095726, 3.1463530737041925, 4.097837201653414, 0.02357192773811878]]\n",
      "2020-12-23 02:45:22,704 : INFO : Removed 0 and 136 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:22,705 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:22,707 : INFO : built Dictionary(133 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 688 corpus positions)\n",
      "2020-12-23 02:45:22,733 : INFO : token count processed\n",
      "2020-12-23 02:45:22,735 : INFO : frequencies processed\n",
      "2020-12-23 02:45:22,862 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:22,863 : INFO : entropies processed\n",
      "2020-12-23 02:45:22,863 : INFO : extropies processed\n",
      "2020-12-23 02:45:22,865 : INFO : token count processed\n",
      "2020-12-23 02:45:22,866 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:22,867 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:22,868 : INFO : vocab #2480\n",
      "2020-12-23 02:45:22,869 : INFO : diff #set()\n",
      "2020-12-23 02:45:23,125 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:23,252 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.2797684663217945, 0.4386410351632822], [0.981889309361577, 0.01811069], [1.584962500721156, 1.1699250014423124], [3.169925001442312, 6.2567074920449475, 6.301047433114367, 3.1255850603728925, 3.131122431672055, 0.04433994106941963]]\n",
      "2020-12-23 02:45:23,255 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:23,256 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:23,257 : INFO : built Dictionary(81 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 198 corpus positions)\n",
      "2020-12-23 02:45:23,271 : INFO : token count processed\n",
      "2020-12-23 02:45:23,273 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:23,274 : INFO : frequencies processed\n",
      "2020-12-23 02:45:23,275 : INFO : token count processed\n",
      "2020-12-23 02:45:23,276 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:23,277 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:23,278 : INFO : vocab #2480\n",
      "2020-12-23 02:45:23,279 : INFO : diff #set()\n",
      "2020-12-23 02:45:23,538 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:23,665 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.2868221612157262, 0.4372880484367781], [0.9868031246587634, 0.013196875], [nan, nan], [3.169925001442312, 5.7680018917339435, 5.910003459345038, 3.0279234338312175, 2.740078457902726, 0.14200156761109461]]\n",
      "2020-12-23 02:45:23,668 : INFO : Removed 0 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:23,669 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:23,670 : INFO : built Dictionary(178 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 739 corpus positions)\n",
      "2020-12-23 02:45:23,717 : INFO : token count processed\n",
      "2020-12-23 02:45:23,719 : INFO : frequencies processed\n",
      "2020-12-23 02:45:23,845 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:23,845 : INFO : entropies processed\n",
      "2020-12-23 02:45:23,846 : INFO : extropies processed\n",
      "2020-12-23 02:45:23,847 : INFO : token count processed\n",
      "2020-12-23 02:45:23,848 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:23,848 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:23,849 : INFO : vocab #2480\n",
      "2020-12-23 02:45:23,850 : INFO : diff #set()\n",
      "2020-12-23 02:45:24,107 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:24,234 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.2812329348400366, 0.4383594435831348], [0.978697944432497, 0.021302056], [0.0, 0.0], [3.169925001442312, 6.846479111193757, 6.888338730770821, 3.128065381865248, 3.7184137293285087, 0.04185961957706397]]\n",
      "2020-12-23 02:45:24,237 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:24,238 : INFO : built Dictionary(29 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 42 corpus positions)\n",
      "2020-12-23 02:45:24,243 : INFO : token count processed\n",
      "2020-12-23 02:45:24,245 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:24,246 : INFO : frequencies processed\n",
      "2020-12-23 02:45:24,247 : INFO : token count processed\n",
      "2020-12-23 02:45:24,248 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:24,248 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:24,249 : INFO : vocab #2480\n",
      "2020-12-23 02:45:24,250 : INFO : diff #set()\n",
      "2020-12-23 02:45:24,505 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:24,632 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/fireException.py')[[1.297106966823011, 0.43533018463787043], [0.997196456650272, 0.0028035433], [nan, nan], [3.169925001442312, 4.165013816065912, 4.70137575590605, 2.6335630616021737, 1.5314507544637381, 0.5363619398401385]]\n",
      "2020-12-23 02:45:24,635 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:24,636 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:45:24,636 : INFO : built Dictionary(55 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 149 corpus positions)\n",
      "2020-12-23 02:45:24,646 : INFO : token count processed\n",
      "2020-12-23 02:45:24,649 : INFO : frequencies processed\n",
      "2020-12-23 02:45:24,776 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:24,777 : INFO : entropies processed\n",
      "2020-12-23 02:45:24,778 : INFO : extropies processed\n",
      "2020-12-23 02:45:24,779 : INFO : token count processed\n",
      "2020-12-23 02:45:24,780 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:24,781 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:24,781 : INFO : vocab #2480\n",
      "2020-12-23 02:45:24,782 : INFO : diff #set()\n",
      "2020-12-23 02:45:25,041 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:25,168 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.229408136437041, 0.448549542659409], [0.9156309142708778, 0.084369086], [1.0, 1.0], [3.169925001442312, 5.449968864419248, 5.5988017303830695, 3.02109213547849, 2.4288767289407573, 0.14883286596382117]]\n",
      "2020-12-23 02:45:25,171 : INFO : Removed 0 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:25,172 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:25,174 : INFO : built Dictionary(148 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 586 corpus positions)\n",
      "2020-12-23 02:45:25,212 : INFO : token count processed\n",
      "2020-12-23 02:45:25,214 : INFO : frequencies processed\n",
      "2020-12-23 02:45:25,342 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:25,343 : INFO : entropies processed\n",
      "2020-12-23 02:45:25,344 : INFO : extropies processed\n",
      "2020-12-23 02:45:25,345 : INFO : token count processed\n",
      "2020-12-23 02:45:25,346 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:25,347 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:25,348 : INFO : vocab #2480\n",
      "2020-12-23 02:45:25,349 : INFO : diff #set()\n",
      "2020-12-23 02:45:25,614 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:25,742 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.275522058809298, 0.43945959395500894], [0.9704025574028492, 0.029597443], [1.0, 1.0], [3.169925001442312, 6.530294129310484, 6.577423023706407, 3.1227961070463888, 3.407498022264095, 0.04712889439592338]]\n",
      "2020-12-23 02:45:25,745 : INFO : Removed 0 and 142 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:25,746 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:25,747 : INFO : built Dictionary(122 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 602 corpus positions)\n",
      "2020-12-23 02:45:25,786 : INFO : token count processed\n",
      "2020-12-23 02:45:25,788 : INFO : frequencies processed\n",
      "2020-12-23 02:45:25,916 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:25,916 : INFO : entropies processed\n",
      "2020-12-23 02:45:25,917 : INFO : extropies processed\n",
      "2020-12-23 02:45:25,918 : INFO : token count processed\n",
      "2020-12-23 02:45:25,919 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:25,920 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:25,921 : INFO : vocab #2480\n",
      "2020-12-23 02:45:25,922 : INFO : diff #set()\n",
      "2020-12-23 02:45:26,181 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:26,309 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.2752215633057595, 0.43951763473402583], [0.968360360711813, 0.03163964], [0.0, 0.0], [3.169925001442312, 6.470272233491701, 6.521039027394722, 3.119158207539292, 3.35111402595241, 0.05076679390302097]]\n",
      "2020-12-23 02:45:26,312 : INFO : Removed 0 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:26,313 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:26,314 : INFO : built Dictionary(123 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 391 corpus positions)\n",
      "2020-12-23 02:45:26,350 : INFO : token count processed\n",
      "2020-12-23 02:45:26,354 : INFO : frequencies processed\n",
      "2020-12-23 02:45:26,483 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:26,483 : INFO : entropies processed\n",
      "2020-12-23 02:45:26,484 : INFO : extropies processed\n",
      "2020-12-23 02:45:26,485 : INFO : token count processed\n",
      "2020-12-23 02:45:26,486 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:26,487 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:26,488 : INFO : vocab #2480\n",
      "2020-12-23 02:45:26,489 : INFO : diff #set()\n",
      "2020-12-23 02:45:26,757 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:26,885 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.2834950825035338, 0.4379251821745284], [0.9822491854429245, 0.017750815], [0.0, 0.0], [3.169925001442312, 6.550038223589686, 6.619973265654869, 3.0999899593771296, 3.4500482642125565, 0.06993504206518253]]\n",
      "2020-12-23 02:45:26,887 : INFO : Removed 0 and 70 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:26,888 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:26,889 : INFO : built Dictionary(77 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 307 corpus positions)\n",
      "2020-12-23 02:45:26,904 : INFO : token count processed\n",
      "2020-12-23 02:45:26,906 : INFO : frequencies processed\n",
      "2020-12-23 02:45:27,045 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:27,045 : INFO : entropies processed\n",
      "2020-12-23 02:45:27,046 : INFO : extropies processed\n",
      "2020-12-23 02:45:27,048 : INFO : token count processed\n",
      "2020-12-23 02:45:27,048 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:27,049 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:27,050 : INFO : vocab #2480\n",
      "2020-12-23 02:45:27,051 : INFO : diff #set()\n",
      "2020-12-23 02:45:27,318 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:27,446 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.2612064824806084, 0.4422417889510786], [0.9613961614668369, 0.03860384], [1.0, 1.0], [3.169925001442312, 5.860525481261383, 5.945045288274954, 3.085405194428742, 2.7751202868326414, 0.08451980701357087]]\n",
      "2020-12-23 02:45:27,448 : INFO : Removed 0 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:27,449 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:27,450 : INFO : built Dictionary(50 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 345 corpus positions)\n",
      "2020-12-23 02:45:27,468 : INFO : token count processed\n",
      "2020-12-23 02:45:27,472 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:27,473 : INFO : frequencies processed\n",
      "2020-12-23 02:45:27,474 : INFO : token count processed\n",
      "2020-12-23 02:45:27,475 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:27,476 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:27,477 : INFO : vocab #2480\n",
      "2020-12-23 02:45:27,478 : INFO : diff #set()\n",
      "2020-12-23 02:45:27,736 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:27,866 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.293352184529724, 0.43604292735573036], [0.9854965768754482, 0.014503423], [nan, nan], [3.169925001442312, 5.945464049777852, 6.024188894055551, 3.0912001571646135, 2.854263892613239, 0.07872484427769866]]\n",
      "2020-12-23 02:45:27,869 : INFO : Removed 0 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:27,870 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:27,871 : INFO : built Dictionary(195 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 892 corpus positions)\n",
      "2020-12-23 02:45:27,913 : INFO : token count processed\n",
      "2020-12-23 02:45:27,916 : INFO : frequencies processed\n",
      "2020-12-23 02:45:28,049 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:28,050 : INFO : entropies processed\n",
      "2020-12-23 02:45:28,051 : INFO : extropies processed\n",
      "2020-12-23 02:45:28,052 : INFO : token count processed\n",
      "2020-12-23 02:45:28,054 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:28,055 : INFO : alphabet_target #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:45:28,056 : INFO : vocab #2480\n",
      "2020-12-23 02:45:28,057 : INFO : diff #set()\n",
      "2020-12-23 02:45:28,315 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:28,444 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.2856677038318394, 0.4375089162451463], [0.9874955378472805, 0.012504462], [1.584962500721156, 1.1699250014423124], [3.169925001442312, 6.811563897304216, 6.845760428779167, 3.1357284699673613, 3.675835427336855, 0.03419653147495083]]\n",
      "2020-12-23 02:45:28,446 : INFO : Removed 0 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:28,447 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:28,450 : INFO : built Dictionary(216 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 987 corpus positions)\n",
      "2020-12-23 02:45:28,506 : INFO : token count processed\n",
      "2020-12-23 02:45:28,514 : INFO : frequencies processed\n",
      "2020-12-23 02:45:28,643 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:28,643 : INFO : entropies processed\n",
      "2020-12-23 02:45:28,644 : INFO : extropies processed\n",
      "2020-12-23 02:45:28,645 : INFO : token count processed\n",
      "2020-12-23 02:45:28,646 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:28,647 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:28,647 : INFO : vocab #2480\n",
      "2020-12-23 02:45:28,648 : INFO : diff #set()\n",
      "2020-12-23 02:45:28,917 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:29,045 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.2776561439927008, 0.43904783548539217], [0.9791547637432814, 0.020845236], [1.584962500721156, 1.1699250014423124], [3.169925001442312, 7.502034948968415, 7.52627059839466, 3.145689352016067, 4.356345596952348, 0.02423564942624523]]\n",
      "2020-12-23 02:45:29,048 : INFO : Removed 0 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:29,049 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:29,052 : INFO : built Dictionary(259 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 1560 corpus positions)\n",
      "2020-12-23 02:45:29,131 : INFO : token count processed\n",
      "2020-12-23 02:45:29,135 : INFO : frequencies processed\n",
      "2020-12-23 02:45:29,264 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:29,265 : INFO : entropies processed\n",
      "2020-12-23 02:45:29,266 : INFO : extropies processed\n",
      "2020-12-23 02:45:29,267 : INFO : token count processed\n",
      "2020-12-23 02:45:29,268 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:29,269 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:29,270 : INFO : vocab #2480\n",
      "2020-12-23 02:45:29,271 : INFO : diff #set()\n",
      "2020-12-23 02:45:29,531 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:29,659 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.2091948441410816, 0.4526536003159977], [0.8523613810539246, 0.14763862], [2.0, 1.2451124978365313], [3.169925001442312, 7.39180093901977, 7.4039481515941565, 3.157777788867926, 4.234023150151844, 0.012147212574386224]]\n",
      "2020-12-23 02:45:29,662 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:29,663 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:29,664 : INFO : built Dictionary(43 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 115 corpus positions)\n",
      "2020-12-23 02:45:29,672 : INFO : token count processed\n",
      "2020-12-23 02:45:29,674 : INFO : frequencies processed\n",
      "2020-12-23 02:45:29,802 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:29,803 : INFO : entropies processed\n",
      "2020-12-23 02:45:29,804 : INFO : extropies processed\n",
      "2020-12-23 02:45:29,805 : INFO : token count processed\n",
      "2020-12-23 02:45:29,806 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:29,807 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:29,808 : INFO : vocab #2480\n",
      "2020-12-23 02:45:29,809 : INFO : diff #set()\n",
      "2020-12-23 02:45:30,066 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:30,194 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.1529671514172704, 0.46447527048506665], [0.837629958987236, 0.16237004], [2.0, 1.2451124978365313], [3.169925001442312, 4.927561309677364, 5.09046877308136, 3.0070175380383155, 1.920543771639048, 0.16290746340399664]]\n",
      "2020-12-23 02:45:30,196 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:30,197 : INFO : built Dictionary(15 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 16 corpus positions)\n",
      "2020-12-23 02:45:30,200 : INFO : token count processed\n",
      "2020-12-23 02:45:30,202 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:30,203 : INFO : frequencies processed\n",
      "2020-12-23 02:45:30,204 : INFO : token count processed\n",
      "2020-12-23 02:45:30,205 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:30,205 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:30,206 : INFO : vocab #2480\n",
      "2020-12-23 02:45:30,207 : INFO : diff #set()\n",
      "2020-12-23 02:45:30,465 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:30,593 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.275915081981708, 0.4393837045665473], [0.9865619763731956, 0.013438024], [nan, nan], [3.169925001442312, 2.5216406363433186, 3.875, 1.8165656377856312, 0.7050749985576878, 1.3533593636566814]]\n",
      "2020-12-23 02:45:30,597 : INFO : Removed 0 and 530 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:30,598 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:30,600 : INFO : built Dictionary(336 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 2888 corpus positions)\n",
      "2020-12-23 02:45:30,709 : INFO : token count processed\n",
      "2020-12-23 02:45:30,711 : INFO : frequencies processed\n",
      "2020-12-23 02:45:30,838 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:30,839 : INFO : entropies processed\n",
      "2020-12-23 02:45:30,839 : INFO : extropies processed\n",
      "2020-12-23 02:45:30,841 : INFO : token count processed\n",
      "2020-12-23 02:45:30,842 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:30,843 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:30,843 : INFO : vocab #2480\n",
      "2020-12-23 02:45:30,844 : INFO : diff #set()\n",
      "2020-12-23 02:45:31,102 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:31,231 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.2774646752790466, 0.43908474667229463], [0.9739242866635323, 0.026075713], [0.0, 0.0], [3.169925001442312, 7.480007711014331, 7.4934368700169545, 3.1564958424396883, 4.323511868574642, 0.01342915900262387]]\n",
      "2020-12-23 02:45:31,233 : INFO : Removed 0 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:31,234 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:31,236 : INFO : built Dictionary(212 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 1037 corpus positions)\n",
      "2020-12-23 02:45:31,287 : INFO : token count processed\n",
      "2020-12-23 02:45:31,290 : INFO : frequencies processed\n",
      "2020-12-23 02:45:31,418 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:31,418 : INFO : entropies processed\n",
      "2020-12-23 02:45:31,419 : INFO : extropies processed\n",
      "2020-12-23 02:45:31,420 : INFO : token count processed\n",
      "2020-12-23 02:45:31,421 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:31,422 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:31,423 : INFO : vocab #2480\n",
      "2020-12-23 02:45:31,424 : INFO : diff #set()\n",
      "2020-12-23 02:45:31,695 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:31,825 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.2762458394131437, 0.43931985846388966], [0.9715998750180006, 0.028400125], [1.0, 1.0], [3.169925001442312, 7.131331012509435, 7.159244849280666, 3.1420111646710804, 3.9893198478383542, 0.027913836771231715]]\n",
      "2020-12-23 02:45:31,828 : INFO : Removed 0 and 166 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:45:31,829 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:31,830 : INFO : built Dictionary(202 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 856 corpus positions)\n",
      "2020-12-23 02:45:31,878 : INFO : token count processed\n",
      "2020-12-23 02:45:31,880 : INFO : frequencies processed\n",
      "2020-12-23 02:45:32,013 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:32,014 : INFO : entropies processed\n",
      "2020-12-23 02:45:32,014 : INFO : extropies processed\n",
      "2020-12-23 02:45:32,016 : INFO : token count processed\n",
      "2020-12-23 02:45:32,017 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:32,018 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:32,019 : INFO : vocab #2480\n",
      "2020-12-23 02:45:32,020 : INFO : diff #set()\n",
      "2020-12-23 02:45:32,289 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:32,416 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.2861324985156022, 0.4374199660996487], [0.9844002826139331, 0.015599717], [0.0, 0.0], [3.169925001442312, 7.203742744794778, 7.2378158696007535, 3.1358518766363357, 4.067890868158441, 0.034073124805975574]]\n",
      "2020-12-23 02:45:32,418 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:32,419 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:32,420 : INFO : built Dictionary(58 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 188 corpus positions)\n",
      "2020-12-23 02:45:32,431 : INFO : token count processed\n",
      "2020-12-23 02:45:32,434 : INFO : frequencies processed\n",
      "2020-12-23 02:45:32,561 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:32,562 : INFO : entropies processed\n",
      "2020-12-23 02:45:32,563 : INFO : extropies processed\n",
      "2020-12-23 02:45:32,564 : INFO : token count processed\n",
      "2020-12-23 02:45:32,565 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:32,566 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:32,566 : INFO : vocab #2480\n",
      "2020-12-23 02:45:32,567 : INFO : diff #set()\n",
      "2020-12-23 02:45:32,826 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:32,954 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.1930456653459858, 0.45598685690944557], [0.8908616751432419, 0.109138325], [1.584962500721156, 1.1699250014423124], [3.169925001442312, 5.195502554608948, 5.321582069936379, 3.043845486114882, 2.1516570684940666, 0.12607951532743034]]\n",
      "2020-12-23 02:45:32,956 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:32,957 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:32,958 : INFO : built Dictionary(64 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 249 corpus positions)\n",
      "2020-12-23 02:45:32,977 : INFO : token count processed\n",
      "2020-12-23 02:45:32,981 : INFO : frequencies processed\n",
      "2020-12-23 02:45:33,113 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:33,114 : INFO : entropies processed\n",
      "2020-12-23 02:45:33,114 : INFO : extropies processed\n",
      "2020-12-23 02:45:33,116 : INFO : token count processed\n",
      "2020-12-23 02:45:33,117 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:33,118 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:33,118 : INFO : vocab #2480\n",
      "2020-12-23 02:45:33,119 : INFO : diff #set()\n",
      "2020-12-23 02:45:33,390 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:33,518 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.2349224908944856, 0.44744281024250143], [0.937576424330473, 0.062423576], [1.584962500721156, 1.1699250014423124], [3.169925001442312, 5.32027245610305, 5.433043500313342, 3.0571539572320194, 2.2631184988710302, 0.11277104421029271]]\n",
      "2020-12-23 02:45:33,520 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:33,521 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:33,522 : INFO : built Dictionary(167 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 489 corpus positions)\n",
      "2020-12-23 02:45:33,566 : INFO : token count processed\n",
      "2020-12-23 02:45:33,568 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:33,569 : INFO : frequencies processed\n",
      "2020-12-23 02:45:33,570 : INFO : token count processed\n",
      "2020-12-23 02:45:33,571 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:33,572 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:33,573 : INFO : vocab #2480\n",
      "2020-12-23 02:45:33,574 : INFO : diff #set()\n",
      "2020-12-23 02:45:33,826 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:33,952 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.2885039422225555, 0.436966693196437], [0.9884823393076658, 0.011517661], [nan, nan], [3.169925001442312, 6.898202761357263, 6.958091717788545, 3.1100360450110305, 3.7881667163462325, 0.05988895643128167]]\n",
      "2020-12-23 02:45:33,955 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:33,956 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:33,957 : INFO : built Dictionary(131 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 511 corpus positions)\n",
      "2020-12-23 02:45:33,987 : INFO : token count processed\n",
      "2020-12-23 02:45:33,991 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:33,992 : INFO : frequencies processed\n",
      "2020-12-23 02:45:33,994 : INFO : token count processed\n",
      "2020-12-23 02:45:33,995 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:33,996 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:33,997 : INFO : vocab #2480\n",
      "2020-12-23 02:45:33,998 : INFO : diff #set()\n",
      "2020-12-23 02:45:34,257 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:34,387 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.2868459770517955, 0.43728349440009123], [0.9962855640333146, 0.003714436], [nan, nan], [3.169925001442312, 6.388500481644799, 6.457479826491349, 3.100945656595762, 3.287554825049037, 0.0689793448465501]]\n",
      "2020-12-23 02:45:34,389 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:34,390 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:34,392 : INFO : built Dictionary(49 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 175 corpus positions)\n",
      "2020-12-23 02:45:34,406 : INFO : token count processed\n",
      "2020-12-23 02:45:34,408 : INFO : frequencies processed\n",
      "2020-12-23 02:45:34,535 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:34,536 : INFO : entropies processed\n",
      "2020-12-23 02:45:34,537 : INFO : extropies processed\n",
      "2020-12-23 02:45:34,538 : INFO : token count processed\n",
      "2020-12-23 02:45:34,539 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:34,539 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:34,540 : INFO : vocab #2480\n",
      "2020-12-23 02:45:34,541 : INFO : diff #set()\n",
      "2020-12-23 02:45:34,798 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:34,926 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.201734239659279, 0.4541874227993798], [0.9058022275567055, 0.09419777], [1.584962500721156, 1.1699250014423124], [3.169925001442312, 4.8191513650620195, 4.974058971446751, 3.0150173950575807, 1.8041339700044388, 0.15490760638473144]]\n",
      "2020-12-23 02:45:34,928 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:34,929 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:34,930 : INFO : built Dictionary(55 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 230 corpus positions)\n",
      "2020-12-23 02:45:34,940 : INFO : token count processed\n",
      "2020-12-23 02:45:34,942 : INFO : frequencies processed\n",
      "2020-12-23 02:45:35,070 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:35,070 : INFO : entropies processed\n",
      "2020-12-23 02:45:35,071 : INFO : extropies processed\n",
      "2020-12-23 02:45:35,072 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:45:35,073 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:35,074 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:35,075 : INFO : vocab #2480\n",
      "2020-12-23 02:45:35,076 : INFO : diff #set()\n",
      "2020-12-23 02:45:35,334 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:35,461 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.241994602747001, 0.4460314038110312], [0.9463688880205154, 0.053631112], [1.0, 1.0], [3.169925001442312, 5.062480936779194, 5.201968977177185, 3.0304369610443205, 2.032043975734873, 0.13948804039799167]]\n",
      "2020-12-23 02:45:35,464 : INFO : Removed 0 and 223 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:35,465 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:35,467 : INFO : built Dictionary(245 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 1784 corpus positions)\n",
      "2020-12-23 02:45:35,531 : INFO : token count processed\n",
      "2020-12-23 02:45:35,535 : INFO : frequencies processed\n",
      "2020-12-23 02:45:35,661 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:35,662 : INFO : entropies processed\n",
      "2020-12-23 02:45:35,663 : INFO : extropies processed\n",
      "2020-12-23 02:45:35,664 : INFO : token count processed\n",
      "2020-12-23 02:45:35,665 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:35,666 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:35,666 : INFO : vocab #2480\n",
      "2020-12-23 02:45:35,667 : INFO : diff #set()\n",
      "2020-12-23 02:45:35,933 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:36,062 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.2842122123428692, 0.43778769529225164], [0.9872389230877161, 0.012761077], [1.584962500721156, 1.1699250014423124], [3.169925001442312, 7.185085743102134, 7.205151445142711, 3.1498592994017365, 4.035226443700399, 0.02006570204057656]]\n",
      "2020-12-23 02:45:36,064 : INFO : Removed 0 and 148 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:36,065 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:36,067 : INFO : built Dictionary(163 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 759 corpus positions)\n",
      "2020-12-23 02:45:36,103 : INFO : token count processed\n",
      "2020-12-23 02:45:36,106 : INFO : frequencies processed\n",
      "2020-12-23 02:45:36,233 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:36,233 : INFO : entropies processed\n",
      "2020-12-23 02:45:36,234 : INFO : extropies processed\n",
      "2020-12-23 02:45:36,235 : INFO : token count processed\n",
      "2020-12-23 02:45:36,236 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:36,237 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:36,237 : INFO : vocab #2480\n",
      "2020-12-23 02:45:36,238 : INFO : diff #set()\n",
      "2020-12-23 02:45:36,494 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:36,623 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.2778356383943477, 0.43901323833220146], [0.9794657882303, 0.020534212], [1.0, 1.0], [3.169925001442312, 6.591225336124281, 6.630220043516395, 3.1309302940501986, 3.4602950420740832, 0.038994707392114414]]\n",
      "2020-12-23 02:45:36,626 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:36,626 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:36,627 : INFO : built Dictionary(41 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 97 corpus positions)\n",
      "2020-12-23 02:45:36,635 : INFO : token count processed\n",
      "2020-12-23 02:45:36,637 : INFO : frequencies processed\n",
      "2020-12-23 02:45:36,765 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:36,766 : INFO : entropies processed\n",
      "2020-12-23 02:45:36,767 : INFO : extropies processed\n",
      "2020-12-23 02:45:36,768 : INFO : token count processed\n",
      "2020-12-23 02:45:36,769 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:36,770 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:36,770 : INFO : vocab #2480\n",
      "2020-12-23 02:45:36,771 : INFO : diff #set()\n",
      "2020-12-23 02:45:37,030 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:37,158 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.2124904887574826, 0.45197934412888363], [0.901958517730236, 0.09804148], [1.0, 1.0], [3.169925001442312, 4.7032114441396695, 4.95144893263138, 2.9216875129506015, 1.781523931189068, 0.24823748849171068]]\n",
      "2020-12-23 02:45:37,161 : INFO : Removed 0 and 9 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:37,162 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:37,163 : INFO : built Dictionary(121 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 337 corpus positions)\n",
      "2020-12-23 02:45:37,190 : INFO : token count processed\n",
      "2020-12-23 02:45:37,199 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:37,200 : INFO : frequencies processed\n",
      "2020-12-23 02:45:37,202 : INFO : token count processed\n",
      "2020-12-23 02:45:37,202 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:37,203 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:37,204 : INFO : vocab #2480\n",
      "2020-12-23 02:45:37,205 : INFO : diff #set()\n",
      "2020-12-23 02:45:37,468 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:37,596 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.2824495882298084, 0.4381257773038337], [0.9924440733157098, 0.0075559267], [nan, nan], [3.169925001442312, 6.14228447828618, 6.238946314122726, 3.0732631656057645, 3.0690213126804142, 0.09666183583654675]]\n",
      "2020-12-23 02:45:37,599 : INFO : Removed 0 and 184 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:37,600 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:37,602 : INFO : built Dictionary(257 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 1127 corpus positions)\n",
      "2020-12-23 02:45:37,675 : INFO : token count processed\n",
      "2020-12-23 02:45:37,677 : INFO : frequencies processed\n",
      "2020-12-23 02:45:37,805 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:37,805 : INFO : entropies processed\n",
      "2020-12-23 02:45:37,806 : INFO : extropies processed\n",
      "2020-12-23 02:45:37,807 : INFO : token count processed\n",
      "2020-12-23 02:45:37,808 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:37,809 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:37,809 : INFO : vocab #2480\n",
      "2020-12-23 02:45:37,810 : INFO : diff #set()\n",
      "2020-12-23 02:45:38,066 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:38,194 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.2784584447855578, 0.43889323603359254], [0.9806459527462721, 0.019354047], [1.0, 1.0], [3.169925001442312, 7.450178124335845, 7.476372755730106, 3.1437303700480523, 4.3064477542877935, 0.026194631394260703]]\n",
      "2020-12-23 02:45:38,197 : INFO : Removed 0 and 16 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:38,198 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:38,199 : INFO : built Dictionary(58 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 175 corpus positions)\n",
      "2020-12-23 02:45:38,209 : INFO : token count processed\n",
      "2020-12-23 02:45:38,212 : INFO : frequencies processed\n",
      "2020-12-23 02:45:38,340 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:38,340 : INFO : entropies processed\n",
      "2020-12-23 02:45:38,341 : INFO : extropies processed\n",
      "2020-12-23 02:45:38,342 : INFO : token count processed\n",
      "2020-12-23 02:45:38,343 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:38,344 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:38,345 : INFO : vocab #2480\n",
      "2020-12-23 02:45:38,346 : INFO : diff #set()\n",
      "2020-12-23 02:45:38,606 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:38,734 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.1862904916780357, 0.4573957595326107], [0.8927549719810486, 0.10724503], [1.584962500721156, 1.1699250014423124], [3.169925001442312, 5.20665021947654, 5.3316173410121435, 3.0449578799067085, 2.1616923395698313, 0.12496712153560363]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:45:38,737 : INFO : Removed 0 and 89 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:38,737 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:38,738 : INFO : built Dictionary(129 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 513 corpus positions)\n",
      "2020-12-23 02:45:38,774 : INFO : token count processed\n",
      "2020-12-23 02:45:38,776 : INFO : frequencies processed\n",
      "2020-12-23 02:45:38,903 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:38,903 : INFO : entropies processed\n",
      "2020-12-23 02:45:38,904 : INFO : extropies processed\n",
      "2020-12-23 02:45:38,906 : INFO : token count processed\n",
      "2020-12-23 02:45:38,907 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:38,907 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:38,908 : INFO : vocab #2480\n",
      "2020-12-23 02:45:38,909 : INFO : diff #set()\n",
      "2020-12-23 02:45:39,174 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:39,302 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.2810217472439123, 0.43840002893802704], [0.9789118524640799, 0.021088148], [0.0, 0.0], [3.169925001442312, 6.524718477352, 6.5806274141841525, 3.1140160646101593, 3.4107024127418404, 0.05590893683215281]]\n",
      "2020-12-23 02:45:39,305 : INFO : Removed 0 and 8 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:39,306 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:39,307 : INFO : built Dictionary(64 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 220 corpus positions)\n",
      "2020-12-23 02:45:39,325 : INFO : token count processed\n",
      "2020-12-23 02:45:39,330 : INFO : frequencies processed\n",
      "2020-12-23 02:45:39,466 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:39,466 : INFO : entropies processed\n",
      "2020-12-23 02:45:39,467 : INFO : extropies processed\n",
      "2020-12-23 02:45:39,468 : INFO : token count processed\n",
      "2020-12-23 02:45:39,469 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:39,470 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:39,471 : INFO : vocab #2480\n",
      "2020-12-23 02:45:39,472 : INFO : diff #set()\n",
      "2020-12-23 02:45:39,737 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:39,865 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.2194430049997758, 0.45056349622282865], [0.9191838875412941, 0.08081611], [1.584962500721156, 1.1699250014423124], [3.169925001442312, 5.321859380715434, 5.436677695288852, 3.0551066868688945, 2.2667526938465397, 0.11481831457341762]]\n",
      "2020-12-23 02:45:39,868 : INFO : Removed 0 and 119 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:39,869 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:39,870 : INFO : built Dictionary(144 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 838 corpus positions)\n",
      "2020-12-23 02:45:39,904 : INFO : token count processed\n",
      "2020-12-23 02:45:39,907 : INFO : frequencies processed\n",
      "2020-12-23 02:45:40,037 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:40,038 : INFO : entropies processed\n",
      "2020-12-23 02:45:40,039 : INFO : extropies processed\n",
      "2020-12-23 02:45:40,040 : INFO : token count processed\n",
      "2020-12-23 02:45:40,041 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:40,042 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:40,043 : INFO : vocab #2480\n",
      "2020-12-23 02:45:40,045 : INFO : diff #set()\n",
      "2020-12-23 02:45:40,304 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:40,431 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.2817499910384327, 0.43826010909499175], [0.9833440631628036, 0.016655937], [1.0, 1.0], [3.169925001442312, 6.500767808767801, 6.540096272507379, 3.1305965377027345, 3.3701712710650664, 0.03932846373957766]]\n",
      "2020-12-23 02:45:40,434 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:40,435 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:40,436 : INFO : built Dictionary(39 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 60 corpus positions)\n",
      "2020-12-23 02:45:40,443 : INFO : token count processed\n",
      "2020-12-23 02:45:40,445 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:40,446 : INFO : frequencies processed\n",
      "2020-12-23 02:45:40,447 : INFO : token count processed\n",
      "2020-12-23 02:45:40,448 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:40,448 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:40,449 : INFO : vocab #2480\n",
      "2020-12-23 02:45:40,450 : INFO : diff #set()\n",
      "2020-12-23 02:45:40,707 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:40,833 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.2803158555158662, 0.4385357395034094], [0.9915078962221742, 0.008492104], [nan, nan], [3.169925001442312, 4.736228843383063, 5.108787083670794, 2.7973667611545805, 1.938862082228482, 0.37255824028773166]]\n",
      "2020-12-23 02:45:40,836 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:40,837 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:40,838 : INFO : built Dictionary(101 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 401 corpus positions)\n",
      "2020-12-23 02:45:40,867 : INFO : token count processed\n",
      "2020-12-23 02:45:40,872 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:40,874 : INFO : frequencies processed\n",
      "2020-12-23 02:45:40,876 : INFO : token count processed\n",
      "2020-12-23 02:45:40,877 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:40,878 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:40,879 : INFO : vocab #2480\n",
      "2020-12-23 02:45:40,880 : INFO : diff #set()\n",
      "2020-12-23 02:45:41,133 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:41,260 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.281591881755002, 0.4382904795535998], [0.982610996812582, 0.017389003], [nan, nan], [3.169925001442312, 5.788442787590127, 5.882464850052144, 3.0759029389802963, 2.7125398486098318, 0.09402206246201672]]\n",
      "2020-12-23 02:45:41,263 : INFO : Removed 0 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:41,264 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:41,265 : INFO : built Dictionary(58 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 87 corpus positions)\n",
      "2020-12-23 02:45:41,281 : INFO : token count processed\n",
      "2020-12-23 02:45:41,283 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:41,284 : INFO : frequencies processed\n",
      "2020-12-23 02:45:41,285 : INFO : token count processed\n",
      "2020-12-23 02:45:41,286 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:41,287 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:41,288 : INFO : vocab #2480\n",
      "2020-12-23 02:45:41,289 : INFO : diff #set()\n",
      "2020-12-23 02:45:41,549 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:41,676 : INFO : Computed distances or similarities ('292', 'sacp-python-common/setup.py')[[1.2673543787003636, 0.4410426572017362], [0.9819468855857849, 0.018053114], [nan, nan], [3.169925001442312, 5.370004292053436, 5.6157792091625796, 2.92415008433317, 2.4458542077202674, 0.24577491710914323]]\n",
      "2020-12-23 02:45:41,679 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:41,680 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:41,682 : INFO : built Dictionary(82 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 332 corpus positions)\n",
      "2020-12-23 02:45:41,703 : INFO : token count processed\n",
      "2020-12-23 02:45:41,706 : INFO : frequencies processed\n",
      "2020-12-23 02:45:41,832 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:41,833 : INFO : entropies processed\n",
      "2020-12-23 02:45:41,834 : INFO : extropies processed\n",
      "2020-12-23 02:45:41,835 : INFO : token count processed\n",
      "2020-12-23 02:45:41,836 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:41,838 : INFO : alphabet_target #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:45:41,839 : INFO : vocab #2480\n",
      "2020-12-23 02:45:41,840 : INFO : diff #set()\n",
      "2020-12-23 02:45:42,108 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:42,236 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.2636039960542709, 0.44177338516061915], [0.9655439481139183, 0.034456052], [1.0, 1.0], [3.169925001442312, 5.695663584743922, 5.7864603996619515, 3.079128186524283, 2.6165353982196393, 0.09079681491802916]]\n",
      "2020-12-23 02:45:42,239 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:42,240 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:42,241 : INFO : built Dictionary(45 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 107 corpus positions)\n",
      "2020-12-23 02:45:42,254 : INFO : token count processed\n",
      "2020-12-23 02:45:42,257 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:42,257 : INFO : frequencies processed\n",
      "2020-12-23 02:45:42,258 : INFO : token count processed\n",
      "2020-12-23 02:45:42,259 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:42,260 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:42,261 : INFO : vocab #2480\n",
      "2020-12-23 02:45:42,262 : INFO : diff #set()\n",
      "2020-12-23 02:45:42,517 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:42,644 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.2792138891099005, 0.4387477650860267], [0.9784537702798843, 0.02154623], [nan, nan], [3.169925001442312, 4.9004417692112465, 5.166106637327175, 2.9042601333263836, 1.996181635884863, 0.2656648681159286]]\n",
      "2020-12-23 02:45:42,647 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:42,648 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:42,648 : INFO : built Dictionary(43 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 114 corpus positions)\n",
      "2020-12-23 02:45:42,656 : INFO : token count processed\n",
      "2020-12-23 02:45:42,658 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:42,659 : INFO : frequencies processed\n",
      "2020-12-23 02:45:42,660 : INFO : token count processed\n",
      "2020-12-23 02:45:42,661 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:42,661 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:42,662 : INFO : vocab #2480\n",
      "2020-12-23 02:45:42,663 : INFO : diff #set()\n",
      "2020-12-23 02:45:42,920 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:43,048 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.28539278885872, 0.437561545164138], [0.9879877008497715, 0.012012299], [nan, nan], [3.169925001442312, 4.778624108914332, 5.046121587353433, 2.9024275230032117, 1.8761965859111207, 0.26749747843910043]]\n",
      "2020-12-23 02:45:43,051 : INFO : Removed 0 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:43,051 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:43,052 : INFO : built Dictionary(44 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 145 corpus positions)\n",
      "2020-12-23 02:45:43,060 : INFO : token count processed\n",
      "2020-12-23 02:45:43,062 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:43,063 : INFO : frequencies processed\n",
      "2020-12-23 02:45:43,064 : INFO : token count processed\n",
      "2020-12-23 02:45:43,065 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:43,066 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:43,066 : INFO : vocab #2480\n",
      "2020-12-23 02:45:43,068 : INFO : diff #set()\n",
      "2020-12-23 02:45:43,326 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:43,454 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.2759735157090977, 0.4393724237553098], [0.983096569776535, 0.01690343], [nan, nan], [3.169925001442312, 4.773880192225086, 5.003212160344094, 2.940593033323304, 1.8332871589017818, 0.22933196811900824]]\n",
      "2020-12-23 02:45:43,457 : INFO : Removed 0 and 346 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:43,458 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:43,459 : INFO : built Dictionary(153 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 1970 corpus positions)\n",
      "2020-12-23 02:45:43,500 : INFO : token count processed\n",
      "2020-12-23 02:45:43,504 : INFO : frequencies processed\n",
      "2020-12-23 02:45:43,632 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:43,633 : INFO : entropies processed\n",
      "2020-12-23 02:45:43,633 : INFO : extropies processed\n",
      "2020-12-23 02:45:43,635 : INFO : token count processed\n",
      "2020-12-23 02:45:43,636 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:43,637 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:43,637 : INFO : vocab #2480\n",
      "2020-12-23 02:45:43,638 : INFO : diff #set()\n",
      "2020-12-23 02:45:43,907 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:44,035 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.2713499051427875, 0.4402668200684542], [0.9663357920944691, 0.033664208], [1.0, 1.0], [3.169925001442312, 6.620773041953877, 6.639762017192491, 3.1509360262036976, 3.469837015750179, 0.018988975238614536]]\n",
      "2020-12-23 02:45:44,038 : INFO : Removed 0 and 132 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:44,039 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:44,040 : INFO : built Dictionary(83 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 533 corpus positions)\n",
      "2020-12-23 02:45:44,063 : INFO : token count processed\n",
      "2020-12-23 02:45:44,065 : INFO : frequencies processed\n",
      "2020-12-23 02:45:44,192 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:44,192 : INFO : entropies processed\n",
      "2020-12-23 02:45:44,193 : INFO : extropies processed\n",
      "2020-12-23 02:45:44,194 : INFO : token count processed\n",
      "2020-12-23 02:45:44,195 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:44,196 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:44,196 : INFO : vocab #2480\n",
      "2020-12-23 02:45:44,197 : INFO : diff #set()\n",
      "2020-12-23 02:45:44,451 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:44,578 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.229552453465642, 0.44852050843010605], [0.9326826184988022, 0.06731738], [1.0, 1.0], [3.169925001442312, 5.828370634755606, 5.882897114888002, 3.115398521309916, 2.71297211344569, 0.05452648013239614]]\n",
      "2020-12-23 02:45:44,580 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:44,581 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:44,582 : INFO : built Dictionary(86 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 296 corpus positions)\n",
      "2020-12-23 02:45:44,604 : INFO : token count processed\n",
      "2020-12-23 02:45:44,609 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:44,611 : INFO : frequencies processed\n",
      "2020-12-23 02:45:44,613 : INFO : token count processed\n",
      "2020-12-23 02:45:44,615 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:44,616 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:44,617 : INFO : vocab #2480\n",
      "2020-12-23 02:45:44,618 : INFO : diff #set()\n",
      "2020-12-23 02:45:44,876 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:45,004 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.276749996720236, 0.43922257667312903], [0.9773852843791246, 0.022614716], [nan, nan], [3.169925001442312, 5.774409284925443, 5.884968577492663, 3.0593657088750925, 2.715043576050351, 0.11055929256722052]]\n",
      "2020-12-23 02:45:45,007 : INFO : Removed 0 and 51 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:45,007 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:45,008 : INFO : built Dictionary(96 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 345 corpus positions)\n",
      "2020-12-23 02:45:45,029 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:45:45,031 : INFO : frequencies processed\n",
      "2020-12-23 02:45:45,158 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:45,159 : INFO : entropies processed\n",
      "2020-12-23 02:45:45,159 : INFO : extropies processed\n",
      "2020-12-23 02:45:45,160 : INFO : token count processed\n",
      "2020-12-23 02:45:45,161 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:45,162 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:45,163 : INFO : vocab #2480\n",
      "2020-12-23 02:45:45,164 : INFO : diff #set()\n",
      "2020-12-23 02:45:45,428 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:45,555 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.2604835700940775, 0.4423832197808815], [0.9567671865224838, 0.043232813], [0.0, 0.0], [3.169925001442312, 5.977819040873918, 6.062299601626284, 3.0854444406899457, 2.892374600183972, 0.08448056075236643]]\n",
      "2020-12-23 02:45:45,558 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:45,559 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:45,560 : INFO : built Dictionary(76 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 292 corpus positions)\n",
      "2020-12-23 02:45:45,579 : INFO : token count processed\n",
      "2020-12-23 02:45:45,582 : INFO : frequencies processed\n",
      "2020-12-23 02:45:45,711 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:45,712 : INFO : entropies processed\n",
      "2020-12-23 02:45:45,713 : INFO : extropies processed\n",
      "2020-12-23 02:45:45,715 : INFO : token count processed\n",
      "2020-12-23 02:45:45,717 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:45,718 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:45,718 : INFO : vocab #2480\n",
      "2020-12-23 02:45:45,720 : INFO : diff #set()\n",
      "2020-12-23 02:45:45,990 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:46,120 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.2506404426493791, 0.44431797325335237], [0.9458839148283005, 0.054116085], [1.0, 1.0], [3.169925001442312, 5.901812829596593, 5.988929536596353, 3.082808294442553, 2.8190045351540407, 0.08711670699976004]]\n",
      "2020-12-23 02:45:46,123 : INFO : Removed 0 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:46,123 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:46,124 : INFO : built Dictionary(78 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 292 corpus positions)\n",
      "2020-12-23 02:45:46,138 : INFO : token count processed\n",
      "2020-12-23 02:45:46,141 : INFO : frequencies processed\n",
      "2020-12-23 02:45:46,269 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:46,270 : INFO : entropies processed\n",
      "2020-12-23 02:45:46,271 : INFO : extropies processed\n",
      "2020-12-23 02:45:46,272 : INFO : token count processed\n",
      "2020-12-23 02:45:46,273 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:46,274 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:46,275 : INFO : vocab #2480\n",
      "2020-12-23 02:45:46,276 : INFO : diff #set()\n",
      "2020-12-23 02:45:46,533 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:46,660 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.2624661817528398, 0.4419955569127016], [0.9663475900888443, 0.03365241], [1.0, 1.0], [3.169925001442312, 5.643202320803383, 5.747410353122325, 3.06571696912337, 2.5774853516800125, 0.10420803231894205]]\n",
      "2020-12-23 02:45:46,662 : INFO : Removed 0 and 30 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:46,663 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:46,664 : INFO : built Dictionary(93 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 326 corpus positions)\n",
      "2020-12-23 02:45:46,689 : INFO : token count processed\n",
      "2020-12-23 02:45:46,694 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:46,696 : INFO : frequencies processed\n",
      "2020-12-23 02:45:46,697 : INFO : token count processed\n",
      "2020-12-23 02:45:46,698 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:46,699 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:46,699 : INFO : vocab #2480\n",
      "2020-12-23 02:45:46,700 : INFO : diff #set()\n",
      "2020-12-23 02:45:46,956 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:47,084 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.2722799586787412, 0.44008661704760554], [0.9775687903165817, 0.02243121], [nan, nan], [3.169925001442312, 5.925214310725336, 6.025701388767059, 3.0694379234005886, 2.855776387324747, 0.10048707804172352]]\n",
      "2020-12-23 02:45:47,087 : INFO : Removed 0 and 293 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:47,088 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:47,089 : INFO : built Dictionary(164 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 1721 corpus positions)\n",
      "2020-12-23 02:45:47,128 : INFO : token count processed\n",
      "2020-12-23 02:45:47,130 : INFO : frequencies processed\n",
      "2020-12-23 02:45:47,257 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:47,258 : INFO : entropies processed\n",
      "2020-12-23 02:45:47,258 : INFO : extropies processed\n",
      "2020-12-23 02:45:47,260 : INFO : token count processed\n",
      "2020-12-23 02:45:47,261 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:47,262 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:47,262 : INFO : vocab #2480\n",
      "2020-12-23 02:45:47,263 : INFO : diff #set()\n",
      "2020-12-23 02:45:47,523 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:47,651 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.268840412201949, 0.4407537853354272], [0.9623450376093388, 0.037654962], [1.0, 1.0], [3.169925001442312, 6.551685682764175, 6.5726985877416375, 3.1489120964648496, 3.4027735862993254, 0.021012904977462554]]\n",
      "2020-12-23 02:45:47,653 : INFO : Removed 0 and 88 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:47,654 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:47,655 : INFO : built Dictionary(138 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 562 corpus positions)\n",
      "2020-12-23 02:45:47,687 : INFO : token count processed\n",
      "2020-12-23 02:45:47,693 : INFO : frequencies processed\n",
      "2020-12-23 02:45:47,821 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:47,821 : INFO : entropies processed\n",
      "2020-12-23 02:45:47,822 : INFO : extropies processed\n",
      "2020-12-23 02:45:47,823 : INFO : token count processed\n",
      "2020-12-23 02:45:47,824 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:47,825 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:47,825 : INFO : vocab #2480\n",
      "2020-12-23 02:45:47,826 : INFO : diff #set()\n",
      "2020-12-23 02:45:48,094 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:48,221 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.2577917441379254, 0.44291064603118296], [0.9599767290055752, 0.04002327], [2.0, 1.2451124978365313], [3.169925001442312, 6.642985062562557, 6.684279241367179, 3.1286308226376907, 3.5143542399248666, 0.041294178804621495]]\n",
      "2020-12-23 02:45:48,224 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:48,225 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:48,226 : INFO : built Dictionary(51 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 157 corpus positions)\n",
      "2020-12-23 02:45:48,235 : INFO : token count processed\n",
      "2020-12-23 02:45:48,238 : INFO : frequencies processed\n",
      "2020-12-23 02:45:48,365 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:48,365 : INFO : entropies processed\n",
      "2020-12-23 02:45:48,366 : INFO : extropies processed\n",
      "2020-12-23 02:45:48,367 : INFO : token count processed\n",
      "2020-12-23 02:45:48,368 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:48,369 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:48,370 : INFO : vocab #2480\n",
      "2020-12-23 02:45:48,371 : INFO : diff #set()\n",
      "2020-12-23 02:45:48,630 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:48,757 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.1839440212562218, 0.4578871941162631], [0.8756072223186493, 0.12439278], [2.0, 1.2451124978365313], [3.169925001442312, 5.2461980344571995, 5.366500120969643, 3.049622914929869, 2.1965751195273304, 0.12030208651244312]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:45:48,759 : INFO : Removed 0 and 23 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:48,760 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:48,761 : INFO : built Dictionary(77 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 217 corpus positions)\n",
      "2020-12-23 02:45:48,775 : INFO : token count processed\n",
      "2020-12-23 02:45:48,777 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:48,778 : INFO : frequencies processed\n",
      "2020-12-23 02:45:48,779 : INFO : token count processed\n",
      "2020-12-23 02:45:48,780 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:48,781 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:48,782 : INFO : vocab #2480\n",
      "2020-12-23 02:45:48,783 : INFO : diff #set()\n",
      "2020-12-23 02:45:49,040 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:49,169 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/test_auth_utility.py')[[1.2795968392115569, 0.43867405972797785], [0.9876711396500468, 0.01232886], [nan, nan], [3.169925001442312, 5.903090303960449, 6.031306560807495, 3.041708744595267, 2.8613815593651832, 0.12821625684704596]]\n",
      "2020-12-23 02:45:49,171 : INFO : Removed 0 and 442 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:49,172 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:49,173 : INFO : built Dictionary(111 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 1216 corpus positions)\n",
      "2020-12-23 02:45:49,200 : INFO : token count processed\n",
      "2020-12-23 02:45:49,204 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:49,206 : INFO : frequencies processed\n",
      "2020-12-23 02:45:49,208 : INFO : token count processed\n",
      "2020-12-23 02:45:49,209 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:49,210 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:49,210 : INFO : vocab #2480\n",
      "2020-12-23 02:45:49,211 : INFO : diff #set()\n",
      "2020-12-23 02:45:49,464 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:49,592 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.2812548408913178, 0.4383552341786094], [0.9854248045012355, 0.0145751955], [nan, nan], [3.169925001442312, 6.16659449033757, 6.1989869239313755, 3.137532567848506, 3.0290619224890634, 0.03239243359380595]]\n",
      "2020-12-23 02:45:49,595 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:49,595 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:49,596 : INFO : built Dictionary(70 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 251 corpus positions)\n",
      "2020-12-23 02:45:49,612 : INFO : token count processed\n",
      "2020-12-23 02:45:49,614 : INFO : frequencies processed\n",
      "2020-12-23 02:45:49,741 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:49,742 : INFO : entropies processed\n",
      "2020-12-23 02:45:49,743 : INFO : extropies processed\n",
      "2020-12-23 02:45:49,744 : INFO : token count processed\n",
      "2020-12-23 02:45:49,745 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:49,746 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:49,747 : INFO : vocab #2480\n",
      "2020-12-23 02:45:49,748 : INFO : diff #set()\n",
      "2020-12-23 02:45:50,017 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:50,145 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.255341216866024, 0.443391887897823], [0.9549033865332603, 0.045096613], [0.0, 0.0], [3.169925001442312, 5.906856253399655, 6.004866213075338, 3.0719150417666308, 2.8349412116330255, 0.09800995967568227]]\n",
      "2020-12-23 02:45:50,148 : INFO : Removed 0 and 68 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:50,148 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:50,149 : INFO : built Dictionary(84 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 250 corpus positions)\n",
      "2020-12-23 02:45:50,170 : INFO : token count processed\n",
      "2020-12-23 02:45:50,174 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:50,177 : INFO : frequencies processed\n",
      "2020-12-23 02:45:50,181 : INFO : token count processed\n",
      "2020-12-23 02:45:50,185 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:50,186 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:50,188 : INFO : vocab #2480\n",
      "2020-12-23 02:45:50,190 : INFO : diff #set()\n",
      "2020-12-23 02:45:50,444 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:50,570 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.2737094243642384, 0.43980993757793585], [0.9830764103680849, 0.01692359], [nan, nan], [3.169925001442312, 5.965115449163356, 6.071809364620913, 3.0632310859847554, 2.901884363178601, 0.1066939154575568]]\n",
      "2020-12-23 02:45:50,572 : INFO : Removed 0 and 13 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:50,573 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:50,575 : INFO : built Dictionary(88 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 364 corpus positions)\n",
      "2020-12-23 02:45:50,603 : INFO : token count processed\n",
      "2020-12-23 02:45:50,605 : INFO : frequencies processed\n",
      "2020-12-23 02:45:50,738 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:50,739 : INFO : entropies processed\n",
      "2020-12-23 02:45:50,740 : INFO : extropies processed\n",
      "2020-12-23 02:45:50,741 : INFO : token count processed\n",
      "2020-12-23 02:45:50,742 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:50,743 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:50,744 : INFO : vocab #2480\n",
      "2020-12-23 02:45:50,745 : INFO : diff #set()\n",
      "2020-12-23 02:45:51,003 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:51,130 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.1832053236343423, 0.4580421223668134], [0.8580318838357925, 0.14196812], [1.584962500721156, 1.1699250014423124], [3.169925001442312, 5.791362404253194, 5.856826084585268, 3.1044613211102376, 2.6869010831429563, 0.0654636803320745]]\n",
      "2020-12-23 02:45:51,133 : INFO : Removed 0 and 41 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:51,134 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:51,135 : INFO : built Dictionary(78 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 376 corpus positions)\n",
      "2020-12-23 02:45:51,149 : INFO : token count processed\n",
      "2020-12-23 02:45:51,152 : INFO : frequencies processed\n",
      "2020-12-23 02:45:51,280 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:51,281 : INFO : entropies processed\n",
      "2020-12-23 02:45:51,282 : INFO : extropies processed\n",
      "2020-12-23 02:45:51,284 : INFO : token count processed\n",
      "2020-12-23 02:45:51,286 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:51,287 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:51,288 : INFO : vocab #2480\n",
      "2020-12-23 02:45:51,290 : INFO : diff #set()\n",
      "2020-12-23 02:45:51,549 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:51,676 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.2036583037658282, 0.4537908614466687], [0.8922096639871597, 0.107790336], [1.584962500721156, 1.1699250014423124], [3.169925001442312, 5.651670454631116, 5.721485063636302, 3.1001103924371263, 2.55156006219399, 0.06981460900518588]]\n",
      "2020-12-23 02:45:51,679 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:51,680 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:51,681 : INFO : built Dictionary(44 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 86 corpus positions)\n",
      "2020-12-23 02:45:51,688 : INFO : token count processed\n",
      "2020-12-23 02:45:51,690 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:51,691 : INFO : frequencies processed\n",
      "2020-12-23 02:45:51,692 : INFO : token count processed\n",
      "2020-12-23 02:45:51,693 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:51,694 : INFO : alphabet_target #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:45:51,695 : INFO : vocab #2480\n",
      "2020-12-23 02:45:51,696 : INFO : diff #set()\n",
      "2020-12-23 02:45:51,953 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:52,080 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.2769385300553333, 0.4391862084988734], [0.977118818089366, 0.022881182], [nan, nan], [3.169925001442312, 4.8226207261920235, 5.131484226558179, 2.861061501076157, 1.9615592251158667, 0.3088635003661553]]\n",
      "2020-12-23 02:45:52,083 : INFO : Removed 0 and 50 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:52,084 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:52,085 : INFO : built Dictionary(88 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 256 corpus positions)\n",
      "2020-12-23 02:45:52,108 : INFO : token count processed\n",
      "2020-12-23 02:45:52,111 : INFO : frequencies processed\n",
      "2020-12-23 02:45:52,240 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:52,241 : INFO : entropies processed\n",
      "2020-12-23 02:45:52,242 : INFO : extropies processed\n",
      "2020-12-23 02:45:52,243 : INFO : token count processed\n",
      "2020-12-23 02:45:52,244 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:52,245 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:52,246 : INFO : vocab #2480\n",
      "2020-12-23 02:45:52,247 : INFO : diff #set()\n",
      "2020-12-23 02:45:52,505 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:52,632 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.266471015100927, 0.4412145548463895], [0.9631437845528126, 0.036856215], [0.0, 0.0], [3.169925001442312, 6.24862851613934, 6.337715456879319, 3.0808380607023347, 3.1677904554370064, 0.08908694073997836]]\n",
      "2020-12-23 02:45:52,635 : INFO : Removed 0 and 32 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:52,636 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:52,637 : INFO : built Dictionary(87 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 372 corpus positions)\n",
      "2020-12-23 02:45:52,663 : INFO : token count processed\n",
      "2020-12-23 02:45:52,668 : INFO : frequencies processed\n",
      "2020-12-23 02:45:52,797 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:52,797 : INFO : entropies processed\n",
      "2020-12-23 02:45:52,798 : INFO : extropies processed\n",
      "2020-12-23 02:45:52,799 : INFO : token count processed\n",
      "2020-12-23 02:45:52,799 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:52,800 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:52,800 : INFO : vocab #2480\n",
      "2020-12-23 02:45:52,801 : INFO : diff #set()\n",
      "2020-12-23 02:45:53,055 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:53,182 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.186133907270369, 0.4574285210408776], [0.8600737154483795, 0.13992628], [1.584962500721156, 1.1699250014423124], [3.169925001442312, 5.850156917433494, 5.911465827728226, 3.1086160911475806, 2.741540826285914, 0.0613089102947324]]\n",
      "2020-12-23 02:45:53,185 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:53,186 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:53,187 : INFO : built Dictionary(82 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 370 corpus positions)\n",
      "2020-12-23 02:45:53,203 : INFO : token count processed\n",
      "2020-12-23 02:45:53,206 : INFO : frequencies processed\n",
      "2020-12-23 02:45:53,333 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:53,334 : INFO : entropies processed\n",
      "2020-12-23 02:45:53,335 : INFO : extropies processed\n",
      "2020-12-23 02:45:53,336 : INFO : token count processed\n",
      "2020-12-23 02:45:53,337 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:53,338 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:53,338 : INFO : vocab #2480\n",
      "2020-12-23 02:45:53,339 : INFO : diff #set()\n",
      "2020-12-23 02:45:53,597 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:53,725 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.2037767502710732, 0.4537664715253013], [0.8916151896119118, 0.10838481], [1.584962500721156, 1.1699250014423124], [3.169925001442312, 5.6831976040360095, 5.752694989442716, 3.1004276160356063, 2.582769988000404, 0.06949738540670669]]\n",
      "2020-12-23 02:45:53,728 : INFO : Removed 0 and 52 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:53,728 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:53,730 : INFO : built Dictionary(70 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 348 corpus positions)\n",
      "2020-12-23 02:45:53,747 : INFO : token count processed\n",
      "2020-12-23 02:45:53,750 : INFO : frequencies processed\n",
      "2020-12-23 02:45:53,886 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:53,886 : INFO : entropies processed\n",
      "2020-12-23 02:45:53,887 : INFO : extropies processed\n",
      "2020-12-23 02:45:53,889 : INFO : token count processed\n",
      "2020-12-23 02:45:53,890 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:53,891 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:53,892 : INFO : vocab #2480\n",
      "2020-12-23 02:45:53,894 : INFO : diff #set()\n",
      "2020-12-23 02:45:54,152 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:54,281 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.2539993144374397, 0.4436558580984232], [0.9516527578234673, 0.048347242], [1.0, 1.0], [3.169925001442312, 5.749308601266266, 5.828665490575245, 3.090568112133333, 2.658740489132933, 0.0793568893089791]]\n",
      "2020-12-23 02:45:54,283 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:54,284 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:54,285 : INFO : built Dictionary(63 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 229 corpus positions)\n",
      "2020-12-23 02:45:54,297 : INFO : token count processed\n",
      "2020-12-23 02:45:54,299 : INFO : frequencies processed\n",
      "2020-12-23 02:45:54,427 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:54,428 : INFO : entropies processed\n",
      "2020-12-23 02:45:54,429 : INFO : extropies processed\n",
      "2020-12-23 02:45:54,430 : INFO : token count processed\n",
      "2020-12-23 02:45:54,431 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:54,432 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:54,433 : INFO : vocab #2480\n",
      "2020-12-23 02:45:54,434 : INFO : diff #set()\n",
      "2020-12-23 02:45:54,690 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:54,817 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.1649139571009486, 0.4619121220591635], [0.8552409410476685, 0.14475906], [1.0, 1.0], [3.169925001442312, 5.015422548793484, 5.141496107912385, 3.0438514423234118, 1.9715711064700727, 0.12607355911890128]]\n",
      "2020-12-23 02:45:54,819 : INFO : Removed 0 and 81 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:54,820 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:54,821 : INFO : built Dictionary(91 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 355 corpus positions)\n",
      "2020-12-23 02:45:54,844 : INFO : token count processed\n",
      "2020-12-23 02:45:54,854 : INFO : frequencies processed\n",
      "2020-12-23 02:45:54,984 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:54,984 : INFO : entropies processed\n",
      "2020-12-23 02:45:54,985 : INFO : extropies processed\n",
      "2020-12-23 02:45:54,986 : INFO : token count processed\n",
      "2020-12-23 02:45:54,986 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:54,987 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:54,988 : INFO : vocab #2480\n",
      "2020-12-23 02:45:54,989 : INFO : diff #set()\n",
      "2020-12-23 02:45:55,251 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:55,378 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.1750505744602433, 0.459759424328861], [0.8588896542787552, 0.14111035], [1.584962500721156, 1.1699250014423124], [3.169925001442312, 6.030001281822029, 6.0855847982510145, 3.114341485013327, 2.9156597968087024, 0.05558351642898529]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:45:55,381 : INFO : Removed 0 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:55,382 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:55,383 : INFO : built Dictionary(81 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 318 corpus positions)\n",
      "2020-12-23 02:45:55,411 : INFO : token count processed\n",
      "2020-12-23 02:45:55,414 : INFO : frequencies processed\n",
      "2020-12-23 02:45:55,541 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:55,541 : INFO : entropies processed\n",
      "2020-12-23 02:45:55,545 : INFO : extropies processed\n",
      "2020-12-23 02:45:55,546 : INFO : token count processed\n",
      "2020-12-23 02:45:55,546 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:55,547 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:55,547 : INFO : vocab #2480\n",
      "2020-12-23 02:45:55,548 : INFO : diff #set()\n",
      "2020-12-23 02:45:55,811 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:55,939 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.272259062746379, 0.44009066413023534], [0.9735123701393604, 0.02648763], [0.0, 0.0], [3.169925001442312, 5.9537092545441395, 6.046747807911026, 3.0768864480754266, 2.876822806468714, 0.09303855336688649]]\n",
      "2020-12-23 02:45:55,941 : INFO : Removed 0 and 90 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:55,942 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:55,944 : INFO : built Dictionary(93 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 319 corpus positions)\n",
      "2020-12-23 02:45:55,968 : INFO : token count processed\n",
      "2020-12-23 02:45:55,970 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:55,972 : INFO : frequencies processed\n",
      "2020-12-23 02:45:55,973 : INFO : token count processed\n",
      "2020-12-23 02:45:55,976 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:55,977 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:55,978 : INFO : vocab #2480\n",
      "2020-12-23 02:45:55,980 : INFO : diff #set()\n",
      "2020-12-23 02:45:56,241 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:56,369 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.273025339847602, 0.4399423017725998], [0.9805381540209055, 0.019461846], [nan, nan], [3.169925001442312, 6.184756445474906, 6.2709695079541525, 3.0837119389630665, 3.1010445065118404, 0.08621306247924654]]\n",
      "2020-12-23 02:45:56,371 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:56,372 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:56,373 : INFO : built Dictionary(99 unique tokens: ['bug', 'chang', 'fix', 'generat', 'html']...) from 2 documents (total 432 corpus positions)\n",
      "2020-12-23 02:45:56,395 : INFO : token count processed\n",
      "2020-12-23 02:45:56,400 : INFO : frequencies processed\n",
      "2020-12-23 02:45:56,528 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:56,529 : INFO : entropies processed\n",
      "2020-12-23 02:45:56,529 : INFO : extropies processed\n",
      "2020-12-23 02:45:56,530 : INFO : token count processed\n",
      "2020-12-23 02:45:56,531 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:56,531 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:56,532 : INFO : vocab #2480\n",
      "2020-12-23 02:45:56,533 : INFO : diff #set()\n",
      "2020-12-23 02:45:56,789 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:56,917 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.200802371695352, 0.4543797357096023], [0.8681151270866394, 0.13188487], [1.584962500721156, 1.1699250014423124], [3.169925001442312, 6.212221456585881, 6.262467887905832, 3.1196785701223595, 3.0925428864635203, 0.05024643131995177]]\n",
      "2020-12-23 02:45:56,919 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:56,920 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:56,921 : INFO : built Dictionary(119 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 412 corpus positions)\n",
      "2020-12-23 02:45:56,940 : INFO : token count processed\n",
      "2020-12-23 02:45:56,942 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:56,943 : INFO : frequencies processed\n",
      "2020-12-23 02:45:56,944 : INFO : token count processed\n",
      "2020-12-23 02:45:56,945 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:56,946 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:56,946 : INFO : vocab #2480\n",
      "2020-12-23 02:45:56,947 : INFO : diff #set()\n",
      "2020-12-23 02:45:57,202 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:57,329 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.295044506364475, 0.43572139765780665], [0.985178922303021, 0.014821078], [nan, nan], [2.321928094887362, 6.301552355933639, 6.346224519451889, 2.277255931369112, 4.024296424564527, 0.044672163518249874]]\n",
      "2020-12-23 02:45:57,332 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:57,333 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:57,334 : INFO : built Dictionary(156 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 648 corpus positions)\n",
      "2020-12-23 02:45:57,357 : INFO : token count processed\n",
      "2020-12-23 02:45:57,360 : INFO : frequencies processed\n",
      "2020-12-23 02:45:57,493 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:57,494 : INFO : entropies processed\n",
      "2020-12-23 02:45:57,495 : INFO : extropies processed\n",
      "2020-12-23 02:45:57,497 : INFO : token count processed\n",
      "2020-12-23 02:45:57,498 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:57,499 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:57,500 : INFO : vocab #2480\n",
      "2020-12-23 02:45:57,501 : INFO : diff #set()\n",
      "2020-12-23 02:45:57,772 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:57,901 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.2811595720409454, 0.43837354135875006], [0.9582063779234886, 0.041793622], [0.0, 0.0], [2.321928094887362, 6.739005504021667, 6.76168711386151, 2.2992464850475196, 4.439759018974147, 0.02268160983984302]]\n",
      "2020-12-23 02:45:57,904 : INFO : Removed 0 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:57,905 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:57,907 : INFO : built Dictionary(99 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 499 corpus positions)\n",
      "2020-12-23 02:45:57,926 : INFO : token count processed\n",
      "2020-12-23 02:45:57,934 : INFO : frequencies processed\n",
      "2020-12-23 02:45:58,064 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:45:58,064 : INFO : entropies processed\n",
      "2020-12-23 02:45:58,065 : INFO : extropies processed\n",
      "2020-12-23 02:45:58,066 : INFO : token count processed\n",
      "2020-12-23 02:45:58,067 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:58,068 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:58,068 : INFO : vocab #2480\n",
      "2020-12-23 02:45:58,069 : INFO : diff #set()\n",
      "2020-12-23 02:45:58,328 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:58,456 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2699893519417491, 0.4405307007914375], [0.9526683539152145, 0.047331646], [1.0, 1.0], [2.321928094887362, 5.870833373337847, 5.897332957252383, 2.295428510972826, 3.5754048623650205, 0.026499583914535663]]\n",
      "2020-12-23 02:45:58,458 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:58,459 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:58,460 : INFO : built Dictionary(61 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 182 corpus positions)\n",
      "2020-12-23 02:45:58,474 : INFO : token count processed\n",
      "2020-12-23 02:45:58,478 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:58,478 : INFO : frequencies processed\n",
      "2020-12-23 02:45:58,480 : INFO : token count processed\n",
      "2020-12-23 02:45:58,483 : INFO : alphabet_source #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:45:58,484 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:58,485 : INFO : vocab #2480\n",
      "2020-12-23 02:45:58,486 : INFO : diff #set()\n",
      "2020-12-23 02:45:58,747 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:58,875 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.290826987519394, 0.43652358098105126], [0.9988046763464808, 0.0011953237], [nan, nan], [2.321928094887362, 5.371881234145534, 5.465426219456887, 2.2283831095760096, 3.143498124569525, 0.09354498531135302]]\n",
      "2020-12-23 02:45:58,877 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:58,878 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:58,879 : INFO : built Dictionary(52 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 134 corpus positions)\n",
      "2020-12-23 02:45:58,885 : INFO : token count processed\n",
      "2020-12-23 02:45:58,887 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:58,888 : INFO : frequencies processed\n",
      "2020-12-23 02:45:58,889 : INFO : token count processed\n",
      "2020-12-23 02:45:58,890 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:58,891 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:58,892 : INFO : vocab #2480\n",
      "2020-12-23 02:45:58,893 : INFO : diff #set()\n",
      "2020-12-23 02:45:59,151 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:59,279 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.2990916473821195, 0.43495438780731455], [0.9970599198713899, 0.0029400801], [nan, nan], [2.321928094887362, 4.85108279267097, 4.985948540410293, 2.1870623471480393, 2.664020445522931, 0.13486574773932336]]\n",
      "2020-12-23 02:45:59,282 : INFO : Removed 0 and 127 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:59,283 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:59,284 : INFO : built Dictionary(92 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 407 corpus positions)\n",
      "2020-12-23 02:45:59,299 : INFO : token count processed\n",
      "2020-12-23 02:45:59,304 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:59,305 : INFO : frequencies processed\n",
      "2020-12-23 02:45:59,307 : INFO : token count processed\n",
      "2020-12-23 02:45:59,308 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:59,309 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:59,309 : INFO : vocab #2480\n",
      "2020-12-23 02:45:59,310 : INFO : diff #set()\n",
      "2020-12-23 02:45:59,567 : INFO : alphabet #2480\n",
      "2020-12-23 02:45:59,697 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.2969503403710114, 0.4353598693119663], [0.9980489058652893, 0.0019510941], [nan, nan], [2.321928094887362, 6.139571208108155, 6.18036750393642, 2.2811317990590965, 3.858439409049058, 0.04079629582826527]]\n",
      "2020-12-23 02:45:59,699 : INFO : Removed 0 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:45:59,700 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:45:59,702 : INFO : built Dictionary(78 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 417 corpus positions)\n",
      "2020-12-23 02:45:59,725 : INFO : token count processed\n",
      "2020-12-23 02:45:59,726 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:45:59,729 : INFO : frequencies processed\n",
      "2020-12-23 02:45:59,730 : INFO : token count processed\n",
      "2020-12-23 02:45:59,731 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:45:59,732 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:45:59,732 : INFO : vocab #2480\n",
      "2020-12-23 02:45:59,733 : INFO : diff #set()\n",
      "2020-12-23 02:45:59,988 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:00,115 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.294897443062141, 0.4357493198762182], [0.9852052070200443, 0.014794793], [nan, nan], [2.321928094887362, 5.609710627339259, 5.6608617378443, 2.270776984382321, 3.338933642956938, 0.05115111050504062]]\n",
      "2020-12-23 02:46:00,118 : INFO : Removed 0 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:00,119 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:00,120 : INFO : built Dictionary(168 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 1077 corpus positions)\n",
      "2020-12-23 02:46:00,144 : INFO : token count processed\n",
      "2020-12-23 02:46:00,146 : INFO : frequencies processed\n",
      "2020-12-23 02:46:00,279 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:00,280 : INFO : entropies processed\n",
      "2020-12-23 02:46:00,281 : INFO : extropies processed\n",
      "2020-12-23 02:46:00,283 : INFO : token count processed\n",
      "2020-12-23 02:46:00,284 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:00,286 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:00,286 : INFO : vocab #2480\n",
      "2020-12-23 02:46:00,287 : INFO : diff #set()\n",
      "2020-12-23 02:46:00,543 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:00,670 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.289549552111904, 0.43676713573531956], [0.9772185813635588, 0.022781419], [1.0, 1.0], [2.321928094887362, 7.2441902753576075, 7.256922069871934, 2.309196300373036, 4.934993974984572, 0.012731794514326822]]\n",
      "2020-12-23 02:46:00,673 : INFO : Removed 0 and 136 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:00,674 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:00,676 : INFO : built Dictionary(131 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 684 corpus positions)\n",
      "2020-12-23 02:46:00,700 : INFO : token count processed\n",
      "2020-12-23 02:46:00,705 : INFO : frequencies processed\n",
      "2020-12-23 02:46:00,838 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:00,839 : INFO : entropies processed\n",
      "2020-12-23 02:46:00,839 : INFO : extropies processed\n",
      "2020-12-23 02:46:00,841 : INFO : token count processed\n",
      "2020-12-23 02:46:00,841 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:00,842 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:00,843 : INFO : vocab #2480\n",
      "2020-12-23 02:46:00,844 : INFO : diff #set()\n",
      "2020-12-23 02:46:01,100 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:01,227 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.300603873330558, 0.43466848491057763], [0.9962700325995684, 0.0037299674], [0.0, 0.0], [2.321928094887362, 6.2567074920449475, 6.283909078755572, 2.294726508176737, 3.96198098386821, 0.02720158671062478]]\n",
      "2020-12-23 02:46:01,230 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:01,231 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:01,232 : INFO : built Dictionary(77 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 194 corpus positions)\n",
      "2020-12-23 02:46:01,248 : INFO : token count processed\n",
      "2020-12-23 02:46:01,251 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:01,251 : INFO : frequencies processed\n",
      "2020-12-23 02:46:01,253 : INFO : token count processed\n",
      "2020-12-23 02:46:01,254 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:01,255 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:01,256 : INFO : vocab #2480\n",
      "2020-12-23 02:46:01,257 : INFO : diff #set()\n",
      "2020-12-23 02:46:01,517 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:01,644 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.3045243470888501, 0.43392902368908903], [0.9945728150196373, 0.005427185], [nan, nan], [2.321928094887362, 5.7680018917339435, 5.848062606230643, 2.241867380390662, 3.526134511343281, 0.0800607144966996]]\n",
      "2020-12-23 02:46:01,647 : INFO : Removed 0 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:01,648 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:01,649 : INFO : built Dictionary(174 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 735 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:46:01,680 : INFO : token count processed\n",
      "2020-12-23 02:46:01,682 : INFO : frequencies processed\n",
      "2020-12-23 02:46:01,812 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:01,812 : INFO : entropies processed\n",
      "2020-12-23 02:46:01,813 : INFO : extropies processed\n",
      "2020-12-23 02:46:01,815 : INFO : token count processed\n",
      "2020-12-23 02:46:01,816 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:01,817 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:01,818 : INFO : vocab #2480\n",
      "2020-12-23 02:46:01,819 : INFO : diff #set()\n",
      "2020-12-23 02:46:02,078 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:02,206 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.2961499854665592, 0.4355116200289538], [0.9867102755233645, 0.0132897245], [0.0, 0.0], [2.321928094887362, 6.846479111193757, 6.868701382841499, 2.29970582323962, 4.546773287954137, 0.02222227164774182]]\n",
      "2020-12-23 02:46:02,208 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:02,209 : INFO : built Dictionary(25 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 38 corpus positions)\n",
      "2020-12-23 02:46:02,219 : INFO : token count processed\n",
      "2020-12-23 02:46:02,223 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:02,224 : INFO : frequencies processed\n",
      "2020-12-23 02:46:02,225 : INFO : token count processed\n",
      "2020-12-23 02:46:02,227 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:02,228 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:02,229 : INFO : vocab #2480\n",
      "2020-12-23 02:46:02,230 : INFO : diff #set()\n",
      "2020-12-23 02:46:02,487 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:02,615 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/fireException.py')[[1.3114329919312286, 0.4326320527096434], [1.0, 0.0], [nan, nan], [2.321928094887362, 4.165013816065912, 4.4842551447948, 2.0026867661584733, 2.162327049907438, 0.31924132872888844]]\n",
      "2020-12-23 02:46:02,618 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:02,619 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:02,620 : INFO : built Dictionary(53 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 145 corpus positions)\n",
      "2020-12-23 02:46:02,633 : INFO : token count processed\n",
      "2020-12-23 02:46:02,635 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:02,636 : INFO : frequencies processed\n",
      "2020-12-23 02:46:02,637 : INFO : token count processed\n",
      "2020-12-23 02:46:02,638 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:02,639 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:02,639 : INFO : vocab #2480\n",
      "2020-12-23 02:46:02,640 : INFO : diff #set()\n",
      "2020-12-23 02:46:02,898 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:03,025 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.2779252863836, 0.4389959609200286], [0.9728716686367989, 0.027128331], [nan, nan], [2.321928094887362, 5.449968864419248, 5.552131818550282, 2.219765140756329, 3.2302037236629197, 0.1021629541310336]]\n",
      "2020-12-23 02:46:03,027 : INFO : Removed 0 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:03,028 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:03,029 : INFO : built Dictionary(144 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 582 corpus positions)\n",
      "2020-12-23 02:46:03,056 : INFO : token count processed\n",
      "2020-12-23 02:46:03,062 : INFO : frequencies processed\n",
      "2020-12-23 02:46:03,187 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:03,188 : INFO : entropies processed\n",
      "2020-12-23 02:46:03,189 : INFO : extropies processed\n",
      "2020-12-23 02:46:03,190 : INFO : token count processed\n",
      "2020-12-23 02:46:03,191 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:03,192 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:03,193 : INFO : vocab #2480\n",
      "2020-12-23 02:46:03,194 : INFO : diff #set()\n",
      "2020-12-23 02:46:03,462 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:03,589 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.272451086434942, 0.440053476164724], [0.9522306211292744, 0.04776938], [1.0, 1.0], [2.321928094887362, 6.530294129310484, 6.55078910630886, 2.301433117888985, 4.228861011421499, 0.020494976998376657]]\n",
      "2020-12-23 02:46:03,592 : INFO : Removed 0 and 142 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:03,593 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:03,594 : INFO : built Dictionary(118 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 598 corpus positions)\n",
      "2020-12-23 02:46:03,614 : INFO : token count processed\n",
      "2020-12-23 02:46:03,623 : INFO : frequencies processed\n",
      "2020-12-23 02:46:03,752 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:03,753 : INFO : entropies processed\n",
      "2020-12-23 02:46:03,753 : INFO : extropies processed\n",
      "2020-12-23 02:46:03,754 : INFO : token count processed\n",
      "2020-12-23 02:46:03,755 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:03,756 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:03,756 : INFO : vocab #2480\n",
      "2020-12-23 02:46:03,757 : INFO : diff #set()\n",
      "2020-12-23 02:46:04,024 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:04,153 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.2921152329891294, 0.4362782401196767], [0.9909426113590598, 0.009057389], [0.0, 0.0], [2.321928094887362, 6.470272233491701, 6.497967634056124, 2.2942326943229396, 4.176039539168762, 0.02769540056442299]]\n",
      "2020-12-23 02:46:04,155 : INFO : Removed 0 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:04,156 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:04,158 : INFO : built Dictionary(120 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 387 corpus positions)\n",
      "2020-12-23 02:46:04,175 : INFO : token count processed\n",
      "2020-12-23 02:46:04,178 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:04,180 : INFO : frequencies processed\n",
      "2020-12-23 02:46:04,181 : INFO : token count processed\n",
      "2020-12-23 02:46:04,184 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:04,185 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:04,187 : INFO : vocab #2480\n",
      "2020-12-23 02:46:04,188 : INFO : diff #set()\n",
      "2020-12-23 02:46:04,442 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:04,570 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.2986739290439415, 0.4350334283453232], [0.9939515735022724, 0.0060484265], [nan, nan], [2.321928094887362, 6.550038223589686, 6.591694907368557, 2.280271411108491, 4.269766812481194, 0.04165668377887055]]\n",
      "2020-12-23 02:46:04,572 : INFO : Removed 0 and 70 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:04,573 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:04,574 : INFO : built Dictionary(74 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 303 corpus positions)\n",
      "2020-12-23 02:46:04,584 : INFO : token count processed\n",
      "2020-12-23 02:46:04,586 : INFO : frequencies processed\n",
      "2020-12-23 02:46:04,716 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:04,716 : INFO : entropies processed\n",
      "2020-12-23 02:46:04,717 : INFO : extropies processed\n",
      "2020-12-23 02:46:04,718 : INFO : token count processed\n",
      "2020-12-23 02:46:04,718 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:04,719 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:04,720 : INFO : vocab #2480\n",
      "2020-12-23 02:46:04,721 : INFO : diff #set()\n",
      "2020-12-23 02:46:04,986 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:05,114 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.256899134608565, 0.44308581835379157], [0.9367171674966812, 0.06328283], [0.0, 0.0], [2.321928094887362, 5.860525481261383, 5.904034666715321, 2.278418909433424, 3.582106571827959, 0.04350918545393867]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:46:05,116 : INFO : Removed 0 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:05,117 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:05,119 : INFO : built Dictionary(45 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 341 corpus positions)\n",
      "2020-12-23 02:46:05,134 : INFO : token count processed\n",
      "2020-12-23 02:46:05,136 : INFO : frequencies processed\n",
      "2020-12-23 02:46:05,265 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:05,265 : INFO : entropies processed\n",
      "2020-12-23 02:46:05,266 : INFO : extropies processed\n",
      "2020-12-23 02:46:05,267 : INFO : token count processed\n",
      "2020-12-23 02:46:05,268 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:05,269 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:05,269 : INFO : vocab #2480\n",
      "2020-12-23 02:46:05,270 : INFO : diff #set()\n",
      "2020-12-23 02:46:05,534 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:05,666 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.0765908837586828, 0.4815585042875535], [0.7385971248149872, 0.26140288], [0.0, 0.0], [2.321928094887362, 5.945464049777852, 5.9749788323064, 2.292413312358814, 3.6530507374190377, 0.029514782528547556]]\n",
      "2020-12-23 02:46:05,669 : INFO : Removed 0 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:05,670 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:05,671 : INFO : built Dictionary(193 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 888 corpus positions)\n",
      "2020-12-23 02:46:05,699 : INFO : token count processed\n",
      "2020-12-23 02:46:05,704 : INFO : frequencies processed\n",
      "2020-12-23 02:46:05,836 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:05,837 : INFO : entropies processed\n",
      "2020-12-23 02:46:05,840 : INFO : extropies processed\n",
      "2020-12-23 02:46:05,841 : INFO : token count processed\n",
      "2020-12-23 02:46:05,842 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:05,843 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:05,844 : INFO : vocab #2480\n",
      "2020-12-23 02:46:05,845 : INFO : diff #set()\n",
      "2020-12-23 02:46:06,116 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:06,249 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.3009569717425693, 0.4346017819023692], [0.9942963253706694, 0.0057036746], [0.0, 0.0], [2.321928094887362, 6.811563897304216, 6.8318936068238845, 2.3015983853676936, 4.509965511936523, 0.02032970951966817]]\n",
      "2020-12-23 02:46:06,252 : INFO : Removed 0 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:06,253 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:06,256 : INFO : built Dictionary(213 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 983 corpus positions)\n",
      "2020-12-23 02:46:06,293 : INFO : token count processed\n",
      "2020-12-23 02:46:06,296 : INFO : frequencies processed\n",
      "2020-12-23 02:46:06,425 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:06,425 : INFO : entropies processed\n",
      "2020-12-23 02:46:06,429 : INFO : extropies processed\n",
      "2020-12-23 02:46:06,430 : INFO : token count processed\n",
      "2020-12-23 02:46:06,431 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:06,432 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:06,434 : INFO : vocab #2480\n",
      "2020-12-23 02:46:06,435 : INFO : diff #set()\n",
      "2020-12-23 02:46:06,694 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:06,823 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.2829229842825822, 0.4380349257880261], [0.9665356613695621, 0.03346434], [1.0, 1.0], [2.321928094887362, 7.502034948968415, 7.514490222643612, 2.3094728212121645, 5.192562127756251, 0.012455273675197276]]\n",
      "2020-12-23 02:46:06,826 : INFO : Removed 0 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:06,827 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:06,828 : INFO : built Dictionary(256 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 1556 corpus positions)\n",
      "2020-12-23 02:46:06,874 : INFO : token count processed\n",
      "2020-12-23 02:46:06,877 : INFO : frequencies processed\n",
      "2020-12-23 02:46:07,006 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:07,007 : INFO : entropies processed\n",
      "2020-12-23 02:46:07,008 : INFO : extropies processed\n",
      "2020-12-23 02:46:07,010 : INFO : token count processed\n",
      "2020-12-23 02:46:07,011 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:07,012 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:07,013 : INFO : vocab #2480\n",
      "2020-12-23 02:46:07,014 : INFO : diff #set()\n",
      "2020-12-23 02:46:07,274 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:07,402 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.2358933682200683, 0.4472485200830806], [0.8508082330226898, 0.14919177], [1.584962500721156, 1.1699250014423124], [2.321928094887362, 7.39180093901977, 7.396809041113305, 2.316919992793827, 5.074880946225942, 0.005008102093534639]]\n",
      "2020-12-23 02:46:07,404 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:07,405 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:07,406 : INFO : built Dictionary(42 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 111 corpus positions)\n",
      "2020-12-23 02:46:07,412 : INFO : token count processed\n",
      "2020-12-23 02:46:07,414 : INFO : frequencies processed\n",
      "2020-12-23 02:46:07,542 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:07,543 : INFO : entropies processed\n",
      "2020-12-23 02:46:07,544 : INFO : extropies processed\n",
      "2020-12-23 02:46:07,545 : INFO : token count processed\n",
      "2020-12-23 02:46:07,546 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:07,547 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:07,548 : INFO : vocab #2480\n",
      "2020-12-23 02:46:07,549 : INFO : diff #set()\n",
      "2020-12-23 02:46:07,805 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:07,933 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.2568329061245291, 0.4430988210452924], [0.9521994367241859, 0.047800563], [0.0, 0.0], [2.321928094887362, 4.927561309677364, 5.049328375772154, 2.2001610287925715, 2.7274002808847917, 0.12176706609479027]]\n",
      "2020-12-23 02:46:07,935 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:07,936 : INFO : built Dictionary(11 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 12 corpus positions)\n",
      "2020-12-23 02:46:07,939 : INFO : token count processed\n",
      "2020-12-23 02:46:07,941 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:07,942 : INFO : frequencies processed\n",
      "2020-12-23 02:46:07,943 : INFO : token count processed\n",
      "2020-12-23 02:46:07,944 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:07,945 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:07,945 : INFO : vocab #2480\n",
      "2020-12-23 02:46:07,946 : INFO : diff #set()\n",
      "2020-12-23 02:46:08,205 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:08,333 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2954639185470753, 0.43564178548837945], [1.0, 0.0], [nan, nan], [2.321928094887362, 2.5216406363433186, 3.4182958340544896, 1.4252728971761912, 1.0963677391671274, 0.896655197711171]]\n",
      "2020-12-23 02:46:08,337 : INFO : Removed 0 and 530 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:08,338 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:08,339 : INFO : built Dictionary(331 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 2884 corpus positions)\n",
      "2020-12-23 02:46:08,403 : INFO : token count processed\n",
      "2020-12-23 02:46:08,405 : INFO : frequencies processed\n",
      "2020-12-23 02:46:08,533 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:08,534 : INFO : entropies processed\n",
      "2020-12-23 02:46:08,535 : INFO : extropies processed\n",
      "2020-12-23 02:46:08,538 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:46:08,539 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:08,540 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:08,541 : INFO : vocab #2480\n",
      "2020-12-23 02:46:08,542 : INFO : diff #set()\n",
      "2020-12-23 02:46:08,811 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:08,943 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.2893288769089104, 0.4368092370154421], [0.9731941763311625, 0.026805824], [1.0, 1.0], [2.321928094887362, 7.480007711014331, 7.485822776903271, 2.316113028998421, 5.1638946820159095, 0.0058150658889406515]]\n",
      "2020-12-23 02:46:08,946 : INFO : Removed 0 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:08,947 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:08,950 : INFO : built Dictionary(209 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 1033 corpus positions)\n",
      "2020-12-23 02:46:08,987 : INFO : token count processed\n",
      "2020-12-23 02:46:08,993 : INFO : frequencies processed\n",
      "2020-12-23 02:46:09,118 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:09,119 : INFO : entropies processed\n",
      "2020-12-23 02:46:09,120 : INFO : extropies processed\n",
      "2020-12-23 02:46:09,121 : INFO : token count processed\n",
      "2020-12-23 02:46:09,122 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:09,123 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:09,123 : INFO : vocab #2480\n",
      "2020-12-23 02:46:09,124 : INFO : diff #set()\n",
      "2020-12-23 02:46:09,390 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:09,519 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.2917237785637978, 0.43635276177423565], [0.979215532541275, 0.020784467], [0.0, 0.0], [2.321928094887362, 7.131331012509435, 7.146937797382236, 2.3063213100145603, 4.825009702494874, 0.015606784872801427]]\n",
      "2020-12-23 02:46:09,521 : INFO : Removed 0 and 166 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:09,522 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:09,524 : INFO : built Dictionary(199 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 852 corpus positions)\n",
      "2020-12-23 02:46:09,557 : INFO : token count processed\n",
      "2020-12-23 02:46:09,559 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:09,561 : INFO : frequencies processed\n",
      "2020-12-23 02:46:09,563 : INFO : token count processed\n",
      "2020-12-23 02:46:09,565 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:09,566 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:09,569 : INFO : vocab #2480\n",
      "2020-12-23 02:46:09,570 : INFO : diff #set()\n",
      "2020-12-23 02:46:09,824 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:09,951 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.303173985543828, 0.43418343827979583], [0.9969169441610575, 0.0030830558], [nan, nan], [2.321928094887362, 7.203742744794778, 7.224503674868972, 2.301167164813169, 4.90257557998161, 0.020760930074193773]]\n",
      "2020-12-23 02:46:09,954 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:09,955 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:09,956 : INFO : built Dictionary(56 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 184 corpus positions)\n",
      "2020-12-23 02:46:09,963 : INFO : token count processed\n",
      "2020-12-23 02:46:09,966 : INFO : frequencies processed\n",
      "2020-12-23 02:46:10,094 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:10,094 : INFO : entropies processed\n",
      "2020-12-23 02:46:10,095 : INFO : extropies processed\n",
      "2020-12-23 02:46:10,096 : INFO : token count processed\n",
      "2020-12-23 02:46:10,097 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:10,098 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:10,099 : INFO : vocab #2480\n",
      "2020-12-23 02:46:10,100 : INFO : diff #set()\n",
      "2020-12-23 02:46:10,358 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:10,486 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.2426616293008714, 0.44589874234025245], [0.9194590374827385, 0.08054096], [0.0, 0.0], [2.321928094887362, 5.195502554608948, 5.27470682830114, 2.2427238211951703, 2.9527787334137776, 0.07920427369219141]]\n",
      "2020-12-23 02:46:10,488 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:10,489 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:10,490 : INFO : built Dictionary(62 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 245 corpus positions)\n",
      "2020-12-23 02:46:10,498 : INFO : token count processed\n",
      "2020-12-23 02:46:10,500 : INFO : frequencies processed\n",
      "2020-12-23 02:46:10,628 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:10,629 : INFO : entropies processed\n",
      "2020-12-23 02:46:10,630 : INFO : extropies processed\n",
      "2020-12-23 02:46:10,631 : INFO : token count processed\n",
      "2020-12-23 02:46:10,632 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:10,633 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:10,634 : INFO : vocab #2480\n",
      "2020-12-23 02:46:10,635 : INFO : diff #set()\n",
      "2020-12-23 02:46:10,891 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:11,017 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.2815042340387452, 0.43830731719913946], [0.9786928072571754, 0.021307193], [0.0, 0.0], [2.321928094887362, 5.32027245610305, 5.394463008954745, 2.247737542035666, 3.072534914067383, 0.07419055285169573]]\n",
      "2020-12-23 02:46:11,019 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:11,020 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:11,021 : INFO : built Dictionary(163 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 485 corpus positions)\n",
      "2020-12-23 02:46:11,047 : INFO : token count processed\n",
      "2020-12-23 02:46:11,051 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:11,052 : INFO : frequencies processed\n",
      "2020-12-23 02:46:11,054 : INFO : token count processed\n",
      "2020-12-23 02:46:11,055 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:11,056 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:11,057 : INFO : vocab #2480\n",
      "2020-12-23 02:46:11,058 : INFO : diff #set()\n",
      "2020-12-23 02:46:11,316 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:11,447 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.2998963328396143, 0.4348022063956811], [0.9930864381603897, 0.006913562], [nan, nan], [2.321928094887362, 6.898202761357263, 6.931671354155984, 2.2884595020886405, 4.6097432592686225, 0.033468592798721275]]\n",
      "2020-12-23 02:46:11,449 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:11,450 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:11,452 : INFO : built Dictionary(127 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 507 corpus positions)\n",
      "2020-12-23 02:46:11,478 : INFO : token count processed\n",
      "2020-12-23 02:46:11,481 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:11,482 : INFO : frequencies processed\n",
      "2020-12-23 02:46:11,486 : INFO : token count processed\n",
      "2020-12-23 02:46:11,487 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:11,488 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:11,489 : INFO : vocab #2480\n",
      "2020-12-23 02:46:11,490 : INFO : diff #set()\n",
      "2020-12-23 02:46:11,745 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:11,872 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.2967417821432492, 0.4353994026558922], [0.9903431087732315, 0.009656891], [nan, nan], [2.321928094887362, 6.388500481644799, 6.427059467294418, 2.283369109237743, 4.105131372407056, 0.038558985649618904]]\n",
      "2020-12-23 02:46:11,874 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:11,875 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:46:11,876 : INFO : built Dictionary(47 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 171 corpus positions)\n",
      "2020-12-23 02:46:11,882 : INFO : token count processed\n",
      "2020-12-23 02:46:11,885 : INFO : frequencies processed\n",
      "2020-12-23 02:46:12,013 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:12,014 : INFO : entropies processed\n",
      "2020-12-23 02:46:12,015 : INFO : extropies processed\n",
      "2020-12-23 02:46:12,016 : INFO : token count processed\n",
      "2020-12-23 02:46:12,017 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:12,018 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:12,019 : INFO : vocab #2480\n",
      "2020-12-23 02:46:12,020 : INFO : diff #set()\n",
      "2020-12-23 02:46:12,277 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:12,406 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.2449011692347003, 0.4454539084858269], [0.9287552237510681, 0.071244776], [0.0, 0.0], [2.321928094887362, 4.8191513650620195, 4.913587583727101, 2.22749187622228, 2.591659488839739, 0.09443621866508156]]\n",
      "2020-12-23 02:46:12,408 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:12,409 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:12,410 : INFO : built Dictionary(53 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 226 corpus positions)\n",
      "2020-12-23 02:46:12,417 : INFO : token count processed\n",
      "2020-12-23 02:46:12,419 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:12,420 : INFO : frequencies processed\n",
      "2020-12-23 02:46:12,421 : INFO : token count processed\n",
      "2020-12-23 02:46:12,422 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:12,423 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:12,424 : INFO : vocab #2480\n",
      "2020-12-23 02:46:12,425 : INFO : diff #set()\n",
      "2020-12-23 02:46:12,682 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:12,809 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.2856734866200805, 0.4375078093410189], [0.986542628146708, 0.013457372], [nan, nan], [2.321928094887362, 5.062480936779194, 5.154788335904384, 2.229620695762171, 2.832860241017022, 0.09230739912519059]]\n",
      "2020-12-23 02:46:12,812 : INFO : Removed 0 and 223 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:12,813 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:12,816 : INFO : built Dictionary(243 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 1780 corpus positions)\n",
      "2020-12-23 02:46:12,868 : INFO : token count processed\n",
      "2020-12-23 02:46:12,872 : INFO : frequencies processed\n",
      "2020-12-23 02:46:13,000 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:13,001 : INFO : entropies processed\n",
      "2020-12-23 02:46:13,002 : INFO : extropies processed\n",
      "2020-12-23 02:46:13,005 : INFO : token count processed\n",
      "2020-12-23 02:46:13,006 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:13,007 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:13,007 : INFO : vocab #2480\n",
      "2020-12-23 02:46:13,008 : INFO : diff #set()\n",
      "2020-12-23 02:46:13,277 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:13,411 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.2959224100433562, 0.4355547886224587], [0.9862720919772983, 0.013727908], [0.0, 0.0], [2.321928094887362, 7.185085743102134, 7.196750187273841, 2.3102636507156564, 4.874822092386479, 0.0116644441717062]]\n",
      "2020-12-23 02:46:13,413 : INFO : Removed 0 and 148 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:13,414 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:13,415 : INFO : built Dictionary(160 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 755 corpus positions)\n",
      "2020-12-23 02:46:13,437 : INFO : token count processed\n",
      "2020-12-23 02:46:13,440 : INFO : frequencies processed\n",
      "2020-12-23 02:46:13,567 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:13,568 : INFO : entropies processed\n",
      "2020-12-23 02:46:13,569 : INFO : extropies processed\n",
      "2020-12-23 02:46:13,570 : INFO : token count processed\n",
      "2020-12-23 02:46:13,571 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:13,572 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:13,573 : INFO : vocab #2480\n",
      "2020-12-23 02:46:13,574 : INFO : diff #set()\n",
      "2020-12-23 02:46:13,833 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:13,960 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.2938128720881397, 0.4359553528399483], [0.9834794234484434, 0.016520577], [0.0, 0.0], [2.321928094887362, 6.591225336124281, 6.612742630761217, 2.300410800250426, 4.290814535873855, 0.021517294636936413]]\n",
      "2020-12-23 02:46:13,962 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:13,963 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:13,964 : INFO : built Dictionary(39 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 93 corpus positions)\n",
      "2020-12-23 02:46:13,969 : INFO : token count processed\n",
      "2020-12-23 02:46:13,972 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:13,972 : INFO : frequencies processed\n",
      "2020-12-23 02:46:13,973 : INFO : token count processed\n",
      "2020-12-23 02:46:13,974 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:13,975 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:13,976 : INFO : vocab #2480\n",
      "2020-12-23 02:46:13,977 : INFO : diff #set()\n",
      "2020-12-23 02:46:14,234 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:14,361 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.2843241782598689, 0.43776623717294394], [0.9906686125323176, 0.0093313875], [nan, nan], [2.321928094887362, 4.7032114441396695, 4.876349949256587, 2.1487895897704448, 2.554421854369225, 0.17313850511691786]]\n",
      "2020-12-23 02:46:14,363 : INFO : Removed 0 and 9 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:14,364 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:14,365 : INFO : built Dictionary(117 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 333 corpus positions)\n",
      "2020-12-23 02:46:14,386 : INFO : token count processed\n",
      "2020-12-23 02:46:14,388 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:14,389 : INFO : frequencies processed\n",
      "2020-12-23 02:46:14,391 : INFO : token count processed\n",
      "2020-12-23 02:46:14,392 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:14,393 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:14,393 : INFO : vocab #2480\n",
      "2020-12-23 02:46:14,395 : INFO : diff #set()\n",
      "2020-12-23 02:46:14,663 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:14,793 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.299007364159334, 0.4349703335403037], [0.9958830010145903, 0.004116999], [nan, nan], [2.321928094887362, 6.14228447828618, 6.19649011115536, 2.267722462018182, 3.874562016267998, 0.054205632869180675]]\n",
      "2020-12-23 02:46:14,795 : INFO : Removed 0 and 184 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:14,796 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:14,797 : INFO : built Dictionary(254 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 1123 corpus positions)\n",
      "2020-12-23 02:46:14,839 : INFO : token count processed\n",
      "2020-12-23 02:46:14,846 : INFO : frequencies processed\n",
      "2020-12-23 02:46:14,973 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:14,974 : INFO : entropies processed\n",
      "2020-12-23 02:46:14,975 : INFO : extropies processed\n",
      "2020-12-23 02:46:14,976 : INFO : token count processed\n",
      "2020-12-23 02:46:14,977 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:14,978 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:14,978 : INFO : vocab #2480\n",
      "2020-12-23 02:46:14,979 : INFO : diff #set()\n",
      "2020-12-23 02:46:15,236 : INFO : alphabet #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:46:15,363 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.2928949023948033, 0.43612988931832625], [0.9786911122500896, 0.021308888], [0.0, 0.0], [2.321928094887362, 7.450178124335845, 7.46430503620536, 2.3078011830178475, 5.142376941317998, 0.01412691186951509]]\n",
      "2020-12-23 02:46:15,366 : INFO : Removed 0 and 16 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:15,367 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:15,368 : INFO : built Dictionary(56 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 171 corpus positions)\n",
      "2020-12-23 02:46:15,381 : INFO : token count processed\n",
      "2020-12-23 02:46:15,383 : INFO : frequencies processed\n",
      "2020-12-23 02:46:15,523 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:15,523 : INFO : entropies processed\n",
      "2020-12-23 02:46:15,524 : INFO : extropies processed\n",
      "2020-12-23 02:46:15,525 : INFO : token count processed\n",
      "2020-12-23 02:46:15,526 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:15,527 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:15,527 : INFO : vocab #2480\n",
      "2020-12-23 02:46:15,528 : INFO : diff #set()\n",
      "2020-12-23 02:46:15,795 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:15,925 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.2359788618785787, 0.44723141933454624], [0.9203472137451172, 0.079652786], [0.0, 0.0], [2.321928094887362, 5.20665021947654, 5.285129785975824, 2.243448528388077, 2.963201691088462, 0.07847956649928456]]\n",
      "2020-12-23 02:46:15,927 : INFO : Removed 0 and 89 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:15,928 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:15,930 : INFO : built Dictionary(126 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 509 corpus positions)\n",
      "2020-12-23 02:46:15,959 : INFO : token count processed\n",
      "2020-12-23 02:46:15,964 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:15,965 : INFO : frequencies processed\n",
      "2020-12-23 02:46:15,966 : INFO : token count processed\n",
      "2020-12-23 02:46:15,967 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:15,968 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:15,969 : INFO : vocab #2480\n",
      "2020-12-23 02:46:15,970 : INFO : diff #set()\n",
      "2020-12-23 02:46:16,229 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:16,359 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.3042666215162058, 0.43397755739828436], [0.9976028178352863, 0.0023971822], [nan, nan], [2.321928094887362, 6.524718477352, 6.559299832064252, 2.287346740175109, 4.23737173717689, 0.034581354712252654]]\n",
      "2020-12-23 02:46:16,362 : INFO : Removed 0 and 8 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:16,363 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:16,364 : INFO : built Dictionary(62 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 216 corpus positions)\n",
      "2020-12-23 02:46:16,372 : INFO : token count processed\n",
      "2020-12-23 02:46:16,374 : INFO : frequencies processed\n",
      "2020-12-23 02:46:16,502 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:16,503 : INFO : entropies processed\n",
      "2020-12-23 02:46:16,504 : INFO : extropies processed\n",
      "2020-12-23 02:46:16,505 : INFO : token count processed\n",
      "2020-12-23 02:46:16,506 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:16,507 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:16,507 : INFO : vocab #2480\n",
      "2020-12-23 02:46:16,508 : INFO : diff #set()\n",
      "2020-12-23 02:46:16,766 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:16,893 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.258676718176832, 0.4427371088356478], [0.93920723721385, 0.060792763], [0.0, 0.0], [2.321928094887362, 5.321859380715434, 5.3917687782814125, 2.2520186973213834, 3.0698406833940504, 0.06990939756597836]]\n",
      "2020-12-23 02:46:16,896 : INFO : Removed 0 and 119 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:16,897 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:16,899 : INFO : built Dictionary(141 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 834 corpus positions)\n",
      "2020-12-23 02:46:16,931 : INFO : token count processed\n",
      "2020-12-23 02:46:16,937 : INFO : frequencies processed\n",
      "2020-12-23 02:46:17,065 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:17,066 : INFO : entropies processed\n",
      "2020-12-23 02:46:17,066 : INFO : extropies processed\n",
      "2020-12-23 02:46:17,067 : INFO : token count processed\n",
      "2020-12-23 02:46:17,068 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:17,068 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:17,069 : INFO : vocab #2480\n",
      "2020-12-23 02:46:17,070 : INFO : diff #set()\n",
      "2020-12-23 02:46:17,327 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:17,455 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.2961143758975653, 0.43551837421386896], [0.9844515295699239, 0.01554847], [0.0, 0.0], [2.321928094887362, 6.500767808767801, 6.522039799740408, 2.300656103914755, 4.200111704853045, 0.02127199097260668]]\n",
      "2020-12-23 02:46:17,457 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:17,458 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:17,460 : INFO : built Dictionary(35 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 56 corpus positions)\n",
      "2020-12-23 02:46:17,470 : INFO : token count processed\n",
      "2020-12-23 02:46:17,472 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:17,472 : INFO : frequencies processed\n",
      "2020-12-23 02:46:17,474 : INFO : token count processed\n",
      "2020-12-23 02:46:17,474 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:17,475 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:17,476 : INFO : vocab #2480\n",
      "2020-12-23 02:46:17,477 : INFO : diff #set()\n",
      "2020-12-23 02:46:17,735 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:17,863 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.2949360401657528, 0.43574199127910096], [0.9944660193286836, 0.0055339807], [nan, nan], [2.321928094887362, 4.736228843383063, 4.953259040701274, 2.104897897569151, 2.6313309458139122, 0.21703019731821183]]\n",
      "2020-12-23 02:46:17,866 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:17,866 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:17,867 : INFO : built Dictionary(97 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 397 corpus positions)\n",
      "2020-12-23 02:46:17,884 : INFO : token count processed\n",
      "2020-12-23 02:46:17,886 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:17,886 : INFO : frequencies processed\n",
      "2020-12-23 02:46:17,888 : INFO : token count processed\n",
      "2020-12-23 02:46:17,889 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:17,890 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:17,890 : INFO : vocab #2480\n",
      "2020-12-23 02:46:17,891 : INFO : diff #set()\n",
      "2020-12-23 02:46:18,150 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:18,278 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.3028243796033505, 0.43424935433949363], [0.9929557889699936, 0.007044211], [nan, nan], [2.321928094887362, 5.788442787590127, 5.841100153960053, 2.269270728517437, 3.5191720590726905, 0.05265736636992546]]\n",
      "2020-12-23 02:46:18,280 : INFO : Removed 0 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:18,281 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:18,282 : INFO : built Dictionary(53 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 83 corpus positions)\n",
      "2020-12-23 02:46:18,289 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:46:18,291 : INFO : frequencies processed\n",
      "2020-12-23 02:46:18,420 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:18,420 : INFO : entropies processed\n",
      "2020-12-23 02:46:18,421 : INFO : extropies processed\n",
      "2020-12-23 02:46:18,422 : INFO : token count processed\n",
      "2020-12-23 02:46:18,423 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:18,424 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:18,425 : INFO : vocab #2480\n",
      "2020-12-23 02:46:18,426 : INFO : diff #set()\n",
      "2020-12-23 02:46:18,683 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:18,811 : INFO : Computed distances or similarities ('290', 'sacp-python-common/setup.py')[[1.2669401572853956, 0.441123245704675], [0.9542643241584301, 0.045735676], [0.0, 0.0], [2.321928094887362, 5.370004292053436, 5.488381712079574, 2.2035506748612246, 3.166453617192212, 0.11837742002613805]]\n",
      "2020-12-23 02:46:18,813 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:18,814 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:18,815 : INFO : built Dictionary(80 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 328 corpus positions)\n",
      "2020-12-23 02:46:18,830 : INFO : token count processed\n",
      "2020-12-23 02:46:18,835 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:18,837 : INFO : frequencies processed\n",
      "2020-12-23 02:46:18,839 : INFO : token count processed\n",
      "2020-12-23 02:46:18,841 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:18,842 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:18,843 : INFO : vocab #2480\n",
      "2020-12-23 02:46:18,844 : INFO : diff #set()\n",
      "2020-12-23 02:46:19,105 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:19,236 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.3003961527250782, 0.43470773449842], [0.9984288284322247, 0.0015711716], [nan, nan], [2.321928094887362, 5.695663584743922, 5.75472046590454, 2.2628712137267444, 3.4327923710171775, 0.05905688116061736]]\n",
      "2020-12-23 02:46:19,238 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:19,239 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:19,240 : INFO : built Dictionary(40 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 103 corpus positions)\n",
      "2020-12-23 02:46:19,246 : INFO : token count processed\n",
      "2020-12-23 02:46:19,248 : INFO : frequencies processed\n",
      "2020-12-23 02:46:19,376 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:19,377 : INFO : entropies processed\n",
      "2020-12-23 02:46:19,377 : INFO : extropies processed\n",
      "2020-12-23 02:46:19,379 : INFO : token count processed\n",
      "2020-12-23 02:46:19,379 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:19,380 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:19,381 : INFO : vocab #2480\n",
      "2020-12-23 02:46:19,382 : INFO : diff #set()\n",
      "2020-12-23 02:46:19,640 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:19,767 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.2663734033382243, 0.4412335577743118], [0.9503937587141991, 0.04960624], [0.0, 0.0], [2.321928094887362, 4.9004417692112465, 5.026539772160876, 2.195830091937732, 2.704611677273514, 0.1260980029496297]]\n",
      "2020-12-23 02:46:19,769 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:19,770 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:19,772 : INFO : built Dictionary(39 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 110 corpus positions)\n",
      "2020-12-23 02:46:19,785 : INFO : token count processed\n",
      "2020-12-23 02:46:19,788 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:19,788 : INFO : frequencies processed\n",
      "2020-12-23 02:46:19,789 : INFO : token count processed\n",
      "2020-12-23 02:46:19,790 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:19,791 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:19,792 : INFO : vocab #2480\n",
      "2020-12-23 02:46:19,794 : INFO : diff #set()\n",
      "2020-12-23 02:46:20,065 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:20,197 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.2977969978414792, 0.43519945449462555], [0.991265831515193, 0.0087341685], [nan, nan], [2.321928094887362, 4.778624108914332, 4.931360852053181, 2.1691913517485135, 2.6094327571658185, 0.15273674313884822]]\n",
      "2020-12-23 02:46:20,199 : INFO : Removed 0 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:20,200 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:20,201 : INFO : built Dictionary(39 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 141 corpus positions)\n",
      "2020-12-23 02:46:20,206 : INFO : token count processed\n",
      "2020-12-23 02:46:20,209 : INFO : frequencies processed\n",
      "2020-12-23 02:46:20,336 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:20,337 : INFO : entropies processed\n",
      "2020-12-23 02:46:20,338 : INFO : extropies processed\n",
      "2020-12-23 02:46:20,339 : INFO : token count processed\n",
      "2020-12-23 02:46:20,340 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:20,341 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:20,342 : INFO : vocab #2480\n",
      "2020-12-23 02:46:20,343 : INFO : diff #set()\n",
      "2020-12-23 02:46:20,602 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:20,730 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.287748651978533, 0.4371109558452418], [0.9789982605725527, 0.02100174], [0.0, 0.0], [2.321928094887362, 4.773880192225086, 4.8905565164997435, 2.205251770612704, 2.5686284216123814, 0.1166763242746578]]\n",
      "2020-12-23 02:46:20,733 : INFO : Removed 0 and 346 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:20,734 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:20,735 : INFO : built Dictionary(149 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 1966 corpus positions)\n",
      "2020-12-23 02:46:20,768 : INFO : token count processed\n",
      "2020-12-23 02:46:20,775 : INFO : frequencies processed\n",
      "2020-12-23 02:46:20,905 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:20,905 : INFO : entropies processed\n",
      "2020-12-23 02:46:20,906 : INFO : extropies processed\n",
      "2020-12-23 02:46:20,907 : INFO : token count processed\n",
      "2020-12-23 02:46:20,908 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:20,908 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:20,909 : INFO : vocab #2480\n",
      "2020-12-23 02:46:20,910 : INFO : diff #set()\n",
      "2020-12-23 02:46:21,178 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:21,307 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.2388985044422482, 0.4466482058100793], [0.8702835291624069, 0.12971647], [1.0, 1.0], [2.321928094887362, 6.620773041953877, 6.628203006846534, 2.3144981299947043, 4.3062749119591714, 0.007429964892657459]]\n",
      "2020-12-23 02:46:21,310 : INFO : Removed 0 and 132 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:21,310 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:21,312 : INFO : built Dictionary(81 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 529 corpus positions)\n",
      "2020-12-23 02:46:21,332 : INFO : token count processed\n",
      "2020-12-23 02:46:21,334 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:21,334 : INFO : frequencies processed\n",
      "2020-12-23 02:46:21,337 : INFO : token count processed\n",
      "2020-12-23 02:46:21,338 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:21,340 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:21,342 : INFO : vocab #2480\n",
      "2020-12-23 02:46:21,344 : INFO : diff #set()\n",
      "2020-12-23 02:46:21,603 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:21,730 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.2885512227162421, 0.43695766565063693], [0.9884564420208335, 0.011543558], [nan, nan], [2.321928094887362, 5.828370634755606, 5.866020866001207, 2.28427786364176, 3.5440927711138452, 0.03765023124560152]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:46:21,733 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:21,734 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:21,735 : INFO : built Dictionary(81 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 292 corpus positions)\n",
      "2020-12-23 02:46:21,745 : INFO : token count processed\n",
      "2020-12-23 02:46:21,748 : INFO : frequencies processed\n",
      "2020-12-23 02:46:21,875 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:21,876 : INFO : entropies processed\n",
      "2020-12-23 02:46:21,877 : INFO : extropies processed\n",
      "2020-12-23 02:46:21,878 : INFO : token count processed\n",
      "2020-12-23 02:46:21,879 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:21,880 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:21,880 : INFO : vocab #2480\n",
      "2020-12-23 02:46:21,881 : INFO : diff #set()\n",
      "2020-12-23 02:46:22,140 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:22,269 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.2917043114001476, 0.43635646842634623], [0.9835703019052744, 0.016429698], [0.0, 0.0], [2.321928094887362, 5.774409284925443, 5.830225803672071, 2.2661115761407347, 3.5082977087847085, 0.05581651874662796]]\n",
      "2020-12-23 02:46:22,271 : INFO : Removed 0 and 51 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:22,272 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:22,274 : INFO : built Dictionary(92 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 341 corpus positions)\n",
      "2020-12-23 02:46:22,298 : INFO : token count processed\n",
      "2020-12-23 02:46:22,301 : INFO : frequencies processed\n",
      "2020-12-23 02:46:22,431 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:22,432 : INFO : entropies processed\n",
      "2020-12-23 02:46:22,432 : INFO : extropies processed\n",
      "2020-12-23 02:46:22,433 : INFO : token count processed\n",
      "2020-12-23 02:46:22,434 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:22,434 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:22,435 : INFO : vocab #2480\n",
      "2020-12-23 02:46:22,436 : INFO : diff #set()\n",
      "2020-12-23 02:46:22,692 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:22,820 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.2661774111878275, 0.44127171820843686], [0.9433058090507984, 0.05669419], [0.0, 0.0], [2.321928094887362, 5.977819040873918, 6.018182313955359, 2.2815648218059206, 3.696254219067997, 0.040363273081441164]]\n",
      "2020-12-23 02:46:22,823 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:22,824 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:22,825 : INFO : built Dictionary(73 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 288 corpus positions)\n",
      "2020-12-23 02:46:22,841 : INFO : token count processed\n",
      "2020-12-23 02:46:22,844 : INFO : frequencies processed\n",
      "2020-12-23 02:46:22,976 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:22,976 : INFO : entropies processed\n",
      "2020-12-23 02:46:22,977 : INFO : extropies processed\n",
      "2020-12-23 02:46:22,978 : INFO : token count processed\n",
      "2020-12-23 02:46:22,979 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:22,980 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:22,981 : INFO : vocab #2480\n",
      "2020-12-23 02:46:22,982 : INFO : diff #set()\n",
      "2020-12-23 02:46:23,242 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:23,369 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.2752899031528815, 0.4395044335292372], [0.9548006169497967, 0.045199383], [0.0, 0.0], [2.321928094887362, 5.901812829596593, 5.9509305840619655, 2.27281034042199, 3.6290024891746033, 0.0491177544653727]]\n",
      "2020-12-23 02:46:23,371 : INFO : Removed 0 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:23,372 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:23,374 : INFO : built Dictionary(75 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 288 corpus positions)\n",
      "2020-12-23 02:46:23,391 : INFO : token count processed\n",
      "2020-12-23 02:46:23,393 : INFO : frequencies processed\n",
      "2020-12-23 02:46:23,522 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:23,523 : INFO : entropies processed\n",
      "2020-12-23 02:46:23,524 : INFO : extropies processed\n",
      "2020-12-23 02:46:23,525 : INFO : token count processed\n",
      "2020-12-23 02:46:23,526 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:23,527 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:23,528 : INFO : vocab #2480\n",
      "2020-12-23 02:46:23,529 : INFO : diff #set()\n",
      "2020-12-23 02:46:23,795 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:23,921 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.284588229619222, 0.43771564040959476], [0.9777074996381998, 0.0222925], [0.0, 0.0], [2.321928094887362, 5.643202320803383, 5.702413000193626, 2.262717415497118, 3.380484905306264, 0.059210679390243826]]\n",
      "2020-12-23 02:46:23,924 : INFO : Removed 0 and 30 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:23,925 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:23,926 : INFO : built Dictionary(89 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 322 corpus positions)\n",
      "2020-12-23 02:46:23,937 : INFO : token count processed\n",
      "2020-12-23 02:46:23,940 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:23,940 : INFO : frequencies processed\n",
      "2020-12-23 02:46:23,942 : INFO : token count processed\n",
      "2020-12-23 02:46:23,943 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:23,943 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:23,944 : INFO : vocab #2480\n",
      "2020-12-23 02:46:23,945 : INFO : diff #set()\n",
      "2020-12-23 02:46:24,204 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:24,332 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.2955996996568373, 0.4356160179623159], [0.9959599422290921, 0.004040058], [nan, nan], [2.321928094887362, 5.925214310725336, 5.981558303231146, 2.2655841023815517, 3.6596302083437835, 0.056343992505810014]]\n",
      "2020-12-23 02:46:24,335 : INFO : Removed 0 and 293 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:24,335 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:24,337 : INFO : built Dictionary(160 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 1717 corpus positions)\n",
      "2020-12-23 02:46:24,365 : INFO : token count processed\n",
      "2020-12-23 02:46:24,370 : INFO : frequencies processed\n",
      "2020-12-23 02:46:24,497 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:24,498 : INFO : entropies processed\n",
      "2020-12-23 02:46:24,499 : INFO : extropies processed\n",
      "2020-12-23 02:46:24,500 : INFO : token count processed\n",
      "2020-12-23 02:46:24,501 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:24,502 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:24,502 : INFO : vocab #2480\n",
      "2020-12-23 02:46:24,503 : INFO : diff #set()\n",
      "2020-12-23 02:46:24,777 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:24,906 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.2302454129991491, 0.44838114862670564], [0.8580887317657471, 0.14191127], [1.0, 1.0], [2.321928094887362, 6.551685682764175, 6.559808137542224, 2.3138056401093126, 4.2378800426548615, 0.008122454778049182]]\n",
      "2020-12-23 02:46:24,908 : INFO : Removed 0 and 88 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:24,909 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:24,911 : INFO : built Dictionary(136 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 558 corpus positions)\n",
      "2020-12-23 02:46:24,940 : INFO : token count processed\n",
      "2020-12-23 02:46:24,942 : INFO : frequencies processed\n",
      "2020-12-23 02:46:25,070 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:25,070 : INFO : entropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:46:25,071 : INFO : extropies processed\n",
      "2020-12-23 02:46:25,072 : INFO : token count processed\n",
      "2020-12-23 02:46:25,073 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:25,074 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:25,074 : INFO : vocab #2480\n",
      "2020-12-23 02:46:25,075 : INFO : diff #set()\n",
      "2020-12-23 02:46:25,338 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:25,467 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.276483185622447, 0.4392740549614801], [0.9502915628254414, 0.049708437], [1.0, 1.0], [2.321928094887362, 6.642985062562557, 6.663771800160044, 2.3011413572898753, 4.341843705272682, 0.020786737597486393]]\n",
      "2020-12-23 02:46:25,469 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:25,470 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:25,471 : INFO : built Dictionary(50 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 153 corpus positions)\n",
      "2020-12-23 02:46:25,484 : INFO : token count processed\n",
      "2020-12-23 02:46:25,486 : INFO : frequencies processed\n",
      "2020-12-23 02:46:25,613 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:25,614 : INFO : entropies processed\n",
      "2020-12-23 02:46:25,615 : INFO : extropies processed\n",
      "2020-12-23 02:46:25,616 : INFO : token count processed\n",
      "2020-12-23 02:46:25,617 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:25,618 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:25,618 : INFO : vocab #2480\n",
      "2020-12-23 02:46:25,619 : INFO : diff #set()\n",
      "2020-12-23 02:46:25,878 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:26,006 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.2676251354676396, 0.44098999625605034], [0.9538821764290333, 0.046117824], [0.0, 0.0], [2.321928094887362, 5.2461980344571995, 5.3362401823698224, 2.2318859469747387, 3.0143120874824603, 0.090042147912623]]\n",
      "2020-12-23 02:46:26,008 : INFO : Removed 0 and 23 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:26,009 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:26,010 : INFO : built Dictionary(73 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 213 corpus positions)\n",
      "2020-12-23 02:46:26,019 : INFO : token count processed\n",
      "2020-12-23 02:46:26,021 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:26,022 : INFO : frequencies processed\n",
      "2020-12-23 02:46:26,023 : INFO : token count processed\n",
      "2020-12-23 02:46:26,024 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:26,025 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:26,026 : INFO : vocab #2480\n",
      "2020-12-23 02:46:26,027 : INFO : diff #set()\n",
      "2020-12-23 02:46:26,284 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:26,411 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/test_auth_utility.py')[[1.3026834572101877, 0.43427593005403714], [0.9938656892627478, 0.0061343107], [nan, nan], [2.321928094887362, 5.903090303960449, 5.975269454648936, 2.249748944198876, 3.653341359761574, 0.07217915068848679]]\n",
      "2020-12-23 02:46:26,414 : INFO : Removed 0 and 442 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:26,415 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:26,416 : INFO : built Dictionary(107 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 1212 corpus positions)\n",
      "2020-12-23 02:46:26,441 : INFO : token count processed\n",
      "2020-12-23 02:46:26,443 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:26,444 : INFO : frequencies processed\n",
      "2020-12-23 02:46:26,445 : INFO : token count processed\n",
      "2020-12-23 02:46:26,446 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:26,447 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:26,448 : INFO : vocab #2480\n",
      "2020-12-23 02:46:26,449 : INFO : diff #set()\n",
      "2020-12-23 02:46:26,702 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:26,829 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.3078258145043558, 0.4333082651711159], [0.9933257917873561, 0.006674208], [nan, nan], [2.321928094887362, 6.16659449033757, 6.18462853282198, 2.303894052402951, 3.862700437934618, 0.01803404248441076]]\n",
      "2020-12-23 02:46:26,831 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:26,832 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:26,833 : INFO : built Dictionary(66 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 247 corpus positions)\n",
      "2020-12-23 02:46:26,842 : INFO : token count processed\n",
      "2020-12-23 02:46:26,844 : INFO : frequencies processed\n",
      "2020-12-23 02:46:26,972 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:26,973 : INFO : entropies processed\n",
      "2020-12-23 02:46:26,973 : INFO : extropies processed\n",
      "2020-12-23 02:46:26,974 : INFO : token count processed\n",
      "2020-12-23 02:46:26,975 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:26,976 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:26,977 : INFO : vocab #2480\n",
      "2020-12-23 02:46:26,978 : INFO : diff #set()\n",
      "2020-12-23 02:46:27,236 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:27,363 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.2767720498227597, 0.4392183223076053], [0.954395804554224, 0.045604195], [0.0, 0.0], [2.321928094887362, 5.906856253399655, 5.956146953392036, 2.272637394894982, 3.6342188585046737, 0.04929069999238056]]\n",
      "2020-12-23 02:46:27,366 : INFO : Removed 0 and 68 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:27,367 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:27,367 : INFO : built Dictionary(80 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 246 corpus positions)\n",
      "2020-12-23 02:46:27,387 : INFO : token count processed\n",
      "2020-12-23 02:46:27,389 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:27,390 : INFO : frequencies processed\n",
      "2020-12-23 02:46:27,391 : INFO : token count processed\n",
      "2020-12-23 02:46:27,392 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:27,393 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:27,393 : INFO : vocab #2480\n",
      "2020-12-23 02:46:27,394 : INFO : diff #set()\n",
      "2020-12-23 02:46:27,646 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:27,773 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.296700053683189, 0.4354073133739482], [0.9941688543185592, 0.0058311457], [nan, nan], [2.321928094887362, 5.965115449163356, 6.024998450066866, 2.2620450939838523, 3.7030703551795034, 0.0598830009035094]]\n",
      "2020-12-23 02:46:27,776 : INFO : Removed 0 and 13 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:27,776 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:27,778 : INFO : built Dictionary(86 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 360 corpus positions)\n",
      "2020-12-23 02:46:27,788 : INFO : token count processed\n",
      "2020-12-23 02:46:27,791 : INFO : frequencies processed\n",
      "2020-12-23 02:46:27,919 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:27,920 : INFO : entropies processed\n",
      "2020-12-23 02:46:27,920 : INFO : extropies processed\n",
      "2020-12-23 02:46:27,922 : INFO : token count processed\n",
      "2020-12-23 02:46:27,922 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:27,923 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:27,924 : INFO : vocab #2480\n",
      "2020-12-23 02:46:27,925 : INFO : diff #set()\n",
      "2020-12-23 02:46:28,183 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:28,311 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.2653470794118709, 0.4414334602800114], [0.9381281472742558, 0.061871853], [0.0, 0.0], [2.321928094887362, 5.791362404253194, 5.836353148636253, 2.2769373505043022, 3.5144250537488912, 0.0449907443830595]]\n",
      "2020-12-23 02:46:28,313 : INFO : Removed 0 and 41 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:46:28,314 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:28,316 : INFO : built Dictionary(76 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 372 corpus positions)\n",
      "2020-12-23 02:46:28,331 : INFO : token count processed\n",
      "2020-12-23 02:46:28,334 : INFO : frequencies processed\n",
      "2020-12-23 02:46:28,462 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:28,463 : INFO : entropies processed\n",
      "2020-12-23 02:46:28,463 : INFO : extropies processed\n",
      "2020-12-23 02:46:28,465 : INFO : token count processed\n",
      "2020-12-23 02:46:28,466 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:28,467 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:28,467 : INFO : vocab #2480\n",
      "2020-12-23 02:46:28,468 : INFO : diff #set()\n",
      "2020-12-23 02:46:28,734 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:28,862 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.284522190383892, 0.437728293561447], [0.9789576306939125, 0.02104237], [0.0, 0.0], [2.321928094887362, 5.651670454631116, 5.700971351480281, 2.2726271980381965, 3.379043256592919, 0.04930089684916528]]\n",
      "2020-12-23 02:46:28,864 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:28,865 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:28,866 : INFO : built Dictionary(40 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 82 corpus positions)\n",
      "2020-12-23 02:46:28,871 : INFO : token count processed\n",
      "2020-12-23 02:46:28,873 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:28,874 : INFO : frequencies processed\n",
      "2020-12-23 02:46:28,875 : INFO : token count processed\n",
      "2020-12-23 02:46:28,876 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:28,877 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:28,878 : INFO : vocab #2480\n",
      "2020-12-23 02:46:28,879 : INFO : diff #set()\n",
      "2020-12-23 02:46:29,138 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:29,265 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.3040615706005523, 0.4340161794111042], [1.0, 0.0], [nan, nan], [2.321928094887362, 4.8226207261920235, 5.000377305705025, 2.144171515374361, 2.678449210817663, 0.1777565795130016]]\n",
      "2020-12-23 02:46:29,267 : INFO : Removed 0 and 50 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:29,268 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:29,269 : INFO : built Dictionary(84 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 252 corpus positions)\n",
      "2020-12-23 02:46:29,290 : INFO : token count processed\n",
      "2020-12-23 02:46:29,292 : INFO : frequencies processed\n",
      "2020-12-23 02:46:29,423 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:29,424 : INFO : entropies processed\n",
      "2020-12-23 02:46:29,424 : INFO : extropies processed\n",
      "2020-12-23 02:46:29,426 : INFO : token count processed\n",
      "2020-12-23 02:46:29,427 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:29,428 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:29,428 : INFO : vocab #2480\n",
      "2020-12-23 02:46:29,429 : INFO : diff #set()\n",
      "2020-12-23 02:46:29,695 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:29,822 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.2814096841129095, 0.43832548225060874], [0.9543108940124512, 0.045689106], [0.0, 0.0], [2.321928094887362, 6.24862851613934, 6.293306229196337, 2.277250381830366, 3.9713781343089747, 0.04467771305699664]]\n",
      "2020-12-23 02:46:29,825 : INFO : Removed 0 and 32 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:29,825 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:29,826 : INFO : built Dictionary(85 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 368 corpus positions)\n",
      "2020-12-23 02:46:29,844 : INFO : token count processed\n",
      "2020-12-23 02:46:29,849 : INFO : frequencies processed\n",
      "2020-12-23 02:46:29,980 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:29,981 : INFO : entropies processed\n",
      "2020-12-23 02:46:29,982 : INFO : extropies processed\n",
      "2020-12-23 02:46:29,983 : INFO : token count processed\n",
      "2020-12-23 02:46:29,984 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:29,985 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:29,985 : INFO : vocab #2480\n",
      "2020-12-23 02:46:29,986 : INFO : diff #set()\n",
      "2020-12-23 02:46:30,253 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:30,382 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.2728321505081943, 0.43997969659853886], [0.945213183760643, 0.054786816], [0.0, 0.0], [2.321928094887362, 5.850156917433494, 5.892644389588948, 2.279440622731908, 3.5707162947015862, 0.042487472155454675]]\n",
      "2020-12-23 02:46:30,385 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:30,386 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:30,387 : INFO : built Dictionary(80 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 366 corpus positions)\n",
      "2020-12-23 02:46:30,398 : INFO : token count processed\n",
      "2020-12-23 02:46:30,400 : INFO : frequencies processed\n",
      "2020-12-23 02:46:30,527 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:30,528 : INFO : entropies processed\n",
      "2020-12-23 02:46:30,529 : INFO : extropies processed\n",
      "2020-12-23 02:46:30,531 : INFO : token count processed\n",
      "2020-12-23 02:46:30,532 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:30,534 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:30,534 : INFO : vocab #2480\n",
      "2020-12-23 02:46:30,536 : INFO : diff #set()\n",
      "2020-12-23 02:46:30,801 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:30,928 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.2864290039985207, 0.4373632412164095], [0.9792955163866282, 0.020704484], [0.0, 0.0], [2.321928094887362, 5.6831976040360095, 5.7324225414963275, 2.2727031574270447, 3.4104944466089653, 0.049224937460317975]]\n",
      "2020-12-23 02:46:30,931 : INFO : Removed 0 and 52 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:30,932 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:30,933 : INFO : built Dictionary(67 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 344 corpus positions)\n",
      "2020-12-23 02:46:30,949 : INFO : token count processed\n",
      "2020-12-23 02:46:30,952 : INFO : frequencies processed\n",
      "2020-12-23 02:46:31,080 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:31,081 : INFO : entropies processed\n",
      "2020-12-23 02:46:31,082 : INFO : extropies processed\n",
      "2020-12-23 02:46:31,083 : INFO : token count processed\n",
      "2020-12-23 02:46:31,084 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:31,085 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:31,085 : INFO : vocab #2480\n",
      "2020-12-23 02:46:31,086 : INFO : diff #set()\n",
      "2020-12-23 02:46:31,343 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:31,471 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.284668576770309, 0.4377002468400194], [0.9658960998058319, 0.0341039], [0.0, 0.0], [2.321928094887362, 5.749308601266266, 5.793924379721136, 2.277312316432492, 3.4719962848337738, 0.04461577845486975]]\n",
      "2020-12-23 02:46:31,474 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:31,474 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:31,475 : INFO : built Dictionary(61 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 225 corpus positions)\n",
      "2020-12-23 02:46:31,483 : INFO : token count processed\n",
      "2020-12-23 02:46:31,485 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:31,486 : INFO : frequencies processed\n",
      "2020-12-23 02:46:31,487 : INFO : token count processed\n",
      "2020-12-23 02:46:31,488 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:31,489 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:31,490 : INFO : vocab #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:46:31,491 : INFO : diff #set()\n",
      "2020-12-23 02:46:31,750 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:31,878 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.2898819231817045, 0.43670373999482803], [0.9863807335495949, 0.013619266], [nan, nan], [2.321928094887362, 5.015422548793484, 5.109037005149054, 2.228313638531792, 2.787108910261692, 0.09361445635557075]]\n",
      "2020-12-23 02:46:31,880 : INFO : Removed 0 and 81 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:31,881 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:31,882 : INFO : built Dictionary(89 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 351 corpus positions)\n",
      "2020-12-23 02:46:31,900 : INFO : token count processed\n",
      "2020-12-23 02:46:31,905 : INFO : frequencies processed\n",
      "2020-12-23 02:46:32,037 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:32,038 : INFO : entropies processed\n",
      "2020-12-23 02:46:32,039 : INFO : extropies processed\n",
      "2020-12-23 02:46:32,040 : INFO : token count processed\n",
      "2020-12-23 02:46:32,041 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:32,042 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:32,043 : INFO : vocab #2480\n",
      "2020-12-23 02:46:32,044 : INFO : diff #set()\n",
      "2020-12-23 02:46:32,313 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:32,441 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.2560375253036817, 0.44325503843974917], [0.9289059191942215, 0.07109408], [0.0, 0.0], [2.321928094887362, 6.030001281822029, 6.068074161733978, 2.2838552149754126, 3.746146066846616, 0.03807287991194919]]\n",
      "2020-12-23 02:46:32,443 : INFO : Removed 0 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:32,444 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:32,445 : INFO : built Dictionary(77 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 314 corpus positions)\n",
      "2020-12-23 02:46:32,455 : INFO : token count processed\n",
      "2020-12-23 02:46:32,458 : INFO : frequencies processed\n",
      "2020-12-23 02:46:32,586 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:32,586 : INFO : entropies processed\n",
      "2020-12-23 02:46:32,587 : INFO : extropies processed\n",
      "2020-12-23 02:46:32,588 : INFO : token count processed\n",
      "2020-12-23 02:46:32,589 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:32,590 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:32,591 : INFO : vocab #2480\n",
      "2020-12-23 02:46:32,592 : INFO : diff #set()\n",
      "2020-12-23 02:46:32,849 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:32,976 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.2989193745164644, 0.43498698174673117], [0.9870516117662191, 0.012948388], [0.0, 0.0], [2.321928094887362, 5.9537092545441395, 6.003385190868302, 2.2722521585632, 3.68145709598094, 0.049675936324162784]]\n",
      "2020-12-23 02:46:32,979 : INFO : Removed 0 and 90 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:32,979 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:32,981 : INFO : built Dictionary(89 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 315 corpus positions)\n",
      "2020-12-23 02:46:33,000 : INFO : token count processed\n",
      "2020-12-23 02:46:33,002 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:33,004 : INFO : frequencies processed\n",
      "2020-12-23 02:46:33,006 : INFO : token count processed\n",
      "2020-12-23 02:46:33,008 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:33,009 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:33,011 : INFO : vocab #2480\n",
      "2020-12-23 02:46:33,012 : INFO : diff #set()\n",
      "2020-12-23 02:46:33,271 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:33,397 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.2998304230856796, 0.4348146671867665], [0.9960147249512374, 0.003985275], [nan, nan], [2.321928094887362, 6.184756445474906, 6.233037609574636, 2.273646930787633, 3.9111095146872734, 0.0482811640997296]]\n",
      "2020-12-23 02:46:33,400 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:33,401 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:33,402 : INFO : built Dictionary(97 unique tokens: ['bug', 'find', 'merg', 'product', 'sec']...) from 2 documents (total 428 corpus positions)\n",
      "2020-12-23 02:46:33,415 : INFO : token count processed\n",
      "2020-12-23 02:46:33,417 : INFO : frequencies processed\n",
      "2020-12-23 02:46:33,545 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:33,546 : INFO : entropies processed\n",
      "2020-12-23 02:46:33,547 : INFO : extropies processed\n",
      "2020-12-23 02:46:33,548 : INFO : token count processed\n",
      "2020-12-23 02:46:33,549 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:33,550 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:33,551 : INFO : vocab #2480\n",
      "2020-12-23 02:46:33,553 : INFO : diff #set()\n",
      "2020-12-23 02:46:33,821 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:33,951 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.2760462094790448, 0.43935839080740197], [0.9458731710910797, 0.05412683], [0.0, 0.0], [2.321928094887362, 6.212221456585881, 6.246951127909132, 2.287198423564111, 3.92502303302177, 0.034729671323251665]]\n",
      "2020-12-23 02:46:33,953 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:33,954 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:33,955 : INFO : built Dictionary(133 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 443 corpus positions)\n",
      "2020-12-23 02:46:34,015 : INFO : token count processed\n",
      "2020-12-23 02:46:34,019 : INFO : frequencies processed\n",
      "2020-12-23 02:46:34,151 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:34,152 : INFO : entropies processed\n",
      "2020-12-23 02:46:34,153 : INFO : extropies processed\n",
      "2020-12-23 02:46:34,155 : INFO : token count processed\n",
      "2020-12-23 02:46:34,156 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:34,157 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:34,157 : INFO : vocab #2480\n",
      "2020-12-23 02:46:34,158 : INFO : diff #set()\n",
      "2020-12-23 02:46:34,424 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:34,552 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.2014842707344822, 0.4542389937977479], [0.8900940269231796, 0.10990597], [2.2516291673878226, 1.2667563532600834], [4.4057645846554525, 6.301552355933639, 6.518099050395499, 4.189217890193593, 2.1123344657400462, 0.2165466944618597]]\n",
      "2020-12-23 02:46:34,555 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:34,556 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:34,557 : INFO : built Dictionary(171 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 679 corpus positions)\n",
      "2020-12-23 02:46:34,641 : INFO : token count processed\n",
      "2020-12-23 02:46:34,644 : INFO : frequencies processed\n",
      "2020-12-23 02:46:34,774 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:34,774 : INFO : entropies processed\n",
      "2020-12-23 02:46:34,775 : INFO : extropies processed\n",
      "2020-12-23 02:46:34,776 : INFO : token count processed\n",
      "2020-12-23 02:46:34,777 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:34,778 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:34,778 : INFO : vocab #2480\n",
      "2020-12-23 02:46:34,779 : INFO : diff #set()\n",
      "2020-12-23 02:46:35,044 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:35,170 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.1738008569002842, 0.4600237399050173], [0.8242135941982269, 0.1757864], [2.121928094887362, 1.2308698765205934], [4.4057645846554525, 6.739005504021667, 6.868276708864412, 4.276493379812708, 2.46251212420896, 0.12927120484274557]]\n",
      "2020-12-23 02:46:35,173 : INFO : Removed 0 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:35,174 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:46:35,175 : INFO : built Dictionary(116 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 530 corpus positions)\n",
      "2020-12-23 02:46:35,225 : INFO : token count processed\n",
      "2020-12-23 02:46:35,228 : INFO : frequencies processed\n",
      "2020-12-23 02:46:35,354 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:35,355 : INFO : entropies processed\n",
      "2020-12-23 02:46:35,356 : INFO : extropies processed\n",
      "2020-12-23 02:46:35,357 : INFO : token count processed\n",
      "2020-12-23 02:46:35,358 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:35,359 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:35,360 : INFO : vocab #2480\n",
      "2020-12-23 02:46:35,360 : INFO : diff #set()\n",
      "2020-12-23 02:46:35,616 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:35,744 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2149875565121275, 0.45146980490250205], [0.9254818111658096, 0.07451819], [1.9502120649147465, 1.2308224193674733], [4.4057645846554525, 5.870833373337847, 6.066719516741372, 4.209878441251927, 1.6609549320859198, 0.19588614340352528]]\n",
      "2020-12-23 02:46:35,746 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:35,747 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:35,749 : INFO : built Dictionary(75 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 213 corpus positions)\n",
      "2020-12-23 02:46:35,781 : INFO : token count processed\n",
      "2020-12-23 02:46:35,784 : INFO : frequencies processed\n",
      "2020-12-23 02:46:35,913 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:35,913 : INFO : entropies processed\n",
      "2020-12-23 02:46:35,914 : INFO : extropies processed\n",
      "2020-12-23 02:46:35,915 : INFO : token count processed\n",
      "2020-12-23 02:46:35,916 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:35,916 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:35,917 : INFO : vocab #2480\n",
      "2020-12-23 02:46:35,918 : INFO : diff #set()\n",
      "2020-12-23 02:46:36,174 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:36,302 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.1369026234561055, 0.4679670421213001], [0.8516363948583603, 0.1483636], [2.2359263506290326, 1.2653331222512114], [4.4057645846554525, 5.371881234145534, 5.764084720896312, 4.013561097904676, 1.3583201362408595, 0.39220348675077776]]\n",
      "2020-12-23 02:46:36,305 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:36,306 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:36,307 : INFO : built Dictionary(67 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 165 corpus positions)\n",
      "2020-12-23 02:46:36,335 : INFO : token count processed\n",
      "2020-12-23 02:46:36,338 : INFO : frequencies processed\n",
      "2020-12-23 02:46:36,470 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:36,471 : INFO : entropies processed\n",
      "2020-12-23 02:46:36,472 : INFO : extropies processed\n",
      "2020-12-23 02:46:36,473 : INFO : token count processed\n",
      "2020-12-23 02:46:36,474 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:36,475 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:36,476 : INFO : vocab #2480\n",
      "2020-12-23 02:46:36,477 : INFO : diff #set()\n",
      "2020-12-23 02:46:36,736 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:36,864 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.153309063050689, 0.4644015191127535], [0.840695321559906, 0.15930468], [1.9219280948873623, 1.2148067842293933], [4.4057645846554525, 4.85108279267097, 5.407334622758407, 3.849512754568016, 1.0015700381029546, 0.5562518300874375]]\n",
      "2020-12-23 02:46:36,867 : INFO : Removed 0 and 127 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:36,868 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:36,869 : INFO : built Dictionary(108 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 438 corpus positions)\n",
      "2020-12-23 02:46:36,915 : INFO : token count processed\n",
      "2020-12-23 02:46:36,919 : INFO : frequencies processed\n",
      "2020-12-23 02:46:37,046 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:37,047 : INFO : entropies processed\n",
      "2020-12-23 02:46:37,047 : INFO : extropies processed\n",
      "2020-12-23 02:46:37,049 : INFO : token count processed\n",
      "2020-12-23 02:46:37,049 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:37,050 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:37,051 : INFO : vocab #2480\n",
      "2020-12-23 02:46:37,052 : INFO : diff #set()\n",
      "2020-12-23 02:46:37,308 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:37,436 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.1187697170761544, 0.4719720090109525], [0.8025230467319489, 0.19747695], [1.5, 1.1225562489182657], [4.4057645846554525, 6.139571208108155, 6.318840745626796, 4.226495047136812, 1.9130761609713431, 0.17926953751864083]]\n",
      "2020-12-23 02:46:37,439 : INFO : Removed 0 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:37,439 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:37,441 : INFO : built Dictionary(92 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 448 corpus positions)\n",
      "2020-12-23 02:46:37,478 : INFO : token count processed\n",
      "2020-12-23 02:46:37,480 : INFO : frequencies processed\n",
      "2020-12-23 02:46:37,608 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:37,609 : INFO : entropies processed\n",
      "2020-12-23 02:46:37,609 : INFO : extropies processed\n",
      "2020-12-23 02:46:37,611 : INFO : token count processed\n",
      "2020-12-23 02:46:37,612 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:37,613 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:37,614 : INFO : vocab #2480\n",
      "2020-12-23 02:46:37,615 : INFO : diff #set()\n",
      "2020-12-23 02:46:37,874 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:38,002 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.1596573555344343, 0.4630364152152912], [0.8444249778985977, 0.15557502], [2.188721875540867, 1.2547137765730974], [4.4057645846554525, 5.609710627339259, 5.838911473346741, 4.176563738647971, 1.4331468886912884, 0.22920084600748147]]\n",
      "2020-12-23 02:46:38,005 : INFO : Removed 0 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:38,006 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:38,007 : INFO : built Dictionary(179 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 1108 corpus positions)\n",
      "2020-12-23 02:46:38,092 : INFO : token count processed\n",
      "2020-12-23 02:46:38,094 : INFO : frequencies processed\n",
      "2020-12-23 02:46:38,223 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:38,224 : INFO : entropies processed\n",
      "2020-12-23 02:46:38,225 : INFO : extropies processed\n",
      "2020-12-23 02:46:38,227 : INFO : token count processed\n",
      "2020-12-23 02:46:38,228 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:38,229 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:38,230 : INFO : vocab #2480\n",
      "2020-12-23 02:46:38,231 : INFO : diff #set()\n",
      "2020-12-23 02:46:38,500 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:38,629 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.105302297577384, 0.47499116927327784], [0.7493428587913513, 0.25065714], [3.1464393446710157, 1.3477439688808162], [4.4057645846554525, 7.2441902753576075, 7.28911155647765, 4.360843303535411, 2.8833469718221973, 0.044921281120042345]]\n",
      "2020-12-23 02:46:38,631 : INFO : Removed 0 and 136 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:38,632 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:38,633 : INFO : built Dictionary(144 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 715 corpus positions)\n",
      "2020-12-23 02:46:38,695 : INFO : token count processed\n",
      "2020-12-23 02:46:38,697 : INFO : frequencies processed\n",
      "2020-12-23 02:46:38,827 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:38,827 : INFO : entropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:46:38,828 : INFO : extropies processed\n",
      "2020-12-23 02:46:38,830 : INFO : token count processed\n",
      "2020-12-23 02:46:38,831 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:38,832 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:38,832 : INFO : vocab #2480\n",
      "2020-12-23 02:46:38,833 : INFO : diff #set()\n",
      "2020-12-23 02:46:39,101 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:39,229 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.159925264401338, 0.46297898194971476], [0.8454131633043289, 0.15458684], [2.6062389286533896, 1.3008455146796718], [4.4057645846554525, 6.2567074920449475, 6.38533549365779, 4.27713658304261, 1.9795709090023372, 0.12862800161284227]]\n",
      "2020-12-23 02:46:39,232 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:39,233 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:39,234 : INFO : built Dictionary(94 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 225 corpus positions)\n",
      "2020-12-23 02:46:39,273 : INFO : token count processed\n",
      "2020-12-23 02:46:39,275 : INFO : frequencies processed\n",
      "2020-12-23 02:46:39,403 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:39,404 : INFO : entropies processed\n",
      "2020-12-23 02:46:39,405 : INFO : extropies processed\n",
      "2020-12-23 02:46:39,406 : INFO : token count processed\n",
      "2020-12-23 02:46:39,407 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:39,408 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:39,409 : INFO : vocab #2480\n",
      "2020-12-23 02:46:39,410 : INFO : diff #set()\n",
      "2020-12-23 02:46:39,668 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:39,796 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.1942247280986125, 0.45574183318339584], [0.8902301490306854, 0.10976985], [1.0, 1.0], [4.4057645846554525, 5.7680018917339435, 6.150410318246852, 4.023356158142544, 1.7446457335913994, 0.3824084265129084]]\n",
      "2020-12-23 02:46:39,798 : INFO : Removed 0 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:39,799 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:39,801 : INFO : built Dictionary(188 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 766 corpus positions)\n",
      "2020-12-23 02:46:39,901 : INFO : token count processed\n",
      "2020-12-23 02:46:39,908 : INFO : frequencies processed\n",
      "2020-12-23 02:46:40,037 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:40,037 : INFO : entropies processed\n",
      "2020-12-23 02:46:40,038 : INFO : extropies processed\n",
      "2020-12-23 02:46:40,039 : INFO : token count processed\n",
      "2020-12-23 02:46:40,040 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:40,041 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:40,041 : INFO : vocab #2480\n",
      "2020-12-23 02:46:40,042 : INFO : diff #set()\n",
      "2020-12-23 02:46:40,300 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:40,427 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.0389757449158668, 0.4904423225697874], [0.624144434928894, 0.37585557], [2.4116022179746714, 1.2794907761529437], [4.4057645846554525, 6.846479111193757, 6.931577341216286, 4.320666354632923, 2.5258127565608337, 0.08509823002252936]]\n",
      "2020-12-23 02:46:40,429 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:40,431 : INFO : built Dictionary(44 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 69 corpus positions)\n",
      "2020-12-23 02:46:40,450 : INFO : token count processed\n",
      "2020-12-23 02:46:40,452 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:40,453 : INFO : frequencies processed\n",
      "2020-12-23 02:46:40,455 : INFO : token count processed\n",
      "2020-12-23 02:46:40,456 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:40,458 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:40,459 : INFO : vocab #2480\n",
      "2020-12-23 02:46:40,460 : INFO : diff #set()\n",
      "2020-12-23 02:46:40,724 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:40,852 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2486405327010799, 0.4447131435449108], [0.9582717642188072, 0.041728236], [nan, nan], [4.4057645846554525, 4.165013816065912, 5.289258876879896, 3.2815195238414683, 0.8834942922244435, 1.1242450608139842]]\n",
      "2020-12-23 02:46:40,855 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:40,855 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:40,856 : INFO : built Dictionary(70 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 176 corpus positions)\n",
      "2020-12-23 02:46:40,879 : INFO : token count processed\n",
      "2020-12-23 02:46:40,881 : INFO : frequencies processed\n",
      "2020-12-23 02:46:41,009 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:41,010 : INFO : entropies processed\n",
      "2020-12-23 02:46:41,011 : INFO : extropies processed\n",
      "2020-12-23 02:46:41,013 : INFO : token count processed\n",
      "2020-12-23 02:46:41,015 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:41,016 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:41,017 : INFO : vocab #2480\n",
      "2020-12-23 02:46:41,019 : INFO : diff #set()\n",
      "2020-12-23 02:46:41,290 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:41,422 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.1942515499836208, 0.45573626232938724], [0.878641664981842, 0.121358335], [0.9182958340544896, 0.9182958340544896], [4.4057645846554525, 5.449968864419248, 5.906525517200822, 3.9492079318738798, 1.5007609325453695, 0.45655665278157365]]\n",
      "2020-12-23 02:46:41,425 : INFO : Removed 0 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:41,426 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:41,427 : INFO : built Dictionary(159 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 613 corpus positions)\n",
      "2020-12-23 02:46:41,502 : INFO : token count processed\n",
      "2020-12-23 02:46:41,504 : INFO : frequencies processed\n",
      "2020-12-23 02:46:41,634 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:41,634 : INFO : entropies processed\n",
      "2020-12-23 02:46:41,635 : INFO : extropies processed\n",
      "2020-12-23 02:46:41,637 : INFO : token count processed\n",
      "2020-12-23 02:46:41,638 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:41,639 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:41,639 : INFO : vocab #2480\n",
      "2020-12-23 02:46:41,640 : INFO : diff #set()\n",
      "2020-12-23 02:46:41,897 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:42,026 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.1831718011629075, 0.45804915557599785], [0.8613778650760651, 0.13862213], [2.3685225277282065, 1.2637056089150573], [4.4057645846554525, 6.530294129310484, 6.665219945116807, 4.27083876884913, 2.259455360461354, 0.1349258158063229]]\n",
      "2020-12-23 02:46:42,028 : INFO : Removed 0 and 142 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:42,029 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:42,030 : INFO : built Dictionary(134 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 629 corpus positions)\n",
      "2020-12-23 02:46:42,093 : INFO : token count processed\n",
      "2020-12-23 02:46:42,096 : INFO : frequencies processed\n",
      "2020-12-23 02:46:42,225 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:42,225 : INFO : entropies processed\n",
      "2020-12-23 02:46:42,226 : INFO : extropies processed\n",
      "2020-12-23 02:46:42,229 : INFO : token count processed\n",
      "2020-12-23 02:46:42,230 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:42,232 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:42,233 : INFO : vocab #2480\n",
      "2020-12-23 02:46:42,235 : INFO : diff #set()\n",
      "2020-12-23 02:46:42,506 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:42,635 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.1644119900704553, 0.4620192479932845], [0.8575370013713837, 0.142463], [1.9182958340544893, 1.2183406773511978], [4.4057645846554525, 6.470272233491701, 6.614159541775494, 4.261877276371659, 2.208394957120041, 0.1438873082837926]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:46:42,638 : INFO : Removed 0 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:42,639 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:42,641 : INFO : built Dictionary(137 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 418 corpus positions)\n",
      "2020-12-23 02:46:42,706 : INFO : token count processed\n",
      "2020-12-23 02:46:42,710 : INFO : frequencies processed\n",
      "2020-12-23 02:46:42,838 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:42,839 : INFO : entropies processed\n",
      "2020-12-23 02:46:42,840 : INFO : extropies processed\n",
      "2020-12-23 02:46:42,841 : INFO : token count processed\n",
      "2020-12-23 02:46:42,842 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:42,842 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:42,843 : INFO : vocab #2480\n",
      "2020-12-23 02:46:42,844 : INFO : diff #set()\n",
      "2020-12-23 02:46:43,100 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:43,229 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.2104160015979115, 0.45240352914433263], [0.8840309679508209, 0.11596903], [0.8112781244591328, 0.8112781244591328], [4.4057645846554525, 6.550038223589686, 6.756740602765115, 4.199062205480024, 2.3509760181096624, 0.20670237917542877]]\n",
      "2020-12-23 02:46:43,231 : INFO : Removed 0 and 70 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:43,232 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:43,233 : INFO : built Dictionary(91 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 334 corpus positions)\n",
      "2020-12-23 02:46:43,271 : INFO : token count processed\n",
      "2020-12-23 02:46:43,276 : INFO : frequencies processed\n",
      "2020-12-23 02:46:43,405 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:43,406 : INFO : entropies processed\n",
      "2020-12-23 02:46:43,406 : INFO : extropies processed\n",
      "2020-12-23 02:46:43,407 : INFO : token count processed\n",
      "2020-12-23 02:46:43,408 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:43,409 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:43,409 : INFO : vocab #2480\n",
      "2020-12-23 02:46:43,410 : INFO : diff #set()\n",
      "2020-12-23 02:46:43,671 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:43,798 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.129189571220739, 0.46966226658092497], [0.7724155336618423, 0.22758447], [1.5, 1.1225562489182657], [4.4057645846554525, 5.860525481261383, 6.101289312142303, 4.165000753774532, 1.6955247274868501, 0.2407638308809199]]\n",
      "2020-12-23 02:46:43,801 : INFO : Removed 0 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:43,802 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:43,803 : INFO : built Dictionary(65 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 372 corpus positions)\n",
      "2020-12-23 02:46:43,827 : INFO : token count processed\n",
      "2020-12-23 02:46:43,833 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:43,837 : INFO : frequencies processed\n",
      "2020-12-23 02:46:43,839 : INFO : token count processed\n",
      "2020-12-23 02:46:43,842 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:43,844 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:43,845 : INFO : vocab #2480\n",
      "2020-12-23 02:46:43,847 : INFO : diff #set()\n",
      "2020-12-23 02:46:44,101 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:44,228 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.294598373371219, 0.4358061138737766], [0.9871501792222261, 0.012849821], [nan, nan], [4.4057645846554525, 5.945464049777852, 6.196176753791114, 4.1550518806421906, 1.7904121691356618, 0.25071270401326196]]\n",
      "2020-12-23 02:46:44,230 : INFO : Removed 0 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:44,231 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:44,234 : INFO : built Dictionary(206 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 919 corpus positions)\n",
      "2020-12-23 02:46:44,342 : INFO : token count processed\n",
      "2020-12-23 02:46:44,345 : INFO : frequencies processed\n",
      "2020-12-23 02:46:44,470 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:44,471 : INFO : entropies processed\n",
      "2020-12-23 02:46:44,471 : INFO : extropies processed\n",
      "2020-12-23 02:46:44,473 : INFO : token count processed\n",
      "2020-12-23 02:46:44,474 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:44,475 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:44,476 : INFO : vocab #2480\n",
      "2020-12-23 02:46:44,477 : INFO : diff #set()\n",
      "2020-12-23 02:46:44,736 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:44,863 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.1645448749171048, 0.46199088389807436], [0.8412147015333176, 0.1587853], [2.606238928653389, 1.3008455146796718], [4.4057645846554525, 6.811563897304216, 6.903075816998156, 4.314252664961513, 2.497311232342703, 0.09151191969393935]]\n",
      "2020-12-23 02:46:44,866 : INFO : Removed 0 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:44,867 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:44,868 : INFO : built Dictionary(221 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 1014 corpus positions)\n",
      "2020-12-23 02:46:44,995 : INFO : token count processed\n",
      "2020-12-23 02:46:45,001 : INFO : frequencies processed\n",
      "2020-12-23 02:46:45,128 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:45,129 : INFO : entropies processed\n",
      "2020-12-23 02:46:45,129 : INFO : extropies processed\n",
      "2020-12-23 02:46:45,131 : INFO : token count processed\n",
      "2020-12-23 02:46:45,132 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:45,133 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:45,133 : INFO : vocab #2480\n",
      "2020-12-23 02:46:45,134 : INFO : diff #set()\n",
      "2020-12-23 02:46:45,396 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:45,524 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.097910484985037, 0.4766647610358516], [0.7415201663970947, 0.25847983], [3.481714572986072, 1.3647091345955626], [4.4057645846554525, 7.502034948968415, 7.542529971498398, 4.365269562125469, 3.1367653868429457, 0.04049502252998316]]\n",
      "2020-12-23 02:46:45,527 : INFO : Removed 0 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:45,528 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:45,531 : INFO : built Dictionary(268 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 1587 corpus positions)\n",
      "2020-12-23 02:46:45,705 : INFO : token count processed\n",
      "2020-12-23 02:46:45,713 : INFO : frequencies processed\n",
      "2020-12-23 02:46:45,842 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:45,843 : INFO : entropies processed\n",
      "2020-12-23 02:46:45,844 : INFO : extropies processed\n",
      "2020-12-23 02:46:45,845 : INFO : token count processed\n",
      "2020-12-23 02:46:45,846 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:45,847 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:45,847 : INFO : vocab #2480\n",
      "2020-12-23 02:46:45,848 : INFO : diff #set()\n",
      "2020-12-23 02:46:46,105 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:46,232 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.179832999764854, 0.4587507392116154], [0.8376515358686447, 0.16234846], [3.1068905956085184, 1.3362845045884086], [4.4057645846554525, 7.39180093901977, 7.443286820535468, 4.354278703139755, 3.037522235880015, 0.05148588151569733]]\n",
      "2020-12-23 02:46:46,234 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:46,235 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:46,236 : INFO : built Dictionary(62 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 142 corpus positions)\n",
      "2020-12-23 02:46:46,256 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:46:46,258 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:46,259 : INFO : frequencies processed\n",
      "2020-12-23 02:46:46,260 : INFO : token count processed\n",
      "2020-12-23 02:46:46,261 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:46,261 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:46,262 : INFO : vocab #2480\n",
      "2020-12-23 02:46:46,263 : INFO : diff #set()\n",
      "2020-12-23 02:46:46,521 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:46,649 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.21856120920847, 0.45074257850058425], [0.9257773905992508, 0.07422261], [nan, nan], [4.4057645846554525, 4.927561309677364, 5.6083902528810174, 3.7249356414517987, 1.202625668225565, 0.6808289432036538]]\n",
      "2020-12-23 02:46:46,652 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:46,653 : INFO : built Dictionary(30 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 43 corpus positions)\n",
      "2020-12-23 02:46:46,657 : INFO : token count processed\n",
      "2020-12-23 02:46:46,660 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:46,660 : INFO : frequencies processed\n",
      "2020-12-23 02:46:46,661 : INFO : token count processed\n",
      "2020-12-23 02:46:46,662 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:46,663 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:46,664 : INFO : vocab #2480\n",
      "2020-12-23 02:46:46,665 : INFO : diff #set()\n",
      "2020-12-23 02:46:46,924 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:47,050 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2474978069350056, 0.4449392550748409], [0.9469987228512764, 0.053001277], [nan, nan], [4.4057645846554525, 2.5216406363433186, 4.739990917392169, 2.1874143036066025, 0.33422633273671654, 2.2183502810488505]]\n",
      "2020-12-23 02:46:47,054 : INFO : Removed 0 and 530 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:47,054 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:47,056 : INFO : built Dictionary(342 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 2915 corpus positions)\n",
      "2020-12-23 02:46:47,270 : INFO : token count processed\n",
      "2020-12-23 02:46:47,276 : INFO : frequencies processed\n",
      "2020-12-23 02:46:47,439 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:47,440 : INFO : entropies processed\n",
      "2020-12-23 02:46:47,441 : INFO : extropies processed\n",
      "2020-12-23 02:46:47,445 : INFO : token count processed\n",
      "2020-12-23 02:46:47,447 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:47,448 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:47,449 : INFO : vocab #2480\n",
      "2020-12-23 02:46:47,451 : INFO : diff #set()\n",
      "2020-12-23 02:46:47,715 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:47,844 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.1575837973011704, 0.4634814190071585], [0.8199140876531601, 0.18008591], [3.127986806877675, 1.3437322713578626], [4.4057645846554525, 7.480007711014331, 7.510839980063989, 4.374932315605794, 3.1050753954085364, 0.0308322690496583]]\n",
      "2020-12-23 02:46:47,847 : INFO : Removed 0 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:47,848 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:47,849 : INFO : built Dictionary(217 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 1064 corpus positions)\n",
      "2020-12-23 02:46:47,967 : INFO : token count processed\n",
      "2020-12-23 02:46:47,970 : INFO : frequencies processed\n",
      "2020-12-23 02:46:48,097 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:48,097 : INFO : entropies processed\n",
      "2020-12-23 02:46:48,098 : INFO : extropies processed\n",
      "2020-12-23 02:46:48,099 : INFO : token count processed\n",
      "2020-12-23 02:46:48,100 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:48,100 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:48,101 : INFO : vocab #2480\n",
      "2020-12-23 02:46:48,102 : INFO : diff #set()\n",
      "2020-12-23 02:46:48,358 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:48,485 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.1059430094618465, 0.4748466580088226], [0.7555500417947769, 0.24444996], [3.4039894464852614, 1.3621878060228698], [4.4057645846554525, 7.131331012509435, 7.180474606736619, 4.356620990428268, 2.774710022081167, 0.04914359422718473]]\n",
      "2020-12-23 02:46:48,488 : INFO : Removed 0 and 166 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:48,489 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:48,491 : INFO : built Dictionary(207 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 883 corpus positions)\n",
      "2020-12-23 02:46:48,605 : INFO : token count processed\n",
      "2020-12-23 02:46:48,612 : INFO : frequencies processed\n",
      "2020-12-23 02:46:48,742 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:48,742 : INFO : entropies processed\n",
      "2020-12-23 02:46:48,743 : INFO : extropies processed\n",
      "2020-12-23 02:46:48,744 : INFO : token count processed\n",
      "2020-12-23 02:46:48,745 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:48,746 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:48,747 : INFO : vocab #2480\n",
      "2020-12-23 02:46:48,748 : INFO : diff #set()\n",
      "2020-12-23 02:46:49,007 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:49,135 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.156319581463814, 0.4637531507834992], [0.8095213025808334, 0.1904787], [3.260828171224455, 1.35212439792718], [4.4057645846554525, 7.203742744794778, 7.270719736063572, 4.33878759338666, 2.864955151408119, 0.06697699126879364]]\n",
      "2020-12-23 02:46:49,137 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:49,138 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:49,140 : INFO : built Dictionary(74 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 215 corpus positions)\n",
      "2020-12-23 02:46:49,170 : INFO : token count processed\n",
      "2020-12-23 02:46:49,172 : INFO : frequencies processed\n",
      "2020-12-23 02:46:49,310 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:49,311 : INFO : entropies processed\n",
      "2020-12-23 02:46:49,311 : INFO : extropies processed\n",
      "2020-12-23 02:46:49,313 : INFO : token count processed\n",
      "2020-12-23 02:46:49,315 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:49,316 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:49,317 : INFO : vocab #2480\n",
      "2020-12-23 02:46:49,318 : INFO : diff #set()\n",
      "2020-12-23 02:46:49,576 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:49,704 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.140835706944653, 0.46710730615903956], [0.8269859403371811, 0.17301406], [0.9182958340544896, 0.9182958340544896], [4.4057645846554525, 5.195502554608948, 5.664378689176194, 3.9368884500882073, 1.258614104520741, 0.4688761345672452]]\n",
      "2020-12-23 02:46:49,707 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:49,707 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:49,708 : INFO : built Dictionary(79 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 276 corpus positions)\n",
      "2020-12-23 02:46:49,736 : INFO : token count processed\n",
      "2020-12-23 02:46:49,739 : INFO : frequencies processed\n",
      "2020-12-23 02:46:49,867 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:49,868 : INFO : entropies processed\n",
      "2020-12-23 02:46:49,868 : INFO : extropies processed\n",
      "2020-12-23 02:46:49,870 : INFO : token count processed\n",
      "2020-12-23 02:46:49,871 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:49,871 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:49,872 : INFO : vocab #2480\n",
      "2020-12-23 02:46:49,873 : INFO : diff #set()\n",
      "2020-12-23 02:46:50,131 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:50,259 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.0764728146200988, 0.4815858859115163], [0.725980818271637, 0.27401918], [1.5304930567574826, 1.1430833436438494], [4.4057645846554525, 5.32027245610305, 5.656021139847298, 4.070015900911204, 1.2502565551918456, 0.3357486837442485]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:46:50,261 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:50,262 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:50,263 : INFO : built Dictionary(173 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 516 corpus positions)\n",
      "2020-12-23 02:46:50,361 : INFO : token count processed\n",
      "2020-12-23 02:46:50,363 : INFO : frequencies processed\n",
      "2020-12-23 02:46:50,492 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:50,492 : INFO : entropies processed\n",
      "2020-12-23 02:46:50,493 : INFO : extropies processed\n",
      "2020-12-23 02:46:50,494 : INFO : token count processed\n",
      "2020-12-23 02:46:50,495 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:50,496 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:50,497 : INFO : vocab #2480\n",
      "2020-12-23 02:46:50,498 : INFO : diff #set()\n",
      "2020-12-23 02:46:50,753 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:50,881 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.1692419312336417, 0.4609905357265996], [0.8289621472358704, 0.17103785], [3.0269868333592873, 1.3385887750658594], [4.4057645846554525, 6.898202761357263, 7.023337126933258, 4.280630219079457, 2.617572542277806, 0.12513436557599533]]\n",
      "2020-12-23 02:46:50,883 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:50,884 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:50,885 : INFO : built Dictionary(141 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 538 corpus positions)\n",
      "2020-12-23 02:46:50,949 : INFO : token count processed\n",
      "2020-12-23 02:46:50,951 : INFO : frequencies processed\n",
      "2020-12-23 02:46:51,078 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:51,079 : INFO : entropies processed\n",
      "2020-12-23 02:46:51,082 : INFO : extropies processed\n",
      "2020-12-23 02:46:51,083 : INFO : token count processed\n",
      "2020-12-23 02:46:51,084 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:51,084 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:51,085 : INFO : vocab #2480\n",
      "2020-12-23 02:46:51,086 : INFO : diff #set()\n",
      "2020-12-23 02:46:51,342 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:51,470 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.1121229499038512, 0.47345728620842004], [0.7680713981389999, 0.2319286], [2.188721875540867, 1.2547137765730974], [4.4057645846554525, 6.388500481644799, 6.532947947819478, 4.261317118480774, 2.1271833631640256, 0.14444746617467885]]\n",
      "2020-12-23 02:46:51,472 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:51,473 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:51,475 : INFO : built Dictionary(67 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 202 corpus positions)\n",
      "2020-12-23 02:46:51,504 : INFO : token count processed\n",
      "2020-12-23 02:46:51,506 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:51,507 : INFO : frequencies processed\n",
      "2020-12-23 02:46:51,508 : INFO : token count processed\n",
      "2020-12-23 02:46:51,509 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:51,510 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:51,511 : INFO : vocab #2480\n",
      "2020-12-23 02:46:51,513 : INFO : diff #set()\n",
      "2020-12-23 02:46:51,787 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:51,915 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.220706242840663, 0.450307195390611], [0.918713390827179, 0.08128661], [nan, nan], [4.4057645846554525, 4.8191513650620195, 5.42006442022884, 3.804851529488632, 1.0142998355733877, 0.6009130551668207]]\n",
      "2020-12-23 02:46:51,917 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:51,918 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:51,919 : INFO : built Dictionary(70 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 257 corpus positions)\n",
      "2020-12-23 02:46:51,947 : INFO : token count processed\n",
      "2020-12-23 02:46:51,953 : INFO : frequencies processed\n",
      "2020-12-23 02:46:52,092 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:52,093 : INFO : entropies processed\n",
      "2020-12-23 02:46:52,094 : INFO : extropies processed\n",
      "2020-12-23 02:46:52,096 : INFO : token count processed\n",
      "2020-12-23 02:46:52,097 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:52,098 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:52,100 : INFO : vocab #2480\n",
      "2020-12-23 02:46:52,101 : INFO : diff #set()\n",
      "2020-12-23 02:46:52,363 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:52,495 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.1391815422606042, 0.46746850617607644], [0.8146336674690247, 0.18536633], [0.9852281360342515, 0.9852281360342515], [4.4057645846554525, 5.062480936779194, 5.48155922673673, 3.9866862946979165, 1.0757946420812772, 0.41907828995753604]]\n",
      "2020-12-23 02:46:52,498 : INFO : Removed 0 and 223 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:52,499 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:52,500 : INFO : built Dictionary(253 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 1811 corpus positions)\n",
      "2020-12-23 02:46:52,656 : INFO : token count processed\n",
      "2020-12-23 02:46:52,662 : INFO : frequencies processed\n",
      "2020-12-23 02:46:52,790 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:52,790 : INFO : entropies processed\n",
      "2020-12-23 02:46:52,791 : INFO : extropies processed\n",
      "2020-12-23 02:46:52,793 : INFO : token count processed\n",
      "2020-12-23 02:46:52,794 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:52,795 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:52,795 : INFO : vocab #2480\n",
      "2020-12-23 02:46:52,797 : INFO : diff #set()\n",
      "2020-12-23 02:46:53,054 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:53,183 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.16790760554452, 0.4612742708418272], [0.8267726451158524, 0.17322735], [3.125, 1.3407118537600056], [4.4057645846554525, 7.185085743102134, 7.238217885393473, 4.352632442364113, 2.8324533007380204, 0.05313214229133845]]\n",
      "2020-12-23 02:46:53,186 : INFO : Removed 0 and 148 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:53,187 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:53,189 : INFO : built Dictionary(175 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 786 corpus positions)\n",
      "2020-12-23 02:46:53,279 : INFO : token count processed\n",
      "2020-12-23 02:46:53,281 : INFO : frequencies processed\n",
      "2020-12-23 02:46:53,409 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:53,410 : INFO : entropies processed\n",
      "2020-12-23 02:46:53,411 : INFO : extropies processed\n",
      "2020-12-23 02:46:53,412 : INFO : token count processed\n",
      "2020-12-23 02:46:53,413 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:53,414 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:53,415 : INFO : vocab #2480\n",
      "2020-12-23 02:46:53,416 : INFO : diff #set()\n",
      "2020-12-23 02:46:53,675 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:53,802 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.1816920171891605, 0.45835983819951637], [0.8563251942396164, 0.1436748], [2.0464393446710156, 1.2127889297821894], [4.4057645846554525, 6.591225336124281, 6.7121897354220845, 4.284800185357648, 2.306425150766632, 0.12096439929780356]]\n",
      "2020-12-23 02:46:53,805 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:53,805 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:53,806 : INFO : built Dictionary(58 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 124 corpus positions)\n",
      "2020-12-23 02:46:53,823 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:46:53,828 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:53,829 : INFO : frequencies processed\n",
      "2020-12-23 02:46:53,831 : INFO : token count processed\n",
      "2020-12-23 02:46:53,832 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:53,834 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:53,834 : INFO : vocab #2480\n",
      "2020-12-23 02:46:53,836 : INFO : diff #set()\n",
      "2020-12-23 02:46:54,103 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:54,232 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.2218625266822956, 0.45007285013857656], [0.9328868538141251, 0.067113146], [nan, nan], [4.4057645846554525, 4.7032114441396695, 5.483670430006843, 3.6253055987882803, 1.0779058453513901, 0.7804589858671731]]\n",
      "2020-12-23 02:46:54,235 : INFO : Removed 0 and 9 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:54,236 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:54,237 : INFO : built Dictionary(130 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 364 corpus positions)\n",
      "2020-12-23 02:46:54,303 : INFO : token count processed\n",
      "2020-12-23 02:46:54,307 : INFO : frequencies processed\n",
      "2020-12-23 02:46:54,433 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:54,434 : INFO : entropies processed\n",
      "2020-12-23 02:46:54,434 : INFO : extropies processed\n",
      "2020-12-23 02:46:54,435 : INFO : token count processed\n",
      "2020-12-23 02:46:54,436 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:54,437 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:54,438 : INFO : vocab #2480\n",
      "2020-12-23 02:46:54,439 : INFO : diff #set()\n",
      "2020-12-23 02:46:54,709 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:54,838 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.1330038237334512, 0.4688224131964632], [0.8405562937259674, 0.1594437], [2.5216406363433186, 1.2998438251349493], [4.4057645846554525, 6.14228447828618, 6.37520395144713, 4.172845111494503, 1.9694393667916774, 0.2329194731609503]]\n",
      "2020-12-23 02:46:54,841 : INFO : Removed 0 and 184 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:54,842 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:54,844 : INFO : built Dictionary(268 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 1154 corpus positions)\n",
      "2020-12-23 02:46:55,010 : INFO : token count processed\n",
      "2020-12-23 02:46:55,012 : INFO : frequencies processed\n",
      "2020-12-23 02:46:55,148 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:55,149 : INFO : entropies processed\n",
      "2020-12-23 02:46:55,150 : INFO : extropies processed\n",
      "2020-12-23 02:46:55,152 : INFO : token count processed\n",
      "2020-12-23 02:46:55,153 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:55,154 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:55,155 : INFO : vocab #2480\n",
      "2020-12-23 02:46:55,157 : INFO : diff #set()\n",
      "2020-12-23 02:46:55,422 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:55,550 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.1943914092741381, 0.45570721602978775], [0.8440034836530685, 0.15599652], [2.5216406363433186, 1.2998438251349493], [4.4057645846554525, 7.450178124335845, 7.529596343295839, 4.326346365695458, 3.123831758640386, 0.07941821895999368]]\n",
      "2020-12-23 02:46:55,553 : INFO : Removed 0 and 16 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:55,554 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:55,555 : INFO : built Dictionary(76 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 202 corpus positions)\n",
      "2020-12-23 02:46:55,586 : INFO : token count processed\n",
      "2020-12-23 02:46:55,588 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:55,589 : INFO : frequencies processed\n",
      "2020-12-23 02:46:55,590 : INFO : token count processed\n",
      "2020-12-23 02:46:55,591 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:55,592 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:55,592 : INFO : vocab #2480\n",
      "2020-12-23 02:46:55,593 : INFO : diff #set()\n",
      "2020-12-23 02:46:55,861 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:55,989 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.2167323680299602, 0.4511144486461906], [0.9077839180827141, 0.09221608], [nan, nan], [4.4057645846554525, 5.20665021947654, 5.720853776264268, 3.8915610278677244, 1.3150891916088154, 0.5142035567877281]]\n",
      "2020-12-23 02:46:55,991 : INFO : Removed 0 and 89 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:55,992 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:55,993 : INFO : built Dictionary(136 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 540 corpus positions)\n",
      "2020-12-23 02:46:56,053 : INFO : token count processed\n",
      "2020-12-23 02:46:56,056 : INFO : frequencies processed\n",
      "2020-12-23 02:46:56,185 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:56,186 : INFO : entropies processed\n",
      "2020-12-23 02:46:56,187 : INFO : extropies processed\n",
      "2020-12-23 02:46:56,188 : INFO : token count processed\n",
      "2020-12-23 02:46:56,189 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:56,190 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:56,191 : INFO : vocab #2480\n",
      "2020-12-23 02:46:56,192 : INFO : diff #set()\n",
      "2020-12-23 02:46:56,448 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:56,575 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.1812633680836584, 0.4584499123911602], [0.8598347902297974, 0.14016521], [2.9232314287976204, 1.3220029247123422], [4.4057645846554525, 6.524718477352, 6.6681281082671315, 4.262354953740321, 2.262363523611679, 0.1434096309151318]]\n",
      "2020-12-23 02:46:56,578 : INFO : Removed 0 and 8 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:56,578 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:56,579 : INFO : built Dictionary(81 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 247 corpus positions)\n",
      "2020-12-23 02:46:56,608 : INFO : token count processed\n",
      "2020-12-23 02:46:56,611 : INFO : frequencies processed\n",
      "2020-12-23 02:46:56,738 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:56,739 : INFO : entropies processed\n",
      "2020-12-23 02:46:56,740 : INFO : extropies processed\n",
      "2020-12-23 02:46:56,741 : INFO : token count processed\n",
      "2020-12-23 02:46:56,742 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:56,743 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:56,744 : INFO : vocab #2480\n",
      "2020-12-23 02:46:56,745 : INFO : diff #set()\n",
      "2020-12-23 02:46:57,002 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:57,129 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.2020757401792108, 0.4541169868746736], [0.9029732421040535, 0.09702676], [0.0, 0.0], [4.4057645846554525, 5.321859380715434, 5.765684506840269, 3.961939458530618, 1.3599199221848162, 0.4438251261248345]]\n",
      "2020-12-23 02:46:57,132 : INFO : Removed 0 and 119 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:57,133 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:57,135 : INFO : built Dictionary(151 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 865 corpus positions)\n",
      "2020-12-23 02:46:57,210 : INFO : token count processed\n",
      "2020-12-23 02:46:57,213 : INFO : frequencies processed\n",
      "2020-12-23 02:46:57,344 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:57,345 : INFO : entropies processed\n",
      "2020-12-23 02:46:57,345 : INFO : extropies processed\n",
      "2020-12-23 02:46:57,347 : INFO : token count processed\n",
      "2020-12-23 02:46:57,348 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:57,349 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:57,349 : INFO : vocab #2480\n",
      "2020-12-23 02:46:57,350 : INFO : diff #set()\n",
      "2020-12-23 02:46:57,620 : INFO : alphabet #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:46:57,749 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.0752594190430746, 0.4818674671820601], [0.7539547085762024, 0.24604529], [3.1158340921632197, 1.343434691179247], [4.4057645846554525, 6.500767808767801, 6.5806913341045385, 4.325841059318715, 2.174926749449086, 0.07992352533673763]]\n",
      "2020-12-23 02:46:57,752 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:57,753 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:57,753 : INFO : built Dictionary(54 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 87 corpus positions)\n",
      "2020-12-23 02:46:57,769 : INFO : token count processed\n",
      "2020-12-23 02:46:57,771 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:46:57,772 : INFO : frequencies processed\n",
      "2020-12-23 02:46:57,773 : INFO : token count processed\n",
      "2020-12-23 02:46:57,774 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:57,775 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:57,776 : INFO : vocab #2480\n",
      "2020-12-23 02:46:57,777 : INFO : diff #set()\n",
      "2020-12-23 02:46:58,036 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:58,165 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.220970198247784, 0.4502536777796216], [0.9342961385846138, 0.06570386], [nan, nan], [4.4057645846554525, 4.736228843383063, 5.5770595675947465, 3.5649338604437677, 1.171294982939294, 0.8408307242116839]]\n",
      "2020-12-23 02:46:58,168 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:58,168 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:58,170 : INFO : built Dictionary(113 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 428 corpus positions)\n",
      "2020-12-23 02:46:58,221 : INFO : token count processed\n",
      "2020-12-23 02:46:58,224 : INFO : frequencies processed\n",
      "2020-12-23 02:46:58,354 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:58,355 : INFO : entropies processed\n",
      "2020-12-23 02:46:58,356 : INFO : extropies processed\n",
      "2020-12-23 02:46:58,358 : INFO : token count processed\n",
      "2020-12-23 02:46:58,359 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:58,360 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:58,361 : INFO : vocab #2480\n",
      "2020-12-23 02:46:58,363 : INFO : diff #set()\n",
      "2020-12-23 02:46:58,623 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:58,752 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.205446363844001, 0.45342295165004226], [0.8931554481387138, 0.10684455], [1.5, 1.1225562489182657], [4.4057645846554525, 5.788442787590127, 6.060236832943407, 4.1339705393021715, 1.6544722482879548, 0.2717940453532801]]\n",
      "2020-12-23 02:46:58,754 : INFO : Removed 0 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:58,755 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:58,756 : INFO : built Dictionary(72 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 114 corpus positions)\n",
      "2020-12-23 02:46:58,787 : INFO : token count processed\n",
      "2020-12-23 02:46:58,792 : INFO : frequencies processed\n",
      "2020-12-23 02:46:58,930 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:58,931 : INFO : entropies processed\n",
      "2020-12-23 02:46:58,932 : INFO : extropies processed\n",
      "2020-12-23 02:46:58,933 : INFO : token count processed\n",
      "2020-12-23 02:46:58,934 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:58,935 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:58,936 : INFO : vocab #2480\n",
      "2020-12-23 02:46:58,937 : INFO : diff #set()\n",
      "2020-12-23 02:46:59,203 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:59,329 : INFO : Computed distances or similarities ('288', 'sacp-python-common/setup.py')[[1.2129337029650513, 0.451888820103434], [0.9031015858054161, 0.096898414], [0.0, 0.0], [4.4057645846554525, 5.370004292053436, 5.931942941777761, 3.843825934931127, 1.5261783571223084, 0.5619386497243246]]\n",
      "2020-12-23 02:46:59,331 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:59,332 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:59,334 : INFO : built Dictionary(96 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 359 corpus positions)\n",
      "2020-12-23 02:46:59,383 : INFO : token count processed\n",
      "2020-12-23 02:46:59,388 : INFO : frequencies processed\n",
      "2020-12-23 02:46:59,517 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:46:59,517 : INFO : entropies processed\n",
      "2020-12-23 02:46:59,518 : INFO : extropies processed\n",
      "2020-12-23 02:46:59,519 : INFO : token count processed\n",
      "2020-12-23 02:46:59,519 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:46:59,520 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:46:59,520 : INFO : vocab #2480\n",
      "2020-12-23 02:46:59,521 : INFO : diff #set()\n",
      "2020-12-23 02:46:59,785 : INFO : alphabet #2480\n",
      "2020-12-23 02:46:59,913 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.1816712529994344, 0.45836420066734007], [0.8848019167780876, 0.11519808], [1.4591479170272448, 1.1091703386755989], [4.4057645846554525, 5.695663584743922, 5.976930591326961, 4.124497578072414, 1.5711660066715085, 0.2812670065830387]]\n",
      "2020-12-23 02:46:59,915 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:46:59,916 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:46:59,917 : INFO : built Dictionary(58 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 134 corpus positions)\n",
      "2020-12-23 02:46:59,935 : INFO : token count processed\n",
      "2020-12-23 02:46:59,937 : INFO : frequencies processed\n",
      "2020-12-23 02:47:00,065 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:00,066 : INFO : entropies processed\n",
      "2020-12-23 02:47:00,067 : INFO : extropies processed\n",
      "2020-12-23 02:47:00,068 : INFO : token count processed\n",
      "2020-12-23 02:47:00,069 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:00,070 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:00,070 : INFO : vocab #2480\n",
      "2020-12-23 02:47:00,071 : INFO : diff #set()\n",
      "2020-12-23 02:47:00,330 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:00,459 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.1572222782675654, 0.46355909174231497], [0.8790518492460251, 0.12094815], [0.9182958340544896, 0.9182958340544896], [4.4057645846554525, 4.9004417692112465, 5.537927902998854, 3.768278450867845, 1.1321633183434017, 0.6374861337876077]]\n",
      "2020-12-23 02:47:00,461 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:00,462 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:00,464 : INFO : built Dictionary(56 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 141 corpus positions)\n",
      "2020-12-23 02:47:00,486 : INFO : token count processed\n",
      "2020-12-23 02:47:00,489 : INFO : frequencies processed\n",
      "2020-12-23 02:47:00,620 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:00,620 : INFO : entropies processed\n",
      "2020-12-23 02:47:00,621 : INFO : extropies processed\n",
      "2020-12-23 02:47:00,622 : INFO : token count processed\n",
      "2020-12-23 02:47:00,623 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:00,624 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:00,625 : INFO : vocab #2480\n",
      "2020-12-23 02:47:00,626 : INFO : diff #set()\n",
      "2020-12-23 02:47:00,881 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:01,010 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.1630176161572003, 0.4623170854135677], [0.8872963413596153, 0.11270366], [0.9182958340544896, 0.9182958340544896], [4.4057645846554525, 4.778624108914332, 5.437727456505461, 3.746661237064324, 1.0319628718500082, 0.6591033475911283]]\n",
      "2020-12-23 02:47:01,013 : INFO : Removed 0 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:01,013 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:47:01,014 : INFO : built Dictionary(57 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 172 corpus positions)\n",
      "2020-12-23 02:47:01,032 : INFO : token count processed\n",
      "2020-12-23 02:47:01,035 : INFO : frequencies processed\n",
      "2020-12-23 02:47:01,175 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:01,176 : INFO : entropies processed\n",
      "2020-12-23 02:47:01,177 : INFO : extropies processed\n",
      "2020-12-23 02:47:01,178 : INFO : token count processed\n",
      "2020-12-23 02:47:01,179 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:01,180 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:01,181 : INFO : vocab #2480\n",
      "2020-12-23 02:47:01,182 : INFO : diff #set()\n",
      "2020-12-23 02:47:01,453 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:01,583 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.177799793500576, 0.45917903150895656], [0.918802909553051, 0.08119709], [0.9182958340544896, 0.9182958340544896], [4.4057645846554525, 4.773880192225086, 5.3748591283392315, 3.8047856485413067, 0.969094543683779, 0.6009789361141458]]\n",
      "2020-12-23 02:47:01,586 : INFO : Removed 0 and 346 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:01,587 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:01,588 : INFO : built Dictionary(163 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 1997 corpus positions)\n",
      "2020-12-23 02:47:01,668 : INFO : token count processed\n",
      "2020-12-23 02:47:01,671 : INFO : frequencies processed\n",
      "2020-12-23 02:47:01,803 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:01,804 : INFO : entropies processed\n",
      "2020-12-23 02:47:01,804 : INFO : extropies processed\n",
      "2020-12-23 02:47:01,806 : INFO : token count processed\n",
      "2020-12-23 02:47:01,807 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:01,808 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:01,808 : INFO : vocab #2480\n",
      "2020-12-23 02:47:01,809 : INFO : diff #set()\n",
      "2020-12-23 02:47:02,066 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:02,194 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.1785407781880588, 0.4590228514481709], [0.8638113439083099, 0.13618866], [2.5654483718208256, 1.2887495548865373], [4.4057645846554525, 6.620773041953877, 6.678986151974938, 4.3475514746343915, 2.273221567319485, 0.058213110021061]]\n",
      "2020-12-23 02:47:02,196 : INFO : Removed 0 and 132 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:02,197 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:02,199 : INFO : built Dictionary(96 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 560 corpus positions)\n",
      "2020-12-23 02:47:02,250 : INFO : token count processed\n",
      "2020-12-23 02:47:02,254 : INFO : frequencies processed\n",
      "2020-12-23 02:47:02,382 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:02,382 : INFO : entropies processed\n",
      "2020-12-23 02:47:02,383 : INFO : extropies processed\n",
      "2020-12-23 02:47:02,384 : INFO : token count processed\n",
      "2020-12-23 02:47:02,385 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:02,385 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:02,386 : INFO : vocab #2480\n",
      "2020-12-23 02:47:02,388 : INFO : diff #set()\n",
      "2020-12-23 02:47:02,647 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:02,774 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.1776681060815943, 0.45920679887228477], [0.8866739124059677, 0.11332609], [1.9219280948873623, 1.2148067842293933], [4.4057645846554525, 5.828370634755606, 6.025261256369091, 4.208873963041968, 1.6194966717136383, 0.19689062161348492]]\n",
      "2020-12-23 02:47:02,776 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:02,777 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:02,778 : INFO : built Dictionary(95 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 323 corpus positions)\n",
      "2020-12-23 02:47:02,818 : INFO : token count processed\n",
      "2020-12-23 02:47:02,824 : INFO : frequencies processed\n",
      "2020-12-23 02:47:02,954 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:02,955 : INFO : entropies processed\n",
      "2020-12-23 02:47:02,956 : INFO : extropies processed\n",
      "2020-12-23 02:47:02,957 : INFO : token count processed\n",
      "2020-12-23 02:47:02,958 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:02,959 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:02,960 : INFO : vocab #2480\n",
      "2020-12-23 02:47:02,961 : INFO : diff #set()\n",
      "2020-12-23 02:47:03,229 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:03,361 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.049939495152778, 0.48781927581988066], [0.6557096838951111, 0.34429032], [2.3553885422075336, 1.265658388865344], [4.4057645846554525, 5.774409284925443, 5.995600325125261, 4.184573544455633, 1.589835740469809, 0.22119104019981872]]\n",
      "2020-12-23 02:47:03,363 : INFO : Removed 0 and 51 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:03,364 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:03,366 : INFO : built Dictionary(108 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 372 corpus positions)\n",
      "2020-12-23 02:47:03,423 : INFO : token count processed\n",
      "2020-12-23 02:47:03,425 : INFO : frequencies processed\n",
      "2020-12-23 02:47:03,552 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:03,553 : INFO : entropies processed\n",
      "2020-12-23 02:47:03,554 : INFO : extropies processed\n",
      "2020-12-23 02:47:03,555 : INFO : token count processed\n",
      "2020-12-23 02:47:03,555 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:03,556 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:03,557 : INFO : vocab #2480\n",
      "2020-12-23 02:47:03,558 : INFO : diff #set()\n",
      "2020-12-23 02:47:03,824 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:03,952 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.1589301707062718, 0.46319237813646386], [0.8732300698757172, 0.12676993], [1.9219280948873623, 1.2148067842293933], [4.4057645846554525, 5.977819040873918, 6.224207926775611, 4.15937569875376, 1.8184433421201582, 0.24638888590169294]]\n",
      "2020-12-23 02:47:03,955 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:03,956 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:03,957 : INFO : built Dictionary(89 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 319 corpus positions)\n",
      "2020-12-23 02:47:03,995 : INFO : token count processed\n",
      "2020-12-23 02:47:03,998 : INFO : frequencies processed\n",
      "2020-12-23 02:47:04,125 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:04,126 : INFO : entropies processed\n",
      "2020-12-23 02:47:04,127 : INFO : extropies processed\n",
      "2020-12-23 02:47:04,128 : INFO : token count processed\n",
      "2020-12-23 02:47:04,129 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:04,130 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:04,131 : INFO : vocab #2480\n",
      "2020-12-23 02:47:04,133 : INFO : diff #set()\n",
      "2020-12-23 02:47:04,391 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:04,518 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.1956807264425953, 0.45543962196187926], [0.8773112371563911, 0.12268876], [2.0, 1.2451124978365313], [4.4057645846554525, 5.901812829596593, 6.188205524349487, 4.119371889902557, 1.7824409396940348, 0.2863926947528945]]\n",
      "2020-12-23 02:47:04,520 : INFO : Removed 0 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:04,521 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:04,522 : INFO : built Dictionary(91 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 319 corpus positions)\n",
      "2020-12-23 02:47:04,555 : INFO : token count processed\n",
      "2020-12-23 02:47:04,558 : INFO : frequencies processed\n",
      "2020-12-23 02:47:04,686 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:04,686 : INFO : entropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:47:04,687 : INFO : extropies processed\n",
      "2020-12-23 02:47:04,688 : INFO : token count processed\n",
      "2020-12-23 02:47:04,689 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:04,690 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:04,690 : INFO : vocab #2480\n",
      "2020-12-23 02:47:04,691 : INFO : diff #set()\n",
      "2020-12-23 02:47:04,948 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:05,087 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.1885631619525598, 0.45692078592231933], [0.89820147305727, 0.10179853], [1.9219280948873623, 1.2148067842293933], [4.4057645846554525, 5.643202320803383, 5.955858622787049, 4.093108282671786, 1.5500940381315962, 0.3126563019836661]]\n",
      "2020-12-23 02:47:05,089 : INFO : Removed 0 and 30 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:05,090 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:05,091 : INFO : built Dictionary(105 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 353 corpus positions)\n",
      "2020-12-23 02:47:05,132 : INFO : token count processed\n",
      "2020-12-23 02:47:05,134 : INFO : frequencies processed\n",
      "2020-12-23 02:47:05,262 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:05,263 : INFO : entropies processed\n",
      "2020-12-23 02:47:05,264 : INFO : extropies processed\n",
      "2020-12-23 02:47:05,265 : INFO : token count processed\n",
      "2020-12-23 02:47:05,266 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:05,267 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:05,267 : INFO : vocab #2480\n",
      "2020-12-23 02:47:05,268 : INFO : diff #set()\n",
      "2020-12-23 02:47:05,527 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:05,655 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.118591595388714, 0.4720116903024542], [0.8229419142007828, 0.17705809], [1.5, 1.1225562489182657], [4.4057645846554525, 5.925214310725336, 6.184137244607258, 4.14684165077353, 1.7783726599518053, 0.2589229338819221]]\n",
      "2020-12-23 02:47:05,658 : INFO : Removed 0 and 293 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:05,659 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:05,660 : INFO : built Dictionary(175 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 1748 corpus positions)\n",
      "2020-12-23 02:47:05,747 : INFO : token count processed\n",
      "2020-12-23 02:47:05,750 : INFO : frequencies processed\n",
      "2020-12-23 02:47:05,880 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:05,881 : INFO : entropies processed\n",
      "2020-12-23 02:47:05,882 : INFO : extropies processed\n",
      "2020-12-23 02:47:05,883 : INFO : token count processed\n",
      "2020-12-23 02:47:05,884 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:05,885 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:05,886 : INFO : vocab #2480\n",
      "2020-12-23 02:47:05,887 : INFO : diff #set()\n",
      "2020-12-23 02:47:06,144 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:06,271 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.1886501650102756, 0.456902622441401], [0.8777749016880989, 0.1222251], [2.321928094887362, 1.2469329688117934], [4.4057645846554525, 6.551685682764175, 6.62027661708921, 4.337173650330418, 2.214512032433757, 0.0685909343250346]]\n",
      "2020-12-23 02:47:06,273 : INFO : Removed 0 and 88 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:06,274 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:06,275 : INFO : built Dictionary(154 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 589 corpus positions)\n",
      "2020-12-23 02:47:06,346 : INFO : token count processed\n",
      "2020-12-23 02:47:06,349 : INFO : frequencies processed\n",
      "2020-12-23 02:47:06,478 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:06,478 : INFO : entropies processed\n",
      "2020-12-23 02:47:06,479 : INFO : extropies processed\n",
      "2020-12-23 02:47:06,481 : INFO : token count processed\n",
      "2020-12-23 02:47:06,482 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:06,483 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:06,484 : INFO : vocab #2480\n",
      "2020-12-23 02:47:06,485 : INFO : diff #set()\n",
      "2020-12-23 02:47:06,757 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:06,886 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.1695146092143367, 0.4609325955920333], [0.8267107903957367, 0.17328921], [1.3787834934861753, 1.0612379896970343], [4.4057645846554525, 6.642985062562557, 6.788794731267432, 4.259954915950578, 2.383030146611979, 0.14580966870487444]]\n",
      "2020-12-23 02:47:06,888 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:06,889 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:06,890 : INFO : built Dictionary(69 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 184 corpus positions)\n",
      "2020-12-23 02:47:06,913 : INFO : token count processed\n",
      "2020-12-23 02:47:06,915 : INFO : frequencies processed\n",
      "2020-12-23 02:47:07,043 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:07,044 : INFO : entropies processed\n",
      "2020-12-23 02:47:07,044 : INFO : extropies processed\n",
      "2020-12-23 02:47:07,046 : INFO : token count processed\n",
      "2020-12-23 02:47:07,046 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:07,047 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:07,048 : INFO : vocab #2480\n",
      "2020-12-23 02:47:07,049 : INFO : diff #set()\n",
      "2020-12-23 02:47:07,307 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:07,435 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.2141268117439432, 0.4516453143947777], [0.9092549756169319, 0.090745024], [0.0, 0.0], [4.4057645846554525, 5.2461980344571995, 5.7673293782130965, 3.8846332408995554, 1.361564793557644, 0.5211313437558971]]\n",
      "2020-12-23 02:47:07,438 : INFO : Removed 0 and 23 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:07,439 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:07,440 : INFO : built Dictionary(91 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 244 corpus positions)\n",
      "2020-12-23 02:47:07,482 : INFO : token count processed\n",
      "2020-12-23 02:47:07,484 : INFO : frequencies processed\n",
      "2020-12-23 02:47:07,615 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:07,615 : INFO : entropies processed\n",
      "2020-12-23 02:47:07,616 : INFO : extropies processed\n",
      "2020-12-23 02:47:07,617 : INFO : token count processed\n",
      "2020-12-23 02:47:07,618 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:07,619 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:07,620 : INFO : vocab #2480\n",
      "2020-12-23 02:47:07,621 : INFO : diff #set()\n",
      "2020-12-23 02:47:07,879 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:08,006 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/test_auth_utility.py')[[1.2151241546072906, 0.45144196451475443], [0.8997045308351517, 0.10029547], [0.0, 0.0], [4.4057645846554525, 5.903090303960449, 6.264254196269677, 4.044600692346224, 1.8584896116142247, 0.36116389230922774]]\n",
      "2020-12-23 02:47:08,009 : INFO : Removed 0 and 442 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:08,009 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:08,011 : INFO : built Dictionary(121 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 1243 corpus positions)\n",
      "2020-12-23 02:47:08,062 : INFO : token count processed\n",
      "2020-12-23 02:47:08,065 : INFO : frequencies processed\n",
      "2020-12-23 02:47:08,193 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:08,194 : INFO : entropies processed\n",
      "2020-12-23 02:47:08,195 : INFO : extropies processed\n",
      "2020-12-23 02:47:08,196 : INFO : token count processed\n",
      "2020-12-23 02:47:08,198 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:08,199 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:08,199 : INFO : vocab #2480\n",
      "2020-12-23 02:47:08,200 : INFO : diff #set()\n",
      "2020-12-23 02:47:08,460 : INFO : alphabet #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:47:08,589 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.198401102271076, 0.4548760455801], [0.9060350805521011, 0.09396492], [2.0, 1.1742577727983858], [4.4057645846554525, 6.16659449033757, 6.262399758067238, 4.309959316925784, 1.8566351734117852, 0.09580526772966813]]\n",
      "2020-12-23 02:47:08,591 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:08,592 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:08,593 : INFO : built Dictionary(82 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 278 corpus positions)\n",
      "2020-12-23 02:47:08,632 : INFO : token count processed\n",
      "2020-12-23 02:47:08,638 : INFO : frequencies processed\n",
      "2020-12-23 02:47:08,771 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:08,771 : INFO : entropies processed\n",
      "2020-12-23 02:47:08,772 : INFO : extropies processed\n",
      "2020-12-23 02:47:08,773 : INFO : token count processed\n",
      "2020-12-23 02:47:08,774 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:08,775 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:08,776 : INFO : vocab #2480\n",
      "2020-12-23 02:47:08,776 : INFO : diff #set()\n",
      "2020-12-23 02:47:09,032 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:09,158 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.108270036626818, 0.4743225405792781], [0.7715584635734558, 0.22844154], [1.75, 1.1484070108583257], [4.4057645846554525, 5.906856253399655, 6.161268014368803, 4.151352823686304, 1.7555034297133503, 0.2544117609691474]]\n",
      "2020-12-23 02:47:09,161 : INFO : Removed 0 and 68 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:09,162 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:09,163 : INFO : built Dictionary(97 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 277 corpus positions)\n",
      "2020-12-23 02:47:09,202 : INFO : token count processed\n",
      "2020-12-23 02:47:09,205 : INFO : frequencies processed\n",
      "2020-12-23 02:47:09,332 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:09,333 : INFO : entropies processed\n",
      "2020-12-23 02:47:09,333 : INFO : extropies processed\n",
      "2020-12-23 02:47:09,334 : INFO : token count processed\n",
      "2020-12-23 02:47:09,335 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:09,336 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:09,336 : INFO : vocab #2480\n",
      "2020-12-23 02:47:09,337 : INFO : diff #set()\n",
      "2020-12-23 02:47:09,596 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:09,722 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.2104841389373855, 0.4523895839762577], [0.9182711243629456, 0.081728876], [0.9182958340544896, 0.9182958340544896], [4.4057645846554525, 5.965115449163356, 6.2654494666293985, 4.10543056718941, 1.859684881973946, 0.30033401746604227]]\n",
      "2020-12-23 02:47:09,725 : INFO : Removed 0 and 13 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:09,726 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:09,727 : INFO : built Dictionary(102 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 391 corpus positions)\n",
      "2020-12-23 02:47:09,768 : INFO : token count processed\n",
      "2020-12-23 02:47:09,774 : INFO : frequencies processed\n",
      "2020-12-23 02:47:09,904 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:09,905 : INFO : entropies processed\n",
      "2020-12-23 02:47:09,906 : INFO : extropies processed\n",
      "2020-12-23 02:47:09,908 : INFO : token count processed\n",
      "2020-12-23 02:47:09,909 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:09,910 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:09,910 : INFO : vocab #2480\n",
      "2020-12-23 02:47:09,911 : INFO : diff #set()\n",
      "2020-12-23 02:47:10,179 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:10,307 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.0972281742247785, 0.4768198388187499], [0.7596442848443985, 0.24035572], [1.9219280948873623, 1.2148067842293933], [4.4057645846554525, 5.791362404253194, 6.043858976253247, 4.153268012655399, 1.6380943915977948, 0.2524965720000534]]\n",
      "2020-12-23 02:47:10,309 : INFO : Removed 0 and 41 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:10,310 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:10,311 : INFO : built Dictionary(92 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 403 corpus positions)\n",
      "2020-12-23 02:47:10,346 : INFO : token count processed\n",
      "2020-12-23 02:47:10,348 : INFO : frequencies processed\n",
      "2020-12-23 02:47:10,476 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:10,477 : INFO : entropies processed\n",
      "2020-12-23 02:47:10,477 : INFO : extropies processed\n",
      "2020-12-23 02:47:10,479 : INFO : token count processed\n",
      "2020-12-23 02:47:10,479 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:10,480 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:10,481 : INFO : vocab #2480\n",
      "2020-12-23 02:47:10,482 : INFO : diff #set()\n",
      "2020-12-23 02:47:10,739 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:10,866 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.1116381362324381, 0.4735659878657947], [0.8057546019554138, 0.1942454], [1.8423709931771086, 1.1893232685884285], [4.4057645846554525, 5.651670454631116, 5.9007702669318345, 4.156664772354734, 1.495005682276382, 0.24909981230071843]]\n",
      "2020-12-23 02:47:10,869 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:10,870 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:10,870 : INFO : built Dictionary(57 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 113 corpus positions)\n",
      "2020-12-23 02:47:10,888 : INFO : token count processed\n",
      "2020-12-23 02:47:10,891 : INFO : frequencies processed\n",
      "2020-12-23 02:47:11,017 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:11,018 : INFO : entropies processed\n",
      "2020-12-23 02:47:11,019 : INFO : extropies processed\n",
      "2020-12-23 02:47:11,021 : INFO : token count processed\n",
      "2020-12-23 02:47:11,023 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:11,024 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:11,025 : INFO : vocab #2480\n",
      "2020-12-23 02:47:11,026 : INFO : diff #set()\n",
      "2020-12-23 02:47:11,292 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:11,422 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.2029206100827012, 0.4539428227340695], [0.892090305685997, 0.107909694], [1.0, 1.0], [4.4057645846554525, 4.8226207261920235, 5.541518330769641, 3.686866980077834, 1.1357537461141884, 0.7188976045776174]]\n",
      "2020-12-23 02:47:11,424 : INFO : Removed 0 and 50 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:11,425 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:11,427 : INFO : built Dictionary(100 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 283 corpus positions)\n",
      "2020-12-23 02:47:11,474 : INFO : token count processed\n",
      "2020-12-23 02:47:11,478 : INFO : frequencies processed\n",
      "2020-12-23 02:47:11,607 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:11,608 : INFO : entropies processed\n",
      "2020-12-23 02:47:11,608 : INFO : extropies processed\n",
      "2020-12-23 02:47:11,609 : INFO : token count processed\n",
      "2020-12-23 02:47:11,610 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:11,610 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:11,611 : INFO : vocab #2480\n",
      "2020-12-23 02:47:11,612 : INFO : diff #set()\n",
      "2020-12-23 02:47:11,868 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:11,996 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.176735242772041, 0.4594035968869206], [0.8470815569162369, 0.15291844], [1.9056390622295665, 1.2149156328132606], [4.4057645846554525, 6.24862851613934, 6.4834747724172885, 4.170918328377503, 2.077710187761836, 0.2348462562779483]]\n",
      "2020-12-23 02:47:11,999 : INFO : Removed 0 and 32 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:47:12,000 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:12,001 : INFO : built Dictionary(103 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 399 corpus positions)\n",
      "2020-12-23 02:47:12,053 : INFO : token count processed\n",
      "2020-12-23 02:47:12,055 : INFO : frequencies processed\n",
      "2020-12-23 02:47:12,185 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:12,185 : INFO : entropies processed\n",
      "2020-12-23 02:47:12,186 : INFO : extropies processed\n",
      "2020-12-23 02:47:12,187 : INFO : token count processed\n",
      "2020-12-23 02:47:12,188 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:12,189 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:12,190 : INFO : vocab #2480\n",
      "2020-12-23 02:47:12,190 : INFO : diff #set()\n",
      "2020-12-23 02:47:12,452 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:12,580 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.19425637020427, 0.455735261193252], [0.9012370631098747, 0.09876294], [1.0, 1.0], [4.4057645846554525, 5.850156917433494, 6.126572920729729, 4.129348581359216, 1.7208083360742767, 0.2764160032962355]]\n",
      "2020-12-23 02:47:12,582 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:12,583 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:12,584 : INFO : built Dictionary(97 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 397 corpus positions)\n",
      "2020-12-23 02:47:12,624 : INFO : token count processed\n",
      "2020-12-23 02:47:12,628 : INFO : frequencies processed\n",
      "2020-12-23 02:47:12,758 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:12,759 : INFO : entropies processed\n",
      "2020-12-23 02:47:12,760 : INFO : extropies processed\n",
      "2020-12-23 02:47:12,761 : INFO : token count processed\n",
      "2020-12-23 02:47:12,762 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:12,763 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:12,764 : INFO : vocab #2480\n",
      "2020-12-23 02:47:12,765 : INFO : diff #set()\n",
      "2020-12-23 02:47:13,028 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:13,157 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.1838419780082061, 0.457908589572978], [0.8883799239993095, 0.111620076], [1.5, 1.1225562489182657], [4.4057645846554525, 5.6831976040360095, 5.957643958947028, 4.1313182297444335, 1.551879374291575, 0.2744463549110181]]\n",
      "2020-12-23 02:47:13,159 : INFO : Removed 0 and 52 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:13,160 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:13,161 : INFO : built Dictionary(84 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 375 corpus positions)\n",
      "2020-12-23 02:47:13,192 : INFO : token count processed\n",
      "2020-12-23 02:47:13,194 : INFO : frequencies processed\n",
      "2020-12-23 02:47:13,322 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:13,323 : INFO : entropies processed\n",
      "2020-12-23 02:47:13,324 : INFO : extropies processed\n",
      "2020-12-23 02:47:13,325 : INFO : token count processed\n",
      "2020-12-23 02:47:13,326 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:13,327 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:13,327 : INFO : vocab #2480\n",
      "2020-12-23 02:47:13,329 : INFO : diff #set()\n",
      "2020-12-23 02:47:13,586 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:13,714 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.1302893030632906, 0.4694198100521046], [0.7612248510122299, 0.23877515], [1.2516291673878228, 0.9667115099633751], [4.4057645846554525, 5.749308601266266, 6.000900584619333, 4.154172601302386, 1.5951359999638806, 0.2515919833530669]]\n",
      "2020-12-23 02:47:13,716 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:13,717 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:13,718 : INFO : built Dictionary(78 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 256 corpus positions)\n",
      "2020-12-23 02:47:13,745 : INFO : token count processed\n",
      "2020-12-23 02:47:13,748 : INFO : frequencies processed\n",
      "2020-12-23 02:47:13,876 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:13,876 : INFO : entropies processed\n",
      "2020-12-23 02:47:13,877 : INFO : extropies processed\n",
      "2020-12-23 02:47:13,878 : INFO : token count processed\n",
      "2020-12-23 02:47:13,879 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:13,880 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:13,881 : INFO : vocab #2480\n",
      "2020-12-23 02:47:13,882 : INFO : diff #set()\n",
      "2020-12-23 02:47:14,139 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:14,267 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.186977544298445, 0.45725206580517835], [0.904423788189888, 0.09557621], [1.0, 1.0], [4.4057645846554525, 5.015422548793484, 5.488570543829962, 3.9326165896189735, 1.0828059591745092, 0.4731479950364781]]\n",
      "2020-12-23 02:47:14,269 : INFO : Removed 0 and 81 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:14,270 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:14,272 : INFO : built Dictionary(107 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 382 corpus positions)\n",
      "2020-12-23 02:47:14,326 : INFO : token count processed\n",
      "2020-12-23 02:47:14,330 : INFO : frequencies processed\n",
      "2020-12-23 02:47:14,458 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:14,458 : INFO : entropies processed\n",
      "2020-12-23 02:47:14,459 : INFO : extropies processed\n",
      "2020-12-23 02:47:14,460 : INFO : token count processed\n",
      "2020-12-23 02:47:14,461 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:14,461 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:14,462 : INFO : vocab #2480\n",
      "2020-12-23 02:47:14,463 : INFO : diff #set()\n",
      "2020-12-23 02:47:14,728 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:14,857 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.1937771475594312, 0.45583481490473915], [0.8971076011657715, 0.1028924], [1.0, 1.0], [4.4057645846554525, 6.030001281822029, 6.2824928550701795, 4.153273011407302, 1.876728270414727, 0.2524915732481503]]\n",
      "2020-12-23 02:47:14,859 : INFO : Removed 0 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:14,860 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:14,861 : INFO : built Dictionary(88 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 345 corpus positions)\n",
      "2020-12-23 02:47:14,901 : INFO : token count processed\n",
      "2020-12-23 02:47:14,904 : INFO : frequencies processed\n",
      "2020-12-23 02:47:15,032 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:15,032 : INFO : entropies processed\n",
      "2020-12-23 02:47:15,033 : INFO : extropies processed\n",
      "2020-12-23 02:47:15,034 : INFO : token count processed\n",
      "2020-12-23 02:47:15,035 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:15,035 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:15,036 : INFO : vocab #2480\n",
      "2020-12-23 02:47:15,036 : INFO : diff #set()\n",
      "2020-12-23 02:47:15,293 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:15,421 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[0.9940019706391383, 0.5015040179120132], [0.654047966003418, 0.34595203], [2.974937501201927, 1.334002234755994], [4.4057645846554525, 5.9537092545441395, 6.116870533413115, 4.242603305786476, 1.7111059487576625, 0.16316127886897558]]\n",
      "2020-12-23 02:47:15,424 : INFO : Removed 0 and 90 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:15,424 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:15,425 : INFO : built Dictionary(105 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 346 corpus positions)\n",
      "2020-12-23 02:47:15,471 : INFO : token count processed\n",
      "2020-12-23 02:47:15,476 : INFO : frequencies processed\n",
      "2020-12-23 02:47:15,602 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:15,602 : INFO : entropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:47:15,603 : INFO : extropies processed\n",
      "2020-12-23 02:47:15,604 : INFO : token count processed\n",
      "2020-12-23 02:47:15,604 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:15,605 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:15,606 : INFO : vocab #2480\n",
      "2020-12-23 02:47:15,607 : INFO : diff #set()\n",
      "2020-12-23 02:47:15,863 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:15,990 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.1698159396341024, 0.4608685841660057], [0.8698849827051163, 0.13011502], [1.4591479170272446, 1.1091703386755989], [4.4057645846554525, 6.184756445474906, 6.410412734718385, 4.180108295411973, 2.0046481500629323, 0.2256562892434788]]\n",
      "2020-12-23 02:47:15,993 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:15,994 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:15,995 : INFO : built Dictionary(114 unique tokens: ['allow', 'append', 'behavior', 'binari', 'bom']...) from 2 documents (total 459 corpus positions)\n",
      "2020-12-23 02:47:16,049 : INFO : token count processed\n",
      "2020-12-23 02:47:16,054 : INFO : frequencies processed\n",
      "2020-12-23 02:47:16,180 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:16,181 : INFO : entropies processed\n",
      "2020-12-23 02:47:16,181 : INFO : extropies processed\n",
      "2020-12-23 02:47:16,182 : INFO : token count processed\n",
      "2020-12-23 02:47:16,183 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:16,184 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:16,185 : INFO : vocab #2480\n",
      "2020-12-23 02:47:16,186 : INFO : diff #set()\n",
      "2020-12-23 02:47:16,451 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:16,580 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.1977900841448874, 0.4550025078437272], [0.8856101706624031, 0.11438983], [1.584962500721156, 1.1699250014423124], [4.4057645846554525, 6.212221456585881, 6.430622818362844, 4.18736322287849, 2.0248582337073913, 0.21840136177696312]]\n",
      "2020-12-23 02:47:16,583 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:16,584 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:16,585 : INFO : built Dictionary(127 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 431 corpus positions)\n",
      "2020-12-23 02:47:16,631 : INFO : token count processed\n",
      "2020-12-23 02:47:16,636 : INFO : frequencies processed\n",
      "2020-12-23 02:47:16,764 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:16,765 : INFO : entropies processed\n",
      "2020-12-23 02:47:16,766 : INFO : extropies processed\n",
      "2020-12-23 02:47:16,767 : INFO : token count processed\n",
      "2020-12-23 02:47:16,768 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:16,769 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:16,770 : INFO : vocab #2480\n",
      "2020-12-23 02:47:16,771 : INFO : diff #set()\n",
      "2020-12-23 02:47:17,029 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:17,156 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.173251506869534, 0.4601400237565935], [0.8229295313358307, 0.17707047], [1.2516291673878228, 0.9667115099633751], [3.7516291673878226, 6.301552355933639, 6.429568046856774, 3.623613476464687, 2.6779388794689516, 0.12801569092313514]]\n",
      "2020-12-23 02:47:17,159 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:17,160 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:17,162 : INFO : built Dictionary(159 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 667 corpus positions)\n",
      "2020-12-23 02:47:17,224 : INFO : token count processed\n",
      "2020-12-23 02:47:17,227 : INFO : frequencies processed\n",
      "2020-12-23 02:47:17,354 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:17,355 : INFO : entropies processed\n",
      "2020-12-23 02:47:17,355 : INFO : extropies processed\n",
      "2020-12-23 02:47:17,356 : INFO : token count processed\n",
      "2020-12-23 02:47:17,357 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:17,358 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:17,359 : INFO : vocab #2480\n",
      "2020-12-23 02:47:17,360 : INFO : diff #set()\n",
      "2020-12-23 02:47:17,620 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:17,748 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.076699003812442, 0.4815334327045863], [0.7305834293365479, 0.26941657], [2.950212064914747, 1.3236480206617924], [3.7516291673878226, 6.739005504021667, 6.7797370778954305, 3.7108975935140593, 3.028107910507608, 0.04073157387376369]]\n",
      "2020-12-23 02:47:17,750 : INFO : Removed 0 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:17,751 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:17,753 : INFO : built Dictionary(108 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 518 corpus positions)\n",
      "2020-12-23 02:47:17,797 : INFO : token count processed\n",
      "2020-12-23 02:47:17,800 : INFO : frequencies processed\n",
      "2020-12-23 02:47:17,932 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:17,933 : INFO : entropies processed\n",
      "2020-12-23 02:47:17,933 : INFO : extropies processed\n",
      "2020-12-23 02:47:17,935 : INFO : token count processed\n",
      "2020-12-23 02:47:17,936 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:17,937 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:17,938 : INFO : vocab #2480\n",
      "2020-12-23 02:47:17,939 : INFO : diff #set()\n",
      "2020-12-23 02:47:18,201 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:18,329 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2303022902723246, 0.4483697139897112], [0.9347375482320786, 0.06526245], [1.9182958340544893, 1.2183406773511978], [3.7516291673878226, 5.870833373337847, 5.996136405123078, 3.626326135602591, 2.2445072377352555, 0.12530303178523106]]\n",
      "2020-12-23 02:47:18,332 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:18,333 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:18,334 : INFO : built Dictionary(67 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 201 corpus positions)\n",
      "2020-12-23 02:47:18,352 : INFO : token count processed\n",
      "2020-12-23 02:47:18,355 : INFO : frequencies processed\n",
      "2020-12-23 02:47:18,483 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:18,484 : INFO : entropies processed\n",
      "2020-12-23 02:47:18,484 : INFO : extropies processed\n",
      "2020-12-23 02:47:18,486 : INFO : token count processed\n",
      "2020-12-23 02:47:18,486 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:18,487 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:18,488 : INFO : vocab #2480\n",
      "2020-12-23 02:47:18,489 : INFO : diff #set()\n",
      "2020-12-23 02:47:18,745 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:18,873 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.1548061848724265, 0.4640788610225769], [0.8778485432267189, 0.12215146], [2.1280852788913944, 1.2238339714721664], [3.7516291673878226, 5.371881234145534, 5.604540554991918, 3.518969846541439, 1.8529113876040957, 0.23265932084638408]]\n",
      "2020-12-23 02:47:18,875 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:18,876 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:18,877 : INFO : built Dictionary(58 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 153 corpus positions)\n",
      "2020-12-23 02:47:18,893 : INFO : token count processed\n",
      "2020-12-23 02:47:18,895 : INFO : frequencies processed\n",
      "2020-12-23 02:47:19,023 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:19,024 : INFO : entropies processed\n",
      "2020-12-23 02:47:19,025 : INFO : extropies processed\n",
      "2020-12-23 02:47:19,026 : INFO : token count processed\n",
      "2020-12-23 02:47:19,027 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:19,027 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:19,028 : INFO : vocab #2480\n",
      "2020-12-23 02:47:19,029 : INFO : diff #set()\n",
      "2020-12-23 02:47:19,287 : INFO : alphabet #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:47:19,415 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.1280006188651224, 0.46992467536654525], [0.8587321192026138, 0.14126788], [2.1556390622295662, 1.2407663947533207], [3.7516291673878226, 4.85108279267097, 5.1715045627530865, 3.4312073973057062, 1.4198753953652639, 0.3204217700821168]]\n",
      "2020-12-23 02:47:19,417 : INFO : Removed 0 and 127 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:19,418 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:19,420 : INFO : built Dictionary(96 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 426 corpus positions)\n",
      "2020-12-23 02:47:19,451 : INFO : token count processed\n",
      "2020-12-23 02:47:19,453 : INFO : frequencies processed\n",
      "2020-12-23 02:47:19,584 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:19,584 : INFO : entropies processed\n",
      "2020-12-23 02:47:19,585 : INFO : extropies processed\n",
      "2020-12-23 02:47:19,586 : INFO : token count processed\n",
      "2020-12-23 02:47:19,588 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:19,589 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:19,590 : INFO : vocab #2480\n",
      "2020-12-23 02:47:19,591 : INFO : diff #set()\n",
      "2020-12-23 02:47:19,849 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:19,977 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.1345148624870445, 0.4684905303656884], [0.8051484674215317, 0.19485153], [2.469670487371862, 1.2675306112133993], [3.7516291673878226, 6.139571208108155, 6.222978157711679, 3.6682222177842982, 2.471348990323856, 0.0834069496035239]]\n",
      "2020-12-23 02:47:19,979 : INFO : Removed 0 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:19,980 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:19,981 : INFO : built Dictionary(87 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 436 corpus positions)\n",
      "2020-12-23 02:47:20,009 : INFO : token count processed\n",
      "2020-12-23 02:47:20,012 : INFO : frequencies processed\n",
      "2020-12-23 02:47:20,139 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:20,140 : INFO : entropies processed\n",
      "2020-12-23 02:47:20,140 : INFO : extropies processed\n",
      "2020-12-23 02:47:20,142 : INFO : token count processed\n",
      "2020-12-23 02:47:20,142 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:20,143 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:20,144 : INFO : vocab #2480\n",
      "2020-12-23 02:47:20,145 : INFO : diff #set()\n",
      "2020-12-23 02:47:20,402 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:20,531 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.1576241729421142, 0.4634727458751124], [0.8510516732931137, 0.14894833], [0.7219280948873623, 0.7219280948873623], [3.7516291673878226, 5.609710627339259, 5.77644705731414, 3.5848927374129413, 2.0248178899263176, 0.1667364299748808]]\n",
      "2020-12-23 02:47:20,533 : INFO : Removed 0 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:20,534 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:20,537 : INFO : built Dictionary(176 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 1096 corpus positions)\n",
      "2020-12-23 02:47:20,608 : INFO : token count processed\n",
      "2020-12-23 02:47:20,611 : INFO : frequencies processed\n",
      "2020-12-23 02:47:20,737 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:20,738 : INFO : entropies processed\n",
      "2020-12-23 02:47:20,739 : INFO : extropies processed\n",
      "2020-12-23 02:47:20,740 : INFO : token count processed\n",
      "2020-12-23 02:47:20,741 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:20,742 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:20,743 : INFO : vocab #2480\n",
      "2020-12-23 02:47:20,744 : INFO : diff #set()\n",
      "2020-12-23 02:47:21,003 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:21,131 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.1551142235195713, 0.46401252847140273], [0.7490128576755524, 0.25098714], [2.084962500721156, 1.2292852896434443], [3.7516291673878226, 7.2441902753576075, 7.275339135517063, 3.7204803072283674, 3.5237099681292405, 0.031148860159455616]]\n",
      "2020-12-23 02:47:21,133 : INFO : Removed 0 and 136 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:21,134 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:21,136 : INFO : built Dictionary(134 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 703 corpus positions)\n",
      "2020-12-23 02:47:21,185 : INFO : token count processed\n",
      "2020-12-23 02:47:21,187 : INFO : frequencies processed\n",
      "2020-12-23 02:47:21,315 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:21,316 : INFO : entropies processed\n",
      "2020-12-23 02:47:21,316 : INFO : extropies processed\n",
      "2020-12-23 02:47:21,318 : INFO : token count processed\n",
      "2020-12-23 02:47:21,319 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:21,320 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:21,321 : INFO : vocab #2480\n",
      "2020-12-23 02:47:21,323 : INFO : diff #set()\n",
      "2020-12-23 02:47:21,584 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:21,712 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.1226827866605398, 0.47110194998717936], [0.824776217341423, 0.17522378], [2.9312089489103235, 1.3173698313927213], [3.7516291673878226, 6.2567074920449475, 6.325561579157648, 3.682775080275122, 2.573932411769825, 0.06885408711270014]]\n",
      "2020-12-23 02:47:21,714 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:21,715 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:21,717 : INFO : built Dictionary(83 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 213 corpus positions)\n",
      "2020-12-23 02:47:21,745 : INFO : token count processed\n",
      "2020-12-23 02:47:21,747 : INFO : frequencies processed\n",
      "2020-12-23 02:47:21,875 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:21,876 : INFO : entropies processed\n",
      "2020-12-23 02:47:21,876 : INFO : extropies processed\n",
      "2020-12-23 02:47:21,878 : INFO : token count processed\n",
      "2020-12-23 02:47:21,879 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:21,879 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:21,880 : INFO : vocab #2480\n",
      "2020-12-23 02:47:21,881 : INFO : diff #set()\n",
      "2020-12-23 02:47:22,139 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:22,266 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.0990020892128338, 0.4764168673957915], [0.7801321744918823, 0.21986783], [2.321928094887362, 1.2877123795494492], [3.7516291673878226, 5.7680018917339435, 5.971306941464347, 3.548324117657419, 2.219677774076524, 0.2033050497304032]]\n",
      "2020-12-23 02:47:22,268 : INFO : Removed 0 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:22,269 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:22,271 : INFO : built Dictionary(177 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 754 corpus positions)\n",
      "2020-12-23 02:47:22,330 : INFO : token count processed\n",
      "2020-12-23 02:47:22,332 : INFO : frequencies processed\n",
      "2020-12-23 02:47:22,461 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:22,462 : INFO : entropies processed\n",
      "2020-12-23 02:47:22,463 : INFO : extropies processed\n",
      "2020-12-23 02:47:22,465 : INFO : token count processed\n",
      "2020-12-23 02:47:22,466 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:22,467 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:22,468 : INFO : vocab #2480\n",
      "2020-12-23 02:47:22,469 : INFO : diff #set()\n",
      "2020-12-23 02:47:22,729 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:22,857 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.0570932203206995, 0.48612284077437223], [0.6750723719596863, 0.32492763], [2.840223928941852, 1.3065729000904318], [3.7516291673878226, 6.846479111193757, 6.876871991284718, 3.721236287296861, 3.1252428238968952, 0.030392880090960972]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:47:22,859 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:22,860 : INFO : built Dictionary(36 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 57 corpus positions)\n",
      "2020-12-23 02:47:22,873 : INFO : token count processed\n",
      "2020-12-23 02:47:22,876 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:47:22,877 : INFO : frequencies processed\n",
      "2020-12-23 02:47:22,878 : INFO : token count processed\n",
      "2020-12-23 02:47:22,880 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:22,881 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:22,882 : INFO : vocab #2480\n",
      "2020-12-23 02:47:22,884 : INFO : diff #set()\n",
      "2020-12-23 02:47:23,155 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:23,288 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2290577881928335, 0.4486200426462389], [0.9520692676305771, 0.047930732], [nan, nan], [3.7516291673878226, 4.165013816065912, 4.972897908749761, 2.9437450747039726, 1.2212687413619387, 0.8078840926838495]]\n",
      "2020-12-23 02:47:23,290 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:23,291 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:23,292 : INFO : built Dictionary(63 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 164 corpus positions)\n",
      "2020-12-23 02:47:23,315 : INFO : token count processed\n",
      "2020-12-23 02:47:23,320 : INFO : frequencies processed\n",
      "2020-12-23 02:47:23,450 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:23,451 : INFO : entropies processed\n",
      "2020-12-23 02:47:23,452 : INFO : extropies processed\n",
      "2020-12-23 02:47:23,453 : INFO : token count processed\n",
      "2020-12-23 02:47:23,454 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:23,455 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:23,456 : INFO : vocab #2480\n",
      "2020-12-23 02:47:23,457 : INFO : diff #set()\n",
      "2020-12-23 02:47:23,715 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:23,843 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.2068915553074895, 0.45312602587790884], [0.8898483738303185, 0.110151626], [0.0, 0.0], [3.7516291673878226, 5.449968864419248, 5.758850744174417, 3.4427472876326544, 2.0072215767865944, 0.3088818797551687]]\n",
      "2020-12-23 02:47:23,846 : INFO : Removed 0 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:23,847 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:23,848 : INFO : built Dictionary(147 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 601 corpus positions)\n",
      "2020-12-23 02:47:23,901 : INFO : token count processed\n",
      "2020-12-23 02:47:23,906 : INFO : frequencies processed\n",
      "2020-12-23 02:47:24,032 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:24,032 : INFO : entropies processed\n",
      "2020-12-23 02:47:24,033 : INFO : extropies processed\n",
      "2020-12-23 02:47:24,034 : INFO : token count processed\n",
      "2020-12-23 02:47:24,034 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:24,035 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:24,036 : INFO : vocab #2480\n",
      "2020-12-23 02:47:24,036 : INFO : diff #set()\n",
      "2020-12-23 02:47:24,294 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:24,422 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.1518934231798308, 0.46470702927392665], [0.8426831662654877, 0.15731683], [3.09306920777189, 1.3315827525040738], [3.7516291673878226, 6.530294129310484, 6.5916529817118334, 3.6902703149864724, 2.840023814324011, 0.06135885240134975]]\n",
      "2020-12-23 02:47:24,425 : INFO : Removed 0 and 142 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:24,426 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:24,427 : INFO : built Dictionary(125 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 617 corpus positions)\n",
      "2020-12-23 02:47:24,468 : INFO : token count processed\n",
      "2020-12-23 02:47:24,470 : INFO : frequencies processed\n",
      "2020-12-23 02:47:24,602 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:24,602 : INFO : entropies processed\n",
      "2020-12-23 02:47:24,603 : INFO : extropies processed\n",
      "2020-12-23 02:47:24,605 : INFO : token count processed\n",
      "2020-12-23 02:47:24,606 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:24,607 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:24,607 : INFO : vocab #2480\n",
      "2020-12-23 02:47:24,608 : INFO : diff #set()\n",
      "2020-12-23 02:47:24,876 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:25,004 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.1284569466658525, 0.46982392646769877], [0.7459567189216614, 0.25404328], [2.0, 1.1742577727983858], [3.7516291673878226, 6.470272233491701, 6.544905533454218, 3.6769958674253065, 2.793276366066395, 0.07463329996251655]]\n",
      "2020-12-23 02:47:25,007 : INFO : Removed 0 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:25,008 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:25,009 : INFO : built Dictionary(123 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 406 corpus positions)\n",
      "2020-12-23 02:47:25,048 : INFO : token count processed\n",
      "2020-12-23 02:47:25,055 : INFO : frequencies processed\n",
      "2020-12-23 02:47:25,188 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:25,189 : INFO : entropies processed\n",
      "2020-12-23 02:47:25,190 : INFO : extropies processed\n",
      "2020-12-23 02:47:25,191 : INFO : token count processed\n",
      "2020-12-23 02:47:25,192 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:25,194 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:25,194 : INFO : vocab #2480\n",
      "2020-12-23 02:47:25,196 : INFO : diff #set()\n",
      "2020-12-23 02:47:25,456 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:25,584 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.0827621772313454, 0.48013163045303553], [0.7530037611722946, 0.24699624], [2.8553885422075336, 1.3250186770664762], [3.7516291673878226, 6.550038223589686, 6.617720189970093, 3.683947201007415, 2.8660910225822707, 0.06768196638040713]]\n",
      "2020-12-23 02:47:25,587 : INFO : Removed 0 and 70 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:25,588 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:25,589 : INFO : built Dictionary(81 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 322 corpus positions)\n",
      "2020-12-23 02:47:25,623 : INFO : token count processed\n",
      "2020-12-23 02:47:25,628 : INFO : frequencies processed\n",
      "2020-12-23 02:47:25,756 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:25,757 : INFO : entropies processed\n",
      "2020-12-23 02:47:25,757 : INFO : extropies processed\n",
      "2020-12-23 02:47:25,758 : INFO : token count processed\n",
      "2020-12-23 02:47:25,759 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:25,760 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:25,760 : INFO : vocab #2480\n",
      "2020-12-23 02:47:25,761 : INFO : diff #set()\n",
      "2020-12-23 02:47:26,017 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:26,144 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.1003120681019318, 0.47611972296274413], [0.7425248622894287, 0.25747514], [2.084962500721156, 1.2292852896434443], [3.7516291673878226, 5.860525481261383, 5.975869659050952, 3.636284989598254, 2.2242404916631293, 0.11534417778956918]]\n",
      "2020-12-23 02:47:26,147 : INFO : Removed 0 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:26,148 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:26,149 : INFO : built Dictionary(57 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 360 corpus positions)\n",
      "2020-12-23 02:47:26,175 : INFO : token count processed\n",
      "2020-12-23 02:47:26,179 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:47:26,180 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:47:26,183 : INFO : token count processed\n",
      "2020-12-23 02:47:26,185 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:26,186 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:26,186 : INFO : vocab #2480\n",
      "2020-12-23 02:47:26,187 : INFO : diff #set()\n",
      "2020-12-23 02:47:26,447 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:26,689 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.2810943570910007, 0.43838607416278247], [0.9463194981217384, 0.053680502], [nan, nan], [3.7516291673878226, 5.945464049777852, 6.112509270199514, 3.584583946966161, 2.360880102811691, 0.16704522042166126]]\n",
      "2020-12-23 02:47:26,692 : INFO : Removed 0 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:26,693 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:26,695 : INFO : built Dictionary(197 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 907 corpus positions)\n",
      "2020-12-23 02:47:26,768 : INFO : token count processed\n",
      "2020-12-23 02:47:26,775 : INFO : frequencies processed\n",
      "2020-12-23 02:47:26,903 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:26,904 : INFO : entropies processed\n",
      "2020-12-23 02:47:26,905 : INFO : extropies processed\n",
      "2020-12-23 02:47:26,906 : INFO : token count processed\n",
      "2020-12-23 02:47:26,907 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:26,908 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:26,909 : INFO : vocab #2480\n",
      "2020-12-23 02:47:26,910 : INFO : diff #set()\n",
      "2020-12-23 02:47:27,166 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:27,293 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.149865427678341, 0.4651453933467403], [0.8341730087995529, 0.16582699], [2.751629167387823, 1.2995901901368234], [3.7516291673878226, 6.811563897304216, 6.865279576173716, 3.6979134885183225, 3.1136504087858934, 0.05371567886949968]]\n",
      "2020-12-23 02:47:27,296 : INFO : Removed 0 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:27,297 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:27,298 : INFO : built Dictionary(217 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 1002 corpus positions)\n",
      "2020-12-23 02:47:27,381 : INFO : token count processed\n",
      "2020-12-23 02:47:27,384 : INFO : frequencies processed\n",
      "2020-12-23 02:47:27,509 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:27,510 : INFO : entropies processed\n",
      "2020-12-23 02:47:27,511 : INFO : extropies processed\n",
      "2020-12-23 02:47:27,512 : INFO : token count processed\n",
      "2020-12-23 02:47:27,513 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:27,514 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:27,515 : INFO : vocab #2480\n",
      "2020-12-23 02:47:27,516 : INFO : diff #set()\n",
      "2020-12-23 02:47:27,777 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:27,904 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.1604287524245582, 0.4628710846760126], [0.7813344597816467, 0.21866554], [2.875, 1.3148610918199455], [3.7516291673878226, 7.502034948968415, 7.52691322937533, 3.7267508869809074, 3.775284061987507, 0.024878280406914755]]\n",
      "2020-12-23 02:47:27,907 : INFO : Removed 0 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:27,908 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:27,910 : INFO : built Dictionary(259 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 1575 corpus positions)\n",
      "2020-12-23 02:47:28,029 : INFO : token count processed\n",
      "2020-12-23 02:47:28,032 : INFO : frequencies processed\n",
      "2020-12-23 02:47:28,165 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:28,166 : INFO : entropies processed\n",
      "2020-12-23 02:47:28,167 : INFO : extropies processed\n",
      "2020-12-23 02:47:28,168 : INFO : token count processed\n",
      "2020-12-23 02:47:28,169 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:28,170 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:28,171 : INFO : vocab #2480\n",
      "2020-12-23 02:47:28,173 : INFO : diff #set()\n",
      "2020-12-23 02:47:28,442 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:28,572 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.1529634274918785, 0.4644760738759796], [0.8174838870763779, 0.18251611], [3.1462863706621045, 1.3327794318170185], [3.7516291673878226, 7.39180093901977, 7.4116372135437825, 3.73179289286381, 3.66000804615596, 0.019836274524012154]]\n",
      "2020-12-23 02:47:28,575 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:28,576 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:28,577 : INFO : built Dictionary(54 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 130 corpus positions)\n",
      "2020-12-23 02:47:28,590 : INFO : token count processed\n",
      "2020-12-23 02:47:28,592 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:47:28,593 : INFO : frequencies processed\n",
      "2020-12-23 02:47:28,594 : INFO : token count processed\n",
      "2020-12-23 02:47:28,595 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:28,596 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:28,596 : INFO : vocab #2480\n",
      "2020-12-23 02:47:28,597 : INFO : diff #set()\n",
      "2020-12-23 02:47:28,854 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:28,982 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.2284853981460215, 0.4487352714233378], [0.9347325637936592, 0.065267436], [nan, nan], [3.7516291673878226, 4.927561309677364, 5.397793901263761, 3.281396575801425, 1.646164733875938, 0.47023259158639696]]\n",
      "2020-12-23 02:47:28,985 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:28,986 : INFO : built Dictionary(22 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 31 corpus positions)\n",
      "2020-12-23 02:47:28,994 : INFO : token count processed\n",
      "2020-12-23 02:47:28,999 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:47:28,999 : INFO : frequencies processed\n",
      "2020-12-23 02:47:29,000 : INFO : token count processed\n",
      "2020-12-23 02:47:29,001 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:29,001 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:29,002 : INFO : vocab #2480\n",
      "2020-12-23 02:47:29,003 : INFO : diff #set()\n",
      "2020-12-23 02:47:29,266 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:29,394 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2603400758422945, 0.4424113038067333], [0.9965940115507692, 0.0034059884], [nan, nan], [3.7516291673878226, 2.5216406363433186, 4.244518891032037, 2.0287509126991043, 0.49288972364421424, 1.7228782546887182]]\n",
      "2020-12-23 02:47:29,397 : INFO : Removed 0 and 530 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:29,398 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:29,401 : INFO : built Dictionary(333 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 2903 corpus positions)\n",
      "2020-12-23 02:47:29,568 : INFO : token count processed\n",
      "2020-12-23 02:47:29,575 : INFO : frequencies processed\n",
      "2020-12-23 02:47:29,708 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:29,709 : INFO : entropies processed\n",
      "2020-12-23 02:47:29,710 : INFO : extropies processed\n",
      "2020-12-23 02:47:29,712 : INFO : token count processed\n",
      "2020-12-23 02:47:29,713 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:29,714 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:29,714 : INFO : vocab #2480\n",
      "2020-12-23 02:47:29,716 : INFO : diff #set()\n",
      "2020-12-23 02:47:29,984 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:30,118 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.1662045475722498, 0.4616369221091051], [0.834742397069931, 0.1652576], [3.1952959344962166, 1.3429064685148835], [3.7516291673878226, 7.480007711014331, 7.49656708432443, 3.735069794077723, 3.7449379169366073, 0.016559373310099268]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:47:30,121 : INFO : Removed 0 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:30,122 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:30,123 : INFO : built Dictionary(210 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 1052 corpus positions)\n",
      "2020-12-23 02:47:30,203 : INFO : token count processed\n",
      "2020-12-23 02:47:30,209 : INFO : frequencies processed\n",
      "2020-12-23 02:47:30,336 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:30,336 : INFO : entropies processed\n",
      "2020-12-23 02:47:30,337 : INFO : extropies processed\n",
      "2020-12-23 02:47:30,338 : INFO : token count processed\n",
      "2020-12-23 02:47:30,339 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:30,340 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:30,340 : INFO : vocab #2480\n",
      "2020-12-23 02:47:30,341 : INFO : diff #set()\n",
      "2020-12-23 02:47:30,601 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:30,730 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.139209654854359, 0.46746236290153687], [0.782339945435524, 0.21766005], [3.1952959344962166, 1.3429064685148835], [3.7516291673878226, 7.131331012509435, 7.154966197482516, 3.7279939824147412, 3.403337030094693, 0.023635184973080925]]\n",
      "2020-12-23 02:47:30,733 : INFO : Removed 0 and 166 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:30,734 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:30,735 : INFO : built Dictionary(201 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 871 corpus positions)\n",
      "2020-12-23 02:47:30,815 : INFO : token count processed\n",
      "2020-12-23 02:47:30,818 : INFO : frequencies processed\n",
      "2020-12-23 02:47:30,951 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:30,952 : INFO : entropies processed\n",
      "2020-12-23 02:47:30,952 : INFO : extropies processed\n",
      "2020-12-23 02:47:30,954 : INFO : token count processed\n",
      "2020-12-23 02:47:30,955 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:30,956 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:30,956 : INFO : vocab #2480\n",
      "2020-12-23 02:47:30,957 : INFO : diff #set()\n",
      "2020-12-23 02:47:31,214 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:31,341 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.1360675253726338, 0.46814999438070265], [0.7879430651664734, 0.21205693], [2.9312089489103235, 1.3173698313927213], [3.7516291673878226, 7.203742744794778, 7.238950611992791, 3.7164213001898103, 3.487321444604968, 0.03520786719801272]]\n",
      "2020-12-23 02:47:31,344 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:31,345 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:31,346 : INFO : built Dictionary(68 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 203 corpus positions)\n",
      "2020-12-23 02:47:31,365 : INFO : token count processed\n",
      "2020-12-23 02:47:31,369 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:47:31,370 : INFO : frequencies processed\n",
      "2020-12-23 02:47:31,372 : INFO : token count processed\n",
      "2020-12-23 02:47:31,373 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:31,374 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:31,375 : INFO : vocab #2480\n",
      "2020-12-23 02:47:31,376 : INFO : diff #set()\n",
      "2020-12-23 02:47:31,641 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:31,779 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.2257692956821873, 0.44928286230739156], [0.9301662370562553, 0.06983376], [nan, nan], [3.7516291673878226, 5.195502554608948, 5.548194471825757, 3.398937250171014, 1.7965653044379342, 0.35269191721680837]]\n",
      "2020-12-23 02:47:31,781 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:31,782 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:31,784 : INFO : built Dictionary(73 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 264 corpus positions)\n",
      "2020-12-23 02:47:31,811 : INFO : token count processed\n",
      "2020-12-23 02:47:31,816 : INFO : frequencies processed\n",
      "2020-12-23 02:47:31,948 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:31,949 : INFO : entropies processed\n",
      "2020-12-23 02:47:31,950 : INFO : extropies processed\n",
      "2020-12-23 02:47:31,952 : INFO : token count processed\n",
      "2020-12-23 02:47:31,954 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:31,955 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:31,956 : INFO : vocab #2480\n",
      "2020-12-23 02:47:31,958 : INFO : diff #set()\n",
      "2020-12-23 02:47:32,227 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:32,354 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.175663213751774, 0.459629961879795], [0.8563606292009354, 0.14363937], [0.0, 0.0], [3.7516291673878226, 5.32027245610305, 5.574979801830788, 3.4969218216600835, 1.8233506344429657, 0.2547073457277387]]\n",
      "2020-12-23 02:47:32,356 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:32,357 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:32,358 : INFO : built Dictionary(164 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 504 corpus positions)\n",
      "2020-12-23 02:47:32,419 : INFO : token count processed\n",
      "2020-12-23 02:47:32,425 : INFO : frequencies processed\n",
      "2020-12-23 02:47:32,552 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:32,552 : INFO : entropies processed\n",
      "2020-12-23 02:47:32,553 : INFO : extropies processed\n",
      "2020-12-23 02:47:32,555 : INFO : token count processed\n",
      "2020-12-23 02:47:32,557 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:32,558 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:32,559 : INFO : vocab #2480\n",
      "2020-12-23 02:47:32,560 : INFO : diff #set()\n",
      "2020-12-23 02:47:32,826 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:32,955 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.0979501233493605, 0.4766557550012238], [0.7573944926261902, 0.2426055], [3.077819531114783, 1.3342657629881893], [3.7516291673878226, 6.898202761357263, 6.9414825994152425, 3.7083493293298426, 3.18985343202742, 0.043279838057979525]]\n",
      "2020-12-23 02:47:32,958 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:32,959 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:32,960 : INFO : built Dictionary(135 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 526 corpus positions)\n",
      "2020-12-23 02:47:33,003 : INFO : token count processed\n",
      "2020-12-23 02:47:33,006 : INFO : frequencies processed\n",
      "2020-12-23 02:47:33,130 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:33,130 : INFO : entropies processed\n",
      "2020-12-23 02:47:33,131 : INFO : extropies processed\n",
      "2020-12-23 02:47:33,132 : INFO : token count processed\n",
      "2020-12-23 02:47:33,133 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:33,134 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:33,134 : INFO : vocab #2480\n",
      "2020-12-23 02:47:33,135 : INFO : diff #set()\n",
      "2020-12-23 02:47:33,393 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:33,521 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.1335772433008158, 0.4686964126280797], [0.7146487832069397, 0.28535122], [1.5219280948873621, 1.1419011889093373], [3.7516291673878226, 6.388500481644799, 6.471619765218507, 3.6685098838141146, 2.719990597830684, 0.08311928357370757]]\n",
      "2020-12-23 02:47:33,524 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:33,525 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:33,525 : INFO : built Dictionary(59 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 190 corpus positions)\n",
      "2020-12-23 02:47:33,541 : INFO : token count processed\n",
      "2020-12-23 02:47:33,543 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:47:33,544 : INFO : frequencies processed\n",
      "2020-12-23 02:47:33,545 : INFO : token count processed\n",
      "2020-12-23 02:47:33,546 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:33,547 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:33,547 : INFO : vocab #2480\n",
      "2020-12-23 02:47:33,548 : INFO : diff #set()\n",
      "2020-12-23 02:47:33,806 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:33,934 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.2231078097830315, 0.4498207399566452], [0.929510198533535, 0.0704898], [nan, nan], [3.7516291673878226, 4.8191513650620195, 5.230411347000098, 3.3403691854497435, 1.4787821796122755, 0.41125998193807867]]\n",
      "2020-12-23 02:47:33,937 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:33,938 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:33,939 : INFO : built Dictionary(63 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 245 corpus positions)\n",
      "2020-12-23 02:47:33,954 : INFO : token count processed\n",
      "2020-12-23 02:47:33,957 : INFO : frequencies processed\n",
      "2020-12-23 02:47:34,086 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:34,086 : INFO : entropies processed\n",
      "2020-12-23 02:47:34,087 : INFO : extropies processed\n",
      "2020-12-23 02:47:34,088 : INFO : token count processed\n",
      "2020-12-23 02:47:34,089 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:34,090 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:34,091 : INFO : vocab #2480\n",
      "2020-12-23 02:47:34,092 : INFO : diff #set()\n",
      "2020-12-23 02:47:34,350 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:34,478 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.1684158491412169, 0.46116615518930176], [0.8529764860868454, 0.14702351], [0.0, 0.0], [3.7516291673878226, 5.062480936779194, 5.351014081057141, 3.463096023109875, 1.5993849136693181, 0.2885331442779471]]\n",
      "2020-12-23 02:47:34,481 : INFO : Removed 0 and 223 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:34,482 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:34,485 : INFO : built Dictionary(248 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 1799 corpus positions)\n",
      "2020-12-23 02:47:34,598 : INFO : token count processed\n",
      "2020-12-23 02:47:34,600 : INFO : frequencies processed\n",
      "2020-12-23 02:47:34,730 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:34,730 : INFO : entropies processed\n",
      "2020-12-23 02:47:34,731 : INFO : extropies processed\n",
      "2020-12-23 02:47:34,733 : INFO : token count processed\n",
      "2020-12-23 02:47:34,734 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:34,735 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:34,735 : INFO : vocab #2480\n",
      "2020-12-23 02:47:34,736 : INFO : diff #set()\n",
      "2020-12-23 02:47:35,003 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:35,131 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.1710442158344885, 0.46060784608001554], [0.7959476113319397, 0.20405239], [2.521640636343318, 1.2812025859357736], [3.7516291673878226, 7.185085743102134, 7.2117935146775665, 3.724921395812391, 3.460164347289744, 0.026707771575432027]]\n",
      "2020-12-23 02:47:35,134 : INFO : Removed 0 and 148 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:35,135 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:35,137 : INFO : built Dictionary(164 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 774 corpus positions)\n",
      "2020-12-23 02:47:35,199 : INFO : token count processed\n",
      "2020-12-23 02:47:35,204 : INFO : frequencies processed\n",
      "2020-12-23 02:47:35,331 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:35,332 : INFO : entropies processed\n",
      "2020-12-23 02:47:35,333 : INFO : extropies processed\n",
      "2020-12-23 02:47:35,334 : INFO : token count processed\n",
      "2020-12-23 02:47:35,335 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:35,336 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:35,336 : INFO : vocab #2480\n",
      "2020-12-23 02:47:35,337 : INFO : diff #set()\n",
      "2020-12-23 02:47:35,591 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:35,719 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.1506660473229344, 0.464972235575468], [0.84303979575634, 0.1569602], [2.751629167387823, 1.2995901901368234], [3.7516291673878226, 6.591225336124281, 6.65167300663076, 3.6911814968813443, 2.900043839242937, 0.0604476705064787]]\n",
      "2020-12-23 02:47:35,722 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:35,723 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:35,724 : INFO : built Dictionary(50 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 112 corpus positions)\n",
      "2020-12-23 02:47:35,741 : INFO : token count processed\n",
      "2020-12-23 02:47:35,743 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:47:35,744 : INFO : frequencies processed\n",
      "2020-12-23 02:47:35,745 : INFO : token count processed\n",
      "2020-12-23 02:47:35,746 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:35,747 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:35,748 : INFO : vocab #2480\n",
      "2020-12-23 02:47:35,749 : INFO : diff #set()\n",
      "2020-12-23 02:47:36,008 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:36,136 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.2306564855770223, 0.44829851950123184], [0.9476096965372562, 0.052390303], [nan, nan], [3.7516291673878226, 4.7032114441396695, 5.247130671905642, 3.207709939621851, 1.495501504517819, 0.5439192277659721]]\n",
      "2020-12-23 02:47:36,139 : INFO : Removed 0 and 9 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:36,139 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:36,140 : INFO : built Dictionary(121 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 352 corpus positions)\n",
      "2020-12-23 02:47:36,179 : INFO : token count processed\n",
      "2020-12-23 02:47:36,182 : INFO : frequencies processed\n",
      "2020-12-23 02:47:36,312 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:36,313 : INFO : entropies processed\n",
      "2020-12-23 02:47:36,314 : INFO : extropies processed\n",
      "2020-12-23 02:47:36,316 : INFO : token count processed\n",
      "2020-12-23 02:47:36,317 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:36,318 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:36,319 : INFO : vocab #2480\n",
      "2020-12-23 02:47:36,321 : INFO : diff #set()\n",
      "2020-12-23 02:47:36,590 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:36,718 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.1333631820062637, 0.46874344154546493], [0.8063923567533493, 0.19360764], [2.5219280948873624, 1.2629960611029938], [3.7516291673878226, 6.14228447828618, 6.270604105695259, 3.6233095399787434, 2.5189749383074367, 0.12831962740907965]]\n",
      "2020-12-23 02:47:36,721 : INFO : Removed 0 and 184 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:36,722 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:36,723 : INFO : built Dictionary(256 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 1142 corpus positions)\n",
      "2020-12-23 02:47:36,829 : INFO : token count processed\n",
      "2020-12-23 02:47:36,834 : INFO : frequencies processed\n",
      "2020-12-23 02:47:36,962 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:36,963 : INFO : entropies processed\n",
      "2020-12-23 02:47:36,964 : INFO : extropies processed\n",
      "2020-12-23 02:47:36,966 : INFO : token count processed\n",
      "2020-12-23 02:47:36,967 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:36,968 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:36,969 : INFO : vocab #2480\n",
      "2020-12-23 02:47:36,970 : INFO : diff #set()\n",
      "2020-12-23 02:47:37,234 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:37,362 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.1545709442631933, 0.4641295301334223], [0.801928922533989, 0.19807108], [3.2776134368191157, 1.3618978811135465], [3.7516291673878226, 7.450178124335845, 7.48558535661712, 3.7162219351065477, 3.7339561892292976, 0.03540723228127529]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:47:37,365 : INFO : Removed 0 and 16 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:37,366 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:37,367 : INFO : built Dictionary(68 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 190 corpus positions)\n",
      "2020-12-23 02:47:37,390 : INFO : token count processed\n",
      "2020-12-23 02:47:37,392 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:47:37,393 : INFO : frequencies processed\n",
      "2020-12-23 02:47:37,394 : INFO : token count processed\n",
      "2020-12-23 02:47:37,396 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:37,397 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:37,398 : INFO : vocab #2480\n",
      "2020-12-23 02:47:37,400 : INFO : diff #set()\n",
      "2020-12-23 02:47:37,670 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:37,802 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.2240382642462866, 0.4496325517757646], [0.9306376203894615, 0.06936238], [nan, nan], [3.7516291673878226, 5.20665021947654, 5.5563632234383995, 3.4019161634259625, 1.804734056050577, 0.34971300396185967]]\n",
      "2020-12-23 02:47:37,805 : INFO : Removed 0 and 89 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:37,805 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:37,807 : INFO : built Dictionary(128 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 528 corpus positions)\n",
      "2020-12-23 02:47:37,849 : INFO : token count processed\n",
      "2020-12-23 02:47:37,852 : INFO : frequencies processed\n",
      "2020-12-23 02:47:37,980 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:37,981 : INFO : entropies processed\n",
      "2020-12-23 02:47:37,982 : INFO : extropies processed\n",
      "2020-12-23 02:47:37,983 : INFO : token count processed\n",
      "2020-12-23 02:47:37,984 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:37,986 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:37,987 : INFO : vocab #2480\n",
      "2020-12-23 02:47:37,988 : INFO : diff #set()\n",
      "2020-12-23 02:47:38,246 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:38,374 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.1042663791565137, 0.4752250047357815], [0.788184642791748, 0.21181536], [2.8962915290459277, 1.3150505832663126], [3.7516291673878226, 6.524718477352, 6.5888929182214815, 3.6874547265183404, 2.837263750833659, 0.06417444086948176]]\n",
      "2020-12-23 02:47:38,376 : INFO : Removed 0 and 8 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:38,377 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:38,378 : INFO : built Dictionary(74 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 235 corpus positions)\n",
      "2020-12-23 02:47:38,398 : INFO : token count processed\n",
      "2020-12-23 02:47:38,400 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:47:38,401 : INFO : frequencies processed\n",
      "2020-12-23 02:47:38,402 : INFO : token count processed\n",
      "2020-12-23 02:47:38,403 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:38,404 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:38,405 : INFO : vocab #2480\n",
      "2020-12-23 02:47:38,406 : INFO : diff #set()\n",
      "2020-12-23 02:47:38,664 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:38,792 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.2342689840124146, 0.447573683900919], [0.9319257140159607, 0.068074286], [nan, nan], [3.7516291673878226, 5.321859380715434, 5.631844760083242, 3.4416437880200146, 1.8802155926954192, 0.30998537936780757]]\n",
      "2020-12-23 02:47:38,795 : INFO : Removed 0 and 119 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:38,796 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:38,797 : INFO : built Dictionary(145 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 853 corpus positions)\n",
      "2020-12-23 02:47:38,851 : INFO : token count processed\n",
      "2020-12-23 02:47:38,853 : INFO : frequencies processed\n",
      "2020-12-23 02:47:38,990 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:38,991 : INFO : entropies processed\n",
      "2020-12-23 02:47:38,992 : INFO : extropies processed\n",
      "2020-12-23 02:47:38,993 : INFO : token count processed\n",
      "2020-12-23 02:47:38,995 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:38,995 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:38,996 : INFO : vocab #2480\n",
      "2020-12-23 02:47:38,997 : INFO : diff #set()\n",
      "2020-12-23 02:47:39,269 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:39,397 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.1384134084169022, 0.46763642430595975], [0.8238174021244049, 0.1761826], [2.751629167387823, 1.2995901901368234], [3.7516291673878226, 6.500767808767801, 6.5575866227191995, 3.6948103534364236, 2.805957455331377, 0.05681881395139854]]\n",
      "2020-12-23 02:47:39,399 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:39,400 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:39,401 : INFO : built Dictionary(45 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 75 corpus positions)\n",
      "2020-12-23 02:47:39,420 : INFO : token count processed\n",
      "2020-12-23 02:47:39,422 : INFO : frequencies processed\n",
      "2020-12-23 02:47:39,550 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:39,550 : INFO : entropies processed\n",
      "2020-12-23 02:47:39,551 : INFO : extropies processed\n",
      "2020-12-23 02:47:39,552 : INFO : token count processed\n",
      "2020-12-23 02:47:39,553 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:39,554 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:39,555 : INFO : vocab #2480\n",
      "2020-12-23 02:47:39,556 : INFO : diff #set()\n",
      "2020-12-23 02:47:39,815 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:39,943 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.1919625140318715, 0.4562121813664647], [0.8546817451715469, 0.14531825], [0.0, 0.0], [3.7516291673878226, 4.736228843383063, 5.277551119208659, 3.210306891562227, 1.5259219518208362, 0.5413222758255962]]\n",
      "2020-12-23 02:47:39,946 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:39,946 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:39,947 : INFO : built Dictionary(103 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 416 corpus positions)\n",
      "2020-12-23 02:47:39,981 : INFO : token count processed\n",
      "2020-12-23 02:47:39,991 : INFO : frequencies processed\n",
      "2020-12-23 02:47:40,123 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:40,123 : INFO : entropies processed\n",
      "2020-12-23 02:47:40,124 : INFO : extropies processed\n",
      "2020-12-23 02:47:40,125 : INFO : token count processed\n",
      "2020-12-23 02:47:40,126 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:40,127 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:40,128 : INFO : vocab #2480\n",
      "2020-12-23 02:47:40,129 : INFO : diff #set()\n",
      "2020-12-23 02:47:40,393 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:40,521 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.1369036244624813, 0.4679668229078609], [0.8287695795297623, 0.17123042], [2.321928094887362, 1.2877123795494492], [3.7516291673878226, 5.788442787590127, 5.947060316351006, 3.593011638626944, 2.1954311489631837, 0.1586175287608791]]\n",
      "2020-12-23 02:47:40,524 : INFO : Removed 0 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:40,525 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:40,526 : INFO : built Dictionary(62 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 102 corpus positions)\n",
      "2020-12-23 02:47:40,548 : INFO : token count processed\n",
      "2020-12-23 02:47:40,550 : INFO : frequencies processed\n",
      "2020-12-23 02:47:40,682 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:40,683 : INFO : entropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:47:40,684 : INFO : extropies processed\n",
      "2020-12-23 02:47:40,686 : INFO : token count processed\n",
      "2020-12-23 02:47:40,687 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:40,689 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:40,690 : INFO : vocab #2480\n",
      "2020-12-23 02:47:40,692 : INFO : diff #set()\n",
      "2020-12-23 02:47:40,963 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:41,095 : INFO : Computed distances or similarities ('273', 'sacp-python-common/setup.py')[[1.1842275192009932, 0.4578277634583633], [0.8848079219460487, 0.11519208], [1.584962500721156, 1.1699250014423124], [3.7516291673878226, 5.370004292053436, 5.704110247947135, 3.4175232114941245, 1.9524810805593122, 0.3341059558936985]]\n",
      "2020-12-23 02:47:41,097 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:41,098 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:41,099 : INFO : built Dictionary(86 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 347 corpus positions)\n",
      "2020-12-23 02:47:41,127 : INFO : token count processed\n",
      "2020-12-23 02:47:41,132 : INFO : frequencies processed\n",
      "2020-12-23 02:47:41,260 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:41,261 : INFO : entropies processed\n",
      "2020-12-23 02:47:41,261 : INFO : extropies processed\n",
      "2020-12-23 02:47:41,262 : INFO : token count processed\n",
      "2020-12-23 02:47:41,263 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:41,264 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:41,264 : INFO : vocab #2480\n",
      "2020-12-23 02:47:41,265 : INFO : diff #set()\n",
      "2020-12-23 02:47:41,528 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:41,656 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.1765817016002633, 0.4594360042927777], [0.8790540471673012, 0.12094595], [2.197159723424149, 1.2560617931988722], [3.7516291673878226, 5.695663584743922, 5.850321992977297, 3.5969707591544475, 2.0986928255894743, 0.15465840823337462]]\n",
      "2020-12-23 02:47:41,658 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:41,659 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:41,660 : INFO : built Dictionary(49 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 122 corpus positions)\n",
      "2020-12-23 02:47:41,672 : INFO : token count processed\n",
      "2020-12-23 02:47:41,675 : INFO : frequencies processed\n",
      "2020-12-23 02:47:41,802 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:41,803 : INFO : entropies processed\n",
      "2020-12-23 02:47:41,804 : INFO : extropies processed\n",
      "2020-12-23 02:47:41,806 : INFO : token count processed\n",
      "2020-12-23 02:47:41,807 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:41,809 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:41,809 : INFO : vocab #2480\n",
      "2020-12-23 02:47:41,810 : INFO : diff #set()\n",
      "2020-12-23 02:47:42,066 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:42,194 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.128388567372753, 0.46983902062318594], [0.8045300394296646, 0.19546996], [1.3787834934861753, 1.0612379896970343], [3.7516291673878226, 4.9004417692112465, 5.27297576142416, 3.3790951751749088, 1.5213465940363373, 0.3725339922129134]]\n",
      "2020-12-23 02:47:42,196 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:42,197 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:42,199 : INFO : built Dictionary(47 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 129 corpus positions)\n",
      "2020-12-23 02:47:42,217 : INFO : token count processed\n",
      "2020-12-23 02:47:42,219 : INFO : frequencies processed\n",
      "2020-12-23 02:47:42,352 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:42,353 : INFO : entropies processed\n",
      "2020-12-23 02:47:42,353 : INFO : extropies processed\n",
      "2020-12-23 02:47:42,355 : INFO : token count processed\n",
      "2020-12-23 02:47:42,357 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:42,358 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:42,359 : INFO : vocab #2480\n",
      "2020-12-23 02:47:42,360 : INFO : diff #set()\n",
      "2020-12-23 02:47:42,613 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:42,740 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.1491216512738025, 0.4653063726789461], [0.859707310795784, 0.14029269], [1.5219280948873621, 1.1419011889093373], [3.7516291673878226, 4.778624108914332, 5.1765466803457985, 3.353706595956356, 1.4249175129579759, 0.39792257143146603]]\n",
      "2020-12-23 02:47:42,742 : INFO : Removed 0 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:42,743 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:42,744 : INFO : built Dictionary(49 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 160 corpus positions)\n",
      "2020-12-23 02:47:42,756 : INFO : token count processed\n",
      "2020-12-23 02:47:42,758 : INFO : frequencies processed\n",
      "2020-12-23 02:47:42,895 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:42,896 : INFO : entropies processed\n",
      "2020-12-23 02:47:42,897 : INFO : extropies processed\n",
      "2020-12-23 02:47:42,899 : INFO : token count processed\n",
      "2020-12-23 02:47:42,900 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:42,901 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:42,902 : INFO : vocab #2480\n",
      "2020-12-23 02:47:42,903 : INFO : diff #set()\n",
      "2020-12-23 02:47:43,169 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:43,296 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.1891055039099931, 0.4568075856617625], [0.9101897180080414, 0.08981028], [1.0, 1.0], [3.7516291673878226, 4.773880192225086, 5.163904839327647, 3.361604520285261, 1.4122756719398244, 0.39002464710256124]]\n",
      "2020-12-23 02:47:43,299 : INFO : Removed 0 and 346 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:43,300 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:43,303 : INFO : built Dictionary(157 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 1985 corpus positions)\n",
      "2020-12-23 02:47:43,361 : INFO : token count processed\n",
      "2020-12-23 02:47:43,363 : INFO : frequencies processed\n",
      "2020-12-23 02:47:43,490 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:43,490 : INFO : entropies processed\n",
      "2020-12-23 02:47:43,491 : INFO : extropies processed\n",
      "2020-12-23 02:47:43,492 : INFO : token count processed\n",
      "2020-12-23 02:47:43,493 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:43,494 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:43,494 : INFO : vocab #2480\n",
      "2020-12-23 02:47:43,495 : INFO : diff #set()\n",
      "2020-12-23 02:47:43,763 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:43,892 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.171402335399899, 0.4605318801113999], [0.8122071474790573, 0.18779285], [2.084962500721156, 1.2292852896434443], [3.7516291673878226, 6.620773041953877, 6.650344728635195, 3.7220574807065034, 2.8987155612473727, 0.02957168668131871]]\n",
      "2020-12-23 02:47:43,895 : INFO : Removed 0 and 132 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:43,896 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:43,898 : INFO : built Dictionary(90 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 548 corpus positions)\n",
      "2020-12-23 02:47:43,936 : INFO : token count processed\n",
      "2020-12-23 02:47:43,938 : INFO : frequencies processed\n",
      "2020-12-23 02:47:44,069 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:44,070 : INFO : entropies processed\n",
      "2020-12-23 02:47:44,073 : INFO : extropies processed\n",
      "2020-12-23 02:47:44,074 : INFO : token count processed\n",
      "2020-12-23 02:47:44,075 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:44,076 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:44,076 : INFO : vocab #2480\n",
      "2020-12-23 02:47:44,077 : INFO : diff #set()\n",
      "2020-12-23 02:47:44,336 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:44,464 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.1931681170669528, 0.4559613976776921], [0.9005569815635681, 0.09944302], [1.0, 1.0], [3.7516291673878226, 5.828370634755606, 5.965812695512029, 3.614187106631399, 2.2141835281242064, 0.13744206075642307]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:47:44,467 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:44,468 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:44,469 : INFO : built Dictionary(89 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 311 corpus positions)\n",
      "2020-12-23 02:47:44,507 : INFO : token count processed\n",
      "2020-12-23 02:47:44,512 : INFO : frequencies processed\n",
      "2020-12-23 02:47:44,640 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:44,640 : INFO : entropies processed\n",
      "2020-12-23 02:47:44,641 : INFO : extropies processed\n",
      "2020-12-23 02:47:44,642 : INFO : token count processed\n",
      "2020-12-23 02:47:44,643 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:44,644 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:44,644 : INFO : vocab #2480\n",
      "2020-12-23 02:47:44,645 : INFO : diff #set()\n",
      "2020-12-23 02:47:44,911 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:45,040 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.1536162496383295, 0.46433527800876145], [0.8064852207899094, 0.19351478], [1.75, 1.1484070108583257], [3.7516291673878226, 5.774409284925443, 5.934009368920207, 3.592029083393059, 2.1823802015323843, 0.15960008399476422]]\n",
      "2020-12-23 02:47:45,043 : INFO : Removed 0 and 51 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:45,043 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:45,044 : INFO : built Dictionary(98 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 360 corpus positions)\n",
      "2020-12-23 02:47:45,076 : INFO : token count processed\n",
      "2020-12-23 02:47:45,078 : INFO : frequencies processed\n",
      "2020-12-23 02:47:45,204 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:45,204 : INFO : entropies processed\n",
      "2020-12-23 02:47:45,205 : INFO : extropies processed\n",
      "2020-12-23 02:47:45,207 : INFO : token count processed\n",
      "2020-12-23 02:47:45,208 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:45,209 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:45,210 : INFO : vocab #2480\n",
      "2020-12-23 02:47:45,211 : INFO : diff #set()\n",
      "2020-12-23 02:47:45,474 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:45,601 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.1480336553845618, 0.46554205400518756], [0.8524626791477203, 0.14753732], [2.5, 1.2968140217166515], [3.7516291673878226, 5.977819040873918, 6.113494938154082, 3.615953270107658, 2.361865770766259, 0.13567589728016394]]\n",
      "2020-12-23 02:47:45,603 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:45,604 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:45,606 : INFO : built Dictionary(83 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 307 corpus positions)\n",
      "2020-12-23 02:47:45,641 : INFO : token count processed\n",
      "2020-12-23 02:47:45,644 : INFO : frequencies processed\n",
      "2020-12-23 02:47:45,771 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:45,771 : INFO : entropies processed\n",
      "2020-12-23 02:47:45,772 : INFO : extropies processed\n",
      "2020-12-23 02:47:45,773 : INFO : token count processed\n",
      "2020-12-23 02:47:45,773 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:45,774 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:45,775 : INFO : vocab #2480\n",
      "2020-12-23 02:47:45,776 : INFO : diff #set()\n",
      "2020-12-23 02:47:46,029 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:46,157 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.2134157274928625, 0.4517904104407447], [0.8907354027032852, 0.1092646], [1.0, 1.0], [3.7516291673878226, 5.901812829596593, 6.098913865245818, 3.554528131738598, 2.347284697857995, 0.19710103564922488]]\n",
      "2020-12-23 02:47:46,159 : INFO : Removed 0 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:46,160 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:46,162 : INFO : built Dictionary(83 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 307 corpus positions)\n",
      "2020-12-23 02:47:46,191 : INFO : token count processed\n",
      "2020-12-23 02:47:46,194 : INFO : frequencies processed\n",
      "2020-12-23 02:47:46,330 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:46,331 : INFO : entropies processed\n",
      "2020-12-23 02:47:46,332 : INFO : extropies processed\n",
      "2020-12-23 02:47:46,333 : INFO : token count processed\n",
      "2020-12-23 02:47:46,334 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:46,335 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:46,339 : INFO : vocab #2480\n",
      "2020-12-23 02:47:46,340 : INFO : diff #set()\n",
      "2020-12-23 02:47:46,600 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:46,728 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.1203929125003644, 0.471610706725482], [0.7959165126085281, 0.20408349], [1.75, 1.1484070108583257], [3.7516291673878226, 5.643202320803383, 5.808652265749996, 3.586179222441209, 2.057023098362173, 0.16544994494661314]]\n",
      "2020-12-23 02:47:46,730 : INFO : Removed 0 and 30 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:46,731 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:46,732 : INFO : built Dictionary(97 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 341 corpus positions)\n",
      "2020-12-23 02:47:46,767 : INFO : token count processed\n",
      "2020-12-23 02:47:46,770 : INFO : frequencies processed\n",
      "2020-12-23 02:47:46,897 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:46,898 : INFO : entropies processed\n",
      "2020-12-23 02:47:46,898 : INFO : extropies processed\n",
      "2020-12-23 02:47:46,899 : INFO : token count processed\n",
      "2020-12-23 02:47:46,900 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:46,900 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:46,901 : INFO : vocab #2480\n",
      "2020-12-23 02:47:46,902 : INFO : diff #set()\n",
      "2020-12-23 02:47:47,158 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:47,286 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.1799697810622944, 0.4587219550872408], [0.8892382830381393, 0.11076172], [1.584962500721156, 1.1699250014423124], [3.7516291673878226, 5.925214310725336, 6.101291175798445, 3.575552302314713, 2.3496620084106223, 0.17607686507310927]]\n",
      "2020-12-23 02:47:47,289 : INFO : Removed 0 and 293 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:47,290 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:47,291 : INFO : built Dictionary(167 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 1736 corpus positions)\n",
      "2020-12-23 02:47:47,350 : INFO : token count processed\n",
      "2020-12-23 02:47:47,352 : INFO : frequencies processed\n",
      "2020-12-23 02:47:47,481 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:47,482 : INFO : entropies processed\n",
      "2020-12-23 02:47:47,483 : INFO : extropies processed\n",
      "2020-12-23 02:47:47,485 : INFO : token count processed\n",
      "2020-12-23 02:47:47,486 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:47,488 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:47,489 : INFO : vocab #2480\n",
      "2020-12-23 02:47:47,490 : INFO : diff #set()\n",
      "2020-12-23 02:47:47,751 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:47,879 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.1567181083875155, 0.46366745663746317], [0.8036383986473083, 0.1963616], [2.3158243335257076, 1.2582718324798057], [3.7516291673878226, 6.551685682764175, 6.58329084530636, 3.7200240048456372, 2.8316616779185373, 0.03160516254218493]]\n",
      "2020-12-23 02:47:47,881 : INFO : Removed 0 and 88 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:47,882 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:47,883 : INFO : built Dictionary(144 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 577 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:47:47,932 : INFO : token count processed\n",
      "2020-12-23 02:47:47,934 : INFO : frequencies processed\n",
      "2020-12-23 02:47:48,061 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:48,062 : INFO : entropies processed\n",
      "2020-12-23 02:47:48,063 : INFO : extropies processed\n",
      "2020-12-23 02:47:48,065 : INFO : token count processed\n",
      "2020-12-23 02:47:48,066 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:48,067 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:48,068 : INFO : vocab #2480\n",
      "2020-12-23 02:47:48,070 : INFO : diff #set()\n",
      "2020-12-23 02:47:48,331 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:48,459 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.182191575846325, 0.4582549080788966], [0.8355524986982346, 0.1644475], [2.0588138903312014, 1.2062416803425784], [3.7516291673878226, 6.642985062562557, 6.722382592169572, 3.6722316377808077, 2.970753424781749, 0.0793975296070144]]\n",
      "2020-12-23 02:47:48,462 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:48,462 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:48,463 : INFO : built Dictionary(62 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 172 corpus positions)\n",
      "2020-12-23 02:47:48,479 : INFO : token count processed\n",
      "2020-12-23 02:47:48,481 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:47:48,482 : INFO : frequencies processed\n",
      "2020-12-23 02:47:48,483 : INFO : token count processed\n",
      "2020-12-23 02:47:48,484 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:48,485 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:48,486 : INFO : vocab #2480\n",
      "2020-12-23 02:47:48,487 : INFO : diff #set()\n",
      "2020-12-23 02:47:48,746 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:48,874 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.2214546372047348, 0.45015548967423613], [0.9158823862671852, 0.084117614], [nan, nan], [3.7516291673878226, 5.2461980344571995, 5.608128602047587, 3.389698599797435, 1.856499434659764, 0.3619305675903872]]\n",
      "2020-12-23 02:47:48,877 : INFO : Removed 0 and 23 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:48,878 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:48,879 : INFO : built Dictionary(81 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 232 corpus positions)\n",
      "2020-12-23 02:47:48,901 : INFO : token count processed\n",
      "2020-12-23 02:47:48,903 : INFO : frequencies processed\n",
      "2020-12-23 02:47:49,032 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:49,032 : INFO : entropies processed\n",
      "2020-12-23 02:47:49,033 : INFO : extropies processed\n",
      "2020-12-23 02:47:49,034 : INFO : token count processed\n",
      "2020-12-23 02:47:49,035 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:49,036 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:49,037 : INFO : vocab #2480\n",
      "2020-12-23 02:47:49,038 : INFO : diff #set()\n",
      "2020-12-23 02:47:49,308 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:49,437 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/test_auth_utility.py')[[1.1633061333967067, 0.4622554268035352], [0.8026362806558609, 0.19736372], [1.2516291673878228, 0.9667115099633751], [3.7516291673878226, 5.903090303960449, 6.100001072581254, 3.554718398767019, 2.348371905193431, 0.19691076862080426]]\n",
      "2020-12-23 02:47:49,440 : INFO : Removed 0 and 442 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:49,440 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:49,442 : INFO : built Dictionary(112 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 1231 corpus positions)\n",
      "2020-12-23 02:47:49,478 : INFO : token count processed\n",
      "2020-12-23 02:47:49,480 : INFO : frequencies processed\n",
      "2020-12-23 02:47:49,608 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:49,609 : INFO : entropies processed\n",
      "2020-12-23 02:47:49,609 : INFO : extropies processed\n",
      "2020-12-23 02:47:49,612 : INFO : token count processed\n",
      "2020-12-23 02:47:49,613 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:49,614 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:49,615 : INFO : vocab #2480\n",
      "2020-12-23 02:47:49,616 : INFO : diff #set()\n",
      "2020-12-23 02:47:49,887 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:50,019 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.1732381597138903, 0.46014284975175074], [0.8812263831496239, 0.11877362], [2.3219280948873626, 1.2469329688117936], [3.7516291673878226, 6.16659449033757, 6.219194433078049, 3.699029224647343, 2.4675652656902263, 0.0525999427404793]]\n",
      "2020-12-23 02:47:50,022 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:50,022 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:50,023 : INFO : built Dictionary(77 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 266 corpus positions)\n",
      "2020-12-23 02:47:50,049 : INFO : token count processed\n",
      "2020-12-23 02:47:50,054 : INFO : frequencies processed\n",
      "2020-12-23 02:47:50,184 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:50,185 : INFO : entropies processed\n",
      "2020-12-23 02:47:50,188 : INFO : extropies processed\n",
      "2020-12-23 02:47:50,189 : INFO : token count processed\n",
      "2020-12-23 02:47:50,189 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:50,190 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:50,190 : INFO : vocab #2480\n",
      "2020-12-23 02:47:50,191 : INFO : diff #set()\n",
      "2020-12-23 02:47:50,448 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:50,577 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.1621427312492922, 0.462504156431059], [0.789117693901062, 0.2108823], [0.0, 0.0], [3.7516291673878226, 5.906856253399655, 6.091312084497973, 3.567173336289505, 2.3396829171101508, 0.18445583109831798]]\n",
      "2020-12-23 02:47:50,580 : INFO : Removed 0 and 68 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:50,581 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:50,582 : INFO : built Dictionary(91 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 265 corpus positions)\n",
      "2020-12-23 02:47:50,618 : INFO : token count processed\n",
      "2020-12-23 02:47:50,622 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:47:50,623 : INFO : frequencies processed\n",
      "2020-12-23 02:47:50,625 : INFO : token count processed\n",
      "2020-12-23 02:47:50,626 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:50,628 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:50,629 : INFO : vocab #2480\n",
      "2020-12-23 02:47:50,630 : INFO : diff #set()\n",
      "2020-12-23 02:47:50,880 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:51,007 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.196681643149457, 0.4552321011643116], [0.902470625936985, 0.097529374], [nan, nan], [3.7516291673878226, 5.965115449163356, 6.179193977188455, 3.537550639362723, 2.4275648098006326, 0.21407852802509897]]\n",
      "2020-12-23 02:47:51,010 : INFO : Removed 0 and 13 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:51,010 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:51,012 : INFO : built Dictionary(97 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 379 corpus positions)\n",
      "2020-12-23 02:47:51,041 : INFO : token count processed\n",
      "2020-12-23 02:47:51,043 : INFO : frequencies processed\n",
      "2020-12-23 02:47:51,169 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:51,169 : INFO : entropies processed\n",
      "2020-12-23 02:47:51,170 : INFO : extropies processed\n",
      "2020-12-23 02:47:51,171 : INFO : token count processed\n",
      "2020-12-23 02:47:51,171 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:51,172 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:51,172 : INFO : vocab #2480\n",
      "2020-12-23 02:47:51,173 : INFO : diff #set()\n",
      "2020-12-23 02:47:51,431 : INFO : alphabet #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:47:51,559 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.2194014172627756, 0.45057193900205594], [0.9213783368468285, 0.07862166], [0.0, 0.0], [3.7516291673878226, 5.791362404253194, 5.991739633487737, 3.551251938153279, 2.2401104660999143, 0.20037722923454293]]\n",
      "2020-12-23 02:47:51,561 : INFO : Removed 0 and 41 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:51,562 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:51,563 : INFO : built Dictionary(86 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 391 corpus positions)\n",
      "2020-12-23 02:47:51,590 : INFO : token count processed\n",
      "2020-12-23 02:47:51,595 : INFO : frequencies processed\n",
      "2020-12-23 02:47:51,725 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:51,725 : INFO : entropies processed\n",
      "2020-12-23 02:47:51,726 : INFO : extropies processed\n",
      "2020-12-23 02:47:51,727 : INFO : token count processed\n",
      "2020-12-23 02:47:51,727 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:51,728 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:51,729 : INFO : vocab #2480\n",
      "2020-12-23 02:47:51,730 : INFO : diff #set()\n",
      "2020-12-23 02:47:51,992 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:52,118 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.2028349579818987, 0.4539604732422343], [0.9097758084535599, 0.09022419], [0.8112781244591328, 0.8112781244591328], [3.7516291673878226, 5.651670454631116, 5.8333146352901375, 3.5699849867288007, 2.081685467902315, 0.18164418065902144]]\n",
      "2020-12-23 02:47:52,120 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:52,121 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:52,123 : INFO : built Dictionary(50 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 101 corpus positions)\n",
      "2020-12-23 02:47:52,143 : INFO : token count processed\n",
      "2020-12-23 02:47:52,146 : INFO : frequencies processed\n",
      "2020-12-23 02:47:52,280 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:52,280 : INFO : entropies processed\n",
      "2020-12-23 02:47:52,281 : INFO : extropies processed\n",
      "2020-12-23 02:47:52,283 : INFO : token count processed\n",
      "2020-12-23 02:47:52,285 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:52,286 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:52,287 : INFO : vocab #2480\n",
      "2020-12-23 02:47:52,289 : INFO : diff #set()\n",
      "2020-12-23 02:47:52,548 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:52,677 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.2088270976401019, 0.45272896238387983], [0.9033917635679245, 0.09660824], [0.0, 0.0], [3.7516291673878226, 4.8226207261920235, 5.322360666454687, 3.25188922712516, 1.5707314990668642, 0.4997399402626632]]\n",
      "2020-12-23 02:47:52,679 : INFO : Removed 0 and 50 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:52,680 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:52,681 : INFO : built Dictionary(91 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 271 corpus positions)\n",
      "2020-12-23 02:47:52,710 : INFO : token count processed\n",
      "2020-12-23 02:47:52,712 : INFO : frequencies processed\n",
      "2020-12-23 02:47:52,843 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:52,844 : INFO : entropies processed\n",
      "2020-12-23 02:47:52,845 : INFO : extropies processed\n",
      "2020-12-23 02:47:52,846 : INFO : token count processed\n",
      "2020-12-23 02:47:52,847 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:52,849 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:52,849 : INFO : vocab #2480\n",
      "2020-12-23 02:47:52,850 : INFO : diff #set()\n",
      "2020-12-23 02:47:53,120 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:53,247 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.1747534001955324, 0.45982224923068965], [0.8367027193307877, 0.16329728], [2.0, 1.1742577727983858], [3.7516291673878226, 6.24862851613934, 6.384311885692771, 3.6159457978343923, 2.6326827183049484, 0.13568336955343074]]\n",
      "2020-12-23 02:47:53,250 : INFO : Removed 0 and 32 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:53,251 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:53,252 : INFO : built Dictionary(96 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 387 corpus positions)\n",
      "2020-12-23 02:47:53,283 : INFO : token count processed\n",
      "2020-12-23 02:47:53,293 : INFO : frequencies processed\n",
      "2020-12-23 02:47:53,425 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:53,426 : INFO : entropies processed\n",
      "2020-12-23 02:47:53,427 : INFO : extropies processed\n",
      "2020-12-23 02:47:53,428 : INFO : token count processed\n",
      "2020-12-23 02:47:53,429 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:53,430 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:53,431 : INFO : vocab #2480\n",
      "2020-12-23 02:47:53,432 : INFO : diff #set()\n",
      "2020-12-23 02:47:53,690 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:53,818 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.215898553741437, 0.45128419724429564], [0.9320578128099442, 0.06794219], [0.0, 0.0], [3.7516291673878226, 5.850156917433494, 6.03992839521153, 3.561857689609787, 2.2882992278237073, 0.18977147777803616]]\n",
      "2020-12-23 02:47:53,820 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:53,821 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:53,822 : INFO : built Dictionary(90 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 385 corpus positions)\n",
      "2020-12-23 02:47:53,857 : INFO : token count processed\n",
      "2020-12-23 02:47:53,860 : INFO : frequencies processed\n",
      "2020-12-23 02:47:53,989 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:53,990 : INFO : entropies processed\n",
      "2020-12-23 02:47:53,991 : INFO : extropies processed\n",
      "2020-12-23 02:47:53,992 : INFO : token count processed\n",
      "2020-12-23 02:47:53,993 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:53,994 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:53,995 : INFO : vocab #2480\n",
      "2020-12-23 02:47:53,996 : INFO : diff #set()\n",
      "2020-12-23 02:47:54,255 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:54,382 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.201153929577063, 0.4543071643300036], [0.9166697114706039, 0.08333029], [0.9182958340544896, 0.9182958340544896], [3.7516291673878226, 5.6831976040360095, 5.867237384747716, 3.567589386676117, 2.115608217359893, 0.18403978071170624]]\n",
      "2020-12-23 02:47:54,385 : INFO : Removed 0 and 52 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:54,386 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:54,387 : INFO : built Dictionary(76 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 363 corpus positions)\n",
      "2020-12-23 02:47:54,415 : INFO : token count processed\n",
      "2020-12-23 02:47:54,417 : INFO : frequencies processed\n",
      "2020-12-23 02:47:54,549 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:54,550 : INFO : entropies processed\n",
      "2020-12-23 02:47:54,551 : INFO : extropies processed\n",
      "2020-12-23 02:47:54,552 : INFO : token count processed\n",
      "2020-12-23 02:47:54,553 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:54,554 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:54,555 : INFO : vocab #2480\n",
      "2020-12-23 02:47:54,556 : INFO : diff #set()\n",
      "2020-12-23 02:47:54,824 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:54,954 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.1040327617328938, 0.4752777704736851], [0.699360579252243, 0.30063942], [1.2516291673878228, 0.9667115099633751], [3.7516291673878226, 5.749308601266266, 5.8999052485824945, 3.601032520071594, 2.148276081194672, 0.15059664731622835]]\n",
      "2020-12-23 02:47:54,956 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:47:54,957 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:54,959 : INFO : built Dictionary(71 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 244 corpus positions)\n",
      "2020-12-23 02:47:54,983 : INFO : token count processed\n",
      "2020-12-23 02:47:54,986 : INFO : frequencies processed\n",
      "2020-12-23 02:47:55,117 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:55,118 : INFO : entropies processed\n",
      "2020-12-23 02:47:55,119 : INFO : extropies processed\n",
      "2020-12-23 02:47:55,120 : INFO : token count processed\n",
      "2020-12-23 02:47:55,121 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:55,122 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:55,122 : INFO : vocab #2480\n",
      "2020-12-23 02:47:55,124 : INFO : diff #set()\n",
      "2020-12-23 02:47:55,385 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:55,513 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.210028086670941, 0.45248293722200716], [0.9293887466192245, 0.07061125], [0.0, 0.0], [3.7516291673878226, 5.015422548793484, 5.342870715148244, 3.4241810010330624, 1.5912415477604216, 0.3274481663547606]]\n",
      "2020-12-23 02:47:55,516 : INFO : Removed 0 and 81 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:55,517 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:55,518 : INFO : built Dictionary(99 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 370 corpus positions)\n",
      "2020-12-23 02:47:55,553 : INFO : token count processed\n",
      "2020-12-23 02:47:55,558 : INFO : frequencies processed\n",
      "2020-12-23 02:47:55,688 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:55,689 : INFO : entropies processed\n",
      "2020-12-23 02:47:55,690 : INFO : extropies processed\n",
      "2020-12-23 02:47:55,691 : INFO : token count processed\n",
      "2020-12-23 02:47:55,692 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:55,694 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:55,694 : INFO : vocab #2480\n",
      "2020-12-23 02:47:55,696 : INFO : diff #set()\n",
      "2020-12-23 02:47:55,963 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:56,091 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.1810376649513763, 0.4584973547544369], [0.858987346291542, 0.14101265], [0.7219280948873623, 0.7219280948873623], [3.7516291673878226, 6.030001281822029, 6.179484735511006, 3.602145713698845, 2.4278555681231837, 0.14948345368897709]]\n",
      "2020-12-23 02:47:56,093 : INFO : Removed 0 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:56,094 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:56,096 : INFO : built Dictionary(85 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 333 corpus positions)\n",
      "2020-12-23 02:47:56,126 : INFO : token count processed\n",
      "2020-12-23 02:47:56,129 : INFO : frequencies processed\n",
      "2020-12-23 02:47:56,256 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:56,257 : INFO : entropies processed\n",
      "2020-12-23 02:47:56,258 : INFO : extropies processed\n",
      "2020-12-23 02:47:56,259 : INFO : token count processed\n",
      "2020-12-23 02:47:56,260 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:56,261 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:56,261 : INFO : vocab #2480\n",
      "2020-12-23 02:47:56,262 : INFO : diff #set()\n",
      "2020-12-23 02:47:56,521 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:56,649 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.172963973387618, 0.460200910943321], [0.8544162958860397, 0.1455837], [1.664497779200461, 1.0957486925807722], [3.7516291673878226, 5.9537092545441395, 6.111423605129992, 3.593914816801971, 2.359794437742169, 0.15771435058585226]]\n",
      "2020-12-23 02:47:56,652 : INFO : Removed 0 and 90 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:56,652 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:56,653 : INFO : built Dictionary(96 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 334 corpus positions)\n",
      "2020-12-23 02:47:56,684 : INFO : token count processed\n",
      "2020-12-23 02:47:56,689 : INFO : frequencies processed\n",
      "2020-12-23 02:47:56,816 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:56,817 : INFO : entropies processed\n",
      "2020-12-23 02:47:56,818 : INFO : extropies processed\n",
      "2020-12-23 02:47:56,819 : INFO : token count processed\n",
      "2020-12-23 02:47:56,819 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:56,820 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:56,821 : INFO : vocab #2480\n",
      "2020-12-23 02:47:56,822 : INFO : diff #set()\n",
      "2020-12-23 02:47:57,080 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:57,207 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.1616081254781017, 0.46261854228495797], [0.865822359919548, 0.13417764], [2.0, 1.2451124978365313], [3.7516291673878226, 6.184756445474906, 6.3327788690683455, 3.6036067437943835, 2.581149701680523, 0.14802242359343953]]\n",
      "2020-12-23 02:47:57,210 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:57,211 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:57,213 : INFO : built Dictionary(108 unique tokens: ['add', 'alway', 'argument', 'check', 'default']...) from 2 documents (total 447 corpus positions)\n",
      "2020-12-23 02:47:57,257 : INFO : token count processed\n",
      "2020-12-23 02:47:57,259 : INFO : frequencies processed\n",
      "2020-12-23 02:47:57,393 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:57,393 : INFO : entropies processed\n",
      "2020-12-23 02:47:57,394 : INFO : extropies processed\n",
      "2020-12-23 02:47:57,396 : INFO : token count processed\n",
      "2020-12-23 02:47:57,397 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:57,398 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:57,398 : INFO : vocab #2480\n",
      "2020-12-23 02:47:57,399 : INFO : diff #set()\n",
      "2020-12-23 02:47:57,663 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:57,792 : INFO : Computed distances or similarities ('273', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.231430142510031, 0.44814309036587047], [0.9317850470542908, 0.06821495], [0.0, 0.0], [3.7516291673878226, 6.212221456585881, 6.365944979656109, 3.5979056443175947, 2.6143158122682864, 0.1537235230702283]]\n",
      "2020-12-23 02:47:57,795 : INFO : Removed 3 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:57,796 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:57,797 : INFO : built Dictionary(126 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 423 corpus positions)\n",
      "2020-12-23 02:47:57,834 : INFO : token count processed\n",
      "2020-12-23 02:47:57,836 : INFO : frequencies processed\n",
      "2020-12-23 02:47:57,963 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:57,964 : INFO : entropies processed\n",
      "2020-12-23 02:47:57,965 : INFO : extropies processed\n",
      "2020-12-23 02:47:57,966 : INFO : token count processed\n",
      "2020-12-23 02:47:57,967 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:57,968 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:57,969 : INFO : vocab #2480\n",
      "2020-12-23 02:47:57,971 : INFO : diff #set()\n",
      "2020-12-23 02:47:58,226 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:58,354 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.1981146218779746, 0.45493532959880095], [0.9040923938155174, 0.095907606], [1.0, 1.0], [3.997670276487613, 6.301552355933639, 6.439165575066401, 3.860057057354851, 2.4414952985787877, 0.1376132191327617]]\n",
      "2020-12-23 02:47:58,356 : INFO : Removed 3 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:58,357 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:58,359 : INFO : built Dictionary(164 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 659 corpus positions)\n",
      "2020-12-23 02:47:58,414 : INFO : token count processed\n",
      "2020-12-23 02:47:58,416 : INFO : frequencies processed\n",
      "2020-12-23 02:47:58,543 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:58,544 : INFO : entropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:47:58,544 : INFO : extropies processed\n",
      "2020-12-23 02:47:58,546 : INFO : token count processed\n",
      "2020-12-23 02:47:58,547 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:58,548 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:58,549 : INFO : vocab #2480\n",
      "2020-12-23 02:47:58,550 : INFO : diff #set()\n",
      "2020-12-23 02:47:58,810 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:58,937 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.2274368134234253, 0.44894651734837093], [0.9035010039806366, 0.096498996], [0.9182958340544896, 0.9182958340544896], [3.997670276487613, 6.739005504021667, 6.832268948780267, 3.9044068317290135, 2.8345986722926537, 0.09326344475860004]]\n",
      "2020-12-23 02:47:58,940 : INFO : Removed 3 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:58,941 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:58,943 : INFO : built Dictionary(110 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 510 corpus positions)\n",
      "2020-12-23 02:47:58,983 : INFO : token count processed\n",
      "2020-12-23 02:47:58,988 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:47:58,988 : INFO : frequencies processed\n",
      "2020-12-23 02:47:58,990 : INFO : token count processed\n",
      "2020-12-23 02:47:58,991 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:58,991 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:58,992 : INFO : vocab #2480\n",
      "2020-12-23 02:47:58,993 : INFO : diff #set()\n",
      "2020-12-23 02:47:59,248 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:59,380 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2549551412754065, 0.4434678019512168], [0.9623576328158379, 0.037642367], [nan, nan], [3.997670276487613, 5.870833373337847, 6.008634752158722, 3.859868897666738, 2.0109644756711087, 0.13780137882087473]]\n",
      "2020-12-23 02:47:59,383 : INFO : Removed 3 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:59,384 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:59,385 : INFO : built Dictionary(70 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 193 corpus positions)\n",
      "2020-12-23 02:47:59,408 : INFO : token count processed\n",
      "2020-12-23 02:47:59,410 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:47:59,411 : INFO : frequencies processed\n",
      "2020-12-23 02:47:59,412 : INFO : token count processed\n",
      "2020-12-23 02:47:59,413 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:59,414 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:59,415 : INFO : vocab #2480\n",
      "2020-12-23 02:47:59,416 : INFO : diff #set()\n",
      "2020-12-23 02:47:59,682 : INFO : alphabet #2480\n",
      "2020-12-23 02:47:59,810 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.242745096723753, 0.44588214749006483], [0.9665513336658478, 0.033448666], [nan, nan], [3.997670276487613, 5.371881234145534, 5.685605146546148, 3.6839463640869994, 1.6879348700585353, 0.31372391240061415]]\n",
      "2020-12-23 02:47:59,813 : INFO : Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:47:59,814 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:47:59,814 : INFO : built Dictionary(60 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 145 corpus positions)\n",
      "2020-12-23 02:47:59,828 : INFO : token count processed\n",
      "2020-12-23 02:47:59,830 : INFO : frequencies processed\n",
      "2020-12-23 02:47:59,958 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:47:59,959 : INFO : entropies processed\n",
      "2020-12-23 02:47:59,959 : INFO : extropies processed\n",
      "2020-12-23 02:47:59,960 : INFO : token count processed\n",
      "2020-12-23 02:47:59,961 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:47:59,962 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:47:59,963 : INFO : vocab #2480\n",
      "2020-12-23 02:47:59,964 : INFO : diff #set()\n",
      "2020-12-23 02:48:00,234 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:00,361 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.2402450588118894, 0.4463797369250079], [0.976856766268611, 0.023143234], [0.0, 0.0], [3.997670276487613, 4.85108279267097, 5.279423955706212, 3.569329113452371, 1.2817536792185993, 0.4283411630352427]]\n",
      "2020-12-23 02:48:00,364 : INFO : Removed 3 and 127 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:00,365 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:00,367 : INFO : built Dictionary(101 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 418 corpus positions)\n",
      "2020-12-23 02:48:00,404 : INFO : token count processed\n",
      "2020-12-23 02:48:00,406 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:00,409 : INFO : frequencies processed\n",
      "2020-12-23 02:48:00,410 : INFO : token count processed\n",
      "2020-12-23 02:48:00,412 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:00,414 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:00,415 : INFO : vocab #2480\n",
      "2020-12-23 02:48:00,418 : INFO : diff #set()\n",
      "2020-12-23 02:48:00,679 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:00,809 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.2462761437416157, 0.4451812404214483], [0.9595785029232502, 0.040421497], [nan, nan], [3.997670276487613, 6.139571208108155, 6.282611886023904, 3.8546295985718633, 2.284941609536291, 0.14304067791574937]]\n",
      "2020-12-23 02:48:00,811 : INFO : Removed 3 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:00,812 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:00,813 : INFO : built Dictionary(87 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 428 corpus positions)\n",
      "2020-12-23 02:48:00,837 : INFO : token count processed\n",
      "2020-12-23 02:48:00,840 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:00,842 : INFO : frequencies processed\n",
      "2020-12-23 02:48:00,843 : INFO : token count processed\n",
      "2020-12-23 02:48:00,846 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:00,847 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:00,849 : INFO : vocab #2480\n",
      "2020-12-23 02:48:00,850 : INFO : diff #set()\n",
      "2020-12-23 02:48:01,105 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:01,231 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.2552335433706112, 0.44341305712641493], [0.9622409678995609, 0.037759032], [nan, nan], [3.997670276487613, 5.609710627339259, 5.789037627979847, 3.818343275847025, 1.7913673514922341, 0.17932700064058782]]\n",
      "2020-12-23 02:48:01,234 : INFO : Removed 3 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:01,235 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:01,236 : INFO : built Dictionary(177 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 1088 corpus positions)\n",
      "2020-12-23 02:48:01,287 : INFO : token count processed\n",
      "2020-12-23 02:48:01,289 : INFO : frequencies processed\n",
      "2020-12-23 02:48:01,417 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:01,418 : INFO : entropies processed\n",
      "2020-12-23 02:48:01,419 : INFO : extropies processed\n",
      "2020-12-23 02:48:01,421 : INFO : token count processed\n",
      "2020-12-23 02:48:01,422 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:01,423 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:01,424 : INFO : vocab #2480\n",
      "2020-12-23 02:48:01,425 : INFO : diff #set()\n",
      "2020-12-23 02:48:01,686 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:01,815 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.238290914178771, 0.44676944970171584], [0.9242869019508362, 0.0757131], [0.8112781244591328, 0.8112781244591328], [3.997670276487613, 7.2441902753576075, 7.292799750667144, 3.949060801178077, 3.295129474179531, 0.04860947530953652]]\n",
      "2020-12-23 02:48:01,818 : INFO : Removed 3 and 136 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:48:01,819 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:01,820 : INFO : built Dictionary(139 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 695 corpus positions)\n",
      "2020-12-23 02:48:01,863 : INFO : token count processed\n",
      "2020-12-23 02:48:01,868 : INFO : frequencies processed\n",
      "2020-12-23 02:48:01,995 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:01,996 : INFO : entropies processed\n",
      "2020-12-23 02:48:01,997 : INFO : extropies processed\n",
      "2020-12-23 02:48:01,998 : INFO : token count processed\n",
      "2020-12-23 02:48:01,998 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:02,000 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:02,000 : INFO : vocab #2480\n",
      "2020-12-23 02:48:02,001 : INFO : diff #set()\n",
      "2020-12-23 02:48:02,263 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:02,391 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.2363689927886676, 0.4471534005455145], [0.9241182655096054, 0.075881734], [0.9182958340544896, 0.9182958340544896], [3.997670276487613, 6.2567074920449475, 6.352905054270207, 3.9014727142623533, 2.3552347777825937, 0.09619756222525933]]\n",
      "2020-12-23 02:48:02,393 : INFO : Removed 3 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:02,394 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:02,395 : INFO : built Dictionary(85 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 205 corpus positions)\n",
      "2020-12-23 02:48:02,417 : INFO : token count processed\n",
      "2020-12-23 02:48:02,420 : INFO : frequencies processed\n",
      "2020-12-23 02:48:02,548 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:02,549 : INFO : entropies processed\n",
      "2020-12-23 02:48:02,549 : INFO : extropies processed\n",
      "2020-12-23 02:48:02,550 : INFO : token count processed\n",
      "2020-12-23 02:48:02,551 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:02,552 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:02,553 : INFO : vocab #2480\n",
      "2020-12-23 02:48:02,554 : INFO : diff #set()\n",
      "2020-12-23 02:48:02,814 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:02,943 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.2264902610734847, 0.44913737889778954], [0.9425951316952705, 0.05740487], [0.0, 0.0], [3.997670276487613, 5.7680018917339435, 6.027267384973863, 3.7384047832476934, 2.0295971084862496, 0.2592654932399192]]\n",
      "2020-12-23 02:48:02,946 : INFO : Removed 3 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:02,947 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:02,949 : INFO : built Dictionary(181 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 746 corpus positions)\n",
      "2020-12-23 02:48:03,015 : INFO : token count processed\n",
      "2020-12-23 02:48:03,021 : INFO : frequencies processed\n",
      "2020-12-23 02:48:03,149 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:03,150 : INFO : entropies processed\n",
      "2020-12-23 02:48:03,150 : INFO : extropies processed\n",
      "2020-12-23 02:48:03,152 : INFO : token count processed\n",
      "2020-12-23 02:48:03,152 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:03,153 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:03,154 : INFO : vocab #2480\n",
      "2020-12-23 02:48:03,155 : INFO : diff #set()\n",
      "2020-12-23 02:48:03,426 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:03,558 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.217742728218567, 0.4509089297311163], [0.9028697833418846, 0.09713022], [1.5, 1.1225562489182657], [3.997670276487613, 6.846479111193757, 6.922005915062908, 3.922143472618462, 2.9243356385752945, 0.07552680386915078]]\n",
      "2020-12-23 02:48:03,561 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:03,562 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:03,563 : INFO : built Dictionary(34 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 49 corpus positions)\n",
      "2020-12-23 02:48:03,573 : INFO : token count processed\n",
      "2020-12-23 02:48:03,576 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:03,577 : INFO : frequencies processed\n",
      "2020-12-23 02:48:03,578 : INFO : token count processed\n",
      "2020-12-23 02:48:03,579 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:03,579 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:03,580 : INFO : vocab #2480\n",
      "2020-12-23 02:48:03,581 : INFO : diff #set()\n",
      "2020-12-23 02:48:03,847 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:03,980 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2480306288228586, 0.4448337968258165], [0.9591599591076374, 0.04084004], [nan, nan], [3.997670276487613, 4.165013816065912, 5.050931304471528, 3.111752788081996, 1.0532610279839152, 0.8859174884056165]]\n",
      "2020-12-23 02:48:03,982 : INFO : Removed 3 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:03,983 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:03,984 : INFO : built Dictionary(61 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 156 corpus positions)\n",
      "2020-12-23 02:48:03,998 : INFO : token count processed\n",
      "2020-12-23 02:48:04,000 : INFO : frequencies processed\n",
      "2020-12-23 02:48:04,128 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:04,129 : INFO : entropies processed\n",
      "2020-12-23 02:48:04,130 : INFO : extropies processed\n",
      "2020-12-23 02:48:04,131 : INFO : token count processed\n",
      "2020-12-23 02:48:04,132 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:04,133 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:04,134 : INFO : vocab #2480\n",
      "2020-12-23 02:48:04,135 : INFO : diff #set()\n",
      "2020-12-23 02:48:04,393 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:04,521 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.2028747284134316, 0.45395227749524664], [0.9207876995205879, 0.0792123], [0.0, 0.0], [3.997670276487613, 5.449968864419248, 5.764742028329246, 3.682897112577616, 1.7670717518416326, 0.3147731639099973]]\n",
      "2020-12-23 02:48:04,524 : INFO : Removed 3 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:04,529 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:04,530 : INFO : built Dictionary(153 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 593 corpus positions)\n",
      "2020-12-23 02:48:04,577 : INFO : token count processed\n",
      "2020-12-23 02:48:04,583 : INFO : frequencies processed\n",
      "2020-12-23 02:48:04,714 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:04,715 : INFO : entropies processed\n",
      "2020-12-23 02:48:04,716 : INFO : extropies processed\n",
      "2020-12-23 02:48:04,717 : INFO : token count processed\n",
      "2020-12-23 02:48:04,718 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:04,719 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:04,719 : INFO : vocab #2480\n",
      "2020-12-23 02:48:04,720 : INFO : diff #set()\n",
      "2020-12-23 02:48:04,981 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:05,117 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.2245840641260315, 0.44952223479712294], [0.9262267649173737, 0.073773235], [1.0, 1.0], [3.997670276487613, 6.530294129310484, 6.628005981984422, 3.8999584238136746, 2.6303357054968086, 0.097711852673938]]\n",
      "2020-12-23 02:48:05,120 : INFO : Removed 3 and 142 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:05,121 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:05,122 : INFO : built Dictionary(128 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 609 corpus positions)\n",
      "2020-12-23 02:48:05,165 : INFO : token count processed\n",
      "2020-12-23 02:48:05,167 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:05,170 : INFO : frequencies processed\n",
      "2020-12-23 02:48:05,172 : INFO : token count processed\n",
      "2020-12-23 02:48:05,173 : INFO : alphabet_source #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:48:05,175 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:05,177 : INFO : vocab #2480\n",
      "2020-12-23 02:48:05,178 : INFO : diff #set()\n",
      "2020-12-23 02:48:05,439 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:05,567 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.239669328511528, 0.4464944834801102], [0.9561278596520424, 0.04387214], [nan, nan], [3.997670276487613, 6.470272233491701, 6.577676523503808, 3.890265986475507, 2.5800062470161946, 0.1074042900121066]]\n",
      "2020-12-23 02:48:05,570 : INFO : Removed 3 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:05,570 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:05,572 : INFO : built Dictionary(127 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 398 corpus positions)\n",
      "2020-12-23 02:48:05,612 : INFO : token count processed\n",
      "2020-12-23 02:48:05,619 : INFO : frequencies processed\n",
      "2020-12-23 02:48:05,751 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:05,752 : INFO : entropies processed\n",
      "2020-12-23 02:48:05,752 : INFO : extropies processed\n",
      "2020-12-23 02:48:05,754 : INFO : token count processed\n",
      "2020-12-23 02:48:05,754 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:05,755 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:05,756 : INFO : vocab #2480\n",
      "2020-12-23 02:48:05,757 : INFO : diff #set()\n",
      "2020-12-23 02:48:06,030 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:06,163 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.2234490044349684, 0.4497517136688835], [0.9115940779447556, 0.08840592], [0.9182958340544896, 0.9182958340544896], [3.997670276487613, 6.550038223589686, 6.677171741288083, 3.870536758789216, 2.67950146480047, 0.12713351769839676]]\n",
      "2020-12-23 02:48:06,166 : INFO : Removed 3 and 70 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:06,167 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:06,167 : INFO : built Dictionary(84 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 314 corpus positions)\n",
      "2020-12-23 02:48:06,188 : INFO : token count processed\n",
      "2020-12-23 02:48:06,190 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:06,191 : INFO : frequencies processed\n",
      "2020-12-23 02:48:06,192 : INFO : token count processed\n",
      "2020-12-23 02:48:06,193 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:06,194 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:06,195 : INFO : vocab #2480\n",
      "2020-12-23 02:48:06,197 : INFO : diff #set()\n",
      "2020-12-23 02:48:06,451 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:06,578 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.2379371690861367, 0.4468400694235534], [0.9647354409098625, 0.03526456], [nan, nan], [3.997670276487613, 5.860525481261383, 6.05161014315486, 3.8065856145941366, 2.0539398666672466, 0.19108466189347695]]\n",
      "2020-12-23 02:48:06,580 : INFO : Removed 3 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:06,581 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:06,582 : INFO : built Dictionary(55 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 352 corpus positions)\n",
      "2020-12-23 02:48:06,598 : INFO : token count processed\n",
      "2020-12-23 02:48:06,601 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:06,603 : INFO : frequencies processed\n",
      "2020-12-23 02:48:06,604 : INFO : token count processed\n",
      "2020-12-23 02:48:06,606 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:06,609 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:06,609 : INFO : vocab #2480\n",
      "2020-12-23 02:48:06,612 : INFO : diff #set()\n",
      "2020-12-23 02:48:06,865 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:06,993 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.282207681549078, 0.43817221722837996], [0.983579084277153, 0.016420916], [nan, nan], [3.997670276487613, 5.945464049777852, 6.099902149902611, 3.8432321763628536, 2.1022318734149983, 0.154438100124759]]\n",
      "2020-12-23 02:48:06,996 : INFO : Removed 3 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:06,997 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:07,000 : INFO : built Dictionary(196 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 899 corpus positions)\n",
      "2020-12-23 02:48:07,064 : INFO : token count processed\n",
      "2020-12-23 02:48:07,067 : INFO : frequencies processed\n",
      "2020-12-23 02:48:07,195 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:07,196 : INFO : entropies processed\n",
      "2020-12-23 02:48:07,196 : INFO : extropies processed\n",
      "2020-12-23 02:48:07,198 : INFO : token count processed\n",
      "2020-12-23 02:48:07,199 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:07,200 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:07,201 : INFO : vocab #2480\n",
      "2020-12-23 02:48:07,202 : INFO : diff #set()\n",
      "2020-12-23 02:48:07,461 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:07,588 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.2150552577311915, 0.45145600612432], [0.9021648988127708, 0.0978351], [2.75, 1.3226647836567116], [3.997670276487613, 6.811563897304216, 6.8705136523604295, 3.9387205214313994, 2.8728433758728165, 0.05894975505621325]]\n",
      "2020-12-23 02:48:07,591 : INFO : Removed 3 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:07,592 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:07,593 : INFO : built Dictionary(219 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 994 corpus positions)\n",
      "2020-12-23 02:48:07,668 : INFO : token count processed\n",
      "2020-12-23 02:48:07,671 : INFO : frequencies processed\n",
      "2020-12-23 02:48:07,798 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:07,798 : INFO : entropies processed\n",
      "2020-12-23 02:48:07,802 : INFO : extropies processed\n",
      "2020-12-23 02:48:07,803 : INFO : token count processed\n",
      "2020-12-23 02:48:07,804 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:07,804 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:07,805 : INFO : vocab #2480\n",
      "2020-12-23 02:48:07,806 : INFO : diff #set()\n",
      "2020-12-23 02:48:08,064 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:08,193 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.2197863187570601, 0.4504938117466806], [0.8947740197181702, 0.10522598], [2.1280852788913944, 1.2238339714721667], [3.997670276487613, 7.502034948968415, 7.544328783758053, 3.9553764416979744, 3.54665850727044, 0.04229383478963822]]\n",
      "2020-12-23 02:48:08,196 : INFO : Removed 3 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:08,197 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:08,200 : INFO : built Dictionary(262 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 1567 corpus positions)\n",
      "2020-12-23 02:48:08,303 : INFO : token count processed\n",
      "2020-12-23 02:48:08,311 : INFO : frequencies processed\n",
      "2020-12-23 02:48:08,443 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:08,443 : INFO : entropies processed\n",
      "2020-12-23 02:48:08,444 : INFO : extropies processed\n",
      "2020-12-23 02:48:08,445 : INFO : token count processed\n",
      "2020-12-23 02:48:08,446 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:08,447 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:08,448 : INFO : vocab #2480\n",
      "2020-12-23 02:48:08,449 : INFO : diff #set()\n",
      "2020-12-23 02:48:08,714 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:08,843 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.2229579330121703, 0.4498510678719736], [0.8856065571308136, 0.11439344], [2.521640636343318, 1.2998438251349493], [3.997670276487613, 7.39180093901977, 7.429020115533822, 3.960451099973561, 3.431349839046209, 0.03721917651405171]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:48:08,845 : INFO : Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:08,846 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:08,847 : INFO : built Dictionary(52 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 122 corpus positions)\n",
      "2020-12-23 02:48:08,859 : INFO : token count processed\n",
      "2020-12-23 02:48:08,861 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:08,862 : INFO : frequencies processed\n",
      "2020-12-23 02:48:08,863 : INFO : token count processed\n",
      "2020-12-23 02:48:08,864 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:08,864 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:08,865 : INFO : vocab #2480\n",
      "2020-12-23 02:48:08,866 : INFO : diff #set()\n",
      "2020-12-23 02:48:09,124 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:09,253 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.2386896062698531, 0.4466898837602677], [0.9554519355297089, 0.044548064], [nan, nan], [3.997670276487613, 4.927561309677364, 5.397297944326001, 3.527933641838975, 1.3996276678383883, 0.4697366346486378]]\n",
      "2020-12-23 02:48:09,255 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:09,256 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:09,257 : INFO : built Dictionary(20 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 23 corpus positions)\n",
      "2020-12-23 02:48:09,260 : INFO : token count processed\n",
      "2020-12-23 02:48:09,263 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:09,263 : INFO : frequencies processed\n",
      "2020-12-23 02:48:09,264 : INFO : token count processed\n",
      "2020-12-23 02:48:09,265 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:09,266 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:09,267 : INFO : vocab #2480\n",
      "2020-12-23 02:48:09,268 : INFO : diff #set()\n",
      "2020-12-23 02:48:09,525 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:09,653 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.260376337771295, 0.4424042064543944], [0.9761867858469486, 0.023813214], [nan, nan], [3.997670276487613, 2.5216406363433186, 4.440636352673266, 2.0786745601576655, 0.4429660761856531, 1.9189957163299476]]\n",
      "2020-12-23 02:48:09,657 : INFO : Removed 3 and 530 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:09,657 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:09,659 : INFO : built Dictionary(337 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 2895 corpus positions)\n",
      "2020-12-23 02:48:09,802 : INFO : token count processed\n",
      "2020-12-23 02:48:09,805 : INFO : frequencies processed\n",
      "2020-12-23 02:48:09,936 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:09,936 : INFO : entropies processed\n",
      "2020-12-23 02:48:09,937 : INFO : extropies processed\n",
      "2020-12-23 02:48:09,939 : INFO : token count processed\n",
      "2020-12-23 02:48:09,940 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:09,941 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:09,941 : INFO : vocab #2480\n",
      "2020-12-23 02:48:09,942 : INFO : diff #set()\n",
      "2020-12-23 02:48:10,199 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:10,327 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.229594674747826, 0.4485120149083165], [0.9113390222191811, 0.08866098], [2.321928094887362, 1.2877123795494492], [3.997670276487613, 7.480007711014331, 7.505423351901284, 3.9722546356006596, 3.5077530754136705, 0.025415640886953028]]\n",
      "2020-12-23 02:48:10,330 : INFO : Removed 3 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:10,331 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:10,332 : INFO : built Dictionary(216 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 1044 corpus positions)\n",
      "2020-12-23 02:48:10,398 : INFO : token count processed\n",
      "2020-12-23 02:48:10,401 : INFO : frequencies processed\n",
      "2020-12-23 02:48:10,526 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:10,527 : INFO : entropies processed\n",
      "2020-12-23 02:48:10,528 : INFO : extropies processed\n",
      "2020-12-23 02:48:10,530 : INFO : token count processed\n",
      "2020-12-23 02:48:10,531 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:10,532 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:10,532 : INFO : vocab #2480\n",
      "2020-12-23 02:48:10,533 : INFO : diff #set()\n",
      "2020-12-23 02:48:10,794 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:10,922 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.2341747709092454, 0.44759255767310835], [0.915111355483532, 0.084888645], [1.5, 1.1225562489182657], [3.997670276487613, 7.131331012509435, 7.189841323649103, 3.939159965347944, 3.1921710471614904, 0.05851031113966876]]\n",
      "2020-12-23 02:48:10,924 : INFO : Removed 3 and 166 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:10,925 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:10,926 : INFO : built Dictionary(206 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 863 corpus positions)\n",
      "2020-12-23 02:48:10,988 : INFO : token count processed\n",
      "2020-12-23 02:48:10,991 : INFO : frequencies processed\n",
      "2020-12-23 02:48:11,118 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:11,119 : INFO : entropies processed\n",
      "2020-12-23 02:48:11,122 : INFO : extropies processed\n",
      "2020-12-23 02:48:11,123 : INFO : token count processed\n",
      "2020-12-23 02:48:11,124 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:11,125 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:11,126 : INFO : vocab #2480\n",
      "2020-12-23 02:48:11,126 : INFO : diff #set()\n",
      "2020-12-23 02:48:11,386 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:11,514 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.236757142235189, 0.4470758050204329], [0.913730077445507, 0.08626992], [0.9182958340544896, 0.9182958340544896], [3.997670276487613, 7.203742744794778, 7.269765020670306, 3.9316480006120855, 3.272094744182693, 0.066022275875528]]\n",
      "2020-12-23 02:48:11,517 : INFO : Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:11,518 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:11,519 : INFO : built Dictionary(63 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 195 corpus positions)\n",
      "2020-12-23 02:48:11,543 : INFO : token count processed\n",
      "2020-12-23 02:48:11,545 : INFO : frequencies processed\n",
      "2020-12-23 02:48:11,675 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:11,676 : INFO : entropies processed\n",
      "2020-12-23 02:48:11,676 : INFO : extropies processed\n",
      "2020-12-23 02:48:11,678 : INFO : token count processed\n",
      "2020-12-23 02:48:11,680 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:11,681 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:11,683 : INFO : vocab #2480\n",
      "2020-12-23 02:48:11,684 : INFO : diff #set()\n",
      "2020-12-23 02:48:11,950 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:12,078 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.2234374289294574, 0.44975405513501715], [0.9386366531252861, 0.061363347], [1.584962500721156, 1.1699250014423124], [3.997670276487613, 5.195502554608948, 5.505478967217547, 3.687693863879014, 1.507808690729934, 0.30997641260859865]]\n",
      "2020-12-23 02:48:12,081 : INFO : Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:12,081 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:12,082 : INFO : built Dictionary(72 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 256 corpus positions)\n",
      "2020-12-23 02:48:12,108 : INFO : token count processed\n",
      "2020-12-23 02:48:12,110 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:12,111 : INFO : frequencies processed\n",
      "2020-12-23 02:48:12,112 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:48:12,114 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:12,115 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:12,116 : INFO : vocab #2480\n",
      "2020-12-23 02:48:12,118 : INFO : diff #set()\n",
      "2020-12-23 02:48:12,396 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:12,531 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.2417685623706494, 0.44607637772496433], [0.9531410373747349, 0.046858963], [nan, nan], [3.997670276487613, 5.32027245610305, 5.600923368364034, 3.717019364226628, 1.6032530918764212, 0.2806509122609846]]\n",
      "2020-12-23 02:48:12,533 : INFO : Removed 3 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:12,534 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:12,536 : INFO : built Dictionary(166 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 496 corpus positions)\n",
      "2020-12-23 02:48:12,601 : INFO : token count processed\n",
      "2020-12-23 02:48:12,603 : INFO : frequencies processed\n",
      "2020-12-23 02:48:12,730 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:12,731 : INFO : entropies processed\n",
      "2020-12-23 02:48:12,731 : INFO : extropies processed\n",
      "2020-12-23 02:48:12,733 : INFO : token count processed\n",
      "2020-12-23 02:48:12,734 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:12,735 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:12,735 : INFO : vocab #2480\n",
      "2020-12-23 02:48:12,736 : INFO : diff #set()\n",
      "2020-12-23 02:48:12,993 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:13,120 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.2046472550443696, 0.45358730187422863], [0.8834734410047531, 0.11652656], [2.521640636343318, 1.2998438251349493], [3.997670276487613, 6.898202761357263, 6.981780167644103, 3.914092870200773, 2.9841098911564896, 0.08357740628683974]]\n",
      "2020-12-23 02:48:13,122 : INFO : Removed 3 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:13,123 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:13,125 : INFO : built Dictionary(134 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 518 corpus positions)\n",
      "2020-12-23 02:48:13,172 : INFO : token count processed\n",
      "2020-12-23 02:48:13,175 : INFO : frequencies processed\n",
      "2020-12-23 02:48:13,423 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:13,424 : INFO : entropies processed\n",
      "2020-12-23 02:48:13,424 : INFO : extropies processed\n",
      "2020-12-23 02:48:13,426 : INFO : token count processed\n",
      "2020-12-23 02:48:13,427 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:13,427 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:13,428 : INFO : vocab #2480\n",
      "2020-12-23 02:48:13,429 : INFO : diff #set()\n",
      "2020-12-23 02:48:13,688 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:13,817 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.1960629318621974, 0.4553603567052739], [0.9021768197417259, 0.09782318], [1.0, 1.0], [3.997670276487613, 6.388500481644799, 6.507564816540139, 3.878605941592273, 2.509894540052526, 0.11906433489533974]]\n",
      "2020-12-23 02:48:13,819 : INFO : Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:13,820 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:13,821 : INFO : built Dictionary(54 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 182 corpus positions)\n",
      "2020-12-23 02:48:13,836 : INFO : token count processed\n",
      "2020-12-23 02:48:13,841 : INFO : frequencies processed\n",
      "2020-12-23 02:48:13,973 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:13,973 : INFO : entropies processed\n",
      "2020-12-23 02:48:13,974 : INFO : extropies processed\n",
      "2020-12-23 02:48:13,976 : INFO : token count processed\n",
      "2020-12-23 02:48:13,977 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:13,978 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:13,978 : INFO : vocab #2480\n",
      "2020-12-23 02:48:13,980 : INFO : diff #set()\n",
      "2020-12-23 02:48:14,250 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:14,380 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.2218833205329105, 0.4500686380597851], [0.9433452449738979, 0.056654755], [1.584962500721156, 1.1699250014423124], [3.997670276487613, 4.8191513650620195, 5.178754368204951, 3.6380672733446815, 1.1810840917173375, 0.35960300314293114]]\n",
      "2020-12-23 02:48:14,382 : INFO : Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:14,383 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:14,385 : INFO : built Dictionary(62 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 237 corpus positions)\n",
      "2020-12-23 02:48:14,405 : INFO : token count processed\n",
      "2020-12-23 02:48:14,407 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:14,408 : INFO : frequencies processed\n",
      "2020-12-23 02:48:14,409 : INFO : token count processed\n",
      "2020-12-23 02:48:14,410 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:14,411 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:14,412 : INFO : vocab #2480\n",
      "2020-12-23 02:48:14,413 : INFO : diff #set()\n",
      "2020-12-23 02:48:14,671 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:14,798 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.2385260623580387, 0.44672251836398585], [0.9535801894962788, 0.04641981], [nan, nan], [3.997670276487613, 5.062480936779194, 5.376605345080974, 3.683545868185832, 1.378935068593361, 0.31412440830178046]]\n",
      "2020-12-23 02:48:14,801 : INFO : Removed 3 and 223 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:14,802 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:14,805 : INFO : built Dictionary(248 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 1791 corpus positions)\n",
      "2020-12-23 02:48:14,909 : INFO : token count processed\n",
      "2020-12-23 02:48:14,912 : INFO : frequencies processed\n",
      "2020-12-23 02:48:15,040 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:15,040 : INFO : entropies processed\n",
      "2020-12-23 02:48:15,041 : INFO : extropies processed\n",
      "2020-12-23 02:48:15,043 : INFO : token count processed\n",
      "2020-12-23 02:48:15,044 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:15,045 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:15,046 : INFO : vocab #2480\n",
      "2020-12-23 02:48:15,047 : INFO : diff #set()\n",
      "2020-12-23 02:48:15,305 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:15,432 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.2299752178008505, 0.4484354767790543], [0.9265231713652611, 0.07347683], [2.321928094887362, 1.2877123795494492], [3.997670276487613, 7.185085743102134, 7.225112415125963, 3.957643604463785, 3.22744213863835, 0.0400266720238287]]\n",
      "2020-12-23 02:48:15,435 : INFO : Removed 3 and 148 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:15,436 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:15,438 : INFO : built Dictionary(168 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 766 corpus positions)\n",
      "2020-12-23 02:48:15,494 : INFO : token count processed\n",
      "2020-12-23 02:48:15,502 : INFO : frequencies processed\n",
      "2020-12-23 02:48:15,630 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:15,630 : INFO : entropies processed\n",
      "2020-12-23 02:48:15,631 : INFO : extropies processed\n",
      "2020-12-23 02:48:15,632 : INFO : token count processed\n",
      "2020-12-23 02:48:15,633 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:15,634 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:15,635 : INFO : vocab #2480\n",
      "2020-12-23 02:48:15,636 : INFO : diff #set()\n",
      "2020-12-23 02:48:15,895 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:16,023 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.2387378103672169, 0.4466802657145329], [0.9277096092700958, 0.07229039], [0.9182958340544896, 0.9182958340544896], [3.997670276487613, 6.591225336124281, 6.674651964887808, 3.9142436477240867, 2.6769816884001947, 0.0834266287635268]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:48:16,026 : INFO : Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:16,027 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:16,027 : INFO : built Dictionary(48 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 104 corpus positions)\n",
      "2020-12-23 02:48:16,038 : INFO : token count processed\n",
      "2020-12-23 02:48:16,040 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:16,041 : INFO : frequencies processed\n",
      "2020-12-23 02:48:16,042 : INFO : token count processed\n",
      "2020-12-23 02:48:16,043 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:16,044 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:16,045 : INFO : vocab #2480\n",
      "2020-12-23 02:48:16,046 : INFO : diff #set()\n",
      "2020-12-23 02:48:16,306 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:16,434 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.2454950248398347, 0.4453361013664804], [0.9667851515114307, 0.03321485], [nan, nan], [3.997670276487613, 4.7032114441396695, 5.250171350962151, 3.4507103696651322, 1.2525010744745377, 0.5469599068224813]]\n",
      "2020-12-23 02:48:16,437 : INFO : Removed 3 and 9 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:16,438 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:16,439 : INFO : built Dictionary(125 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 344 corpus positions)\n",
      "2020-12-23 02:48:16,475 : INFO : token count processed\n",
      "2020-12-23 02:48:16,485 : INFO : frequencies processed\n",
      "2020-12-23 02:48:16,615 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:16,616 : INFO : entropies processed\n",
      "2020-12-23 02:48:16,617 : INFO : extropies processed\n",
      "2020-12-23 02:48:16,618 : INFO : token count processed\n",
      "2020-12-23 02:48:16,619 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:16,621 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:16,621 : INFO : vocab #2480\n",
      "2020-12-23 02:48:16,622 : INFO : diff #set()\n",
      "2020-12-23 02:48:16,891 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:17,018 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.214387250488655, 0.45159219543886336], [0.912135899066925, 0.0878641], [0.0, 0.0], [3.997670276487613, 6.14228447828618, 6.3227535726012105, 3.8172011821725826, 2.3250832961135974, 0.1804690943150309]]\n",
      "2020-12-23 02:48:17,021 : INFO : Removed 3 and 184 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:17,021 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:17,024 : INFO : built Dictionary(259 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 1134 corpus positions)\n",
      "2020-12-23 02:48:17,125 : INFO : token count processed\n",
      "2020-12-23 02:48:17,131 : INFO : frequencies processed\n",
      "2020-12-23 02:48:17,261 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:17,261 : INFO : entropies processed\n",
      "2020-12-23 02:48:17,262 : INFO : extropies processed\n",
      "2020-12-23 02:48:17,264 : INFO : token count processed\n",
      "2020-12-23 02:48:17,265 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:17,266 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:17,267 : INFO : vocab #2480\n",
      "2020-12-23 02:48:17,268 : INFO : diff #set()\n",
      "2020-12-23 02:48:17,537 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:17,665 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.228943426026263, 0.44864306035025275], [0.9102151989936829, 0.0897848], [2.321928094887362, 1.2877123795494492], [3.997670276487613, 7.450178124335845, 7.500424875998623, 3.9474235248248357, 3.5027545995110096, 0.05024675166277781]]\n",
      "2020-12-23 02:48:17,668 : INFO : Removed 3 and 16 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:17,669 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:17,670 : INFO : built Dictionary(63 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 182 corpus positions)\n",
      "2020-12-23 02:48:17,696 : INFO : token count processed\n",
      "2020-12-23 02:48:17,698 : INFO : frequencies processed\n",
      "2020-12-23 02:48:17,827 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:17,828 : INFO : entropies processed\n",
      "2020-12-23 02:48:17,828 : INFO : extropies processed\n",
      "2020-12-23 02:48:17,829 : INFO : token count processed\n",
      "2020-12-23 02:48:17,830 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:17,831 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:17,831 : INFO : vocab #2480\n",
      "2020-12-23 02:48:17,832 : INFO : diff #set()\n",
      "2020-12-23 02:48:18,090 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:18,218 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.2196357389018866, 0.45052437319950833], [0.9386957511305809, 0.06130425], [1.584962500721156, 1.1699250014423124], [3.997670276487613, 5.20665021947654, 5.513921481857556, 3.690399014106596, 1.5162512053699433, 0.30727126238101654]]\n",
      "2020-12-23 02:48:18,221 : INFO : Removed 3 and 89 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:18,222 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:18,223 : INFO : built Dictionary(133 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 520 corpus positions)\n",
      "2020-12-23 02:48:18,265 : INFO : token count processed\n",
      "2020-12-23 02:48:18,271 : INFO : frequencies processed\n",
      "2020-12-23 02:48:18,405 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:18,405 : INFO : entropies processed\n",
      "2020-12-23 02:48:18,406 : INFO : extropies processed\n",
      "2020-12-23 02:48:18,407 : INFO : token count processed\n",
      "2020-12-23 02:48:18,408 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:18,409 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:18,409 : INFO : vocab #2480\n",
      "2020-12-23 02:48:18,410 : INFO : diff #set()\n",
      "2020-12-23 02:48:18,667 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:18,795 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.2268331896953728, 0.44906821248555145], [0.9160387516021729, 0.08396125], [0.9182958340544896, 0.9182958340544896], [3.997670276487613, 6.524718477352, 6.633439755155653, 3.8889489986839596, 2.6357694786680397, 0.10872127780365304]]\n",
      "2020-12-23 02:48:18,798 : INFO : Removed 3 and 8 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:18,799 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:18,800 : INFO : built Dictionary(69 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 227 corpus positions)\n",
      "2020-12-23 02:48:18,825 : INFO : token count processed\n",
      "2020-12-23 02:48:18,827 : INFO : frequencies processed\n",
      "2020-12-23 02:48:18,958 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:18,959 : INFO : entropies processed\n",
      "2020-12-23 02:48:18,960 : INFO : extropies processed\n",
      "2020-12-23 02:48:18,962 : INFO : token count processed\n",
      "2020-12-23 02:48:18,963 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:18,965 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:18,965 : INFO : vocab #2480\n",
      "2020-12-23 02:48:18,967 : INFO : diff #set()\n",
      "2020-12-23 02:48:19,225 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:19,351 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.2285745144712166, 0.4487173273796834], [0.9367107972502708, 0.0632892], [1.584962500721156, 1.1699250014423124], [3.997670276487613, 5.321859380715434, 5.592523439030045, 3.7270062181730017, 1.594853162542432, 0.27066405831461093]]\n",
      "2020-12-23 02:48:19,354 : INFO : Removed 3 and 119 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:19,355 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:19,357 : INFO : built Dictionary(148 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 845 corpus positions)\n",
      "2020-12-23 02:48:19,405 : INFO : token count processed\n",
      "2020-12-23 02:48:19,412 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:48:19,542 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:19,543 : INFO : entropies processed\n",
      "2020-12-23 02:48:19,543 : INFO : extropies processed\n",
      "2020-12-23 02:48:19,545 : INFO : token count processed\n",
      "2020-12-23 02:48:19,546 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:19,547 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:19,547 : INFO : vocab #2480\n",
      "2020-12-23 02:48:19,549 : INFO : diff #set()\n",
      "2020-12-23 02:48:19,807 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:19,936 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.233242486035706, 0.44777940875338135], [0.9107180312275887, 0.08928197], [1.5, 1.1225562489182657], [3.997670276487613, 6.500767808767801, 6.581110191968801, 3.917327893286613, 2.5834399154811876, 0.08034238320099973]]\n",
      "2020-12-23 02:48:19,938 : INFO : Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:19,939 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:19,940 : INFO : built Dictionary(42 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 67 corpus positions)\n",
      "2020-12-23 02:48:19,957 : INFO : token count processed\n",
      "2020-12-23 02:48:19,959 : INFO : frequencies processed\n",
      "2020-12-23 02:48:20,086 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:20,087 : INFO : entropies processed\n",
      "2020-12-23 02:48:20,088 : INFO : extropies processed\n",
      "2020-12-23 02:48:20,090 : INFO : token count processed\n",
      "2020-12-23 02:48:20,092 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:20,093 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:20,094 : INFO : vocab #2480\n",
      "2020-12-23 02:48:20,096 : INFO : diff #set()\n",
      "2020-12-23 02:48:20,358 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:20,486 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.181069699084059, 0.45849062064359997], [0.9101403430104256, 0.08985966], [1.0, 1.0], [3.997670276487613, 4.736228843383063, 5.320256232327482, 3.413642887543194, 1.3225859558398692, 0.5840273889444196]]\n",
      "2020-12-23 02:48:20,489 : INFO : Removed 3 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:20,490 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:20,492 : INFO : built Dictionary(104 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 408 corpus positions)\n",
      "2020-12-23 02:48:20,525 : INFO : token count processed\n",
      "2020-12-23 02:48:20,527 : INFO : frequencies processed\n",
      "2020-12-23 02:48:20,657 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:20,658 : INFO : entropies processed\n",
      "2020-12-23 02:48:20,659 : INFO : extropies processed\n",
      "2020-12-23 02:48:20,660 : INFO : token count processed\n",
      "2020-12-23 02:48:20,661 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:20,662 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:20,663 : INFO : vocab #2480\n",
      "2020-12-23 02:48:20,664 : INFO : diff #set()\n",
      "2020-12-23 02:48:20,923 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:21,051 : INFO : Computed distances or similarities ('283', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.2272294634423817, 0.44898831324474797], [0.9366499483585358, 0.06335005], [1.0, 1.0], [3.997670276487613, 5.788442787590127, 5.956914292993652, 3.829198771084089, 1.9592440165060387, 0.16847150540352462]]\n",
      "2020-12-23 02:48:21,054 : INFO : Removed 3 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:21,055 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:21,056 : INFO : built Dictionary(59 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 94 corpus positions)\n",
      "2020-12-23 02:48:21,070 : INFO : token count processed\n",
      "2020-12-23 02:48:21,073 : INFO : frequencies processed\n",
      "2020-12-23 02:48:21,201 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:21,202 : INFO : entropies processed\n",
      "2020-12-23 02:48:21,202 : INFO : extropies processed\n",
      "2020-12-23 02:48:21,203 : INFO : token count processed\n",
      "2020-12-23 02:48:21,204 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:21,206 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:21,207 : INFO : vocab #2480\n",
      "2020-12-23 02:48:21,209 : INFO : diff #set()\n",
      "2020-12-23 02:48:21,477 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:21,605 : INFO : Computed distances or similarities ('283', 'sacp-python-common/setup.py')[[1.1606627736010269, 0.46282095115350613], [0.8759521767497063, 0.12404782], [2.0, 1.2451124978365313], [3.997670276487613, 5.370004292053436, 5.721606566161333, 3.6460680023797165, 1.7239362896737203, 0.351602274107897]]\n",
      "2020-12-23 02:48:21,608 : INFO : Removed 3 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:21,608 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:21,609 : INFO : built Dictionary(89 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 339 corpus positions)\n",
      "2020-12-23 02:48:21,636 : INFO : token count processed\n",
      "2020-12-23 02:48:21,640 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:21,641 : INFO : frequencies processed\n",
      "2020-12-23 02:48:21,643 : INFO : token count processed\n",
      "2020-12-23 02:48:21,644 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:21,646 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:21,647 : INFO : vocab #2480\n",
      "2020-12-23 02:48:21,648 : INFO : diff #set()\n",
      "2020-12-23 02:48:21,905 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:22,034 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.2498872343524825, 0.444466720256671], [0.9678513668477535, 0.032148633], [nan, nan], [3.997670276487613, 5.695663584743922, 5.900231615346864, 3.793102245884671, 1.9025613388592508, 0.2045680306029416]]\n",
      "2020-12-23 02:48:22,036 : INFO : Removed 3 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:22,037 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:22,038 : INFO : built Dictionary(50 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 114 corpus positions)\n",
      "2020-12-23 02:48:22,049 : INFO : token count processed\n",
      "2020-12-23 02:48:22,051 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:22,052 : INFO : frequencies processed\n",
      "2020-12-23 02:48:22,053 : INFO : token count processed\n",
      "2020-12-23 02:48:22,054 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:22,055 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:22,055 : INFO : vocab #2480\n",
      "2020-12-23 02:48:22,056 : INFO : diff #set()\n",
      "2020-12-23 02:48:22,313 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:22,440 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.2420293616874398, 0.4460244888351331], [0.9736518263816833, 0.026348174], [nan, nan], [3.997670276487613, 4.9004417692112465, 5.385828427987745, 3.5122836177111143, 1.3881581515001318, 0.48538665877649834]]\n",
      "2020-12-23 02:48:22,443 : INFO : Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:22,444 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:22,445 : INFO : built Dictionary(48 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 121 corpus positions)\n",
      "2020-12-23 02:48:22,462 : INFO : token count processed\n",
      "2020-12-23 02:48:22,464 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:22,465 : INFO : frequencies processed\n",
      "2020-12-23 02:48:22,466 : INFO : token count processed\n",
      "2020-12-23 02:48:22,467 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:22,468 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:22,469 : INFO : vocab #2480\n",
      "2020-12-23 02:48:22,470 : INFO : diff #set()\n",
      "2020-12-23 02:48:22,727 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:22,855 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.2505501054718804, 0.444335808195804], [0.9846530938521028, 0.015346906], [nan, nan], [3.997670276487613, 4.778624108914332, 5.2706426869842105, 3.5056516984177346, 1.2729724104965974, 0.49201857806987803]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:48:22,857 : INFO : Removed 3 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:22,858 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:22,859 : INFO : built Dictionary(49 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 152 corpus positions)\n",
      "2020-12-23 02:48:22,869 : INFO : token count processed\n",
      "2020-12-23 02:48:22,872 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:22,872 : INFO : frequencies processed\n",
      "2020-12-23 02:48:22,874 : INFO : token count processed\n",
      "2020-12-23 02:48:22,876 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:22,877 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:22,878 : INFO : vocab #2480\n",
      "2020-12-23 02:48:22,880 : INFO : diff #set()\n",
      "2020-12-23 02:48:23,150 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:23,280 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.250427514652199, 0.44436001314823453], [0.9859128277748823, 0.014087172], [nan, nan], [3.997670276487613, 4.773880192225086, 5.204346168481623, 3.567204300231076, 1.2066758919940095, 0.4304659762565368]]\n",
      "2020-12-23 02:48:23,283 : INFO : Removed 3 and 346 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:23,284 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:23,286 : INFO : built Dictionary(158 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 1977 corpus positions)\n",
      "2020-12-23 02:48:23,333 : INFO : token count processed\n",
      "2020-12-23 02:48:23,336 : INFO : frequencies processed\n",
      "2020-12-23 02:48:23,466 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:23,467 : INFO : entropies processed\n",
      "2020-12-23 02:48:23,468 : INFO : extropies processed\n",
      "2020-12-23 02:48:23,470 : INFO : token count processed\n",
      "2020-12-23 02:48:23,470 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:23,471 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:23,472 : INFO : vocab #2480\n",
      "2020-12-23 02:48:23,473 : INFO : diff #set()\n",
      "2020-12-23 02:48:23,730 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:23,859 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.25152183545131, 0.4441440381587742], [0.9652453772723675, 0.034754623], [1.0, 1.0], [3.997670276487613, 6.620773041953877, 6.666017428031166, 3.952425890410323, 2.668347151543553, 0.04524438607728953]]\n",
      "2020-12-23 02:48:23,861 : INFO : Removed 3 and 132 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:23,862 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:23,863 : INFO : built Dictionary(90 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 540 corpus positions)\n",
      "2020-12-23 02:48:23,888 : INFO : token count processed\n",
      "2020-12-23 02:48:23,890 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:23,891 : INFO : frequencies processed\n",
      "2020-12-23 02:48:23,894 : INFO : token count processed\n",
      "2020-12-23 02:48:23,896 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:23,898 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:23,898 : INFO : vocab #2480\n",
      "2020-12-23 02:48:23,901 : INFO : diff #set()\n",
      "2020-12-23 02:48:24,161 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:24,289 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.2438370838663317, 0.4456651542084823], [0.960138838738203, 0.03986116], [nan, nan], [3.997670276487613, 5.828370634755606, 5.961857964335305, 3.8641829469079134, 1.964187687847692, 0.13348732957969922]]\n",
      "2020-12-23 02:48:24,292 : INFO : Removed 3 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:24,293 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:24,294 : INFO : built Dictionary(91 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 303 corpus positions)\n",
      "2020-12-23 02:48:24,323 : INFO : token count processed\n",
      "2020-12-23 02:48:24,325 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:24,328 : INFO : frequencies processed\n",
      "2020-12-23 02:48:24,330 : INFO : token count processed\n",
      "2020-12-23 02:48:24,331 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:24,332 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:24,332 : INFO : vocab #2480\n",
      "2020-12-23 02:48:24,333 : INFO : diff #set()\n",
      "2020-12-23 02:48:24,588 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:24,715 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.2548427649419651, 0.4434899033972056], [0.9654429964721203, 0.034557004], [nan, nan], [3.997670276487613, 5.774409284925443, 5.987909017017709, 3.784170544395347, 1.990238740530096, 0.21349973209226647]]\n",
      "2020-12-23 02:48:24,717 : INFO : Removed 3 and 51 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:24,718 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:24,720 : INFO : built Dictionary(102 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 352 corpus positions)\n",
      "2020-12-23 02:48:24,753 : INFO : token count processed\n",
      "2020-12-23 02:48:24,755 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:24,758 : INFO : frequencies processed\n",
      "2020-12-23 02:48:24,759 : INFO : token count processed\n",
      "2020-12-23 02:48:24,762 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:24,762 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:24,765 : INFO : vocab #2480\n",
      "2020-12-23 02:48:24,765 : INFO : diff #set()\n",
      "2020-12-23 02:48:25,020 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:25,146 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.248758122235302, 0.4446898891046512], [0.9621195420622826, 0.037880458], [nan, nan], [3.997670276487613, 5.977819040873918, 6.157788246466349, 3.8177010708951817, 2.1601179699787356, 0.1799692055924309]]\n",
      "2020-12-23 02:48:25,149 : INFO : Removed 3 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:25,150 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:25,151 : INFO : built Dictionary(82 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 299 corpus positions)\n",
      "2020-12-23 02:48:25,183 : INFO : token count processed\n",
      "2020-12-23 02:48:25,185 : INFO : frequencies processed\n",
      "2020-12-23 02:48:25,312 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:25,313 : INFO : entropies processed\n",
      "2020-12-23 02:48:25,313 : INFO : extropies processed\n",
      "2020-12-23 02:48:25,315 : INFO : token count processed\n",
      "2020-12-23 02:48:25,316 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:25,317 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:25,318 : INFO : vocab #2480\n",
      "2020-12-23 02:48:25,319 : INFO : diff #set()\n",
      "2020-12-23 02:48:25,579 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:25,706 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.2395241420668108, 0.44652342933759154], [0.9442500919103622, 0.055749908], [0.0, 0.0], [3.997670276487613, 5.901812829596593, 6.098715498526385, 3.8007676075578214, 2.101045222038772, 0.19690266892979214]]\n",
      "2020-12-23 02:48:25,709 : INFO : Removed 3 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:25,710 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:25,711 : INFO : built Dictionary(85 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 299 corpus positions)\n",
      "2020-12-23 02:48:25,742 : INFO : token count processed\n",
      "2020-12-23 02:48:25,745 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:25,748 : INFO : frequencies processed\n",
      "2020-12-23 02:48:25,749 : INFO : token count processed\n",
      "2020-12-23 02:48:25,752 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:25,753 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:25,753 : INFO : vocab #2480\n",
      "2020-12-23 02:48:25,754 : INFO : diff #set()\n",
      "2020-12-23 02:48:26,010 : INFO : alphabet #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:48:26,137 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.2504654104271942, 0.44435253053286217], [0.9777113925665617, 0.022288607], [nan, nan], [3.997670276487613, 5.643202320803383, 5.869163277093972, 3.771709320197023, 1.871493000606359, 0.22596095629058954]]\n",
      "2020-12-23 02:48:26,139 : INFO : Removed 3 and 30 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:26,140 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:26,141 : INFO : built Dictionary(97 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 333 corpus positions)\n",
      "2020-12-23 02:48:26,173 : INFO : token count processed\n",
      "2020-12-23 02:48:26,178 : INFO : frequencies processed\n",
      "2020-12-23 02:48:26,307 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:26,308 : INFO : entropies processed\n",
      "2020-12-23 02:48:26,308 : INFO : extropies processed\n",
      "2020-12-23 02:48:26,309 : INFO : token count processed\n",
      "2020-12-23 02:48:26,310 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:26,311 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:26,311 : INFO : vocab #2480\n",
      "2020-12-23 02:48:26,312 : INFO : diff #set()\n",
      "2020-12-23 02:48:26,569 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:26,696 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.1693666357872894, 0.46096403600172814], [0.901746578514576, 0.09825342], [0.0, 0.0], [3.997670276487613, 5.925214310725336, 6.103582012445803, 3.8193025747671454, 2.10591173595819, 0.17836770172046723]]\n",
      "2020-12-23 02:48:26,699 : INFO : Removed 3 and 293 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:26,700 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:26,703 : INFO : built Dictionary(168 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 1728 corpus positions)\n",
      "2020-12-23 02:48:26,767 : INFO : token count processed\n",
      "2020-12-23 02:48:26,769 : INFO : frequencies processed\n",
      "2020-12-23 02:48:26,900 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:26,900 : INFO : entropies processed\n",
      "2020-12-23 02:48:26,901 : INFO : extropies processed\n",
      "2020-12-23 02:48:26,902 : INFO : token count processed\n",
      "2020-12-23 02:48:26,903 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:26,904 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:26,904 : INFO : vocab #2480\n",
      "2020-12-23 02:48:26,905 : INFO : diff #set()\n",
      "2020-12-23 02:48:27,162 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:27,290 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.2515089427448245, 0.4441465814392438], [0.9627692215144634, 0.03723078], [1.584962500721156, 1.1699250014423124], [3.997670276487613, 6.551685682764175, 6.60083439418586, 3.9485215650659278, 2.6031641176982467, 0.049148711421684865]]\n",
      "2020-12-23 02:48:27,293 : INFO : Removed 3 and 88 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:27,294 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:27,295 : INFO : built Dictionary(147 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 569 corpus positions)\n",
      "2020-12-23 02:48:27,339 : INFO : token count processed\n",
      "2020-12-23 02:48:27,348 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:27,349 : INFO : frequencies processed\n",
      "2020-12-23 02:48:27,351 : INFO : token count processed\n",
      "2020-12-23 02:48:27,352 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:27,353 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:27,353 : INFO : vocab #2480\n",
      "2020-12-23 02:48:27,354 : INFO : diff #set()\n",
      "2020-12-23 02:48:27,609 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:27,736 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.2494429743752387, 0.44455450144395886], [0.950974877923727, 0.049025122], [nan, nan], [3.997670276487613, 6.642985062562557, 6.755108285787972, 3.885547053262198, 2.7574380093003588, 0.11212322322541457]]\n",
      "2020-12-23 02:48:27,738 : INFO : Removed 3 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:27,739 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:27,741 : INFO : built Dictionary(60 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 164 corpus positions)\n",
      "2020-12-23 02:48:27,760 : INFO : token count processed\n",
      "2020-12-23 02:48:27,762 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:27,763 : INFO : frequencies processed\n",
      "2020-12-23 02:48:27,764 : INFO : token count processed\n",
      "2020-12-23 02:48:27,765 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:27,766 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:27,767 : INFO : vocab #2480\n",
      "2020-12-23 02:48:27,769 : INFO : diff #set()\n",
      "2020-12-23 02:48:28,031 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:28,158 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.2487667007465961, 0.44468819271825644], [0.9550965875387192, 0.044903412], [nan, nan], [3.997670276487613, 5.2461980344571995, 5.599699442953889, 3.644168867990923, 1.6020291664662758, 0.35350140849668943]]\n",
      "2020-12-23 02:48:28,161 : INFO : Removed 3 and 23 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:28,162 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:28,163 : INFO : built Dictionary(80 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 224 corpus positions)\n",
      "2020-12-23 02:48:28,183 : INFO : token count processed\n",
      "2020-12-23 02:48:28,190 : INFO : frequencies processed\n",
      "2020-12-23 02:48:28,327 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:28,328 : INFO : entropies processed\n",
      "2020-12-23 02:48:28,329 : INFO : extropies processed\n",
      "2020-12-23 02:48:28,330 : INFO : token count processed\n",
      "2020-12-23 02:48:28,331 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:28,332 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:28,333 : INFO : vocab #2480\n",
      "2020-12-23 02:48:28,334 : INFO : diff #set()\n",
      "2020-12-23 02:48:28,605 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:28,733 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/test_auth_utility.py')[[1.21265828740315, 0.45194506792715544], [0.9260861873626709, 0.07391381], [1.0, 1.0], [3.997670276487613, 5.903090303960449, 6.124164961227144, 3.7765956192209194, 2.1264946847395305, 0.22107465726669417]]\n",
      "2020-12-23 02:48:28,736 : INFO : Removed 3 and 442 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:28,737 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:28,738 : INFO : built Dictionary(116 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 1223 corpus positions)\n",
      "2020-12-23 02:48:28,772 : INFO : token count processed\n",
      "2020-12-23 02:48:28,774 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:28,777 : INFO : frequencies processed\n",
      "2020-12-23 02:48:28,779 : INFO : token count processed\n",
      "2020-12-23 02:48:28,781 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:28,784 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:28,784 : INFO : vocab #2480\n",
      "2020-12-23 02:48:28,786 : INFO : diff #set()\n",
      "2020-12-23 02:48:29,046 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:29,175 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.2528803334732945, 0.4438762170995062], [0.9667784161865711, 0.033221584], [nan, nan], [3.997670276487613, 6.16659449033757, 6.231767437399164, 3.9324973294260186, 2.2340971609115505, 0.06517294706159404]]\n",
      "2020-12-23 02:48:29,178 : INFO : Removed 3 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:29,179 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:29,180 : INFO : built Dictionary(75 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 258 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:48:29,199 : INFO : token count processed\n",
      "2020-12-23 02:48:29,202 : INFO : frequencies processed\n",
      "2020-12-23 02:48:29,330 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:29,331 : INFO : entropies processed\n",
      "2020-12-23 02:48:29,332 : INFO : extropies processed\n",
      "2020-12-23 02:48:29,333 : INFO : token count processed\n",
      "2020-12-23 02:48:29,334 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:29,335 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:29,335 : INFO : vocab #2480\n",
      "2020-12-23 02:48:29,336 : INFO : diff #set()\n",
      "2020-12-23 02:48:29,596 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:29,723 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.236614692076594, 0.44710427931220736], [0.936864085495472, 0.063135915], [0.0, 0.0], [3.997670276487613, 5.906856253399655, 6.110929469921887, 3.793597059965382, 2.1132591934342737, 0.20407321652223143]]\n",
      "2020-12-23 02:48:29,726 : INFO : Removed 3 and 68 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:29,727 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:29,727 : INFO : built Dictionary(87 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 257 corpus positions)\n",
      "2020-12-23 02:48:29,752 : INFO : token count processed\n",
      "2020-12-23 02:48:29,755 : INFO : frequencies processed\n",
      "2020-12-23 02:48:29,883 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:29,884 : INFO : entropies processed\n",
      "2020-12-23 02:48:29,884 : INFO : extropies processed\n",
      "2020-12-23 02:48:29,885 : INFO : token count processed\n",
      "2020-12-23 02:48:29,886 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:29,887 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:29,888 : INFO : vocab #2480\n",
      "2020-12-23 02:48:29,889 : INFO : diff #set()\n",
      "2020-12-23 02:48:30,147 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:30,273 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.2214186865319672, 0.45016277483520195], [0.9417228102684021, 0.05827719], [1.0, 1.0], [3.997670276487613, 5.965115449163356, 6.158112098114614, 3.8046736275363546, 2.160441821627001, 0.192996648951258]]\n",
      "2020-12-23 02:48:30,276 : INFO : Removed 3 and 13 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:30,277 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:30,278 : INFO : built Dictionary(94 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 371 corpus positions)\n",
      "2020-12-23 02:48:30,311 : INFO : token count processed\n",
      "2020-12-23 02:48:30,316 : INFO : frequencies processed\n",
      "2020-12-23 02:48:30,444 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:30,445 : INFO : entropies processed\n",
      "2020-12-23 02:48:30,445 : INFO : extropies processed\n",
      "2020-12-23 02:48:30,446 : INFO : token count processed\n",
      "2020-12-23 02:48:30,447 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:30,448 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:30,448 : INFO : vocab #2480\n",
      "2020-12-23 02:48:30,449 : INFO : diff #set()\n",
      "2020-12-23 02:48:30,707 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:30,835 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.2305847837491168, 0.4483129299928347], [0.9331003203988075, 0.06689968], [1.0, 1.0], [3.997670276487613, 5.791362404253194, 5.969072062186678, 3.819960618554129, 1.9714017856990647, 0.1777096579334838]]\n",
      "2020-12-23 02:48:30,837 : INFO : Removed 3 and 41 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:30,838 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:30,839 : INFO : built Dictionary(84 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 383 corpus positions)\n",
      "2020-12-23 02:48:30,860 : INFO : token count processed\n",
      "2020-12-23 02:48:30,870 : INFO : frequencies processed\n",
      "2020-12-23 02:48:30,997 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:30,998 : INFO : entropies processed\n",
      "2020-12-23 02:48:30,998 : INFO : extropies processed\n",
      "2020-12-23 02:48:30,999 : INFO : token count processed\n",
      "2020-12-23 02:48:31,000 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:31,001 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:31,002 : INFO : vocab #2480\n",
      "2020-12-23 02:48:31,003 : INFO : diff #set()\n",
      "2020-12-23 02:48:31,262 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:31,390 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.2356921235483627, 0.4472887789275999], [0.9538716226816177, 0.046128377], [1.0, 1.0], [3.997670276487613, 5.651670454631116, 5.831247103796044, 3.8180936273226846, 1.833576827308431, 0.179576649164928]]\n",
      "2020-12-23 02:48:31,393 : INFO : Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:31,394 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:31,395 : INFO : built Dictionary(49 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 93 corpus positions)\n",
      "2020-12-23 02:48:31,412 : INFO : token count processed\n",
      "2020-12-23 02:48:31,414 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:31,415 : INFO : frequencies processed\n",
      "2020-12-23 02:48:31,416 : INFO : token count processed\n",
      "2020-12-23 02:48:31,417 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:31,418 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:31,419 : INFO : vocab #2480\n",
      "2020-12-23 02:48:31,420 : INFO : diff #set()\n",
      "2020-12-23 02:48:31,687 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:31,816 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.251025386847558, 0.44424199115783725], [0.9688943568617105, 0.031105643], [nan, nan], [3.997670276487613, 4.8226207261920235, 5.374636100636189, 3.445654902043448, 1.3769658241485758, 0.5520153744441654]]\n",
      "2020-12-23 02:48:31,819 : INFO : Removed 3 and 50 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:31,820 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:31,820 : INFO : built Dictionary(92 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 263 corpus positions)\n",
      "2020-12-23 02:48:31,853 : INFO : token count processed\n",
      "2020-12-23 02:48:31,855 : INFO : frequencies processed\n",
      "2020-12-23 02:48:31,982 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:31,982 : INFO : entropies processed\n",
      "2020-12-23 02:48:31,983 : INFO : extropies processed\n",
      "2020-12-23 02:48:31,984 : INFO : token count processed\n",
      "2020-12-23 02:48:31,984 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:31,985 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:31,986 : INFO : vocab #2480\n",
      "2020-12-23 02:48:31,987 : INFO : diff #set()\n",
      "2020-12-23 02:48:32,245 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:32,373 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.232935653108144, 0.4478409391726295], [0.9353972673416138, 0.06460273], [1.0, 1.0], [3.997670276487613, 6.24862851613934, 6.42857506119417, 3.8177237314327837, 2.430904784706557, 0.17994654505482988]]\n",
      "2020-12-23 02:48:32,375 : INFO : Removed 3 and 32 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:32,376 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:32,377 : INFO : built Dictionary(93 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 379 corpus positions)\n",
      "2020-12-23 02:48:32,407 : INFO : token count processed\n",
      "2020-12-23 02:48:32,410 : INFO : frequencies processed\n",
      "2020-12-23 02:48:32,537 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:32,537 : INFO : entropies processed\n",
      "2020-12-23 02:48:32,538 : INFO : extropies processed\n",
      "2020-12-23 02:48:32,539 : INFO : token count processed\n",
      "2020-12-23 02:48:32,540 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:32,540 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:32,541 : INFO : vocab #2480\n",
      "2020-12-23 02:48:32,542 : INFO : diff #set()\n",
      "2020-12-23 02:48:32,808 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:32,936 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.230281810551264, 0.44837383117644114], [0.9381410256028175, 0.061858974], [1.0, 1.0], [3.997670276487613, 5.850156917433494, 6.018153326769687, 3.8296738671514206, 2.0204830502820736, 0.16799640933619298]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:48:32,939 : INFO : Removed 3 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:32,940 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:32,941 : INFO : built Dictionary(88 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 377 corpus positions)\n",
      "2020-12-23 02:48:32,971 : INFO : token count processed\n",
      "2020-12-23 02:48:32,978 : INFO : frequencies processed\n",
      "2020-12-23 02:48:33,106 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:33,107 : INFO : entropies processed\n",
      "2020-12-23 02:48:33,108 : INFO : extropies processed\n",
      "2020-12-23 02:48:33,109 : INFO : token count processed\n",
      "2020-12-23 02:48:33,109 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:33,110 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:33,111 : INFO : vocab #2480\n",
      "2020-12-23 02:48:33,112 : INFO : diff #set()\n",
      "2020-12-23 02:48:33,369 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:33,495 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.2346803989479165, 0.44749128352797035], [0.9538198038935661, 0.046180196], [1.0, 1.0], [3.997670276487613, 5.6831976040360095, 5.862432305830728, 3.818435574692895, 1.8647620293431149, 0.17923470179471845]]\n",
      "2020-12-23 02:48:33,498 : INFO : Removed 3 and 52 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:33,499 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:33,500 : INFO : built Dictionary(76 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 355 corpus positions)\n",
      "2020-12-23 02:48:33,518 : INFO : token count processed\n",
      "2020-12-23 02:48:33,521 : INFO : frequencies processed\n",
      "2020-12-23 02:48:33,649 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:33,650 : INFO : entropies processed\n",
      "2020-12-23 02:48:33,651 : INFO : extropies processed\n",
      "2020-12-23 02:48:33,652 : INFO : token count processed\n",
      "2020-12-23 02:48:33,653 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:33,654 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:33,655 : INFO : vocab #2480\n",
      "2020-12-23 02:48:33,656 : INFO : diff #set()\n",
      "2020-12-23 02:48:33,912 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:34,037 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.2482806128991428, 0.44478433620014485], [0.9590940065681934, 0.040905993], [0.0, 0.0], [3.997670276487613, 5.749308601266266, 5.9339046844951335, 3.8130741932587453, 1.9362344080075204, 0.18459608322886734]]\n",
      "2020-12-23 02:48:34,040 : INFO : Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:34,041 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:34,042 : INFO : built Dictionary(70 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 236 corpus positions)\n",
      "2020-12-23 02:48:34,066 : INFO : token count processed\n",
      "2020-12-23 02:48:34,068 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:34,069 : INFO : frequencies processed\n",
      "2020-12-23 02:48:34,070 : INFO : token count processed\n",
      "2020-12-23 02:48:34,071 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:34,072 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:34,073 : INFO : vocab #2480\n",
      "2020-12-23 02:48:34,074 : INFO : diff #set()\n",
      "2020-12-23 02:48:34,333 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:34,461 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.2500351178466955, 0.4444375076941062], [0.9685547947883606, 0.031445205], [nan, nan], [3.997670276487613, 5.015422548793484, 5.334086554590561, 3.679006270690536, 1.336416278102948, 0.31866400579707754]]\n",
      "2020-12-23 02:48:34,464 : INFO : Removed 3 and 81 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:34,465 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:34,466 : INFO : built Dictionary(97 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 362 corpus positions)\n",
      "2020-12-23 02:48:34,492 : INFO : token count processed\n",
      "2020-12-23 02:48:34,494 : INFO : frequencies processed\n",
      "2020-12-23 02:48:34,622 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:34,623 : INFO : entropies processed\n",
      "2020-12-23 02:48:34,626 : INFO : extropies processed\n",
      "2020-12-23 02:48:34,627 : INFO : token count processed\n",
      "2020-12-23 02:48:34,627 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:34,628 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:34,628 : INFO : vocab #2480\n",
      "2020-12-23 02:48:34,629 : INFO : diff #set()\n",
      "2020-12-23 02:48:34,885 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:35,011 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.224732407135156, 0.4494922610884808], [0.9330888912081718, 0.06691111], [1.0, 1.0], [3.997670276487613, 6.030001281822029, 6.182962769835598, 3.844708788474044, 2.1852924933479847, 0.15296148801356857]]\n",
      "2020-12-23 02:48:35,013 : INFO : Removed 3 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:35,014 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:35,015 : INFO : built Dictionary(87 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 325 corpus positions)\n",
      "2020-12-23 02:48:35,040 : INFO : token count processed\n",
      "2020-12-23 02:48:35,044 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:35,045 : INFO : frequencies processed\n",
      "2020-12-23 02:48:35,047 : INFO : token count processed\n",
      "2020-12-23 02:48:35,049 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:35,049 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:35,050 : INFO : vocab #2480\n",
      "2020-12-23 02:48:35,051 : INFO : diff #set()\n",
      "2020-12-23 02:48:35,311 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:35,442 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.243250292883594, 0.44578173161165463], [0.9570098221302032, 0.042990178], [nan, nan], [3.997670276487613, 5.9537092545441395, 6.1444415357433915, 3.8069379952883615, 2.1467712592557784, 0.190732281199252]]\n",
      "2020-12-23 02:48:35,444 : INFO : Removed 3 and 90 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:35,445 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:35,447 : INFO : built Dictionary(97 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 326 corpus positions)\n",
      "2020-12-23 02:48:35,478 : INFO : token count processed\n",
      "2020-12-23 02:48:35,480 : INFO : frequencies processed\n",
      "2020-12-23 02:48:35,607 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:35,608 : INFO : entropies processed\n",
      "2020-12-23 02:48:35,608 : INFO : extropies processed\n",
      "2020-12-23 02:48:35,610 : INFO : token count processed\n",
      "2020-12-23 02:48:35,611 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:35,612 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:35,613 : INFO : vocab #2480\n",
      "2020-12-23 02:48:35,613 : INFO : diff #set()\n",
      "2020-12-23 02:48:35,876 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:36,004 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.2191659347269377, 0.4506197505789702], [0.9297746866941452, 0.07022531], [0.0, 0.0], [3.997670276487613, 6.184756445474906, 6.345294088295403, 3.8371326336671165, 2.34762381180779, 0.16053764282049698]]\n",
      "2020-12-23 02:48:36,006 : INFO : Removed 3 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:36,007 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:36,009 : INFO : built Dictionary(105 unique tokens: ['autom', 'chang', 'com', 'consol', 'fix']...) from 2 documents (total 439 corpus positions)\n",
      "2020-12-23 02:48:36,047 : INFO : token count processed\n",
      "2020-12-23 02:48:36,052 : INFO : frequencies processed\n",
      "2020-12-23 02:48:36,179 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:36,179 : INFO : entropies processed\n",
      "2020-12-23 02:48:36,180 : INFO : extropies processed\n",
      "2020-12-23 02:48:36,181 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:48:36,182 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:36,183 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:36,183 : INFO : vocab #2480\n",
      "2020-12-23 02:48:36,184 : INFO : diff #set()\n",
      "2020-12-23 02:48:36,451 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:36,578 : INFO : Computed distances or similarities ('283', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.22858375045974, 0.44871546774659354], [0.9188753366470337, 0.08112466], [1.0, 1.0], [3.997670276487613, 6.212221456585881, 6.348454866002158, 3.861436867071336, 2.350784589514545, 0.1362334094162776]]\n",
      "2020-12-23 02:48:36,580 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:36,581 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:36,582 : INFO : built Dictionary(119 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 413 corpus positions)\n",
      "2020-12-23 02:48:36,602 : INFO : token count processed\n",
      "2020-12-23 02:48:36,604 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:36,606 : INFO : frequencies processed\n",
      "2020-12-23 02:48:36,607 : INFO : token count processed\n",
      "2020-12-23 02:48:36,609 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:36,610 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:36,611 : INFO : vocab #2480\n",
      "2020-12-23 02:48:36,612 : INFO : diff #set()\n",
      "2020-12-23 02:48:36,869 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:36,997 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.303390283511136, 0.434142666641654], [0.9962070190813392, 0.003792981], [nan, nan], [2.2516291673878226, 6.301552355933639, 6.350493034095955, 2.2026884892255065, 4.0988638667081325, 0.04894067816231562]]\n",
      "2020-12-23 02:48:36,999 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:37,000 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:37,001 : INFO : built Dictionary(157 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 649 corpus positions)\n",
      "2020-12-23 02:48:37,032 : INFO : token count processed\n",
      "2020-12-23 02:48:37,034 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:37,035 : INFO : frequencies processed\n",
      "2020-12-23 02:48:37,036 : INFO : token count processed\n",
      "2020-12-23 02:48:37,037 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:37,038 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:37,039 : INFO : vocab #2480\n",
      "2020-12-23 02:48:37,040 : INFO : diff #set()\n",
      "2020-12-23 02:48:37,297 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:37,424 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.3127475071598733, 0.4323861540890953], [0.9992617688840255, 0.0007382311], [nan, nan], [2.2516291673878226, 6.739005504021667, 6.771593195217958, 2.2190414761915314, 4.519964027830136, 0.03258769119629168]]\n",
      "2020-12-23 02:48:37,427 : INFO : Removed 0 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:37,428 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:37,430 : INFO : built Dictionary(101 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 500 corpus positions)\n",
      "2020-12-23 02:48:37,453 : INFO : token count processed\n",
      "2020-12-23 02:48:37,457 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:37,458 : INFO : frequencies processed\n",
      "2020-12-23 02:48:37,461 : INFO : token count processed\n",
      "2020-12-23 02:48:37,462 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:37,463 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:37,463 : INFO : vocab #2480\n",
      "2020-12-23 02:48:37,464 : INFO : diff #set()\n",
      "2020-12-23 02:48:37,725 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:37,854 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.3148819537067804, 0.43198747063482756], [1.0, 0.0], [nan, nan], [2.2516291673878226, 5.870833373337847, 5.914313120783648, 2.2081494199420213, 3.6626839533958253, 0.043479747445800854]]\n",
      "2020-12-23 02:48:37,857 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:37,858 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:37,858 : INFO : built Dictionary(61 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 183 corpus positions)\n",
      "2020-12-23 02:48:37,866 : INFO : token count processed\n",
      "2020-12-23 02:48:37,868 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:37,869 : INFO : frequencies processed\n",
      "2020-12-23 02:48:37,870 : INFO : token count processed\n",
      "2020-12-23 02:48:37,871 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:37,872 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:37,872 : INFO : vocab #2480\n",
      "2020-12-23 02:48:37,873 : INFO : diff #set()\n",
      "2020-12-23 02:48:38,129 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:38,257 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.31318331495934, 0.43230469177821196], [1.0, 0.0], [nan, nan], [2.2516291673878226, 5.371881234145534, 5.473525979694796, 2.1499844218385613, 3.2218968123069733, 0.10164474554926173]]\n",
      "2020-12-23 02:48:38,259 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:38,260 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:38,261 : INFO : built Dictionary(52 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 135 corpus positions)\n",
      "2020-12-23 02:48:38,267 : INFO : token count processed\n",
      "2020-12-23 02:48:38,270 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:38,270 : INFO : frequencies processed\n",
      "2020-12-23 02:48:38,271 : INFO : token count processed\n",
      "2020-12-23 02:48:38,272 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:38,273 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:38,274 : INFO : vocab #2480\n",
      "2020-12-23 02:48:38,275 : INFO : diff #set()\n",
      "2020-12-23 02:48:38,534 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:38,662 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.3155073520704996, 0.4318707945823673], [0.997179135447368, 0.0028208646], [nan, nan], [2.2516291673878226, 4.85108279267097, 4.997263924732154, 2.1054480353266385, 2.7456347573443316, 0.14618113206118455]]\n",
      "2020-12-23 02:48:38,664 : INFO : Removed 0 and 127 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:38,665 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:38,667 : INFO : built Dictionary(92 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 408 corpus positions)\n",
      "2020-12-23 02:48:38,688 : INFO : token count processed\n",
      "2020-12-23 02:48:38,690 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:38,692 : INFO : frequencies processed\n",
      "2020-12-23 02:48:38,693 : INFO : token count processed\n",
      "2020-12-23 02:48:38,695 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:38,697 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:38,699 : INFO : vocab #2480\n",
      "2020-12-23 02:48:38,700 : INFO : diff #set()\n",
      "2020-12-23 02:48:38,957 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:39,085 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.3138051654283278, 0.43218850702793793], [1.0, 0.0], [nan, nan], [2.2516291673878226, 6.139571208108155, 6.184712127234105, 2.2064882482618717, 3.9330829598462826, 0.04514091912595042]]\n",
      "2020-12-23 02:48:39,087 : INFO : Removed 0 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:39,088 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:39,089 : INFO : built Dictionary(78 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 418 corpus positions)\n",
      "2020-12-23 02:48:39,100 : INFO : token count processed\n",
      "2020-12-23 02:48:39,104 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:48:39,105 : INFO : frequencies processed\n",
      "2020-12-23 02:48:39,107 : INFO : token count processed\n",
      "2020-12-23 02:48:39,108 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:39,109 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:39,110 : INFO : vocab #2480\n",
      "2020-12-23 02:48:39,112 : INFO : diff #set()\n",
      "2020-12-23 02:48:39,371 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:39,498 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.3143221500309326, 0.43209196264514615], [1.0, 0.0], [nan, nan], [2.2516291673878226, 5.609710627339259, 5.666592308583599, 2.1947474861434824, 3.4149631411957766, 0.05688168124433979]]\n",
      "2020-12-23 02:48:39,501 : INFO : Removed 0 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:39,501 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:39,503 : INFO : built Dictionary(170 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 1078 corpus positions)\n",
      "2020-12-23 02:48:39,530 : INFO : token count processed\n",
      "2020-12-23 02:48:39,532 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:39,535 : INFO : frequencies processed\n",
      "2020-12-23 02:48:39,537 : INFO : token count processed\n",
      "2020-12-23 02:48:39,539 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:39,541 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:39,542 : INFO : vocab #2480\n",
      "2020-12-23 02:48:39,543 : INFO : diff #set()\n",
      "2020-12-23 02:48:39,911 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:40,039 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.3154267440032297, 0.43188582950849996], [0.9996885004511569, 0.00031149955], [nan, nan], [2.2516291673878226, 7.2441902753576075, 7.261953963099239, 2.233865479646192, 5.010324795711416, 0.017763687741631173]]\n",
      "2020-12-23 02:48:40,041 : INFO : Removed 0 and 136 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:40,042 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:40,044 : INFO : built Dictionary(132 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 685 corpus positions)\n",
      "2020-12-23 02:48:40,070 : INFO : token count processed\n",
      "2020-12-23 02:48:40,074 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:40,076 : INFO : frequencies processed\n",
      "2020-12-23 02:48:40,079 : INFO : token count processed\n",
      "2020-12-23 02:48:40,081 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:40,083 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:40,084 : INFO : vocab #2480\n",
      "2020-12-23 02:48:40,088 : INFO : diff #set()\n",
      "2020-12-23 02:48:40,341 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:40,467 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.313252182746486, 0.43229182164337854], [0.999800236633746, 0.00019976337], [nan, nan], [2.2516291673878226, 6.2567074920449475, 6.289803287520927, 2.2185333719118425, 4.038174120133105, 0.03309579547597963]]\n",
      "2020-12-23 02:48:40,470 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:40,471 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:40,472 : INFO : built Dictionary(77 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 195 corpus positions)\n",
      "2020-12-23 02:48:40,482 : INFO : token count processed\n",
      "2020-12-23 02:48:40,485 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:40,485 : INFO : frequencies processed\n",
      "2020-12-23 02:48:40,486 : INFO : token count processed\n",
      "2020-12-23 02:48:40,487 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:40,488 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:40,489 : INFO : vocab #2480\n",
      "2020-12-23 02:48:40,490 : INFO : diff #set()\n",
      "2020-12-23 02:48:40,748 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:40,876 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.312452281112098, 0.43244135594403826], [1.0, 0.0], [nan, nan], [2.2516291673878226, 5.7680018917339435, 5.8542849880417345, 2.165346071080031, 3.602655820653912, 0.08628309630779096]]\n",
      "2020-12-23 02:48:40,879 : INFO : Removed 0 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:40,880 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:40,882 : INFO : built Dictionary(175 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 736 corpus positions)\n",
      "2020-12-23 02:48:40,914 : INFO : token count processed\n",
      "2020-12-23 02:48:40,917 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:40,919 : INFO : frequencies processed\n",
      "2020-12-23 02:48:40,921 : INFO : token count processed\n",
      "2020-12-23 02:48:40,922 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:40,924 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:40,925 : INFO : vocab #2480\n",
      "2020-12-23 02:48:40,926 : INFO : diff #set()\n",
      "2020-12-23 02:48:41,178 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:41,304 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.314284077166314, 0.43209907109780277], [0.9973719511181116, 0.0026280489], [nan, nan], [2.2516291673878226, 6.846479111193757, 6.8746585700337945, 2.2234497085477845, 4.623029402645972, 0.028179458840037697]]\n",
      "2020-12-23 02:48:41,307 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:41,308 : INFO : built Dictionary(25 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 39 corpus positions)\n",
      "2020-12-23 02:48:41,311 : INFO : token count processed\n",
      "2020-12-23 02:48:41,314 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:41,314 : INFO : frequencies processed\n",
      "2020-12-23 02:48:41,315 : INFO : token count processed\n",
      "2020-12-23 02:48:41,316 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:41,317 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:41,318 : INFO : vocab #2480\n",
      "2020-12-23 02:48:41,319 : INFO : diff #set()\n",
      "2020-12-23 02:48:41,580 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:41,707 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/fireException.py')[[1.3014325419043313, 0.43451197538579395], [0.9911001697182655, 0.00889983], [nan, nan], [2.2516291673878226, 4.165013816065912, 4.490029141717277, 1.9266138417364571, 2.2383999743294543, 0.32501532565136504]]\n",
      "2020-12-23 02:48:41,710 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:41,711 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:41,712 : INFO : built Dictionary(53 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 146 corpus positions)\n",
      "2020-12-23 02:48:41,720 : INFO : token count processed\n",
      "2020-12-23 02:48:41,722 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:41,723 : INFO : frequencies processed\n",
      "2020-12-23 02:48:41,724 : INFO : token count processed\n",
      "2020-12-23 02:48:41,725 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:41,725 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:41,726 : INFO : vocab #2480\n",
      "2020-12-23 02:48:41,727 : INFO : diff #set()\n",
      "2020-12-23 02:48:41,986 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:42,113 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.3091835130636078, 0.433053498928413], [1.0, 0.0], [nan, nan], [2.2516291673878226, 5.449968864419248, 5.559707627469337, 2.141890404337734, 3.3080784600815147, 0.109738763050089]]\n",
      "2020-12-23 02:48:42,116 : INFO : Removed 0 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:42,117 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:42,118 : INFO : built Dictionary(146 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 583 corpus positions)\n",
      "2020-12-23 02:48:42,144 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:48:42,147 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:42,147 : INFO : frequencies processed\n",
      "2020-12-23 02:48:42,151 : INFO : token count processed\n",
      "2020-12-23 02:48:42,153 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:42,155 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:42,157 : INFO : vocab #2480\n",
      "2020-12-23 02:48:42,158 : INFO : diff #set()\n",
      "2020-12-23 02:48:42,419 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:42,547 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.3108203838541048, 0.4327467452628875], [1.0, 0.0], [nan, nan], [2.2516291673878226, 6.530294129310484, 6.563868444688679, 2.218054852009627, 4.312239277300856, 0.03357431537819533]]\n",
      "2020-12-23 02:48:42,550 : INFO : Removed 0 and 142 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:42,551 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:42,552 : INFO : built Dictionary(119 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 599 corpus positions)\n",
      "2020-12-23 02:48:42,571 : INFO : token count processed\n",
      "2020-12-23 02:48:42,573 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:42,574 : INFO : frequencies processed\n",
      "2020-12-23 02:48:42,575 : INFO : token count processed\n",
      "2020-12-23 02:48:42,576 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:42,577 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:42,578 : INFO : vocab #2480\n",
      "2020-12-23 02:48:42,578 : INFO : diff #set()\n",
      "2020-12-23 02:48:42,836 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:42,963 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.3096154692511528, 0.4329725070313243], [0.9978174741845578, 0.0021825258], [nan, nan], [2.2516291673878226, 6.470272233491701, 6.5040095817248105, 2.2178918191547137, 4.252380414336988, 0.033737348233109365]]\n",
      "2020-12-23 02:48:42,966 : INFO : Removed 0 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:42,966 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:42,967 : INFO : built Dictionary(120 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 388 corpus positions)\n",
      "2020-12-23 02:48:42,998 : INFO : token count processed\n",
      "2020-12-23 02:48:43,003 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:43,004 : INFO : frequencies processed\n",
      "2020-12-23 02:48:43,006 : INFO : token count processed\n",
      "2020-12-23 02:48:43,008 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:43,009 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:43,009 : INFO : vocab #2480\n",
      "2020-12-23 02:48:43,011 : INFO : diff #set()\n",
      "2020-12-23 02:48:43,274 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:43,405 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.3114320684308556, 0.4326322255617326], [0.9995200775156263, 0.00047992248], [nan, nan], [2.2516291673878226, 6.550038223589686, 6.595400118326941, 2.2062672726505674, 4.343770950939119, 0.045361894737254715]]\n",
      "2020-12-23 02:48:43,408 : INFO : Removed 0 and 70 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:43,409 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:43,410 : INFO : built Dictionary(75 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 304 corpus positions)\n",
      "2020-12-23 02:48:43,420 : INFO : token count processed\n",
      "2020-12-23 02:48:43,422 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:43,423 : INFO : frequencies processed\n",
      "2020-12-23 02:48:43,424 : INFO : token count processed\n",
      "2020-12-23 02:48:43,425 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:43,426 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:43,427 : INFO : vocab #2480\n",
      "2020-12-23 02:48:43,428 : INFO : diff #set()\n",
      "2020-12-23 02:48:43,686 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:43,814 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.311537037717599, 0.43261257928507835], [1.0, 0.0], [nan, nan], [2.2516291673878226, 5.860525481261383, 5.921232901799956, 2.1909217468492495, 3.6696037344121337, 0.06070742053857359]]\n",
      "2020-12-23 02:48:43,817 : INFO : Removed 0 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:43,818 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:43,818 : INFO : built Dictionary(46 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 342 corpus positions)\n",
      "2020-12-23 02:48:43,826 : INFO : token count processed\n",
      "2020-12-23 02:48:43,828 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:43,829 : INFO : frequencies processed\n",
      "2020-12-23 02:48:43,830 : INFO : token count processed\n",
      "2020-12-23 02:48:43,831 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:43,831 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:43,832 : INFO : vocab #2480\n",
      "2020-12-23 02:48:43,833 : INFO : diff #set()\n",
      "2020-12-23 02:48:44,091 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:44,218 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.3146009721611454, 0.43203991185845697], [1.0, 0.0], [nan, nan], [2.2516291673878226, 5.945464049777852, 5.994284416553132, 2.2028088006125426, 3.7426552491653093, 0.04882036677527957]]\n",
      "2020-12-23 02:48:44,221 : INFO : Removed 0 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:44,222 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:44,223 : INFO : built Dictionary(194 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 889 corpus positions)\n",
      "2020-12-23 02:48:44,256 : INFO : token count processed\n",
      "2020-12-23 02:48:44,258 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:44,259 : INFO : frequencies processed\n",
      "2020-12-23 02:48:44,260 : INFO : token count processed\n",
      "2020-12-23 02:48:44,261 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:44,262 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:44,262 : INFO : vocab #2480\n",
      "2020-12-23 02:48:44,263 : INFO : diff #set()\n",
      "2020-12-23 02:48:44,520 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:44,649 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.3115270473716802, 0.4326144490228004], [0.999824455473572, 0.00017554453], [nan, nan], [2.2516291673878226, 6.811563897304216, 6.83632155012749, 2.2268715145645483, 4.584692382739668, 0.0247576528232738]]\n",
      "2020-12-23 02:48:44,652 : INFO : Removed 0 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:44,653 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:44,654 : INFO : built Dictionary(215 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 984 corpus positions)\n",
      "2020-12-23 02:48:44,694 : INFO : token count processed\n",
      "2020-12-23 02:48:44,697 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:44,698 : INFO : frequencies processed\n",
      "2020-12-23 02:48:44,699 : INFO : token count processed\n",
      "2020-12-23 02:48:44,700 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:44,701 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:44,701 : INFO : vocab #2480\n",
      "2020-12-23 02:48:44,702 : INFO : diff #set()\n",
      "2020-12-23 02:48:44,959 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:45,086 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.3095408059781133, 0.43298650424861845], [0.9971097570378333, 0.002890243], [nan, nan], [2.2516291673878226, 7.502034948968415, 7.519683611109407, 2.23398050524683, 5.268054443721585, 0.017648662140992144]]\n",
      "2020-12-23 02:48:45,089 : INFO : Removed 0 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:45,090 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:48:45,091 : INFO : built Dictionary(259 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 1557 corpus positions)\n",
      "2020-12-23 02:48:45,138 : INFO : token count processed\n",
      "2020-12-23 02:48:45,143 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:45,144 : INFO : frequencies processed\n",
      "2020-12-23 02:48:45,147 : INFO : token count processed\n",
      "2020-12-23 02:48:45,149 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:45,149 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:45,150 : INFO : vocab #2480\n",
      "2020-12-23 02:48:45,151 : INFO : diff #set()\n",
      "2020-12-23 02:48:45,414 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:45,545 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.309338826762079, 0.4330243740811732], [0.9994364848244004, 0.0005635152], [nan, nan], [2.2516291673878226, 7.39180093901977, 7.406525351307998, 2.2369047550995944, 5.154896183920176, 0.01472441228822774]]\n",
      "2020-12-23 02:48:45,547 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:45,548 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:45,549 : INFO : built Dictionary(43 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 112 corpus positions)\n",
      "2020-12-23 02:48:45,555 : INFO : token count processed\n",
      "2020-12-23 02:48:45,557 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:45,558 : INFO : frequencies processed\n",
      "2020-12-23 02:48:45,559 : INFO : token count processed\n",
      "2020-12-23 02:48:45,560 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:45,560 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:45,561 : INFO : vocab #2480\n",
      "2020-12-23 02:48:45,562 : INFO : diff #set()\n",
      "2020-12-23 02:48:45,821 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:45,949 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.304539700002886, 0.43392613284064824], [1.0, 0.0], [nan, nan], [2.2516291673878226, 4.927561309677364, 5.084195024791942, 2.0949954522732437, 2.8325658574041195, 0.15663371511457846]]\n",
      "2020-12-23 02:48:45,951 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:45,952 : INFO : built Dictionary(11 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 13 corpus positions)\n",
      "2020-12-23 02:48:45,956 : INFO : token count processed\n",
      "2020-12-23 02:48:45,961 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:45,962 : INFO : frequencies processed\n",
      "2020-12-23 02:48:45,963 : INFO : token count processed\n",
      "2020-12-23 02:48:45,965 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:45,966 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:45,967 : INFO : vocab #2480\n",
      "2020-12-23 02:48:45,969 : INFO : diff #set()\n",
      "2020-12-23 02:48:46,242 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:46,371 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.3148528700927666, 0.43199289808856206], [1.0, 0.0], [nan, nan], [2.2516291673878226, 2.5216406363433186, 3.3927474104487847, 1.3805223932823565, 1.141118243060962, 0.8711067741054661]]\n",
      "2020-12-23 02:48:46,375 : INFO : Removed 0 and 530 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:46,376 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:46,380 : INFO : built Dictionary(332 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 2885 corpus positions)\n",
      "2020-12-23 02:48:46,458 : INFO : token count processed\n",
      "2020-12-23 02:48:46,461 : INFO : frequencies processed\n",
      "2020-12-23 02:48:46,588 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:46,588 : INFO : entropies processed\n",
      "2020-12-23 02:48:46,589 : INFO : extropies processed\n",
      "2020-12-23 02:48:46,591 : INFO : token count processed\n",
      "2020-12-23 02:48:46,592 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:46,593 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:46,594 : INFO : vocab #2480\n",
      "2020-12-23 02:48:46,595 : INFO : diff #set()\n",
      "2020-12-23 02:48:46,866 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:46,995 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.3040628455436574, 0.43401593925014836], [0.9963301287498325, 0.0036698713], [0.0, 0.0], [2.2516291673878226, 7.480007711014331, 7.488849448774753, 2.2427874296274, 5.23722028138693, 0.008841737760421964]]\n",
      "2020-12-23 02:48:46,998 : INFO : Removed 0 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:46,998 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:47,000 : INFO : built Dictionary(210 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 1034 corpus positions)\n",
      "2020-12-23 02:48:47,037 : INFO : token count processed\n",
      "2020-12-23 02:48:47,041 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:47,042 : INFO : frequencies processed\n",
      "2020-12-23 02:48:47,044 : INFO : token count processed\n",
      "2020-12-23 02:48:47,045 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:47,046 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:47,046 : INFO : vocab #2480\n",
      "2020-12-23 02:48:47,047 : INFO : diff #set()\n",
      "2020-12-23 02:48:47,304 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:47,431 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.3109497862577928, 0.43272251346462065], [0.9968211243394762, 0.0031788757], [nan, nan], [2.2516291673878226, 7.131331012509435, 7.1522671295548275, 2.2306930503424294, 4.900637962167005, 0.020936117045392777]]\n",
      "2020-12-23 02:48:47,433 : INFO : Removed 0 and 166 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:47,434 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:47,436 : INFO : built Dictionary(199 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 853 corpus positions)\n",
      "2020-12-23 02:48:47,466 : INFO : token count processed\n",
      "2020-12-23 02:48:47,471 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:47,472 : INFO : frequencies processed\n",
      "2020-12-23 02:48:47,474 : INFO : token count processed\n",
      "2020-12-23 02:48:47,475 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:47,476 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:47,476 : INFO : vocab #2480\n",
      "2020-12-23 02:48:47,477 : INFO : diff #set()\n",
      "2020-12-23 02:48:47,732 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:47,860 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.3144936172965838, 0.4320599514843501], [0.9997642243397422, 0.00023577566], [nan, nan], [2.2516291673878226, 7.203742744794778, 7.226672881515722, 2.2286990306668786, 4.975043714127899, 0.02293013672094446]]\n",
      "2020-12-23 02:48:47,862 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:47,863 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:47,864 : INFO : built Dictionary(57 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 185 corpus positions)\n",
      "2020-12-23 02:48:47,877 : INFO : token count processed\n",
      "2020-12-23 02:48:47,881 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:47,882 : INFO : frequencies processed\n",
      "2020-12-23 02:48:47,883 : INFO : token count processed\n",
      "2020-12-23 02:48:47,884 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:47,886 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:47,886 : INFO : vocab #2480\n",
      "2020-12-23 02:48:47,888 : INFO : diff #set()\n",
      "2020-12-23 02:48:48,149 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:48,278 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.2992027574478744, 0.43493336842984853], [0.9932448701001704, 0.00675513], [nan, nan], [2.2516291673878226, 5.195502554608948, 5.306131405141738, 2.141000316855033, 3.054502237753915, 0.11062885053278926]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:48:48,280 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:48,281 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:48,282 : INFO : built Dictionary(63 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 246 corpus positions)\n",
      "2020-12-23 02:48:48,291 : INFO : token count processed\n",
      "2020-12-23 02:48:48,293 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:48,294 : INFO : frequencies processed\n",
      "2020-12-23 02:48:48,295 : INFO : token count processed\n",
      "2020-12-23 02:48:48,296 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:48,297 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:48,297 : INFO : vocab #2480\n",
      "2020-12-23 02:48:48,298 : INFO : diff #set()\n",
      "2020-12-23 02:48:48,557 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:48,684 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.3070836962604608, 0.4334476471837127], [0.9991338709369302, 0.00086612906], [nan, nan], [2.2516291673878226, 5.32027245610305, 5.410631733161254, 2.161269890329618, 3.1590025657734313, 0.09035927705820423]]\n",
      "2020-12-23 02:48:48,687 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:48,687 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:48,689 : INFO : built Dictionary(162 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 486 corpus positions)\n",
      "2020-12-23 02:48:48,722 : INFO : token count processed\n",
      "2020-12-23 02:48:48,725 : INFO : frequencies processed\n",
      "2020-12-23 02:48:48,852 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:48,853 : INFO : entropies processed\n",
      "2020-12-23 02:48:48,853 : INFO : extropies processed\n",
      "2020-12-23 02:48:48,854 : INFO : token count processed\n",
      "2020-12-23 02:48:48,855 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:48,856 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:48,857 : INFO : vocab #2480\n",
      "2020-12-23 02:48:48,858 : INFO : diff #set()\n",
      "2020-12-23 02:48:49,116 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:49,243 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.309321473333875, 0.43302762804883116], [0.9955241926945746, 0.0044758073], [0.0, 0.0], [2.2516291673878226, 6.898202761357263, 6.9309119720648615, 2.2189199566802236, 4.6792828046770385, 0.03270921070759858]]\n",
      "2020-12-23 02:48:49,246 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:49,247 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:49,249 : INFO : built Dictionary(127 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 508 corpus positions)\n",
      "2020-12-23 02:48:49,273 : INFO : token count processed\n",
      "2020-12-23 02:48:49,281 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:49,283 : INFO : frequencies processed\n",
      "2020-12-23 02:48:49,284 : INFO : token count processed\n",
      "2020-12-23 02:48:49,285 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:49,287 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:49,288 : INFO : vocab #2480\n",
      "2020-12-23 02:48:49,289 : INFO : diff #set()\n",
      "2020-12-23 02:48:49,548 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:49,678 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.30624367958122, 0.43360552436574495], [0.9980115501675755, 0.0019884498], [nan, nan], [2.2516291673878226, 6.388500481644799, 6.430947409276996, 2.2091822397556253, 4.179318241889174, 0.04244692763219682]]\n",
      "2020-12-23 02:48:49,681 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:49,682 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:49,683 : INFO : built Dictionary(48 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 172 corpus positions)\n",
      "2020-12-23 02:48:49,689 : INFO : token count processed\n",
      "2020-12-23 02:48:49,691 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:49,692 : INFO : frequencies processed\n",
      "2020-12-23 02:48:49,693 : INFO : token count processed\n",
      "2020-12-23 02:48:49,694 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:49,695 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:49,696 : INFO : vocab #2480\n",
      "2020-12-23 02:48:49,697 : INFO : diff #set()\n",
      "2020-12-23 02:48:49,956 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:50,084 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.2994563555965184, 0.4348854013106862], [0.9945392878726125, 0.005460712], [nan, nan], [2.2516291673878226, 4.8191513650620195, 4.947458638747652, 2.1233218937021894, 2.6958294713598296, 0.12830727368563277]]\n",
      "2020-12-23 02:48:50,086 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:50,086 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:50,087 : INFO : built Dictionary(53 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 227 corpus positions)\n",
      "2020-12-23 02:48:50,093 : INFO : token count processed\n",
      "2020-12-23 02:48:50,095 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:50,096 : INFO : frequencies processed\n",
      "2020-12-23 02:48:50,097 : INFO : token count processed\n",
      "2020-12-23 02:48:50,097 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:50,098 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:50,099 : INFO : vocab #2480\n",
      "2020-12-23 02:48:50,099 : INFO : diff #set()\n",
      "2020-12-23 02:48:50,357 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:50,485 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.3062898994678969, 0.4335968345656451], [1.0, 0.0], [nan, nan], [2.2516291673878226, 5.062480936779194, 5.16407617923166, 2.1500339249353555, 2.9124470118438377, 0.10159524245246665]]\n",
      "2020-12-23 02:48:50,488 : INFO : Removed 0 and 223 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:50,489 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:50,492 : INFO : built Dictionary(244 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 1781 corpus positions)\n",
      "2020-12-23 02:48:50,544 : INFO : token count processed\n",
      "2020-12-23 02:48:50,549 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:50,550 : INFO : frequencies processed\n",
      "2020-12-23 02:48:50,551 : INFO : token count processed\n",
      "2020-12-23 02:48:50,552 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:50,553 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:50,554 : INFO : vocab #2480\n",
      "2020-12-23 02:48:50,555 : INFO : diff #set()\n",
      "2020-12-23 02:48:50,811 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:50,943 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.3061826056293389, 0.433617007412606], [0.9971804297529161, 0.0028195702], [nan, nan], [2.2516291673878226, 7.185085743102134, 7.199728799616771, 2.236986110873186, 4.948099632228949, 0.014643056514636932]]\n",
      "2020-12-23 02:48:50,946 : INFO : Removed 0 and 148 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:50,947 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:50,948 : INFO : built Dictionary(161 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 756 corpus positions)\n",
      "2020-12-23 02:48:50,974 : INFO : token count processed\n",
      "2020-12-23 02:48:50,976 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:50,979 : INFO : frequencies processed\n",
      "2020-12-23 02:48:50,980 : INFO : token count processed\n",
      "2020-12-23 02:48:50,983 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:50,984 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:50,984 : INFO : vocab #2480\n",
      "2020-12-23 02:48:50,986 : INFO : diff #set()\n",
      "2020-12-23 02:48:51,243 : INFO : alphabet #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:48:51,371 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.3113740052112672, 0.43264309356485847], [0.9971981099806726, 0.00280189], [nan, nan], [2.2516291673878226, 6.591225336124281, 6.619987617107725, 2.222866886404379, 4.368358449719903, 0.028762280983444022]]\n",
      "2020-12-23 02:48:51,373 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:51,374 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:51,375 : INFO : built Dictionary(39 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 94 corpus positions)\n",
      "2020-12-23 02:48:51,380 : INFO : token count processed\n",
      "2020-12-23 02:48:51,383 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:51,384 : INFO : frequencies processed\n",
      "2020-12-23 02:48:51,385 : INFO : token count processed\n",
      "2020-12-23 02:48:51,386 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:51,386 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:51,387 : INFO : vocab #2480\n",
      "2020-12-23 02:48:51,388 : INFO : diff #set()\n",
      "2020-12-23 02:48:51,647 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:51,775 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.3139892821390093, 0.4321541191736283], [1.0, 0.0], [nan, nan], [2.2516291673878226, 4.7032114441396695, 4.888229746988014, 2.066610864539479, 2.636600579600191, 0.18501830284834409]]\n",
      "2020-12-23 02:48:51,778 : INFO : Removed 0 and 9 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:51,779 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:51,780 : INFO : built Dictionary(116 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 334 corpus positions)\n",
      "2020-12-23 02:48:51,794 : INFO : token count processed\n",
      "2020-12-23 02:48:51,797 : INFO : frequencies processed\n",
      "2020-12-23 02:48:51,925 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:51,926 : INFO : entropies processed\n",
      "2020-12-23 02:48:51,926 : INFO : extropies processed\n",
      "2020-12-23 02:48:51,928 : INFO : token count processed\n",
      "2020-12-23 02:48:51,929 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:51,930 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:51,930 : INFO : vocab #2480\n",
      "2020-12-23 02:48:51,931 : INFO : diff #set()\n",
      "2020-12-23 02:48:52,189 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:52,317 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.2994332560555006, 0.43488977006248153], [0.9943689284846187, 0.0056310715], [0.0, 0.0], [2.2516291673878226, 6.14228447828618, 6.195516830091635, 2.1983968155823677, 3.9438876627038124, 0.05323235180545538]]\n",
      "2020-12-23 02:48:52,319 : INFO : Removed 0 and 184 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:52,320 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:52,322 : INFO : built Dictionary(255 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 1124 corpus positions)\n",
      "2020-12-23 02:48:52,375 : INFO : token count processed\n",
      "2020-12-23 02:48:52,378 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:52,379 : INFO : frequencies processed\n",
      "2020-12-23 02:48:52,381 : INFO : token count processed\n",
      "2020-12-23 02:48:52,383 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:52,384 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:52,385 : INFO : vocab #2480\n",
      "2020-12-23 02:48:52,386 : INFO : diff #set()\n",
      "2020-12-23 02:48:52,646 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:52,777 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.3050031112232585, 0.43383889380925955], [0.994155309163034, 0.005844691], [nan, nan], [2.2516291673878226, 7.450178124335845, 7.468568107840294, 2.2332391838833736, 5.216938940452472, 0.018389983504449425]]\n",
      "2020-12-23 02:48:52,779 : INFO : Removed 0 and 16 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:52,780 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:52,781 : INFO : built Dictionary(57 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 172 corpus positions)\n",
      "2020-12-23 02:48:52,788 : INFO : token count processed\n",
      "2020-12-23 02:48:52,790 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:52,791 : INFO : frequencies processed\n",
      "2020-12-23 02:48:52,792 : INFO : token count processed\n",
      "2020-12-23 02:48:52,793 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:52,794 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:52,795 : INFO : vocab #2480\n",
      "2020-12-23 02:48:52,796 : INFO : diff #set()\n",
      "2020-12-23 02:48:53,054 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:53,182 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.295392079929518, 0.43565541971840643], [0.9933193637989461, 0.006680636], [nan, nan], [2.2516291673878226, 5.20665021947654, 5.316246912467891, 2.142032474396471, 3.0646177450800685, 0.10959669299135122]]\n",
      "2020-12-23 02:48:53,185 : INFO : Removed 0 and 89 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:53,186 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:53,188 : INFO : built Dictionary(126 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 510 corpus positions)\n",
      "2020-12-23 02:48:53,213 : INFO : token count processed\n",
      "2020-12-23 02:48:53,215 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:53,216 : INFO : frequencies processed\n",
      "2020-12-23 02:48:53,218 : INFO : token count processed\n",
      "2020-12-23 02:48:53,219 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:53,220 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:53,221 : INFO : vocab #2480\n",
      "2020-12-23 02:48:53,222 : INFO : diff #set()\n",
      "2020-12-23 02:48:53,475 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:53,602 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.313058067625103, 0.43232810018761647], [0.9996619058947545, 0.0003380941], [nan, nan], [2.2516291673878226, 6.524718477352, 6.562820036534957, 2.213527608204865, 4.311190869147135, 0.03810155918295699]]\n",
      "2020-12-23 02:48:53,604 : INFO : Removed 0 and 8 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:53,605 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:53,606 : INFO : built Dictionary(62 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 217 corpus positions)\n",
      "2020-12-23 02:48:53,614 : INFO : token count processed\n",
      "2020-12-23 02:48:53,617 : INFO : frequencies processed\n",
      "2020-12-23 02:48:53,745 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:48:53,746 : INFO : entropies processed\n",
      "2020-12-23 02:48:53,747 : INFO : extropies processed\n",
      "2020-12-23 02:48:53,748 : INFO : token count processed\n",
      "2020-12-23 02:48:53,749 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:53,750 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:53,750 : INFO : vocab #2480\n",
      "2020-12-23 02:48:53,751 : INFO : diff #set()\n",
      "2020-12-23 02:48:54,009 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:54,137 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.3018691351129126, 0.4344295619355213], [0.9883965374901891, 0.0116034625], [0.0, 0.0], [2.2516291673878226, 5.321859380715434, 5.408487139323807, 2.1650014087794496, 3.156857971935984, 0.08662775860837257]]\n",
      "2020-12-23 02:48:54,139 : INFO : Removed 0 and 119 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:54,140 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:54,141 : INFO : built Dictionary(142 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 835 corpus positions)\n",
      "2020-12-23 02:48:54,171 : INFO : token count processed\n",
      "2020-12-23 02:48:54,173 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:48:54,174 : INFO : frequencies processed\n",
      "2020-12-23 02:48:54,175 : INFO : token count processed\n",
      "2020-12-23 02:48:54,176 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:54,177 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:54,177 : INFO : vocab #2480\n",
      "2020-12-23 02:48:54,178 : INFO : diff #set()\n",
      "2020-12-23 02:48:54,431 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:54,558 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.3116573848103912, 0.43259005706073655], [0.9980368674732745, 0.0019631325], [nan, nan], [2.2516291673878226, 6.500767808767801, 6.529081601452393, 2.2233153747032297, 4.277452434064571, 0.028313792684592443]]\n",
      "2020-12-23 02:48:54,560 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:54,561 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:54,563 : INFO : built Dictionary(35 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 57 corpus positions)\n",
      "2020-12-23 02:48:54,574 : INFO : token count processed\n",
      "2020-12-23 02:48:54,576 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:54,577 : INFO : frequencies processed\n",
      "2020-12-23 02:48:54,579 : INFO : token count processed\n",
      "2020-12-23 02:48:54,580 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:54,581 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:54,582 : INFO : vocab #2480\n",
      "2020-12-23 02:48:54,583 : INFO : diff #set()\n",
      "2020-12-23 02:48:54,842 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:54,970 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.2993653153249975, 0.4349026200121914], [1.0, 0.0], [nan, nan], [2.2516291673878226, 4.736228843383063, 4.959033314310027, 2.028824696460859, 2.707404146922204, 0.2228044709269641]]\n",
      "2020-12-23 02:48:54,972 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:54,973 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:54,974 : INFO : built Dictionary(97 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 398 corpus positions)\n",
      "2020-12-23 02:48:54,994 : INFO : token count processed\n",
      "2020-12-23 02:48:54,997 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:54,997 : INFO : frequencies processed\n",
      "2020-12-23 02:48:54,998 : INFO : token count processed\n",
      "2020-12-23 02:48:54,999 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:55,000 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:55,001 : INFO : vocab #2480\n",
      "2020-12-23 02:48:55,001 : INFO : diff #set()\n",
      "2020-12-23 02:48:55,259 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:55,388 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.308927553491836, 0.43310150571319594], [0.9973608541768044, 0.0026391458], [nan, nan], [2.2516291673878226, 5.788442787590127, 5.846649530543436, 2.193422424434514, 3.5950203631556135, 0.058206742953308854]]\n",
      "2020-12-23 02:48:55,390 : INFO : Removed 0 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:55,391 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:55,392 : INFO : built Dictionary(54 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 84 corpus positions)\n",
      "2020-12-23 02:48:55,399 : INFO : token count processed\n",
      "2020-12-23 02:48:55,401 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:55,402 : INFO : frequencies processed\n",
      "2020-12-23 02:48:55,403 : INFO : token count processed\n",
      "2020-12-23 02:48:55,403 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:55,404 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:55,405 : INFO : vocab #2480\n",
      "2020-12-23 02:48:55,406 : INFO : diff #set()\n",
      "2020-12-23 02:48:55,665 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:55,792 : INFO : Computed distances or similarities ('285', 'sacp-python-common/setup.py')[[1.3126907784632322, 0.43239676022079065], [1.0, 0.0], [nan, nan], [2.2516291673878226, 5.370004292053436, 5.51547195209715, 2.106161507344109, 3.2638427847093276, 0.14546766004371392]]\n",
      "2020-12-23 02:48:55,795 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:55,795 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:55,796 : INFO : built Dictionary(80 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 329 corpus positions)\n",
      "2020-12-23 02:48:55,814 : INFO : token count processed\n",
      "2020-12-23 02:48:55,816 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:55,817 : INFO : frequencies processed\n",
      "2020-12-23 02:48:55,818 : INFO : token count processed\n",
      "2020-12-23 02:48:55,819 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:55,820 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:55,820 : INFO : vocab #2480\n",
      "2020-12-23 02:48:55,821 : INFO : diff #set()\n",
      "2020-12-23 02:48:56,073 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:56,199 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.3132360826976182, 0.43229483038057814], [1.0, 0.0], [nan, nan], [2.2516291673878226, 5.695663584743922, 5.760781285838369, 2.186511466293376, 3.509152118450546, 0.06511770109444637]]\n",
      "2020-12-23 02:48:56,202 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:56,203 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:56,203 : INFO : built Dictionary(41 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 104 corpus positions)\n",
      "2020-12-23 02:48:56,209 : INFO : token count processed\n",
      "2020-12-23 02:48:56,211 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:56,212 : INFO : frequencies processed\n",
      "2020-12-23 02:48:56,213 : INFO : token count processed\n",
      "2020-12-23 02:48:56,214 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:56,215 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:56,215 : INFO : vocab #2480\n",
      "2020-12-23 02:48:56,216 : INFO : diff #set()\n",
      "2020-12-23 02:48:56,475 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:56,603 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.3065430698696656, 0.4335492421810733], [1.0, 0.0], [nan, nan], [2.2516291673878226, 4.9004417692112465, 5.062828942704713, 2.0892419938943556, 2.8111997753168905, 0.16238717349346654]]\n",
      "2020-12-23 02:48:56,605 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:56,606 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:56,608 : INFO : built Dictionary(39 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 111 corpus positions)\n",
      "2020-12-23 02:48:56,618 : INFO : token count processed\n",
      "2020-12-23 02:48:56,620 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:56,621 : INFO : frequencies processed\n",
      "2020-12-23 02:48:56,622 : INFO : token count processed\n",
      "2020-12-23 02:48:56,623 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:56,624 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:56,625 : INFO : vocab #2480\n",
      "2020-12-23 02:48:56,626 : INFO : diff #set()\n",
      "2020-12-23 02:48:56,887 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:57,014 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.3143565497497787, 0.43208554019393297], [1.0, 0.0], [nan, nan], [2.2516291673878226, 4.778624108914332, 4.943096624069069, 2.0871566522330856, 2.6914674566812464, 0.1644725151547366]]\n",
      "2020-12-23 02:48:57,017 : INFO : Removed 0 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:57,018 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:57,019 : INFO : built Dictionary(40 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 142 corpus positions)\n",
      "2020-12-23 02:48:57,030 : INFO : token count processed\n",
      "2020-12-23 02:48:57,032 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:48:57,033 : INFO : frequencies processed\n",
      "2020-12-23 02:48:57,034 : INFO : token count processed\n",
      "2020-12-23 02:48:57,035 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:57,037 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:57,038 : INFO : vocab #2480\n",
      "2020-12-23 02:48:57,039 : INFO : diff #set()\n",
      "2020-12-23 02:48:57,313 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:57,442 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.3089430657445127, 0.4330985959922544], [1.0, 0.0], [nan, nan], [2.2516291673878226, 4.773880192225086, 4.915834737434494, 2.1096746221784137, 2.6642055700466716, 0.14195454520940842]]\n",
      "2020-12-23 02:48:57,445 : INFO : Removed 0 and 346 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:57,446 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:57,447 : INFO : built Dictionary(151 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 1967 corpus positions)\n",
      "2020-12-23 02:48:57,477 : INFO : token count processed\n",
      "2020-12-23 02:48:57,482 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:57,483 : INFO : frequencies processed\n",
      "2020-12-23 02:48:57,485 : INFO : token count processed\n",
      "2020-12-23 02:48:57,486 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:57,487 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:57,488 : INFO : vocab #2480\n",
      "2020-12-23 02:48:57,489 : INFO : diff #set()\n",
      "2020-12-23 02:48:57,744 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:57,871 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.3123378802036278, 0.43246275060456935], [1.0, 0.0], [nan, nan], [2.2516291673878226, 6.620773041953877, 6.635461149337424, 2.236941060004275, 4.383831981949601, 0.014688107383546978]]\n",
      "2020-12-23 02:48:57,873 : INFO : Removed 0 and 132 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:57,874 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:57,876 : INFO : built Dictionary(81 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 530 corpus positions)\n",
      "2020-12-23 02:48:57,893 : INFO : token count processed\n",
      "2020-12-23 02:48:57,896 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:57,897 : INFO : frequencies processed\n",
      "2020-12-23 02:48:57,899 : INFO : token count processed\n",
      "2020-12-23 02:48:57,900 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:57,901 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:57,902 : INFO : vocab #2480\n",
      "2020-12-23 02:48:57,903 : INFO : diff #set()\n",
      "2020-12-23 02:48:58,159 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:58,286 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.3053226290815583, 0.43377876371187163], [1.0, 0.0], [nan, nan], [2.2516291673878226, 5.828370634755606, 5.870471427003917, 2.209528375139511, 3.6188422596160943, 0.04210079224831098]]\n",
      "2020-12-23 02:48:58,288 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:58,289 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:58,290 : INFO : built Dictionary(82 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 293 corpus positions)\n",
      "2020-12-23 02:48:58,301 : INFO : token count processed\n",
      "2020-12-23 02:48:58,303 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:58,304 : INFO : frequencies processed\n",
      "2020-12-23 02:48:58,306 : INFO : token count processed\n",
      "2020-12-23 02:48:58,307 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:58,308 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:58,309 : INFO : vocab #2480\n",
      "2020-12-23 02:48:58,310 : INFO : diff #set()\n",
      "2020-12-23 02:48:58,698 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:58,826 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.309228242093133, 0.43304511081744734], [0.9935580142773688, 0.0064419857], [nan, nan], [2.2516291673878226, 5.774409284925443, 5.842465380016715, 2.183573072296551, 3.590836212628892, 0.06805609509127208]]\n",
      "2020-12-23 02:48:58,828 : INFO : Removed 0 and 51 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:58,829 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:58,830 : INFO : built Dictionary(93 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 342 corpus positions)\n",
      "2020-12-23 02:48:58,843 : INFO : token count processed\n",
      "2020-12-23 02:48:58,845 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:58,846 : INFO : frequencies processed\n",
      "2020-12-23 02:48:58,847 : INFO : token count processed\n",
      "2020-12-23 02:48:58,848 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:58,849 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:58,850 : INFO : vocab #2480\n",
      "2020-12-23 02:48:58,851 : INFO : diff #set()\n",
      "2020-12-23 02:48:59,109 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:59,237 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.3073166651842387, 0.43340388213255954], [1.0, 0.0], [nan, nan], [2.2516291673878226, 5.977819040873918, 6.034900853887919, 2.194547354373821, 3.7832716865000964, 0.05708181301400117]]\n",
      "2020-12-23 02:48:59,239 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:59,240 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:59,241 : INFO : built Dictionary(74 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 289 corpus positions)\n",
      "2020-12-23 02:48:59,251 : INFO : token count processed\n",
      "2020-12-23 02:48:59,253 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:59,254 : INFO : frequencies processed\n",
      "2020-12-23 02:48:59,255 : INFO : token count processed\n",
      "2020-12-23 02:48:59,256 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:59,257 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:59,258 : INFO : vocab #2480\n",
      "2020-12-23 02:48:59,259 : INFO : diff #set()\n",
      "2020-12-23 02:48:59,517 : INFO : alphabet #2480\n",
      "2020-12-23 02:48:59,645 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.3088303819882505, 0.43311973361111505], [1.0, 0.0], [nan, nan], [2.2516291673878226, 5.901812829596593, 5.966321371615472, 2.1871206253689435, 3.71469220422765, 0.06450854201887957]]\n",
      "2020-12-23 02:48:59,647 : INFO : Removed 0 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:48:59,648 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:48:59,650 : INFO : built Dictionary(76 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 289 corpus positions)\n",
      "2020-12-23 02:48:59,666 : INFO : token count processed\n",
      "2020-12-23 02:48:59,669 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:48:59,669 : INFO : frequencies processed\n",
      "2020-12-23 02:48:59,670 : INFO : token count processed\n",
      "2020-12-23 02:48:59,671 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:48:59,672 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:48:59,673 : INFO : vocab #2480\n",
      "2020-12-23 02:48:59,674 : INFO : diff #set()\n",
      "2020-12-23 02:48:59,942 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:00,071 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.3040438306360855, 0.43401952111472053], [1.0, 0.0], [nan, nan], [2.2516291673878226, 5.643202320803383, 5.715361635717393, 2.1794698524738116, 3.4637324683295705, 0.07215931491401051]]\n",
      "2020-12-23 02:49:00,074 : INFO : Removed 0 and 30 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:00,074 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:00,075 : INFO : built Dictionary(89 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 323 corpus positions)\n",
      "2020-12-23 02:49:00,093 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:49:00,096 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:00,097 : INFO : frequencies processed\n",
      "2020-12-23 02:49:00,099 : INFO : token count processed\n",
      "2020-12-23 02:49:00,101 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:00,101 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:00,102 : INFO : vocab #2480\n",
      "2020-12-23 02:49:00,103 : INFO : diff #set()\n",
      "2020-12-23 02:49:00,357 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:00,487 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.303433983946664, 0.434134430146167], [0.9985521093476564, 0.0014478907], [nan, nan], [2.2516291673878226, 5.925214310725336, 5.987004806633886, 2.189838671479272, 3.735375639246063, 0.061790495908550014]]\n",
      "2020-12-23 02:49:00,490 : INFO : Removed 0 and 293 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:00,491 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:00,492 : INFO : built Dictionary(162 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 1718 corpus positions)\n",
      "2020-12-23 02:49:00,516 : INFO : token count processed\n",
      "2020-12-23 02:49:00,519 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:00,519 : INFO : frequencies processed\n",
      "2020-12-23 02:49:00,521 : INFO : token count processed\n",
      "2020-12-23 02:49:00,522 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:00,523 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:00,523 : INFO : vocab #2480\n",
      "2020-12-23 02:49:00,525 : INFO : diff #set()\n",
      "2020-12-23 02:49:00,783 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:00,911 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.3115518722726769, 0.4326098029618594], [1.0, 0.0], [nan, nan], [2.2516291673878226, 6.551685682764175, 6.568182606808035, 2.235132243343962, 4.316553439420213, 0.01649692404386016]]\n",
      "2020-12-23 02:49:00,913 : INFO : Removed 0 and 88 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:00,914 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:00,915 : INFO : built Dictionary(138 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 559 corpus positions)\n",
      "2020-12-23 02:49:00,938 : INFO : token count processed\n",
      "2020-12-23 02:49:00,940 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:00,941 : INFO : frequencies processed\n",
      "2020-12-23 02:49:00,942 : INFO : token count processed\n",
      "2020-12-23 02:49:00,943 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:00,944 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:00,944 : INFO : vocab #2480\n",
      "2020-12-23 02:49:00,945 : INFO : diff #set()\n",
      "2020-12-23 02:49:01,204 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:01,332 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.3022915750524395, 0.43434984987825564], [0.9995024690870196, 0.0004975309], [nan, nan], [2.2516291673878226, 6.642985062562557, 6.6781994827034605, 2.216414747246919, 4.426570315315638, 0.03521442014090326]]\n",
      "2020-12-23 02:49:01,335 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:01,335 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:01,336 : INFO : built Dictionary(51 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 154 corpus positions)\n",
      "2020-12-23 02:49:01,345 : INFO : token count processed\n",
      "2020-12-23 02:49:01,348 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:01,349 : INFO : frequencies processed\n",
      "2020-12-23 02:49:01,351 : INFO : token count processed\n",
      "2020-12-23 02:49:01,352 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:01,353 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:01,354 : INFO : vocab #2480\n",
      "2020-12-23 02:49:01,355 : INFO : diff #set()\n",
      "2020-12-23 02:49:01,625 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:01,752 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.308209078295835, 0.4332363170230255], [1.0, 0.0], [nan, nan], [2.2516291673878226, 5.2461980344571995, 5.361533977033267, 2.1362932248117543, 3.1099048096454447, 0.11533594257606783]]\n",
      "2020-12-23 02:49:01,755 : INFO : Removed 0 and 23 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:01,756 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:01,757 : INFO : built Dictionary(73 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 214 corpus positions)\n",
      "2020-12-23 02:49:01,766 : INFO : token count processed\n",
      "2020-12-23 02:49:01,769 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:01,769 : INFO : frequencies processed\n",
      "2020-12-23 02:49:01,771 : INFO : token count processed\n",
      "2020-12-23 02:49:01,771 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:01,772 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:01,773 : INFO : vocab #2480\n",
      "2020-12-23 02:49:01,774 : INFO : diff #set()\n",
      "2020-12-23 02:49:02,031 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:02,158 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/test_auth_utility.py')[[1.3068078238789385, 0.43349948341968175], [1.0, 0.0], [nan, nan], [2.2516291673878226, 5.903090303960449, 5.980978825471064, 2.173740645877208, 3.7293496580832417, 0.07788852151061487]]\n",
      "2020-12-23 02:49:02,161 : INFO : Removed 0 and 442 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:02,162 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:02,163 : INFO : built Dictionary(107 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 1213 corpus positions)\n",
      "2020-12-23 02:49:02,177 : INFO : token count processed\n",
      "2020-12-23 02:49:02,179 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:02,180 : INFO : frequencies processed\n",
      "2020-12-23 02:49:02,182 : INFO : token count processed\n",
      "2020-12-23 02:49:02,183 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:02,184 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:02,184 : INFO : vocab #2480\n",
      "2020-12-23 02:49:02,185 : INFO : diff #set()\n",
      "2020-12-23 02:49:02,443 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:02,570 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.3026074705065085, 0.4342902612836691], [0.9996276335441507, 0.00037236646], [nan, nan], [2.2516291673878226, 6.16659449033757, 6.1870153884488195, 2.2312082692765722, 3.935386221060997, 0.02042089811124992]]\n",
      "2020-12-23 02:49:02,573 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:02,574 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:02,575 : INFO : built Dictionary(67 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 248 corpus positions)\n",
      "2020-12-23 02:49:02,584 : INFO : token count processed\n",
      "2020-12-23 02:49:02,586 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:02,587 : INFO : frequencies processed\n",
      "2020-12-23 02:49:02,588 : INFO : token count processed\n",
      "2020-12-23 02:49:02,589 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:02,589 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:02,590 : INFO : vocab #2480\n",
      "2020-12-23 02:49:02,591 : INFO : diff #set()\n",
      "2020-12-23 02:49:02,849 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:02,975 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.3046041882366706, 0.4339139905690848], [0.9953723978251219, 0.004627602], [nan, nan], [2.2516291673878226, 5.906856253399655, 5.974674041125734, 2.183811379661744, 3.7230448737379116, 0.06781778772607883]]\n",
      "2020-12-23 02:49:02,978 : INFO : Removed 0 and 68 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:02,979 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:49:02,980 : INFO : built Dictionary(80 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 247 corpus positions)\n",
      "2020-12-23 02:49:02,991 : INFO : token count processed\n",
      "2020-12-23 02:49:02,993 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:02,994 : INFO : frequencies processed\n",
      "2020-12-23 02:49:02,995 : INFO : token count processed\n",
      "2020-12-23 02:49:02,997 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:02,998 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:02,999 : INFO : vocab #2480\n",
      "2020-12-23 02:49:03,001 : INFO : diff #set()\n",
      "2020-12-23 02:49:03,272 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:03,405 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.3021076714835222, 0.4343845478589543], [0.9987220135517418, 0.0012779864], [nan, nan], [2.2516291673878226, 5.965115449163356, 6.030441663368311, 2.1863029531828673, 3.7788124959804885, 0.06532621420495488]]\n",
      "2020-12-23 02:49:03,407 : INFO : Removed 0 and 13 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:03,408 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:03,409 : INFO : built Dictionary(87 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 361 corpus positions)\n",
      "2020-12-23 02:49:03,421 : INFO : token count processed\n",
      "2020-12-23 02:49:03,423 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:03,424 : INFO : frequencies processed\n",
      "2020-12-23 02:49:03,425 : INFO : token count processed\n",
      "2020-12-23 02:49:03,426 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:03,427 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:03,428 : INFO : vocab #2480\n",
      "2020-12-23 02:49:03,430 : INFO : diff #set()\n",
      "2020-12-23 02:49:03,688 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:03,816 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.3048221134998117, 0.43387296318566043], [0.9949019537307322, 0.0050980463], [nan, nan], [2.2516291673878226, 5.791362404253194, 5.853179392872113, 2.1898121787689027, 3.601550225484291, 0.06181698861891949]]\n",
      "2020-12-23 02:49:03,818 : INFO : Removed 0 and 41 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:03,819 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:03,820 : INFO : built Dictionary(77 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 373 corpus positions)\n",
      "2020-12-23 02:49:03,829 : INFO : token count processed\n",
      "2020-12-23 02:49:03,832 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:03,832 : INFO : frequencies processed\n",
      "2020-12-23 02:49:03,833 : INFO : token count processed\n",
      "2020-12-23 02:49:03,834 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:03,835 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:03,836 : INFO : vocab #2480\n",
      "2020-12-23 02:49:03,837 : INFO : diff #set()\n",
      "2020-12-23 02:49:04,095 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:04,223 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.3085322087395057, 0.4331756759616603], [0.9993804086116143, 0.0006195914], [nan, nan], [2.2516291673878226, 5.651670454631116, 5.711680208622816, 2.191619413396122, 3.4600510412349936, 0.060009753991700165]]\n",
      "2020-12-23 02:49:04,225 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:04,226 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:04,227 : INFO : built Dictionary(40 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 83 corpus positions)\n",
      "2020-12-23 02:49:04,232 : INFO : token count processed\n",
      "2020-12-23 02:49:04,235 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:04,235 : INFO : frequencies processed\n",
      "2020-12-23 02:49:04,236 : INFO : token count processed\n",
      "2020-12-23 02:49:04,237 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:04,238 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:04,239 : INFO : vocab #2480\n",
      "2020-12-23 02:49:04,240 : INFO : diff #set()\n",
      "2020-12-23 02:49:04,517 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:04,647 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.3116304566039503, 0.4325950963066625], [1.0, 0.0], [nan, nan], [2.2516291673878226, 4.8226207261920235, 5.010210798632597, 2.0640390949472494, 2.7585816312447746, 0.18759007244057369]]\n",
      "2020-12-23 02:49:04,650 : INFO : Removed 0 and 50 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:04,651 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:04,652 : INFO : built Dictionary(85 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 253 corpus positions)\n",
      "2020-12-23 02:49:04,672 : INFO : token count processed\n",
      "2020-12-23 02:49:04,674 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:04,675 : INFO : frequencies processed\n",
      "2020-12-23 02:49:04,677 : INFO : token count processed\n",
      "2020-12-23 02:49:04,678 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:04,679 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:04,680 : INFO : vocab #2480\n",
      "2020-12-23 02:49:04,680 : INFO : diff #set()\n",
      "2020-12-23 02:49:04,935 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:05,062 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.3036639842589208, 0.4340910856935135], [0.9952497989870608, 0.004750201], [nan, nan], [2.2516291673878226, 6.24862851613934, 6.309807285174457, 2.1904503983527066, 4.058178117786634, 0.06117876903511643]]\n",
      "2020-12-23 02:49:05,064 : INFO : Removed 0 and 32 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:05,065 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:05,066 : INFO : built Dictionary(86 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 369 corpus positions)\n",
      "2020-12-23 02:49:05,081 : INFO : token count processed\n",
      "2020-12-23 02:49:05,083 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:05,084 : INFO : frequencies processed\n",
      "2020-12-23 02:49:05,085 : INFO : token count processed\n",
      "2020-12-23 02:49:05,086 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:05,087 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:05,088 : INFO : vocab #2480\n",
      "2020-12-23 02:49:05,089 : INFO : diff #set()\n",
      "2020-12-23 02:49:05,347 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:05,476 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.303971231770847, 0.43403319720767247], [0.9951040842570364, 0.0048959157], [nan, nan], [2.2516291673878226, 5.850156917433494, 5.908448424898226, 2.193337659923091, 3.6568192575104033, 0.058291507464732106]]\n",
      "2020-12-23 02:49:05,478 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:05,479 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:05,480 : INFO : built Dictionary(81 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 367 corpus positions)\n",
      "2020-12-23 02:49:05,491 : INFO : token count processed\n",
      "2020-12-23 02:49:05,493 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:05,494 : INFO : frequencies processed\n",
      "2020-12-23 02:49:05,495 : INFO : token count processed\n",
      "2020-12-23 02:49:05,496 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:05,497 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:05,498 : INFO : vocab #2480\n",
      "2020-12-23 02:49:05,499 : INFO : diff #set()\n",
      "2020-12-23 02:49:05,759 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:05,886 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.3070607861940813, 0.4334519514978549], [0.9993702928186394, 0.0006297072], [nan, nan], [2.2516291673878226, 5.6831976040360095, 5.743124960478183, 2.1917018109456494, 3.4914957930903605, 0.05992735644217362]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:49:05,888 : INFO : Removed 0 and 52 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:05,889 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:05,891 : INFO : built Dictionary(68 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 345 corpus positions)\n",
      "2020-12-23 02:49:05,905 : INFO : token count processed\n",
      "2020-12-23 02:49:05,907 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:05,908 : INFO : frequencies processed\n",
      "2020-12-23 02:49:05,909 : INFO : token count processed\n",
      "2020-12-23 02:49:05,910 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:05,912 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:05,912 : INFO : vocab #2480\n",
      "2020-12-23 02:49:05,914 : INFO : diff #set()\n",
      "2020-12-23 02:49:06,175 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:06,302 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.307828375841866, 0.43330778426502925], [0.9936671736650169, 0.0063328263], [nan, nan], [2.2516291673878226, 5.749308601266266, 5.809491336558411, 2.1914464320956775, 3.5578621691705883, 0.060182735292144685]]\n",
      "2020-12-23 02:49:06,305 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:06,306 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:06,307 : INFO : built Dictionary(61 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 226 corpus positions)\n",
      "2020-12-23 02:49:06,321 : INFO : token count processed\n",
      "2020-12-23 02:49:06,324 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:06,325 : INFO : frequencies processed\n",
      "2020-12-23 02:49:06,326 : INFO : token count processed\n",
      "2020-12-23 02:49:06,327 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:06,328 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:06,328 : INFO : vocab #2480\n",
      "2020-12-23 02:49:06,329 : INFO : diff #set()\n",
      "2020-12-23 02:49:06,605 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:06,737 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.3150198695563322, 0.4319617352535499], [1.0, 0.0], [nan, nan], [2.2516291673878226, 5.015422548793484, 5.118539313977495, 2.148512402203812, 2.8669101465896722, 0.10311676518401125]]\n",
      "2020-12-23 02:49:06,740 : INFO : Removed 0 and 81 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:06,741 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:06,742 : INFO : built Dictionary(90 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 352 corpus positions)\n",
      "2020-12-23 02:49:06,763 : INFO : token count processed\n",
      "2020-12-23 02:49:06,767 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:06,768 : INFO : frequencies processed\n",
      "2020-12-23 02:49:06,769 : INFO : token count processed\n",
      "2020-12-23 02:49:06,770 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:06,771 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:06,772 : INFO : vocab #2480\n",
      "2020-12-23 02:49:06,773 : INFO : diff #set()\n",
      "2020-12-23 02:49:07,027 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:07,153 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.2924507958537814, 0.43621437886851927], [0.9904601871967316, 0.009539813], [nan, nan], [2.2516291673878226, 6.030001281822029, 6.0830388093688885, 2.198591639840963, 3.831409641981066, 0.05303752754685931]]\n",
      "2020-12-23 02:49:07,156 : INFO : Removed 0 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:07,157 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:07,158 : INFO : built Dictionary(78 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 315 corpus positions)\n",
      "2020-12-23 02:49:07,168 : INFO : token count processed\n",
      "2020-12-23 02:49:07,171 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:07,171 : INFO : frequencies processed\n",
      "2020-12-23 02:49:07,172 : INFO : token count processed\n",
      "2020-12-23 02:49:07,173 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:07,174 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:07,175 : INFO : vocab #2480\n",
      "2020-12-23 02:49:07,176 : INFO : diff #set()\n",
      "2020-12-23 02:49:07,435 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:07,563 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.3060027058665926, 0.43365083547211253], [0.9970003545749933, 0.0029996454], [nan, nan], [2.2516291673878226, 5.9537092545441395, 6.014299516827576, 2.191038905104387, 3.762670349439753, 0.060590262283436225]]\n",
      "2020-12-23 02:49:07,566 : INFO : Removed 0 and 90 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:07,567 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:07,568 : INFO : built Dictionary(89 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 316 corpus positions)\n",
      "2020-12-23 02:49:07,587 : INFO : token count processed\n",
      "2020-12-23 02:49:07,589 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:07,589 : INFO : frequencies processed\n",
      "2020-12-23 02:49:07,590 : INFO : token count processed\n",
      "2020-12-23 02:49:07,591 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:07,592 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:07,593 : INFO : vocab #2480\n",
      "2020-12-23 02:49:07,594 : INFO : diff #set()\n",
      "2020-12-23 02:49:07,846 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:07,974 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.3045838537293115, 0.4339178192113884], [0.9989486702252179, 0.0010513298], [nan, nan], [2.2516291673878226, 6.184756445474906, 6.237651440042498, 2.198734172820231, 3.9860222726546755, 0.05289499456759206]]\n",
      "2020-12-23 02:49:07,976 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:07,977 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:07,979 : INFO : built Dictionary(97 unique tokens: ['commit', 'last', 'merg', 'prod', 'revert']...) from 2 documents (total 429 corpus positions)\n",
      "2020-12-23 02:49:08,000 : INFO : token count processed\n",
      "2020-12-23 02:49:08,003 : INFO : frequencies processed\n",
      "2020-12-23 02:49:08,132 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:08,132 : INFO : entropies processed\n",
      "2020-12-23 02:49:08,133 : INFO : extropies processed\n",
      "2020-12-23 02:49:08,134 : INFO : token count processed\n",
      "2020-12-23 02:49:08,135 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:08,136 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:08,137 : INFO : vocab #2480\n",
      "2020-12-23 02:49:08,137 : INFO : diff #set()\n",
      "2020-12-23 02:49:08,405 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:08,533 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.3001358900289237, 0.4347569221170777], [0.9794645439833403, 0.020535456], [0.0, 0.0], [2.2516291673878226, 6.212221456585881, 6.2526647743923665, 2.2111858495813372, 4.001035607004544, 0.0404433178064858]]\n",
      "2020-12-23 02:49:08,535 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:08,536 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:08,537 : INFO : built Dictionary(117 unique tokens: ['chang', 'product', 'revert', '\"])', '\"],']...) from 2 documents (total 410 corpus positions)\n",
      "2020-12-23 02:49:08,551 : INFO : token count processed\n",
      "2020-12-23 02:49:08,555 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:08,556 : INFO : frequencies processed\n",
      "2020-12-23 02:49:08,558 : INFO : token count processed\n",
      "2020-12-23 02:49:08,560 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:08,562 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:08,563 : INFO : vocab #2480\n",
      "2020-12-23 02:49:08,564 : INFO : diff #set()\n",
      "2020-12-23 02:49:08,816 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:08,944 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.3195227307605104, 0.4311231732021554], [0.999418426246848, 0.00058157375], [nan, nan], [1.584962500721156, 6.301552355933639, 6.32845599723888, 1.5580588594159153, 4.743493496517724, 0.02690364130524081]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:49:08,947 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:08,948 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:08,949 : INFO : built Dictionary(155 unique tokens: ['chang', 'product', 'revert', '\")).', '\"--']...) from 2 documents (total 646 corpus positions)\n",
      "2020-12-23 02:49:08,967 : INFO : token count processed\n",
      "2020-12-23 02:49:08,969 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:08,970 : INFO : frequencies processed\n",
      "2020-12-23 02:49:08,972 : INFO : token count processed\n",
      "2020-12-23 02:49:08,973 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:08,975 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:08,975 : INFO : vocab #2480\n",
      "2020-12-23 02:49:08,977 : INFO : diff #set()\n",
      "2020-12-23 02:49:09,236 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:09,364 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.3227632148346327, 0.43052171379905124], [1.0, 0.0], [nan, nan], [1.584962500721156, 6.739005504021667, 6.7567824027877785, 1.5671856019550452, 5.1718199020666225, 0.01777689876611177]]\n",
      "2020-12-23 02:49:09,367 : INFO : Removed 0 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:09,368 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:09,369 : INFO : built Dictionary(99 unique tokens: ['chang', 'product', 'revert', '\".\"', '\"])']...) from 2 documents (total 497 corpus positions)\n",
      "2020-12-23 02:49:09,379 : INFO : token count processed\n",
      "2020-12-23 02:49:09,381 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:09,382 : INFO : frequencies processed\n",
      "2020-12-23 02:49:09,383 : INFO : token count processed\n",
      "2020-12-23 02:49:09,384 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:09,385 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:09,386 : INFO : vocab #2480\n",
      "2020-12-23 02:49:09,387 : INFO : diff #set()\n",
      "2020-12-23 02:49:09,645 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:09,772 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.3319766847120835, 0.42882075389337115], [0.9995392026612535, 0.00046079734], [nan, nan], [1.584962500721156, 5.870833373337847, 5.8942744541682055, 1.5615214198907976, 4.309311953447049, 0.02344108083035845]]\n",
      "2020-12-23 02:49:09,774 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:09,775 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:09,776 : INFO : built Dictionary(59 unique tokens: ['chang', 'product', 'revert', '\"].', 'add']...) from 2 documents (total 180 corpus positions)\n",
      "2020-12-23 02:49:09,781 : INFO : token count processed\n",
      "2020-12-23 02:49:09,784 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:09,784 : INFO : frequencies processed\n",
      "2020-12-23 02:49:09,785 : INFO : token count processed\n",
      "2020-12-23 02:49:09,786 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:09,787 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:09,788 : INFO : vocab #2480\n",
      "2020-12-23 02:49:09,789 : INFO : diff #set()\n",
      "2020-12-23 02:49:10,047 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:10,175 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.3183589598857381, 0.4313395886067987], [1.0, 0.0], [nan, nan], [1.584962500721156, 5.371881234145534, 5.428471654387997, 1.528372080478693, 3.843509153666841, 0.05659042024246297]]\n",
      "2020-12-23 02:49:10,177 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:10,178 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:10,178 : INFO : built Dictionary(49 unique tokens: ['chang', 'product', 'revert', 'add', 'append']...) from 2 documents (total 132 corpus positions)\n",
      "2020-12-23 02:49:10,183 : INFO : token count processed\n",
      "2020-12-23 02:49:10,185 : INFO : frequencies processed\n",
      "2020-12-23 02:49:10,313 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:10,313 : INFO : entropies processed\n",
      "2020-12-23 02:49:10,314 : INFO : extropies processed\n",
      "2020-12-23 02:49:10,315 : INFO : token count processed\n",
      "2020-12-23 02:49:10,316 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:10,317 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:10,318 : INFO : vocab #2480\n",
      "2020-12-23 02:49:10,319 : INFO : diff #set()\n",
      "2020-12-23 02:49:10,576 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:10,704 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.318754287297079, 0.4312660489635916], [0.9809911232441664, 0.019008877], [0.0, 0.0], [1.584962500721156, 4.85108279267097, 4.917936175001393, 1.5181091183907327, 3.332973674280237, 0.06685338233042337]]\n",
      "2020-12-23 02:49:10,707 : INFO : Removed 0 and 127 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:10,708 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:10,709 : INFO : built Dictionary(90 unique tokens: ['chang', 'product', 'revert', '\":\")', '\"],']...) from 2 documents (total 405 corpus positions)\n",
      "2020-12-23 02:49:10,717 : INFO : token count processed\n",
      "2020-12-23 02:49:10,722 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:10,723 : INFO : frequencies processed\n",
      "2020-12-23 02:49:10,726 : INFO : token count processed\n",
      "2020-12-23 02:49:10,727 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:10,728 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:10,729 : INFO : vocab #2480\n",
      "2020-12-23 02:49:10,730 : INFO : diff #set()\n",
      "2020-12-23 02:49:10,989 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:11,117 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.325317595819274, 0.4300487820665513], [0.9994058591546491, 0.00059414085], [nan, nan], [1.584962500721156, 6.139571208108155, 6.164125705390089, 1.560408003439222, 4.579163204668933, 0.02455449728193404]]\n",
      "2020-12-23 02:49:11,119 : INFO : Removed 0 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:11,120 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:11,121 : INFO : built Dictionary(76 unique tokens: ['chang', 'product', 'revert', '\"),', '\"])']...) from 2 documents (total 415 corpus positions)\n",
      "2020-12-23 02:49:11,131 : INFO : token count processed\n",
      "2020-12-23 02:49:11,133 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:11,136 : INFO : frequencies processed\n",
      "2020-12-23 02:49:11,137 : INFO : token count processed\n",
      "2020-12-23 02:49:11,140 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:11,141 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:11,142 : INFO : vocab #2480\n",
      "2020-12-23 02:49:11,142 : INFO : diff #set()\n",
      "2020-12-23 02:49:11,398 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:11,525 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.3220267664446612, 0.43065825702394295], [1.0, 0.0], [nan, nan], [1.584962500721156, 5.609710627339259, 5.640515686525264, 1.5541574415351516, 4.055553185804108, 0.030805059186004513]]\n",
      "2020-12-23 02:49:11,528 : INFO : Removed 0 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:11,529 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:11,530 : INFO : built Dictionary(168 unique tokens: ['chang', 'product', 'revert', '\"\"\"', '\"\",']...) from 2 documents (total 1075 corpus positions)\n",
      "2020-12-23 02:49:11,548 : INFO : token count processed\n",
      "2020-12-23 02:49:11,550 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:11,551 : INFO : frequencies processed\n",
      "2020-12-23 02:49:11,552 : INFO : token count processed\n",
      "2020-12-23 02:49:11,553 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:11,554 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:11,555 : INFO : vocab #2480\n",
      "2020-12-23 02:49:11,556 : INFO : diff #set()\n",
      "2020-12-23 02:49:11,815 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:11,943 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.3284421431227864, 0.42947169761274445], [0.9985423523467034, 0.0014576477], [nan, nan], [1.584962500721156, 7.2441902753576075, 7.253761443456057, 1.5753913326227078, 5.668798942734901, 0.009571168098449157]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:49:11,946 : INFO : Removed 0 and 136 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:11,947 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:11,948 : INFO : built Dictionary(130 unique tokens: ['chang', 'product', 'revert', '\"--', '(\"--']...) from 2 documents (total 682 corpus positions)\n",
      "2020-12-23 02:49:11,962 : INFO : token count processed\n",
      "2020-12-23 02:49:11,965 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:11,965 : INFO : frequencies processed\n",
      "2020-12-23 02:49:11,967 : INFO : token count processed\n",
      "2020-12-23 02:49:11,968 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:11,969 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:11,969 : INFO : vocab #2480\n",
      "2020-12-23 02:49:11,970 : INFO : diff #set()\n",
      "2020-12-23 02:49:12,228 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:12,355 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.3197981550776985, 0.4310719869360816], [0.9996467129094526, 0.0003532871], [nan, nan], [1.584962500721156, 6.2567074920449475, 6.274528870331851, 1.5671411224342524, 4.689566369610695, 0.017821378286903666]]\n",
      "2020-12-23 02:49:12,357 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:12,358 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:12,359 : INFO : built Dictionary(75 unique tokens: ['chang', 'product', 'revert', '__doc__', '__main__']...) from 2 documents (total 192 corpus positions)\n",
      "2020-12-23 02:49:12,367 : INFO : token count processed\n",
      "2020-12-23 02:49:12,369 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:12,370 : INFO : frequencies processed\n",
      "2020-12-23 02:49:12,371 : INFO : token count processed\n",
      "2020-12-23 02:49:12,372 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:12,373 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:12,373 : INFO : vocab #2480\n",
      "2020-12-23 02:49:12,374 : INFO : diff #set()\n",
      "2020-12-23 02:49:12,632 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:12,760 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.3215663127810333, 0.4307436727069353], [1.0, 0.0], [nan, nan], [1.584962500721156, 5.7680018917339435, 5.816398754556717, 1.5365656378983825, 4.231436253835561, 0.048396862822773556]]\n",
      "2020-12-23 02:49:12,762 : INFO : Removed 0 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:12,763 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:12,764 : INFO : built Dictionary(173 unique tokens: ['chang', 'product', 'revert', '\"\"\"', '\"])']...) from 2 documents (total 733 corpus positions)\n",
      "2020-12-23 02:49:12,785 : INFO : token count processed\n",
      "2020-12-23 02:49:12,787 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:12,788 : INFO : frequencies processed\n",
      "2020-12-23 02:49:12,791 : INFO : token count processed\n",
      "2020-12-23 02:49:12,795 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:12,799 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:12,799 : INFO : vocab #2480\n",
      "2020-12-23 02:49:12,802 : INFO : diff #set()\n",
      "2020-12-23 02:49:13,057 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:13,185 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.3217236142689568, 0.4307144889487075], [0.9996142105665058, 0.00038578943], [nan, nan], [1.584962500721156, 6.846479111193757, 6.861791766455839, 1.5696498454590735, 5.276829265734683, 0.015312655262082586]]\n",
      "2020-12-23 02:49:13,187 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:13,188 : INFO : built Dictionary(23 unique tokens: ['chang', 'product', 'revert', 'buddi', 'def']...) from 2 documents (total 36 corpus positions)\n",
      "2020-12-23 02:49:13,191 : INFO : token count processed\n",
      "2020-12-23 02:49:13,193 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:13,194 : INFO : frequencies processed\n",
      "2020-12-23 02:49:13,195 : INFO : token count processed\n",
      "2020-12-23 02:49:13,196 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:13,197 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:13,197 : INFO : vocab #2480\n",
      "2020-12-23 02:49:13,198 : INFO : diff #set()\n",
      "2020-12-23 02:49:13,455 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:13,584 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/fireException.py')[[1.3309457532439781, 0.42901041287996494], [1.0, 0.0], [nan, nan], [1.584962500721156, 4.165013816065912, 4.363826390090815, 1.3861499266962527, 2.778863889369659, 0.19881257402490338]]\n",
      "2020-12-23 02:49:13,586 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:13,587 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:13,588 : INFO : built Dictionary(51 unique tokens: ['chang', 'product', 'revert', 'autoescap', 'bin']...) from 2 documents (total 143 corpus positions)\n",
      "2020-12-23 02:49:13,593 : INFO : token count processed\n",
      "2020-12-23 02:49:13,596 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:13,596 : INFO : frequencies processed\n",
      "2020-12-23 02:49:13,597 : INFO : token count processed\n",
      "2020-12-23 02:49:13,598 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:13,599 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:13,600 : INFO : vocab #2480\n",
      "2020-12-23 02:49:13,601 : INFO : diff #set()\n",
      "2020-12-23 02:49:13,858 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:13,986 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.3205694189131731, 0.4309287159650431], [1.0, 0.0], [nan, nan], [1.584962500721156, 5.449968864419248, 5.51186343324884, 1.523067931891564, 3.9269009325276842, 0.06189456882959199]]\n",
      "2020-12-23 02:49:13,989 : INFO : Removed 0 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:13,990 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:13,991 : INFO : built Dictionary(144 unique tokens: ['chang', 'product', 'revert', '\"\"\"', '\"))).']...) from 2 documents (total 580 corpus positions)\n",
      "2020-12-23 02:49:14,010 : INFO : token count processed\n",
      "2020-12-23 02:49:14,012 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:14,014 : INFO : frequencies processed\n",
      "2020-12-23 02:49:14,018 : INFO : token count processed\n",
      "2020-12-23 02:49:14,019 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:14,021 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:14,023 : INFO : vocab #2480\n",
      "2020-12-23 02:49:14,024 : INFO : diff #set()\n",
      "2020-12-23 02:49:14,278 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:14,405 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.3285643786022243, 0.42944915295847375], [0.9989148412132636, 0.0010851588], [nan, nan], [1.584962500721156, 6.530294129310484, 6.548513792396278, 1.5667428376353616, 4.963551291675122, 0.01821966308579448]]\n",
      "2020-12-23 02:49:14,408 : INFO : Removed 0 and 142 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:14,408 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:14,409 : INFO : built Dictionary(117 unique tokens: ['chang', 'product', 'revert', '\"):', '\"],']...) from 2 documents (total 596 corpus positions)\n",
      "2020-12-23 02:49:14,424 : INFO : token count processed\n",
      "2020-12-23 02:49:14,430 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:14,431 : INFO : frequencies processed\n",
      "2020-12-23 02:49:14,433 : INFO : token count processed\n",
      "2020-12-23 02:49:14,434 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:14,435 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:14,436 : INFO : vocab #2480\n",
      "2020-12-23 02:49:14,437 : INFO : diff #set()\n",
      "2020-12-23 02:49:14,694 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:14,826 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.3207805078464605, 0.4308895204087773], [0.9995423028303776, 0.00045769717], [nan, nan], [1.584962500721156, 6.470272233491701, 6.488552573024436, 1.56668216118842, 4.90359007230328, 0.018280339532735113]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:49:14,829 : INFO : Removed 0 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:14,830 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:14,831 : INFO : built Dictionary(118 unique tokens: ['chang', 'product', 'revert', '\"\":', '\"--']...) from 2 documents (total 385 corpus positions)\n",
      "2020-12-23 02:49:14,850 : INFO : token count processed\n",
      "2020-12-23 02:49:14,853 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:14,856 : INFO : frequencies processed\n",
      "2020-12-23 02:49:14,858 : INFO : token count processed\n",
      "2020-12-23 02:49:14,861 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:14,862 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:14,864 : INFO : vocab #2480\n",
      "2020-12-23 02:49:14,865 : INFO : diff #set()\n",
      "2020-12-23 02:49:15,119 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:15,246 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.3213437970015776, 0.4307849622669746], [1.0, 0.0], [nan, nan], [1.584962500721156, 6.550038223589686, 6.575123784010567, 1.559876940300275, 4.990161283289411, 0.02508556042088106]]\n",
      "2020-12-23 02:49:15,249 : INFO : Removed 0 and 70 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:15,250 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:15,251 : INFO : built Dictionary(73 unique tokens: ['chang', 'product', 'revert', '\")).', '\"])),']...) from 2 documents (total 301 corpus positions)\n",
      "2020-12-23 02:49:15,258 : INFO : token count processed\n",
      "2020-12-23 02:49:15,260 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:15,261 : INFO : frequencies processed\n",
      "2020-12-23 02:49:15,262 : INFO : token count processed\n",
      "2020-12-23 02:49:15,263 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:15,263 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:15,264 : INFO : vocab #2480\n",
      "2020-12-23 02:49:15,265 : INFO : diff #set()\n",
      "2020-12-23 02:49:15,538 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:15,669 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.3182681653297579, 0.4313564819442521], [1.0, 0.0], [nan, nan], [1.584962500721156, 5.860525481261383, 5.8937728894919825, 1.5517150924905563, 4.3088103887708264, 0.033247408230599795]]\n",
      "2020-12-23 02:49:15,671 : INFO : Removed 0 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:15,672 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:15,674 : INFO : built Dictionary(44 unique tokens: ['chang', 'product', 'revert', '\"]),', '\"],']...) from 2 documents (total 339 corpus positions)\n",
      "2020-12-23 02:49:15,687 : INFO : token count processed\n",
      "2020-12-23 02:49:15,690 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:15,693 : INFO : frequencies processed\n",
      "2020-12-23 02:49:15,695 : INFO : token count processed\n",
      "2020-12-23 02:49:15,696 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:15,698 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:15,701 : INFO : vocab #2480\n",
      "2020-12-23 02:49:15,701 : INFO : diff #set()\n",
      "2020-12-23 02:49:15,956 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:16,083 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.3181871606964202, 0.43137155487462203], [0.9793989304453135, 0.02060107], [nan, nan], [1.584962500721156, 5.945464049777852, 5.971965802179039, 1.5584607483199697, 4.387003301457883, 0.02650175240118635]]\n",
      "2020-12-23 02:49:16,086 : INFO : Removed 0 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:16,087 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:16,088 : INFO : built Dictionary(192 unique tokens: ['chang', 'product', 'revert', '\"\"\"', '\"))']...) from 2 documents (total 886 corpus positions)\n",
      "2020-12-23 02:49:16,114 : INFO : token count processed\n",
      "2020-12-23 02:49:16,116 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:16,117 : INFO : frequencies processed\n",
      "2020-12-23 02:49:16,118 : INFO : token count processed\n",
      "2020-12-23 02:49:16,119 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:16,120 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:16,121 : INFO : vocab #2480\n",
      "2020-12-23 02:49:16,122 : INFO : diff #set()\n",
      "2020-12-23 02:49:16,380 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:16,508 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.321527215001881, 0.4307509270354127], [0.9996895446092822, 0.0003104554], [nan, nan], [1.584962500721156, 6.811563897304216, 6.8249283163044785, 1.5715980817208939, 5.239965815583322, 0.013364419000262195]]\n",
      "2020-12-23 02:49:16,511 : INFO : Removed 0 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:16,512 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:16,513 : INFO : built Dictionary(213 unique tokens: ['chang', 'product', 'revert', '\"\"\"', '\"\".']...) from 2 documents (total 981 corpus positions)\n",
      "2020-12-23 02:49:16,542 : INFO : token count processed\n",
      "2020-12-23 02:49:16,544 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:16,545 : INFO : frequencies processed\n",
      "2020-12-23 02:49:16,549 : INFO : token count processed\n",
      "2020-12-23 02:49:16,550 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:16,551 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:16,551 : INFO : vocab #2480\n",
      "2020-12-23 02:49:16,552 : INFO : diff #set()\n",
      "2020-12-23 02:49:16,808 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:16,936 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.3212938824526455, 0.43079422539269974], [0.9988879336742684, 0.0011120663], [nan, nan], [1.584962500721156, 7.502034948968415, 7.5116083367581, 1.5753891129314708, 5.926645836036944, 0.009573387789685306]]\n",
      "2020-12-23 02:49:16,939 : INFO : Removed 0 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:16,940 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:16,943 : INFO : built Dictionary(257 unique tokens: ['chang', 'product', 'revert', '\"\"\"', '\"\":']...) from 2 documents (total 1554 corpus positions)\n",
      "2020-12-23 02:49:16,988 : INFO : token count processed\n",
      "2020-12-23 02:49:16,993 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:16,994 : INFO : frequencies processed\n",
      "2020-12-23 02:49:16,999 : INFO : token count processed\n",
      "2020-12-23 02:49:17,000 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:17,003 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:17,003 : INFO : vocab #2480\n",
      "2020-12-23 02:49:17,004 : INFO : diff #set()\n",
      "2020-12-23 02:49:17,281 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:17,412 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.3220773241787427, 0.4306488804603755], [0.9987379452213645, 0.0012620548], [nan, nan], [1.584962500721156, 7.39180093901977, 7.399709606384812, 1.5770538333561142, 5.814747105663656, 0.007908667365041921]]\n",
      "2020-12-23 02:49:17,414 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:17,415 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:17,416 : INFO : built Dictionary(41 unique tokens: ['chang', 'product', 'revert', 'aggreg', 'autoescap']...) from 2 documents (total 109 corpus positions)\n",
      "2020-12-23 02:49:17,420 : INFO : token count processed\n",
      "2020-12-23 02:49:17,422 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:17,423 : INFO : frequencies processed\n",
      "2020-12-23 02:49:17,424 : INFO : token count processed\n",
      "2020-12-23 02:49:17,425 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:17,427 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:17,428 : INFO : vocab #2480\n",
      "2020-12-23 02:49:17,429 : INFO : diff #set()\n",
      "2020-12-23 02:49:17,687 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:17,816 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.3120488167316335, 0.43251681917928686], [1.0, 0.0], [nan, nan], [1.584962500721156, 4.927561309677364, 5.016476868255518, 1.4960469421430016, 3.431514367534362, 0.08891555857815447]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:49:17,818 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:17,819 : INFO : built Dictionary(9 unique tokens: ['chang', 'product', 'revert', 'dirnam', 'file']...) from 2 documents (total 10 corpus positions)\n",
      "2020-12-23 02:49:17,823 : INFO : token count processed\n",
      "2020-12-23 02:49:17,828 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:17,829 : INFO : frequencies processed\n",
      "2020-12-23 02:49:17,830 : INFO : token count processed\n",
      "2020-12-23 02:49:17,832 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:17,833 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:17,834 : INFO : vocab #2480\n",
      "2020-12-23 02:49:17,835 : INFO : diff #set()\n",
      "2020-12-23 02:49:18,092 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:18,219 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.3223546519131777, 0.4305974538282474], [1.0, 0.0], [nan, nan], [1.584962500721156, 2.5216406363433186, 3.121928094887362, 0.9846750421771131, 1.536965594166206, 0.6002874585440434]]\n",
      "2020-12-23 02:49:18,222 : INFO : Removed 0 and 530 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:18,223 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:18,227 : INFO : built Dictionary(330 unique tokens: ['chang', 'product', 'revert', '\"\"\"', '\"\",']...) from 2 documents (total 2882 corpus positions)\n",
      "2020-12-23 02:49:18,279 : INFO : token count processed\n",
      "2020-12-23 02:49:18,281 : INFO : frequencies processed\n",
      "2020-12-23 02:49:18,411 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:18,412 : INFO : entropies processed\n",
      "2020-12-23 02:49:18,413 : INFO : extropies processed\n",
      "2020-12-23 02:49:18,415 : INFO : token count processed\n",
      "2020-12-23 02:49:18,416 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:18,417 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:18,418 : INFO : vocab #2480\n",
      "2020-12-23 02:49:18,419 : INFO : diff #set()\n",
      "2020-12-23 02:49:18,689 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:18,818 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.3108141678850695, 0.4327479093289582], [0.9800453316420317, 0.019954668], [0.0, 0.0], [1.584962500721156, 7.480007711014331, 7.4834364107288724, 1.5815338010066142, 5.898473910007716, 0.0034286997145418496]]\n",
      "2020-12-23 02:49:18,821 : INFO : Removed 0 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:18,822 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:18,824 : INFO : built Dictionary(207 unique tokens: ['chang', 'product', 'revert', '\")).', '\"):']...) from 2 documents (total 1031 corpus positions)\n",
      "2020-12-23 02:49:18,851 : INFO : token count processed\n",
      "2020-12-23 02:49:18,854 : INFO : frequencies processed\n",
      "2020-12-23 02:49:18,981 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:18,982 : INFO : entropies processed\n",
      "2020-12-23 02:49:18,983 : INFO : extropies processed\n",
      "2020-12-23 02:49:18,984 : INFO : token count processed\n",
      "2020-12-23 02:49:18,985 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:18,986 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:18,987 : INFO : vocab #2480\n",
      "2020-12-23 02:49:18,988 : INFO : diff #set()\n",
      "2020-12-23 02:49:19,245 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:19,373 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.3118244587484174, 0.43255879407962616], [0.9779096059501171, 0.022090394], [0.0, 0.0], [1.584962500721156, 7.131331012509435, 7.13941192610478, 1.5768815871258104, 5.554449425383624, 0.008080913595345685]]\n",
      "2020-12-23 02:49:19,376 : INFO : Removed 0 and 166 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:19,377 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:19,378 : INFO : built Dictionary(197 unique tokens: ['chang', 'product', 'revert', '\")).', '\")):']...) from 2 documents (total 850 corpus positions)\n",
      "2020-12-23 02:49:19,398 : INFO : token count processed\n",
      "2020-12-23 02:49:19,401 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:19,401 : INFO : frequencies processed\n",
      "2020-12-23 02:49:19,403 : INFO : token count processed\n",
      "2020-12-23 02:49:19,404 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:19,405 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:19,405 : INFO : vocab #2480\n",
      "2020-12-23 02:49:19,406 : INFO : diff #set()\n",
      "2020-12-23 02:49:19,664 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:19,793 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.3236794281275022, 0.4303519615895697], [0.9988908014493063, 0.0011091986], [nan, nan], [1.584962500721156, 7.203742744794778, 7.21621962940165, 1.5724856161142853, 5.6312571286804936, 0.012476884606871685]]\n",
      "2020-12-23 02:49:19,795 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:19,796 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:19,797 : INFO : built Dictionary(54 unique tokens: ['chang', 'product', 'revert', 'argv', 'autoescap']...) from 2 documents (total 182 corpus positions)\n",
      "2020-12-23 02:49:19,803 : INFO : token count processed\n",
      "2020-12-23 02:49:19,805 : INFO : frequencies processed\n",
      "2020-12-23 02:49:19,933 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:19,934 : INFO : entropies processed\n",
      "2020-12-23 02:49:19,935 : INFO : extropies processed\n",
      "2020-12-23 02:49:19,936 : INFO : token count processed\n",
      "2020-12-23 02:49:19,937 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:19,938 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:19,939 : INFO : vocab #2480\n",
      "2020-12-23 02:49:19,940 : INFO : diff #set()\n",
      "2020-12-23 02:49:20,198 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:20,326 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.270798667631574, 0.4403736950590131], [0.92064119130373, 0.07935881], [0.0, 0.0], [1.584962500721156, 5.195502554608948, 5.234362095334276, 1.546102959995828, 3.6493995946131204, 0.038859540725328046]]\n",
      "2020-12-23 02:49:20,328 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:20,329 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:20,330 : INFO : built Dictionary(60 unique tokens: ['chang', 'product', 'revert', ')).', '__main__']...) from 2 documents (total 243 corpus positions)\n",
      "2020-12-23 02:49:20,336 : INFO : token count processed\n",
      "2020-12-23 02:49:20,338 : INFO : frequencies processed\n",
      "2020-12-23 02:49:20,466 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:20,467 : INFO : entropies processed\n",
      "2020-12-23 02:49:20,467 : INFO : extropies processed\n",
      "2020-12-23 02:49:20,468 : INFO : token count processed\n",
      "2020-12-23 02:49:20,469 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:20,470 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:20,471 : INFO : vocab #2480\n",
      "2020-12-23 02:49:20,472 : INFO : diff #set()\n",
      "2020-12-23 02:49:20,729 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:20,858 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.312231089563332, 0.4324827239429824], [0.9898247746750712, 0.010175225], [0.0, 0.0], [1.584962500721156, 5.32027245610305, 5.361800173175238, 1.5434347836489675, 3.776837672454082, 0.04152771707218861]]\n",
      "2020-12-23 02:49:20,861 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:20,861 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:20,863 : INFO : built Dictionary(161 unique tokens: ['chang', 'product', 'revert', '\"):', '\"--']...) from 2 documents (total 483 corpus positions)\n",
      "2020-12-23 02:49:20,883 : INFO : token count processed\n",
      "2020-12-23 02:49:20,885 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:20,886 : INFO : frequencies processed\n",
      "2020-12-23 02:49:20,890 : INFO : token count processed\n",
      "2020-12-23 02:49:20,891 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:20,893 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:20,895 : INFO : vocab #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:49:20,896 : INFO : diff #set()\n",
      "2020-12-23 02:49:21,155 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:21,283 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.3203208219129117, 0.4309748852641779], [0.9976729871705174, 0.0023270128], [nan, nan], [1.584962500721156, 6.898202761357263, 6.918343605747669, 1.56482165633075, 5.333381105026513, 0.020140844390406087]]\n",
      "2020-12-23 02:49:21,285 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:21,286 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:21,287 : INFO : built Dictionary(125 unique tokens: ['chang', 'product', 'revert', '\"\"\"', '\"))']...) from 2 documents (total 505 corpus positions)\n",
      "2020-12-23 02:49:21,305 : INFO : token count processed\n",
      "2020-12-23 02:49:21,307 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:21,308 : INFO : frequencies processed\n",
      "2020-12-23 02:49:21,309 : INFO : token count processed\n",
      "2020-12-23 02:49:21,310 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:21,311 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:21,311 : INFO : vocab #2480\n",
      "2020-12-23 02:49:21,312 : INFO : diff #set()\n",
      "2020-12-23 02:49:21,569 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:21,697 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.3185090065793361, 0.43131167364985673], [0.9981509773060679, 0.0018490227], [nan, nan], [1.584962500721156, 6.388500481644799, 6.411707758722255, 1.5617552236437007, 4.8267452580010985, 0.0232072770774554]]\n",
      "2020-12-23 02:49:21,700 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:21,700 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:21,701 : INFO : built Dictionary(45 unique tokens: ['chang', 'product', 'revert', 'asset', 'autoescap']...) from 2 documents (total 169 corpus positions)\n",
      "2020-12-23 02:49:21,706 : INFO : token count processed\n",
      "2020-12-23 02:49:21,709 : INFO : frequencies processed\n",
      "2020-12-23 02:49:21,837 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:21,838 : INFO : entropies processed\n",
      "2020-12-23 02:49:21,839 : INFO : extropies processed\n",
      "2020-12-23 02:49:21,840 : INFO : token count processed\n",
      "2020-12-23 02:49:21,841 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:21,841 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:21,842 : INFO : vocab #2480\n",
      "2020-12-23 02:49:21,843 : INFO : diff #set()\n",
      "2020-12-23 02:49:22,102 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:22,229 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.2707399748333883, 0.4403850775883634], [0.9358479306101799, 0.06415207], [0.0, 0.0], [1.584962500721156, 4.8191513650620195, 4.867152857268882, 1.536961008514294, 3.2821903565477255, 0.04800149220686212]]\n",
      "2020-12-23 02:49:22,232 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:22,233 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:22,234 : INFO : built Dictionary(51 unique tokens: ['chang', 'product', 'revert', '\"),', 'autoescap']...) from 2 documents (total 224 corpus positions)\n",
      "2020-12-23 02:49:22,245 : INFO : token count processed\n",
      "2020-12-23 02:49:22,247 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:22,248 : INFO : frequencies processed\n",
      "2020-12-23 02:49:22,249 : INFO : token count processed\n",
      "2020-12-23 02:49:22,250 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:22,251 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:22,252 : INFO : vocab #2480\n",
      "2020-12-23 02:49:22,253 : INFO : diff #set()\n",
      "2020-12-23 02:49:22,519 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:22,647 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.3124518245096501, 0.4324414413312362], [1.0, 0.0], [nan, nan], [1.584962500721156, 5.062480936779194, 5.11827206070707, 1.52917137679328, 3.5333095599859137, 0.05579112392787611]]\n",
      "2020-12-23 02:49:22,650 : INFO : Removed 0 and 223 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:22,651 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:22,654 : INFO : built Dictionary(241 unique tokens: ['chang', 'product', 'revert', '\"\"\"', '\"\")']...) from 2 documents (total 1778 corpus positions)\n",
      "2020-12-23 02:49:22,692 : INFO : token count processed\n",
      "2020-12-23 02:49:22,695 : INFO : frequencies processed\n",
      "2020-12-23 02:49:22,825 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:22,826 : INFO : entropies processed\n",
      "2020-12-23 02:49:22,827 : INFO : extropies processed\n",
      "2020-12-23 02:49:22,828 : INFO : token count processed\n",
      "2020-12-23 02:49:22,830 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:22,831 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:22,832 : INFO : vocab #2480\n",
      "2020-12-23 02:49:22,834 : INFO : diff #set()\n",
      "2020-12-23 02:49:23,093 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:23,220 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.317432259710435, 0.43151207367975053], [0.9973887884989381, 0.0026112115], [0.0, 0.0], [1.584962500721156, 7.185085743102134, 7.1919168766277854, 1.5781313671955042, 5.606954375906629, 0.006831133525651012]]\n",
      "2020-12-23 02:49:23,223 : INFO : Removed 0 and 148 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:23,224 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:23,225 : INFO : built Dictionary(158 unique tokens: ['chang', 'product', 'revert', '\"\",', '\")).']...) from 2 documents (total 753 corpus positions)\n",
      "2020-12-23 02:49:23,244 : INFO : token count processed\n",
      "2020-12-23 02:49:23,246 : INFO : frequencies processed\n",
      "2020-12-23 02:49:23,375 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:23,376 : INFO : entropies processed\n",
      "2020-12-23 02:49:23,378 : INFO : extropies processed\n",
      "2020-12-23 02:49:23,379 : INFO : token count processed\n",
      "2020-12-23 02:49:23,380 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:23,380 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:23,381 : INFO : vocab #2480\n",
      "2020-12-23 02:49:23,382 : INFO : diff #set()\n",
      "2020-12-23 02:49:23,639 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:23,766 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.310526369678834, 0.43280181222904685], [0.9827795084565878, 0.017220492], [0.0, 0.0], [1.584962500721156, 6.591225336124281, 6.602427568080092, 1.5737602687653443, 5.017465067358936, 0.011202231955810937]]\n",
      "2020-12-23 02:49:23,769 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:23,770 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:23,771 : INFO : built Dictionary(37 unique tokens: ['chang', 'product', 'revert', 'autoescap', 'class']...) from 2 documents (total 91 corpus positions)\n",
      "2020-12-23 02:49:23,779 : INFO : token count processed\n",
      "2020-12-23 02:49:23,783 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:23,784 : INFO : frequencies processed\n",
      "2020-12-23 02:49:23,784 : INFO : token count processed\n",
      "2020-12-23 02:49:23,785 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:23,786 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:23,786 : INFO : vocab #2480\n",
      "2020-12-23 02:49:23,787 : INFO : diff #set()\n",
      "2020-12-23 02:49:24,050 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:24,178 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.3149614549252813, 0.43197263516954604], [1.0, 0.0], [nan, nan], [1.584962500721156, 4.7032114441396695, 4.8088395992355055, 1.47933434562532, 3.2238770985143494, 0.10562815509583601]]\n",
      "2020-12-23 02:49:24,180 : INFO : Removed 0 and 9 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:24,181 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:24,183 : INFO : built Dictionary(115 unique tokens: ['chang', 'product', 'revert', '\"\"\"', '\"=\"']...) from 2 documents (total 331 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:49:24,200 : INFO : token count processed\n",
      "2020-12-23 02:49:24,204 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:24,204 : INFO : frequencies processed\n",
      "2020-12-23 02:49:24,208 : INFO : token count processed\n",
      "2020-12-23 02:49:24,209 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:24,210 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:24,210 : INFO : vocab #2480\n",
      "2020-12-23 02:49:24,211 : INFO : diff #set()\n",
      "2020-12-23 02:49:24,476 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:24,606 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.31139939768049, 0.4326383406535058], [0.997593856183812, 0.0024061438], [nan, nan], [1.584962500721156, 6.14228447828618, 6.174961694738162, 1.552285284269174, 4.589999194017006, 0.03267721645198218]]\n",
      "2020-12-23 02:49:24,609 : INFO : Removed 0 and 184 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:24,610 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:24,612 : INFO : built Dictionary(252 unique tokens: ['chang', 'product', 'revert', '\"\"\"', '\"--']...) from 2 documents (total 1121 corpus positions)\n",
      "2020-12-23 02:49:24,644 : INFO : token count processed\n",
      "2020-12-23 02:49:24,647 : INFO : frequencies processed\n",
      "2020-12-23 02:49:24,774 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:24,775 : INFO : entropies processed\n",
      "2020-12-23 02:49:24,776 : INFO : extropies processed\n",
      "2020-12-23 02:49:24,778 : INFO : token count processed\n",
      "2020-12-23 02:49:24,779 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:24,780 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:24,781 : INFO : vocab #2480\n",
      "2020-12-23 02:49:24,782 : INFO : diff #set()\n",
      "2020-12-23 02:49:25,042 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:25,171 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.3110709275832535, 0.43269983108901194], [0.9908655546605587, 0.009134445], [0.0, 0.0], [1.584962500721156, 7.450178124335845, 7.458624158356567, 1.5765164667004328, 5.873661657635411, 0.008446034020722415]]\n",
      "2020-12-23 02:49:25,176 : INFO : Removed 0 and 16 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:25,177 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:25,178 : INFO : built Dictionary(54 unique tokens: ['chang', 'product', 'revert', 'argv', 'autoescap']...) from 2 documents (total 169 corpus positions)\n",
      "2020-12-23 02:49:25,188 : INFO : token count processed\n",
      "2020-12-23 02:49:25,190 : INFO : frequencies processed\n",
      "2020-12-23 02:49:25,318 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:25,319 : INFO : entropies processed\n",
      "2020-12-23 02:49:25,319 : INFO : extropies processed\n",
      "2020-12-23 02:49:25,321 : INFO : token count processed\n",
      "2020-12-23 02:49:25,321 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:25,322 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:25,323 : INFO : vocab #2480\n",
      "2020-12-23 02:49:25,324 : INFO : diff #set()\n",
      "2020-12-23 02:49:25,581 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:25,709 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.265060058857382, 0.4414893971970234], [0.9215163365006447, 0.07848366], [0.0, 0.0], [1.584962500721156, 5.20665021947654, 5.245165276616264, 1.5464474435814317, 3.660202775895108, 0.03851505713972436]]\n",
      "2020-12-23 02:49:25,712 : INFO : Removed 0 and 89 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:25,712 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:25,714 : INFO : built Dictionary(124 unique tokens: ['chang', 'product', 'revert', '\"--', '(\"--']...) from 2 documents (total 507 corpus positions)\n",
      "2020-12-23 02:49:25,735 : INFO : token count processed\n",
      "2020-12-23 02:49:25,738 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:25,739 : INFO : frequencies processed\n",
      "2020-12-23 02:49:25,740 : INFO : token count processed\n",
      "2020-12-23 02:49:25,741 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:25,742 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:25,743 : INFO : vocab #2480\n",
      "2020-12-23 02:49:25,744 : INFO : diff #set()\n",
      "2020-12-23 02:49:26,001 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:26,130 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.3207201134865856, 0.4309007338664496], [0.9995036883046851, 0.0004963117], [nan, nan], [1.584962500721156, 6.524718477352, 6.545524725837928, 1.564156252235228, 4.960562225116772, 0.020806248485928158]]\n",
      "2020-12-23 02:49:26,132 : INFO : Removed 0 and 8 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:26,133 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:26,134 : INFO : built Dictionary(60 unique tokens: ['chang', 'product', 'revert', '(),', 'array']...) from 2 documents (total 214 corpus positions)\n",
      "2020-12-23 02:49:26,142 : INFO : token count processed\n",
      "2020-12-23 02:49:26,147 : INFO : frequencies processed\n",
      "2020-12-23 02:49:26,283 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:26,284 : INFO : entropies processed\n",
      "2020-12-23 02:49:26,284 : INFO : extropies processed\n",
      "2020-12-23 02:49:26,286 : INFO : token count processed\n",
      "2020-12-23 02:49:26,286 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:26,287 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:26,288 : INFO : vocab #2480\n",
      "2020-12-23 02:49:26,289 : INFO : diff #set()\n",
      "2020-12-23 02:49:26,553 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:26,681 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.2803270985581934, 0.4385335773241833], [0.9429499320685863, 0.057050068], [0.0, 0.0], [1.584962500721156, 5.321859380715434, 5.3570676852425985, 1.5497541961939918, 3.7721051845214424, 0.03520830452716428]]\n",
      "2020-12-23 02:49:26,683 : INFO : Removed 0 and 119 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:26,684 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:26,686 : INFO : built Dictionary(139 unique tokens: ['chang', 'product', 'revert', '\"--', '\"{}']...) from 2 documents (total 832 corpus positions)\n",
      "2020-12-23 02:49:26,706 : INFO : token count processed\n",
      "2020-12-23 02:49:26,710 : INFO : frequencies processed\n",
      "2020-12-23 02:49:26,837 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:26,837 : INFO : entropies processed\n",
      "2020-12-23 02:49:26,838 : INFO : extropies processed\n",
      "2020-12-23 02:49:26,839 : INFO : token count processed\n",
      "2020-12-23 02:49:26,840 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:26,841 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:26,841 : INFO : vocab #2480\n",
      "2020-12-23 02:49:26,842 : INFO : diff #set()\n",
      "2020-12-23 02:49:27,099 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:27,226 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.3107540932877275, 0.4327591598365215], [0.9825321566313505, 0.017467843], [0.0, 0.0], [1.584962500721156, 6.500767808767801, 6.511912622581315, 1.573817686907642, 4.926950121860159, 0.011144813813514176]]\n",
      "2020-12-23 02:49:27,228 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:27,229 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:27,230 : INFO : built Dictionary(33 unique tokens: ['chang', 'product', 'revert', 'accept', 'api']...) from 2 documents (total 54 corpus positions)\n",
      "2020-12-23 02:49:27,240 : INFO : token count processed\n",
      "2020-12-23 02:49:27,243 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:27,243 : INFO : frequencies processed\n",
      "2020-12-23 02:49:27,244 : INFO : token count processed\n",
      "2020-12-23 02:49:27,245 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:27,246 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:27,247 : INFO : vocab #2480\n",
      "2020-12-23 02:49:27,248 : INFO : diff #set()\n",
      "2020-12-23 02:49:27,517 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:27,651 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.3131994757741605, 0.4323016715475128], [1.0, 0.0], [nan, nan], [1.584962500721156, 4.736228843383063, 4.869742159207975, 1.4514491848962434, 3.284779658486819, 0.13351331582491266]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:49:27,653 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:27,654 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:27,655 : INFO : built Dictionary(95 unique tokens: ['chang', 'product', 'revert', '(\",\")', '(\"--']...) from 2 documents (total 395 corpus positions)\n",
      "2020-12-23 02:49:27,672 : INFO : token count processed\n",
      "2020-12-23 02:49:27,677 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:27,678 : INFO : frequencies processed\n",
      "2020-12-23 02:49:27,681 : INFO : token count processed\n",
      "2020-12-23 02:49:27,683 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:27,685 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:27,687 : INFO : vocab #2480\n",
      "2020-12-23 02:49:27,688 : INFO : diff #set()\n",
      "2020-12-23 02:49:27,951 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:28,081 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.3189712379915628, 0.4312257019910647], [0.9994099817704409, 0.00059001823], [nan, nan], [1.584962500721156, 5.788442787590127, 5.820165808942752, 1.5532394793685311, 4.235203308221596, 0.03172302135262495]]\n",
      "2020-12-23 02:49:28,084 : INFO : Removed 0 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:28,084 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:28,085 : INFO : built Dictionary(52 unique tokens: ['chang', 'product', 'revert', 'author', 'buddi']...) from 2 documents (total 81 corpus positions)\n",
      "2020-12-23 02:49:28,090 : INFO : token count processed\n",
      "2020-12-23 02:49:28,093 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:28,093 : INFO : frequencies processed\n",
      "2020-12-23 02:49:28,094 : INFO : token count processed\n",
      "2020-12-23 02:49:28,095 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:28,096 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:28,097 : INFO : vocab #2480\n",
      "2020-12-23 02:49:28,098 : INFO : diff #set()\n",
      "2020-12-23 02:49:28,355 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:28,482 : INFO : Computed distances or similarities ('286', 'sacp-python-common/setup.py')[[1.3094134200870222, 0.4330103875304918], [1.0, 0.0], [nan, nan], [1.584962500721156, 5.370004292053436, 5.455882542044965, 1.4990842507296271, 3.870920041323809, 0.08587824999152893]]\n",
      "2020-12-23 02:49:28,484 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:28,485 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:28,486 : INFO : built Dictionary(78 unique tokens: ['chang', 'product', 'revert', '\"--', '\"])']...) from 2 documents (total 326 corpus positions)\n",
      "2020-12-23 02:49:28,494 : INFO : token count processed\n",
      "2020-12-23 02:49:28,497 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:28,497 : INFO : frequencies processed\n",
      "2020-12-23 02:49:28,498 : INFO : token count processed\n",
      "2020-12-23 02:49:28,499 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:28,500 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:28,501 : INFO : vocab #2480\n",
      "2020-12-23 02:49:28,502 : INFO : diff #set()\n",
      "2020-12-23 02:49:28,760 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:28,887 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.3176312231966765, 0.4314750293278815], [0.9950516419485211, 0.004948358], [nan, nan], [1.584962500721156, 5.695663584743922, 5.731262966700586, 1.549363118764492, 4.14630046597943, 0.03559938195666401]]\n",
      "2020-12-23 02:49:28,889 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:28,890 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:28,892 : INFO : built Dictionary(39 unique tokens: ['chang', 'product', 'revert', 'actual', 'assert']...) from 2 documents (total 101 corpus positions)\n",
      "2020-12-23 02:49:28,901 : INFO : token count processed\n",
      "2020-12-23 02:49:28,903 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:28,903 : INFO : frequencies processed\n",
      "2020-12-23 02:49:28,905 : INFO : token count processed\n",
      "2020-12-23 02:49:28,906 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:28,907 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:28,907 : INFO : vocab #2480\n",
      "2020-12-23 02:49:28,909 : INFO : diff #set()\n",
      "2020-12-23 02:49:29,169 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:29,297 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.3143930926235512, 0.4320787178233492], [0.9920962387695909, 0.007903761], [nan, nan], [1.584962500721156, 4.9004417692112465, 4.992890046984738, 1.4925142229476647, 3.407927546263582, 0.0924482777734914]]\n",
      "2020-12-23 02:49:29,299 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:29,300 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:29,301 : INFO : built Dictionary(37 unique tokens: ['chang', 'product', 'revert', 'actual', 'assert']...) from 2 documents (total 108 corpus positions)\n",
      "2020-12-23 02:49:29,305 : INFO : token count processed\n",
      "2020-12-23 02:49:29,307 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:29,307 : INFO : frequencies processed\n",
      "2020-12-23 02:49:29,308 : INFO : token count processed\n",
      "2020-12-23 02:49:29,309 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:29,310 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:29,311 : INFO : vocab #2480\n",
      "2020-12-23 02:49:29,311 : INFO : diff #set()\n",
      "2020-12-23 02:49:29,580 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:29,711 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.3205592238006334, 0.4309306092012557], [0.9948305799625814, 0.00516942], [nan, nan], [1.584962500721156, 4.778624108914332, 4.87156499724284, 1.4920216123926489, 3.2866024965216836, 0.09294088832850722]]\n",
      "2020-12-23 02:49:29,714 : INFO : Removed 0 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:29,715 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:29,716 : INFO : built Dictionary(38 unique tokens: ['chang', 'product', 'revert', 'actual', 'assert']...) from 2 documents (total 139 corpus positions)\n",
      "2020-12-23 02:49:29,726 : INFO : token count processed\n",
      "2020-12-23 02:49:29,730 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:29,730 : INFO : frequencies processed\n",
      "2020-12-23 02:49:29,731 : INFO : token count processed\n",
      "2020-12-23 02:49:29,732 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:29,733 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:29,734 : INFO : vocab #2480\n",
      "2020-12-23 02:49:29,736 : INFO : diff #set()\n",
      "2020-12-23 02:49:30,008 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:30,136 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.3118383081657614, 0.4325562027706909], [0.992330954875797, 0.007669045], [nan, nan], [1.584962500721156, 4.773880192225086, 4.852860629625752, 1.5059820633204897, 3.267898128904596, 0.0789804374006664]]\n",
      "2020-12-23 02:49:30,139 : INFO : Removed 0 and 346 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:30,140 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:30,141 : INFO : built Dictionary(149 unique tokens: ['chang', 'product', 'revert', '\"))', '\"):']...) from 2 documents (total 1964 corpus positions)\n",
      "2020-12-23 02:49:30,160 : INFO : token count processed\n",
      "2020-12-23 02:49:30,164 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:30,164 : INFO : frequencies processed\n",
      "2020-12-23 02:49:30,169 : INFO : token count processed\n",
      "2020-12-23 02:49:30,170 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:30,172 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:30,174 : INFO : vocab #2480\n",
      "2020-12-23 02:49:30,175 : INFO : diff #set()\n",
      "2020-12-23 02:49:30,441 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:30,572 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.3216724875834207, 0.4307239739231603], [1.0, 0.0], [nan, nan], [1.584962500721156, 6.620773041953877, 6.628558317167468, 1.5771772255075645, 5.043595816446312, 0.007785275213591625]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:49:30,574 : INFO : Removed 0 and 132 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:30,575 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:30,577 : INFO : built Dictionary(79 unique tokens: ['chang', 'product', 'revert', '\"))', '\"):']...) from 2 documents (total 527 corpus positions)\n",
      "2020-12-23 02:49:30,595 : INFO : token count processed\n",
      "2020-12-23 02:49:30,600 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:30,601 : INFO : frequencies processed\n",
      "2020-12-23 02:49:30,603 : INFO : token count processed\n",
      "2020-12-23 02:49:30,604 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:30,604 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:30,605 : INFO : vocab #2480\n",
      "2020-12-23 02:49:30,606 : INFO : diff #set()\n",
      "2020-12-23 02:49:30,858 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:30,985 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.3132230413388768, 0.43229726754805586], [0.999186270928476, 0.0008137291], [nan, nan], [1.584962500721156, 5.828370634755606, 5.851019361184401, 1.5623137742923614, 4.2660568604632445, 0.022648726428794674]]\n",
      "2020-12-23 02:49:30,987 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:30,988 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:30,989 : INFO : built Dictionary(80 unique tokens: ['chang', 'product', 'revert', '\"):', '\"].']...) from 2 documents (total 290 corpus positions)\n",
      "2020-12-23 02:49:30,997 : INFO : token count processed\n",
      "2020-12-23 02:49:30,999 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:31,000 : INFO : frequencies processed\n",
      "2020-12-23 02:49:31,001 : INFO : token count processed\n",
      "2020-12-23 02:49:31,002 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:31,003 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:31,004 : INFO : vocab #2480\n",
      "2020-12-23 02:49:31,005 : INFO : diff #set()\n",
      "2020-12-23 02:49:31,262 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:31,389 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.3204987988119812, 0.4309418304857417], [0.9982826834311709, 0.0017173166], [nan, nan], [1.584962500721156, 5.774409284925443, 5.811828342957583, 1.5475434426890153, 4.226865842236427, 0.03741905803214074]]\n",
      "2020-12-23 02:49:31,392 : INFO : Removed 0 and 51 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:31,392 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:31,393 : INFO : built Dictionary(91 unique tokens: ['chang', 'product', 'revert', '\"])', '\"],']...) from 2 documents (total 339 corpus positions)\n",
      "2020-12-23 02:49:31,405 : INFO : token count processed\n",
      "2020-12-23 02:49:31,407 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:31,408 : INFO : frequencies processed\n",
      "2020-12-23 02:49:31,409 : INFO : token count processed\n",
      "2020-12-23 02:49:31,410 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:31,411 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:31,412 : INFO : vocab #2480\n",
      "2020-12-23 02:49:31,414 : INFO : diff #set()\n",
      "2020-12-23 02:49:31,671 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:31,798 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.3143371034516567, 0.43208917080773435], [0.9984773385804147, 0.0015226614], [nan, nan], [1.584962500721156, 5.977819040873918, 6.0091009116735075, 1.5536806299215664, 4.424138410952351, 0.03128187079958966]]\n",
      "2020-12-23 02:49:31,800 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:31,801 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:31,802 : INFO : built Dictionary(72 unique tokens: ['chang', 'product', 'revert', '\").', '\"]),']...) from 2 documents (total 286 corpus positions)\n",
      "2020-12-23 02:49:31,809 : INFO : token count processed\n",
      "2020-12-23 02:49:31,811 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:31,812 : INFO : frequencies processed\n",
      "2020-12-23 02:49:31,813 : INFO : token count processed\n",
      "2020-12-23 02:49:31,814 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:31,815 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:31,816 : INFO : vocab #2480\n",
      "2020-12-23 02:49:31,817 : INFO : diff #set()\n",
      "2020-12-23 02:49:32,076 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:32,203 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.319845178184671, 0.43106324913566935], [0.9987621944164857, 0.0012378056], [nan, nan], [1.584962500721156, 5.901812829596593, 5.937340746893776, 1.5494345834239729, 4.35237824617262, 0.03552791729718319]]\n",
      "2020-12-23 02:49:32,206 : INFO : Removed 0 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:32,207 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:32,208 : INFO : built Dictionary(74 unique tokens: ['chang', 'product', 'revert', '\"],', '\"][']...) from 2 documents (total 286 corpus positions)\n",
      "2020-12-23 02:49:32,215 : INFO : token count processed\n",
      "2020-12-23 02:49:32,217 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:32,218 : INFO : frequencies processed\n",
      "2020-12-23 02:49:32,219 : INFO : token count processed\n",
      "2020-12-23 02:49:32,220 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:32,220 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:32,221 : INFO : vocab #2480\n",
      "2020-12-23 02:49:32,222 : INFO : diff #set()\n",
      "2020-12-23 02:49:32,480 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:32,608 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.3104361874469306, 0.4328187055904003], [0.9991140657220967, 0.0008859343], [nan, nan], [1.584962500721156, 5.643202320803383, 5.6828229933746695, 1.5453418281498692, 4.097860492653513, 0.039620672571286875]]\n",
      "2020-12-23 02:49:32,611 : INFO : Removed 0 and 30 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:32,611 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:32,612 : INFO : built Dictionary(87 unique tokens: ['chang', 'product', 'revert', '\"))', '\")).']...) from 2 documents (total 320 corpus positions)\n",
      "2020-12-23 02:49:32,631 : INFO : token count processed\n",
      "2020-12-23 02:49:32,633 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:32,634 : INFO : frequencies processed\n",
      "2020-12-23 02:49:32,635 : INFO : token count processed\n",
      "2020-12-23 02:49:32,636 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:32,636 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:32,637 : INFO : vocab #2480\n",
      "2020-12-23 02:49:32,638 : INFO : diff #set()\n",
      "2020-12-23 02:49:32,891 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:33,017 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.3168465879483011, 0.4316211548929343], [1.0, 0.0], [nan, nan], [1.584962500721156, 5.925214310725336, 5.959178520338144, 1.550998291108348, 4.374216019616988, 0.033964209612808105]]\n",
      "2020-12-23 02:49:33,020 : INFO : Removed 0 and 293 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:33,020 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:33,022 : INFO : built Dictionary(160 unique tokens: ['chang', 'product', 'revert', '\"))', '\"):']...) from 2 documents (total 1715 corpus positions)\n",
      "2020-12-23 02:49:33,048 : INFO : token count processed\n",
      "2020-12-23 02:49:33,050 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:33,053 : INFO : frequencies processed\n",
      "2020-12-23 02:49:33,055 : INFO : token count processed\n",
      "2020-12-23 02:49:33,057 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:33,058 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:33,059 : INFO : vocab #2480\n",
      "2020-12-23 02:49:33,059 : INFO : diff #set()\n",
      "2020-12-23 02:49:33,313 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:33,440 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.3215274069878467, 0.4307508914131183], [1.0, 0.0], [nan, nan], [1.584962500721156, 6.551685682764175, 6.560442866027911, 1.5762053174574202, 4.975480365306755, 0.008757183263735868]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:49:33,443 : INFO : Removed 0 and 88 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:33,444 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:33,445 : INFO : built Dictionary(136 unique tokens: ['chang', 'product', 'revert', '\")}', '\"])']...) from 2 documents (total 556 corpus positions)\n",
      "2020-12-23 02:49:33,460 : INFO : token count processed\n",
      "2020-12-23 02:49:33,462 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:33,463 : INFO : frequencies processed\n",
      "2020-12-23 02:49:33,464 : INFO : token count processed\n",
      "2020-12-23 02:49:33,465 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:33,466 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:33,467 : INFO : vocab #2480\n",
      "2020-12-23 02:49:33,468 : INFO : diff #set()\n",
      "2020-12-23 02:49:33,726 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:33,854 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.3112750364874606, 0.43266161932841235], [0.9980835324386135, 0.0019164676], [nan, nan], [1.584962500721156, 6.642985062562557, 6.66221143513323, 1.5657361281504834, 5.077248934412074, 0.019226372570672723]]\n",
      "2020-12-23 02:49:33,857 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:33,858 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:33,859 : INFO : built Dictionary(49 unique tokens: ['chang', 'product', 'revert', 'assert', 'assur']...) from 2 documents (total 151 corpus positions)\n",
      "2020-12-23 02:49:33,869 : INFO : token count processed\n",
      "2020-12-23 02:49:33,871 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:33,872 : INFO : frequencies processed\n",
      "2020-12-23 02:49:33,873 : INFO : token count processed\n",
      "2020-12-23 02:49:33,874 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:33,875 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:33,876 : INFO : vocab #2480\n",
      "2020-12-23 02:49:33,877 : INFO : diff #set()\n",
      "2020-12-23 02:49:34,149 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:34,278 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.3165901551450978, 0.43166893279720675], [1.0, 0.0], [nan, nan], [1.584962500721156, 5.2461980344571995, 5.3107366452443445, 1.520423889934011, 3.7257741445231884, 0.06453861078714507]]\n",
      "2020-12-23 02:49:34,280 : INFO : Removed 0 and 23 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:34,281 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:34,282 : INFO : built Dictionary(71 unique tokens: ['chang', 'product', 'revert', '__main__', '__name__']...) from 2 documents (total 211 corpus positions)\n",
      "2020-12-23 02:49:34,289 : INFO : token count processed\n",
      "2020-12-23 02:49:34,291 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:34,292 : INFO : frequencies processed\n",
      "2020-12-23 02:49:34,294 : INFO : token count processed\n",
      "2020-12-23 02:49:34,295 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:34,297 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:34,297 : INFO : vocab #2480\n",
      "2020-12-23 02:49:34,299 : INFO : diff #set()\n",
      "2020-12-23 02:49:34,562 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:34,689 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/test_auth_utility.py')[[1.3193199506811064, 0.43116086666108033], [1.0, 0.0], [nan, nan], [1.584962500721156, 5.903090303960449, 5.94668878645415, 1.5413640182274557, 4.361726285732994, 0.04359848249370035]]\n",
      "2020-12-23 02:49:34,692 : INFO : Removed 0 and 442 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:34,693 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:34,695 : INFO : built Dictionary(105 unique tokens: ['chang', 'product', 'revert', '\":\"', '\":{\"']...) from 2 documents (total 1210 corpus positions)\n",
      "2020-12-23 02:49:34,712 : INFO : token count processed\n",
      "2020-12-23 02:49:34,720 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:34,721 : INFO : frequencies processed\n",
      "2020-12-23 02:49:34,725 : INFO : token count processed\n",
      "2020-12-23 02:49:34,726 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:34,727 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:34,727 : INFO : vocab #2480\n",
      "2020-12-23 02:49:34,728 : INFO : diff #set()\n",
      "2020-12-23 02:49:34,980 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:35,108 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.3114140853955734, 0.4326355914841892], [0.9993009518948384, 0.0006990481], [nan, nan], [1.584962500721156, 6.16659449033757, 6.177426429396605, 1.5741305616621206, 4.592463928675449, 0.010831939059035456]]\n",
      "2020-12-23 02:49:35,110 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:35,111 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:35,112 : INFO : built Dictionary(64 unique tokens: ['chang', 'product', 'revert', '\":\"', '\"},']...) from 2 documents (total 245 corpus positions)\n",
      "2020-12-23 02:49:35,118 : INFO : token count processed\n",
      "2020-12-23 02:49:35,121 : INFO : frequencies processed\n",
      "2020-12-23 02:49:35,249 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:35,250 : INFO : entropies processed\n",
      "2020-12-23 02:49:35,251 : INFO : extropies processed\n",
      "2020-12-23 02:49:35,252 : INFO : token count processed\n",
      "2020-12-23 02:49:35,253 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:35,254 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:35,255 : INFO : vocab #2480\n",
      "2020-12-23 02:49:35,256 : INFO : diff #set()\n",
      "2020-12-23 02:49:35,514 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:35,642 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.287012986773664, 0.43725156165847595], [0.9446444027125835, 0.055355597], [0.0, 0.0], [1.584962500721156, 5.906856253399655, 5.931396453787886, 1.5604223003329256, 4.34643395306673, 0.02454020038823046]]\n",
      "2020-12-23 02:49:35,645 : INFO : Removed 0 and 68 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:35,645 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:35,646 : INFO : built Dictionary(78 unique tokens: ['chang', 'product', 'revert', '())', '/\",']...) from 2 documents (total 244 corpus positions)\n",
      "2020-12-23 02:49:35,662 : INFO : token count processed\n",
      "2020-12-23 02:49:35,664 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:35,665 : INFO : frequencies processed\n",
      "2020-12-23 02:49:35,666 : INFO : token count processed\n",
      "2020-12-23 02:49:35,667 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:35,668 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:35,669 : INFO : vocab #2480\n",
      "2020-12-23 02:49:35,670 : INFO : diff #set()\n",
      "2020-12-23 02:49:35,923 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:36,050 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.3130627087273103, 0.43232723273214607], [0.9980849010171369, 0.001915099], [nan, nan], [1.584962500721156, 5.965115449163356, 6.0012310591475835, 1.5488468907369288, 4.416268558426427, 0.03611560998422725]]\n",
      "2020-12-23 02:49:36,052 : INFO : Removed 0 and 13 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:36,053 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:36,054 : INFO : built Dictionary(84 unique tokens: ['chang', 'product', 'revert', '\"))', '\"):']...) from 2 documents (total 358 corpus positions)\n",
      "2020-12-23 02:49:36,062 : INFO : token count processed\n",
      "2020-12-23 02:49:36,065 : INFO : frequencies processed\n",
      "2020-12-23 02:49:36,193 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:36,193 : INFO : entropies processed\n",
      "2020-12-23 02:49:36,194 : INFO : extropies processed\n",
      "2020-12-23 02:49:36,195 : INFO : token count processed\n",
      "2020-12-23 02:49:36,196 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:36,197 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:36,198 : INFO : vocab #2480\n",
      "2020-12-23 02:49:36,199 : INFO : diff #set()\n",
      "2020-12-23 02:49:36,457 : INFO : alphabet #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:49:36,584 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.2899226045965426, 0.4366959817736671], [0.949048139154911, 0.05095186], [0.0, 0.0], [1.584962500721156, 5.791362404253194, 5.814005445316539, 1.562319459657811, 4.229042944595383, 0.022643041063345137]]\n",
      "2020-12-23 02:49:36,587 : INFO : Removed 0 and 41 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:36,588 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:36,590 : INFO : built Dictionary(74 unique tokens: ['chang', 'product', 'revert', '\"))', '\"):']...) from 2 documents (total 370 corpus positions)\n",
      "2020-12-23 02:49:36,603 : INFO : token count processed\n",
      "2020-12-23 02:49:36,606 : INFO : frequencies processed\n",
      "2020-12-23 02:49:36,738 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:36,739 : INFO : entropies processed\n",
      "2020-12-23 02:49:36,740 : INFO : extropies processed\n",
      "2020-12-23 02:49:36,742 : INFO : token count processed\n",
      "2020-12-23 02:49:36,744 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:36,746 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:36,747 : INFO : vocab #2480\n",
      "2020-12-23 02:49:36,748 : INFO : diff #set()\n",
      "2020-12-23 02:49:37,018 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:37,150 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.3125338136478442, 0.43242610944683957], [0.9920577649027109, 0.007942235], [0.0, 0.0], [1.584962500721156, 5.651670454631116, 5.679422822039651, 1.557210133312621, 4.094460321318495, 0.02775236740853515]]\n",
      "2020-12-23 02:49:37,152 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:37,153 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:37,154 : INFO : built Dictionary(38 unique tokens: ['chang', 'product', 'revert', 'arg', 'assert']...) from 2 documents (total 80 corpus positions)\n",
      "2020-12-23 02:49:37,158 : INFO : token count processed\n",
      "2020-12-23 02:49:37,160 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:37,160 : INFO : frequencies processed\n",
      "2020-12-23 02:49:37,161 : INFO : token count processed\n",
      "2020-12-23 02:49:37,162 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:37,163 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:37,164 : INFO : vocab #2480\n",
      "2020-12-23 02:49:37,166 : INFO : diff #set()\n",
      "2020-12-23 02:49:37,439 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:37,566 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.3178391728367735, 0.4314363186709425], [0.9971500367391855, 0.0028499633], [nan, nan], [1.584962500721156, 4.8226207261920235, 4.931245602535515, 1.4763376243776642, 3.3462831018143593, 0.10862487634349183]]\n",
      "2020-12-23 02:49:37,569 : INFO : Removed 0 and 50 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:37,570 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:37,571 : INFO : built Dictionary(82 unique tokens: ['chang', 'product', 'revert', '\")):', '.\",']...) from 2 documents (total 250 corpus positions)\n",
      "2020-12-23 02:49:37,587 : INFO : token count processed\n",
      "2020-12-23 02:49:37,590 : INFO : frequencies processed\n",
      "2020-12-23 02:49:37,718 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:37,718 : INFO : entropies processed\n",
      "2020-12-23 02:49:37,719 : INFO : extropies processed\n",
      "2020-12-23 02:49:37,720 : INFO : token count processed\n",
      "2020-12-23 02:49:37,720 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:37,721 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:37,721 : INFO : vocab #2480\n",
      "2020-12-23 02:49:37,722 : INFO : diff #set()\n",
      "2020-12-23 02:49:37,979 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:38,106 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.2973852508884336, 0.4352774527534225], [0.9492213912308216, 0.05077861], [0.0, 0.0], [1.584962500721156, 6.24862851613934, 6.270752856966281, 1.5628381598942154, 4.685790356245125, 0.022124340826940703]]\n",
      "2020-12-23 02:49:38,108 : INFO : Removed 0 and 32 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:38,109 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:38,110 : INFO : built Dictionary(83 unique tokens: ['chang', 'product', 'revert', '\"))', '\"):']...) from 2 documents (total 366 corpus positions)\n",
      "2020-12-23 02:49:38,118 : INFO : token count processed\n",
      "2020-12-23 02:49:38,120 : INFO : frequencies processed\n",
      "2020-12-23 02:49:38,249 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:38,250 : INFO : entropies processed\n",
      "2020-12-23 02:49:38,251 : INFO : extropies processed\n",
      "2020-12-23 02:49:38,252 : INFO : token count processed\n",
      "2020-12-23 02:49:38,253 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:38,254 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:38,254 : INFO : vocab #2480\n",
      "2020-12-23 02:49:38,255 : INFO : diff #set()\n",
      "2020-12-23 02:49:38,513 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:38,639 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.292282048930786, 0.43624649090038503], [0.9510683119297028, 0.048931688], [0.0, 0.0], [1.584962500721156, 5.850156917433494, 5.871587635432316, 1.5635317827223334, 4.28662513471116, 0.02143071799882268]]\n",
      "2020-12-23 02:49:38,642 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:38,642 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:38,643 : INFO : built Dictionary(78 unique tokens: ['chang', 'product', 'revert', '\"))', '\"):']...) from 2 documents (total 364 corpus positions)\n",
      "2020-12-23 02:49:38,651 : INFO : token count processed\n",
      "2020-12-23 02:49:38,654 : INFO : frequencies processed\n",
      "2020-12-23 02:49:38,781 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:38,782 : INFO : entropies processed\n",
      "2020-12-23 02:49:38,783 : INFO : extropies processed\n",
      "2020-12-23 02:49:38,784 : INFO : token count processed\n",
      "2020-12-23 02:49:38,785 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:38,786 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:38,787 : INFO : vocab #2480\n",
      "2020-12-23 02:49:38,788 : INFO : diff #set()\n",
      "2020-12-23 02:49:39,055 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:39,185 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.310845333643233, 0.43274207297267264], [0.991928094998002, 0.008071905], [0.0, 0.0], [1.584962500721156, 5.6831976040360095, 5.710885949865319, 1.5572741548918465, 4.125923449144163, 0.02768834582930957]]\n",
      "2020-12-23 02:49:39,188 : INFO : Removed 0 and 52 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:39,188 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:39,189 : INFO : built Dictionary(65 unique tokens: ['chang', 'product', 'revert', '\"\",', '99999']...) from 2 documents (total 342 corpus positions)\n",
      "2020-12-23 02:49:39,196 : INFO : token count processed\n",
      "2020-12-23 02:49:39,198 : INFO : frequencies processed\n",
      "2020-12-23 02:49:39,327 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:39,328 : INFO : entropies processed\n",
      "2020-12-23 02:49:39,328 : INFO : extropies processed\n",
      "2020-12-23 02:49:39,330 : INFO : token count processed\n",
      "2020-12-23 02:49:39,331 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:39,332 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:39,332 : INFO : vocab #2480\n",
      "2020-12-23 02:49:39,333 : INFO : diff #set()\n",
      "2020-12-23 02:49:39,591 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:39,718 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.296871533389515, 0.43537480675912704], [0.9592145308852196, 0.04078547], [0.0, 0.0], [1.584962500721156, 5.749308601266266, 5.772226522040246, 1.5620445799471758, 4.18726402131909, 0.02291792077398025]]\n",
      "2020-12-23 02:49:39,721 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:39,722 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:39,723 : INFO : built Dictionary(59 unique tokens: ['chang', 'product', 'revert', 'assert', 'call']...) from 2 documents (total 223 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:49:39,729 : INFO : token count processed\n",
      "2020-12-23 02:49:39,731 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:39,732 : INFO : frequencies processed\n",
      "2020-12-23 02:49:39,733 : INFO : token count processed\n",
      "2020-12-23 02:49:39,734 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:39,735 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:39,736 : INFO : vocab #2480\n",
      "2020-12-23 02:49:39,737 : INFO : diff #set()\n",
      "2020-12-23 02:49:39,994 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:40,122 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.315271594808194, 0.4319147707087228], [0.9990493244840764, 0.0009506755], [nan, nan], [1.584962500721156, 5.015422548793484, 5.072006340173021, 1.5283787093416183, 3.4870438394518652, 0.05658379137953773]]\n",
      "2020-12-23 02:49:40,124 : INFO : Removed 0 and 81 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:40,125 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:40,127 : INFO : built Dictionary(87 unique tokens: ['chang', 'product', 'revert', '\"))', '\"):']...) from 2 documents (total 349 corpus positions)\n",
      "2020-12-23 02:49:40,142 : INFO : token count processed\n",
      "2020-12-23 02:49:40,144 : INFO : frequencies processed\n",
      "2020-12-23 02:49:40,273 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:40,273 : INFO : entropies processed\n",
      "2020-12-23 02:49:40,277 : INFO : extropies processed\n",
      "2020-12-23 02:49:40,278 : INFO : token count processed\n",
      "2020-12-23 02:49:40,278 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:40,279 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:40,280 : INFO : vocab #2480\n",
      "2020-12-23 02:49:40,280 : INFO : diff #set()\n",
      "2020-12-23 02:49:40,544 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:40,673 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.2782167831878661, 0.4389397915859081], [0.9394496120512486, 0.060550388], [0.0, 0.0], [1.584962500721156, 6.030001281822029, 6.0488827029254155, 1.5660810796177698, 4.4639202022042594, 0.01888142110338631]]\n",
      "2020-12-23 02:49:40,675 : INFO : Removed 0 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:40,676 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:40,678 : INFO : built Dictionary(75 unique tokens: ['chang', 'product', 'revert', '__main__', '__name__']...) from 2 documents (total 312 corpus positions)\n",
      "2020-12-23 02:49:40,692 : INFO : token count processed\n",
      "2020-12-23 02:49:40,694 : INFO : frequencies processed\n",
      "2020-12-23 02:49:40,829 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:40,829 : INFO : entropies processed\n",
      "2020-12-23 02:49:40,830 : INFO : extropies processed\n",
      "2020-12-23 02:49:40,831 : INFO : token count processed\n",
      "2020-12-23 02:49:40,832 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:40,833 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:40,834 : INFO : vocab #2480\n",
      "2020-12-23 02:49:40,835 : INFO : diff #set()\n",
      "2020-12-23 02:49:41,093 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:41,220 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.3095414024349077, 0.4329863924265302], [0.9870346868410707, 0.012965313], [0.0, 0.0], [1.584962500721156, 5.9537092545441395, 5.981406906547992, 1.5572648487173035, 4.396444405826836, 0.027697652003852546]]\n",
      "2020-12-23 02:49:41,222 : INFO : Removed 0 and 90 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:41,223 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:41,224 : INFO : built Dictionary(87 unique tokens: ['chang', 'product', 'revert', '())', '()):']...) from 2 documents (total 313 corpus positions)\n",
      "2020-12-23 02:49:41,241 : INFO : token count processed\n",
      "2020-12-23 02:49:41,243 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:41,245 : INFO : frequencies processed\n",
      "2020-12-23 02:49:41,246 : INFO : token count processed\n",
      "2020-12-23 02:49:41,247 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:41,249 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:41,249 : INFO : vocab #2480\n",
      "2020-12-23 02:49:41,251 : INFO : diff #set()\n",
      "2020-12-23 02:49:41,505 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:41,632 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.3124125916405793, 0.43244877822194067], [0.9984245523810387, 0.0015754476], [nan, nan], [1.584962500721156, 6.184756445474906, 6.213842237659661, 1.5558767085364007, 4.628879736938505, 0.029085792184755377]]\n",
      "2020-12-23 02:49:41,635 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:41,636 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:41,637 : INFO : built Dictionary(95 unique tokens: ['chang', 'product', 'revert', '\"))', '\"):']...) from 2 documents (total 426 corpus positions)\n",
      "2020-12-23 02:49:41,653 : INFO : token count processed\n",
      "2020-12-23 02:49:41,658 : INFO : frequencies processed\n",
      "2020-12-23 02:49:41,787 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:41,788 : INFO : entropies processed\n",
      "2020-12-23 02:49:41,788 : INFO : extropies processed\n",
      "2020-12-23 02:49:41,789 : INFO : token count processed\n",
      "2020-12-23 02:49:41,790 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:41,791 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:41,792 : INFO : vocab #2480\n",
      "2020-12-23 02:49:41,792 : INFO : diff #set()\n",
      "2020-12-23 02:49:42,061 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:42,191 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.292307955376478, 0.436241560674497], [0.9494232274591923, 0.050576773], [0.0, 0.0], [1.584962500721156, 6.212221456585881, 6.229890454626467, 1.5672935026805694, 4.644927953905311, 0.017668998040586636]]\n",
      "2020-12-23 02:49:42,193 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:42,194 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:42,196 : INFO : built Dictionary(120 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 413 corpus positions)\n",
      "2020-12-23 02:49:42,223 : INFO : token count processed\n",
      "2020-12-23 02:49:42,227 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:42,229 : INFO : frequencies processed\n",
      "2020-12-23 02:49:42,232 : INFO : token count processed\n",
      "2020-12-23 02:49:42,233 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:42,234 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:42,235 : INFO : vocab #2480\n",
      "2020-12-23 02:49:42,236 : INFO : diff #set()\n",
      "2020-12-23 02:49:42,493 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:42,621 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.2381779819440772, 0.4467919924453022], [0.9223630279302597, 0.07763697], [nan, nan], [2.584962500721156, 6.301552355933639, 6.355059244141616, 2.531455612513179, 3.77009674342046, 0.053506888207977]]\n",
      "2020-12-23 02:49:42,624 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:42,625 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:42,627 : INFO : built Dictionary(157 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 649 corpus positions)\n",
      "2020-12-23 02:49:42,659 : INFO : token count processed\n",
      "2020-12-23 02:49:42,661 : INFO : frequencies processed\n",
      "2020-12-23 02:49:42,789 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:42,790 : INFO : entropies processed\n",
      "2020-12-23 02:49:42,791 : INFO : extropies processed\n",
      "2020-12-23 02:49:42,792 : INFO : token count processed\n",
      "2020-12-23 02:49:42,793 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:42,794 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:42,795 : INFO : vocab #2480\n",
      "2020-12-23 02:49:42,796 : INFO : diff #set()\n",
      "2020-12-23 02:49:43,061 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:43,188 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.2110581959560713, 0.4522721300728114], [0.8308412283658981, 0.16915877], [0.0, 0.0], [2.584962500721156, 6.739005504021667, 6.768550994294555, 2.5554170104482683, 4.183588493573399, 0.029545490272888664]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:49:43,190 : INFO : Removed 0 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:43,191 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:43,193 : INFO : built Dictionary(102 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 500 corpus positions)\n",
      "2020-12-23 02:49:43,218 : INFO : token count processed\n",
      "2020-12-23 02:49:43,220 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:43,223 : INFO : frequencies processed\n",
      "2020-12-23 02:49:43,225 : INFO : token count processed\n",
      "2020-12-23 02:49:43,226 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:43,226 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:43,227 : INFO : vocab #2480\n",
      "2020-12-23 02:49:43,228 : INFO : diff #set()\n",
      "2020-12-23 02:49:43,486 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:43,614 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2534010802107647, 0.44377364011313436], [0.9270941913127899, 0.07290581], [nan, nan], [2.584962500721156, 5.870833373337847, 5.917523393656841, 2.5382724804021617, 3.3325608929356854, 0.046690020318994385]]\n",
      "2020-12-23 02:49:43,616 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:43,617 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:43,618 : INFO : built Dictionary(62 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 183 corpus positions)\n",
      "2020-12-23 02:49:43,627 : INFO : token count processed\n",
      "2020-12-23 02:49:43,629 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:43,630 : INFO : frequencies processed\n",
      "2020-12-23 02:49:43,631 : INFO : token count processed\n",
      "2020-12-23 02:49:43,632 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:43,633 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:43,633 : INFO : vocab #2480\n",
      "2020-12-23 02:49:43,634 : INFO : diff #set()\n",
      "2020-12-23 02:49:43,892 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:44,019 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.2298495503935756, 0.44846074921220436], [0.9304393380880356, 0.06956066], [nan, nan], [2.584962500721156, 5.371881234145534, 5.483678263958755, 2.4731654709079356, 2.8987157632375986, 0.11179702981322048]]\n",
      "2020-12-23 02:49:44,022 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:44,023 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:44,024 : INFO : built Dictionary(52 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 135 corpus positions)\n",
      "2020-12-23 02:49:44,040 : INFO : token count processed\n",
      "2020-12-23 02:49:44,045 : INFO : frequencies processed\n",
      "2020-12-23 02:49:44,184 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:44,185 : INFO : entropies processed\n",
      "2020-12-23 02:49:44,186 : INFO : extropies processed\n",
      "2020-12-23 02:49:44,188 : INFO : token count processed\n",
      "2020-12-23 02:49:44,189 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:44,191 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:44,192 : INFO : vocab #2480\n",
      "2020-12-23 02:49:44,193 : INFO : diff #set()\n",
      "2020-12-23 02:49:44,466 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:44,596 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.22932653002913, 0.4485659621997739], [0.9426521845161915, 0.057347815], [0.0, 0.0], [2.584962500721156, 4.85108279267097, 4.997263924732154, 2.4387813686599715, 2.412301424010998, 0.14618113206118455]]\n",
      "2020-12-23 02:49:44,598 : INFO : Removed 0 and 127 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:44,599 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:44,600 : INFO : built Dictionary(92 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 408 corpus positions)\n",
      "2020-12-23 02:49:44,616 : INFO : token count processed\n",
      "2020-12-23 02:49:44,622 : INFO : frequencies processed\n",
      "2020-12-23 02:49:44,751 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:44,752 : INFO : entropies processed\n",
      "2020-12-23 02:49:44,753 : INFO : extropies processed\n",
      "2020-12-23 02:49:44,754 : INFO : token count processed\n",
      "2020-12-23 02:49:44,756 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:44,757 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:44,757 : INFO : vocab #2480\n",
      "2020-12-23 02:49:44,758 : INFO : diff #set()\n",
      "2020-12-23 02:49:45,017 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:45,145 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.2323434020506345, 0.4479597534507451], [0.9014422371983528, 0.09855776], [0.0, 0.0], [2.584962500721156, 6.139571208108155, 6.182384814154037, 2.5421488946752735, 3.5974223134328813, 0.04281360604588258]]\n",
      "2020-12-23 02:49:45,148 : INFO : Removed 0 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:45,148 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:45,149 : INFO : built Dictionary(79 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 418 corpus positions)\n",
      "2020-12-23 02:49:45,161 : INFO : token count processed\n",
      "2020-12-23 02:49:45,170 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:45,171 : INFO : frequencies processed\n",
      "2020-12-23 02:49:45,173 : INFO : token count processed\n",
      "2020-12-23 02:49:45,175 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:45,177 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:45,178 : INFO : vocab #2480\n",
      "2020-12-23 02:49:45,181 : INFO : diff #set()\n",
      "2020-12-23 02:49:45,448 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:45,580 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.2258363751852968, 0.44926932237629186], [0.9093162938952446, 0.090683706], [nan, nan], [2.584962500721156, 5.609710627339259, 5.67097827349588, 2.5236948545645355, 3.086015772774724, 0.061267646156620614]]\n",
      "2020-12-23 02:49:45,583 : INFO : Removed 0 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:45,584 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:45,586 : INFO : built Dictionary(170 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 1078 corpus positions)\n",
      "2020-12-23 02:49:45,625 : INFO : token count processed\n",
      "2020-12-23 02:49:45,628 : INFO : frequencies processed\n",
      "2020-12-23 02:49:45,755 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:45,755 : INFO : entropies processed\n",
      "2020-12-23 02:49:45,756 : INFO : extropies processed\n",
      "2020-12-23 02:49:45,758 : INFO : token count processed\n",
      "2020-12-23 02:49:45,759 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:45,759 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:45,760 : INFO : vocab #2480\n",
      "2020-12-23 02:49:45,761 : INFO : diff #set()\n",
      "2020-12-23 02:49:46,022 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:46,149 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.2418725648108295, 0.44605568384944333], [0.8875390440225601, 0.112460956], [0.0, 0.0], [2.584962500721156, 7.2441902753576075, 7.260870759549819, 2.5682820165289453, 4.675908258828663, 0.016680484192211686]]\n",
      "2020-12-23 02:49:46,152 : INFO : Removed 0 and 136 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:46,153 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:46,154 : INFO : built Dictionary(132 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 685 corpus positions)\n",
      "2020-12-23 02:49:46,179 : INFO : token count processed\n",
      "2020-12-23 02:49:46,183 : INFO : frequencies processed\n",
      "2020-12-23 02:49:46,311 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:46,311 : INFO : entropies processed\n",
      "2020-12-23 02:49:46,312 : INFO : extropies processed\n",
      "2020-12-23 02:49:46,313 : INFO : token count processed\n",
      "2020-12-23 02:49:46,314 : INFO : alphabet_source #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:49:46,315 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:46,315 : INFO : vocab #2480\n",
      "2020-12-23 02:49:46,316 : INFO : diff #set()\n",
      "2020-12-23 02:49:46,579 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:46,708 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.2004372098944607, 0.45445513987102715], [0.8420045226812363, 0.15799548], [0.0, 0.0], [2.584962500721156, 6.2567074920449475, 6.288883814314882, 2.552786178451221, 3.7039213135937263, 0.03217632226993494]]\n",
      "2020-12-23 02:49:46,710 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:46,711 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:46,712 : INFO : built Dictionary(76 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 195 corpus positions)\n",
      "2020-12-23 02:49:46,723 : INFO : token count processed\n",
      "2020-12-23 02:49:46,726 : INFO : frequencies processed\n",
      "2020-12-23 02:49:46,854 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:46,855 : INFO : entropies processed\n",
      "2020-12-23 02:49:46,856 : INFO : extropies processed\n",
      "2020-12-23 02:49:46,857 : INFO : token count processed\n",
      "2020-12-23 02:49:46,858 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:46,859 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:46,859 : INFO : vocab #2480\n",
      "2020-12-23 02:49:46,861 : INFO : diff #set()\n",
      "2020-12-23 02:49:47,119 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:47,247 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.2019777497503852, 0.4541371955794555], [0.8671996593475342, 0.13280034], [1.0, 1.0], [2.584962500721156, 5.7680018917339435, 5.841290235673038, 2.5116741567820613, 3.256327734951882, 0.07328834393909478]]\n",
      "2020-12-23 02:49:47,250 : INFO : Removed 0 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:47,250 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:47,252 : INFO : built Dictionary(174 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 736 corpus positions)\n",
      "2020-12-23 02:49:47,286 : INFO : token count processed\n",
      "2020-12-23 02:49:47,291 : INFO : frequencies processed\n",
      "2020-12-23 02:49:47,417 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:47,418 : INFO : entropies processed\n",
      "2020-12-23 02:49:47,419 : INFO : extropies processed\n",
      "2020-12-23 02:49:47,420 : INFO : token count processed\n",
      "2020-12-23 02:49:47,421 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:47,421 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:47,422 : INFO : vocab #2480\n",
      "2020-12-23 02:49:47,423 : INFO : diff #set()\n",
      "2020-12-23 02:49:47,683 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:47,811 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.1991310698060451, 0.45472505651434236], [0.8116021901369095, 0.18839781], [1.0, 1.0], [2.584962500721156, 6.846479111193757, 6.867497699181058, 2.563943912733855, 4.282535198459902, 0.021018587987301274]]\n",
      "2020-12-23 02:49:47,813 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:47,814 : INFO : built Dictionary(26 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 39 corpus positions)\n",
      "2020-12-23 02:49:47,818 : INFO : token count processed\n",
      "2020-12-23 02:49:47,820 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:47,821 : INFO : frequencies processed\n",
      "2020-12-23 02:49:47,822 : INFO : token count processed\n",
      "2020-12-23 02:49:47,823 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:47,824 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:47,825 : INFO : vocab #2480\n",
      "2020-12-23 02:49:47,826 : INFO : diff #set()\n",
      "2020-12-23 02:49:48,084 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:48,212 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2174052014319534, 0.45097756573955056], [0.9054432213306427, 0.09455678], [nan, nan], [2.584962500721156, 4.165013816065912, 4.541311192999328, 2.20866512378774, 1.9563486922781719, 0.37629737693341614]]\n",
      "2020-12-23 02:49:48,214 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:48,215 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:48,216 : INFO : built Dictionary(54 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 146 corpus positions)\n",
      "2020-12-23 02:49:48,224 : INFO : token count processed\n",
      "2020-12-23 02:49:48,226 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:48,227 : INFO : frequencies processed\n",
      "2020-12-23 02:49:48,228 : INFO : token count processed\n",
      "2020-12-23 02:49:48,230 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:48,231 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:48,232 : INFO : vocab #2480\n",
      "2020-12-23 02:49:48,235 : INFO : diff #set()\n",
      "2020-12-23 02:49:48,502 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:48,629 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.2281761624976608, 0.44879754878943495], [0.9107974469661713, 0.08920255], [nan, nan], [2.584962500721156, 5.449968864419248, 5.571977566119645, 2.4629537990207604, 2.987015065398489, 0.12200870170039657]]\n",
      "2020-12-23 02:49:48,631 : INFO : Removed 0 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:48,632 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:48,633 : INFO : built Dictionary(145 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 583 corpus positions)\n",
      "2020-12-23 02:49:48,658 : INFO : token count processed\n",
      "2020-12-23 02:49:48,660 : INFO : frequencies processed\n",
      "2020-12-23 02:49:48,787 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:48,788 : INFO : entropies processed\n",
      "2020-12-23 02:49:48,789 : INFO : extropies processed\n",
      "2020-12-23 02:49:48,790 : INFO : token count processed\n",
      "2020-12-23 02:49:48,791 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:48,792 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:48,793 : INFO : vocab #2480\n",
      "2020-12-23 02:49:48,794 : INFO : diff #set()\n",
      "2020-12-23 02:49:49,052 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:49,179 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.2300301028601281, 0.4484244399739038], [0.8861265629529953, 0.11387344], [1.0, 1.0], [2.584962500721156, 6.530294129310484, 6.560094626192565, 2.555162003839075, 3.9751321254714087, 0.029800496882081084]]\n",
      "2020-12-23 02:49:49,182 : INFO : Removed 0 and 142 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:49,182 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:49,184 : INFO : built Dictionary(120 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 599 corpus positions)\n",
      "2020-12-23 02:49:49,212 : INFO : token count processed\n",
      "2020-12-23 02:49:49,215 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:49,216 : INFO : frequencies processed\n",
      "2020-12-23 02:49:49,219 : INFO : token count processed\n",
      "2020-12-23 02:49:49,219 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:49,221 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:49,223 : INFO : vocab #2480\n",
      "2020-12-23 02:49:49,224 : INFO : diff #set()\n",
      "2020-12-23 02:49:49,490 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:49,623 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.2405534205420337, 0.44631830280488494], [0.914931982755661, 0.08506802], [nan, nan], [2.584962500721156, 6.470272233491701, 6.5067086370554446, 2.5485260971574117, 3.9217461363342885, 0.03643640356374345]]\n",
      "2020-12-23 02:49:49,625 : INFO : Removed 0 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:49,626 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:49,627 : INFO : built Dictionary(120 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 388 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:49:49,646 : INFO : token count processed\n",
      "2020-12-23 02:49:49,648 : INFO : frequencies processed\n",
      "2020-12-23 02:49:49,776 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:49,777 : INFO : entropies processed\n",
      "2020-12-23 02:49:49,777 : INFO : extropies processed\n",
      "2020-12-23 02:49:49,779 : INFO : token count processed\n",
      "2020-12-23 02:49:49,780 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:49,781 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:49,782 : INFO : vocab #2480\n",
      "2020-12-23 02:49:49,782 : INFO : diff #set()\n",
      "2020-12-23 02:49:50,040 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:50,168 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.2138841990400893, 0.45169480880417623], [0.8453693091869354, 0.15463069], [0.0, 0.0], [2.584962500721156, 6.550038223589686, 6.593688355283486, 2.541312369027356, 4.00872585456233, 0.043650131693800276]]\n",
      "2020-12-23 02:49:50,170 : INFO : Removed 0 and 70 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:50,171 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:50,173 : INFO : built Dictionary(76 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 304 corpus positions)\n",
      "2020-12-23 02:49:50,190 : INFO : token count processed\n",
      "2020-12-23 02:49:50,192 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:50,193 : INFO : frequencies processed\n",
      "2020-12-23 02:49:50,195 : INFO : token count processed\n",
      "2020-12-23 02:49:50,197 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:50,198 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:50,199 : INFO : vocab #2480\n",
      "2020-12-23 02:49:50,201 : INFO : diff #set()\n",
      "2020-12-23 02:49:50,472 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:50,602 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.239992648604792, 0.4464300365551926], [0.9298992678523064, 0.07010073], [nan, nan], [2.584962500721156, 5.860525481261383, 5.926580495382843, 2.518907486599695, 3.3416179946616866, 0.06605501412145998]]\n",
      "2020-12-23 02:49:50,605 : INFO : Removed 0 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:50,606 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:50,607 : INFO : built Dictionary(47 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 342 corpus positions)\n",
      "2020-12-23 02:49:50,627 : INFO : token count processed\n",
      "2020-12-23 02:49:50,632 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:50,633 : INFO : frequencies processed\n",
      "2020-12-23 02:49:50,636 : INFO : token count processed\n",
      "2020-12-23 02:49:50,637 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:50,638 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:50,639 : INFO : vocab #2480\n",
      "2020-12-23 02:49:50,639 : INFO : diff #set()\n",
      "2020-12-23 02:49:50,893 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:51,020 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.3001917943036811, 0.43474635570670833], [0.9661132022738457, 0.033886798], [nan, nan], [2.584962500721156, 5.945464049777852, 5.998205985180583, 2.5322205653184255, 3.413243484459427, 0.05274193540273053]]\n",
      "2020-12-23 02:49:51,023 : INFO : Removed 0 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:51,024 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:51,026 : INFO : built Dictionary(192 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 889 corpus positions)\n",
      "2020-12-23 02:49:51,063 : INFO : token count processed\n",
      "2020-12-23 02:49:51,065 : INFO : frequencies processed\n",
      "2020-12-23 02:49:51,192 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:51,192 : INFO : entropies processed\n",
      "2020-12-23 02:49:51,193 : INFO : extropies processed\n",
      "2020-12-23 02:49:51,195 : INFO : token count processed\n",
      "2020-12-23 02:49:51,195 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:51,196 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:51,197 : INFO : vocab #2480\n",
      "2020-12-23 02:49:51,198 : INFO : diff #set()\n",
      "2020-12-23 02:49:51,465 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:51,593 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.19936731327821, 0.4546762125465418], [0.8215570896863937, 0.17844291], [1.584962500721156, 1.1699250014423124], [2.584962500721156, 6.811563897304216, 6.830601817048366, 2.565924580977006, 4.24563931632721, 0.019037919744150145]]\n",
      "2020-12-23 02:49:51,596 : INFO : Removed 0 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:51,597 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:51,598 : INFO : built Dictionary(215 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 984 corpus positions)\n",
      "2020-12-23 02:49:51,643 : INFO : token count processed\n",
      "2020-12-23 02:49:51,645 : INFO : frequencies processed\n",
      "2020-12-23 02:49:51,779 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:51,780 : INFO : entropies processed\n",
      "2020-12-23 02:49:51,780 : INFO : extropies processed\n",
      "2020-12-23 02:49:51,782 : INFO : token count processed\n",
      "2020-12-23 02:49:51,783 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:51,784 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:51,785 : INFO : vocab #2480\n",
      "2020-12-23 02:49:51,786 : INFO : diff #set()\n",
      "2020-12-23 02:49:52,043 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:52,171 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.2201117070208445, 0.4504277856098936], [0.8483620434999466, 0.15163796], [0.0, 0.0], [2.584962500721156, 7.502034948968415, 7.518506112591165, 2.568491337098406, 4.933543611870009, 0.01647116362275014]]\n",
      "2020-12-23 02:49:52,174 : INFO : Removed 0 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:52,175 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:52,177 : INFO : built Dictionary(257 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 1557 corpus positions)\n",
      "2020-12-23 02:49:52,228 : INFO : token count processed\n",
      "2020-12-23 02:49:52,231 : INFO : frequencies processed\n",
      "2020-12-23 02:49:52,358 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:52,359 : INFO : entropies processed\n",
      "2020-12-23 02:49:52,359 : INFO : extropies processed\n",
      "2020-12-23 02:49:52,361 : INFO : token count processed\n",
      "2020-12-23 02:49:52,362 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:52,363 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:52,364 : INFO : vocab #2480\n",
      "2020-12-23 02:49:52,365 : INFO : diff #set()\n",
      "2020-12-23 02:49:52,622 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:52,751 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.1926686514081897, 0.45606526063925507], [0.7792379856109619, 0.22076201], [1.584962500721156, 1.1699250014423124], [2.584962500721156, 7.39180093901977, 7.3997798366449805, 2.576983603095946, 4.814817335923824, 0.007978897625210202]]\n",
      "2020-12-23 02:49:52,753 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:52,754 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:52,755 : INFO : built Dictionary(44 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 112 corpus positions)\n",
      "2020-12-23 02:49:52,761 : INFO : token count processed\n",
      "2020-12-23 02:49:52,764 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:52,764 : INFO : frequencies processed\n",
      "2020-12-23 02:49:52,765 : INFO : token count processed\n",
      "2020-12-23 02:49:52,766 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:52,767 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:52,768 : INFO : vocab #2480\n",
      "2020-12-23 02:49:52,769 : INFO : diff #set()\n",
      "2020-12-23 02:49:53,027 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:53,155 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.226043500039828, 0.449227519579967], [0.9215138629078865, 0.07848614], [nan, nan], [2.584962500721156, 4.927561309677364, 5.101738884441065, 2.4107849259574543, 2.5167763837199093, 0.17417757476370177]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:49:53,158 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:53,158 : INFO : built Dictionary(12 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 13 corpus positions)\n",
      "2020-12-23 02:49:53,161 : INFO : token count processed\n",
      "2020-12-23 02:49:53,163 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:53,164 : INFO : frequencies processed\n",
      "2020-12-23 02:49:53,165 : INFO : token count processed\n",
      "2020-12-23 02:49:53,166 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:53,167 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:53,167 : INFO : vocab #2480\n",
      "2020-12-23 02:49:53,168 : INFO : diff #set()\n",
      "2020-12-23 02:49:53,430 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:53,558 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2203288011744982, 0.45038374472781917], [0.9501453936100006, 0.049854606], [nan, nan], [2.584962500721156, 2.5216406363433186, 3.5465935642949384, 1.5600095727695367, 0.9616310635737824, 1.0249529279516199]]\n",
      "2020-12-23 02:49:53,562 : INFO : Removed 0 and 530 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:53,562 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:53,564 : INFO : built Dictionary(332 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 2885 corpus positions)\n",
      "2020-12-23 02:49:53,645 : INFO : token count processed\n",
      "2020-12-23 02:49:53,648 : INFO : frequencies processed\n",
      "2020-12-23 02:49:53,775 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:53,775 : INFO : entropies processed\n",
      "2020-12-23 02:49:53,776 : INFO : extropies processed\n",
      "2020-12-23 02:49:53,778 : INFO : token count processed\n",
      "2020-12-23 02:49:53,779 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:53,780 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:53,780 : INFO : vocab #2480\n",
      "2020-12-23 02:49:53,781 : INFO : diff #set()\n",
      "2020-12-23 02:49:54,039 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:54,166 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.215599023550707, 0.451345207039045], [0.8298896849155426, 0.17011032], [1.0, 1.0], [2.584962500721156, 7.480007711014331, 7.486948991991004, 2.5780212197444827, 4.901986491269848, 0.006941280976673347]]\n",
      "2020-12-23 02:49:54,169 : INFO : Removed 0 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:54,170 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:54,171 : INFO : built Dictionary(210 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 1034 corpus positions)\n",
      "2020-12-23 02:49:54,212 : INFO : token count processed\n",
      "2020-12-23 02:49:54,215 : INFO : frequencies processed\n",
      "2020-12-23 02:49:54,351 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:54,352 : INFO : entropies processed\n",
      "2020-12-23 02:49:54,353 : INFO : extropies processed\n",
      "2020-12-23 02:49:54,354 : INFO : token count processed\n",
      "2020-12-23 02:49:54,355 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:54,356 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:54,357 : INFO : vocab #2480\n",
      "2020-12-23 02:49:54,358 : INFO : diff #set()\n",
      "2020-12-23 02:49:54,633 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:54,762 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.2223343566636202, 0.4499772939213769], [0.8415244221687317, 0.15847558], [0.0, 0.0], [2.584962500721156, 7.131331012509435, 7.151641704946572, 2.564651808284019, 4.566679204225416, 0.020310692437137234]]\n",
      "2020-12-23 02:49:54,764 : INFO : Removed 0 and 166 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:54,765 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:54,766 : INFO : built Dictionary(199 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 853 corpus positions)\n",
      "2020-12-23 02:49:54,806 : INFO : token count processed\n",
      "2020-12-23 02:49:54,822 : INFO : frequencies processed\n",
      "2020-12-23 02:49:54,954 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:54,955 : INFO : entropies processed\n",
      "2020-12-23 02:49:54,956 : INFO : extropies processed\n",
      "2020-12-23 02:49:54,958 : INFO : token count processed\n",
      "2020-12-23 02:49:54,959 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:54,960 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:54,961 : INFO : vocab #2480\n",
      "2020-12-23 02:49:54,962 : INFO : diff #set()\n",
      "2020-12-23 02:49:55,223 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:55,352 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.2232620128997673, 0.44978954086284906], [0.8415032029151917, 0.1584968], [0.0, 0.0], [2.584962500721156, 7.203742744794778, 7.225093253964754, 2.5636119915511806, 4.640130753243598, 0.021350509169976384]]\n",
      "2020-12-23 02:49:55,355 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:55,355 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:55,356 : INFO : built Dictionary(57 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 185 corpus positions)\n",
      "2020-12-23 02:49:55,364 : INFO : token count processed\n",
      "2020-12-23 02:49:55,367 : INFO : frequencies processed\n",
      "2020-12-23 02:49:55,495 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:55,496 : INFO : entropies processed\n",
      "2020-12-23 02:49:55,497 : INFO : extropies processed\n",
      "2020-12-23 02:49:55,498 : INFO : token count processed\n",
      "2020-12-23 02:49:55,499 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:55,500 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:55,500 : INFO : vocab #2480\n",
      "2020-12-23 02:49:55,501 : INFO : diff #set()\n",
      "2020-12-23 02:49:55,760 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:55,888 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.2190251688440343, 0.45064833605331933], [0.9013124406337738, 0.09868756], [0.0, 0.0], [2.584962500721156, 5.195502554608948, 5.306131405141738, 2.474333650188367, 2.7211689044205816, 0.11062885053278926]]\n",
      "2020-12-23 02:49:55,891 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:55,892 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:55,893 : INFO : built Dictionary(64 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 246 corpus positions)\n",
      "2020-12-23 02:49:55,902 : INFO : token count processed\n",
      "2020-12-23 02:49:55,904 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:55,905 : INFO : frequencies processed\n",
      "2020-12-23 02:49:55,906 : INFO : token count processed\n",
      "2020-12-23 02:49:55,907 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:55,908 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:55,908 : INFO : vocab #2480\n",
      "2020-12-23 02:49:55,910 : INFO : diff #set()\n",
      "2020-12-23 02:49:56,170 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:56,298 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.2253582581510403, 0.449365847650463], [0.9088658317923546, 0.09113417], [nan, nan], [2.584962500721156, 5.32027245610305, 5.418728899153157, 2.4865060576710487, 2.833766398432001, 0.09845644305010737]]\n",
      "2020-12-23 02:49:56,300 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:56,301 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:56,302 : INFO : built Dictionary(162 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 486 corpus positions)\n",
      "2020-12-23 02:49:56,336 : INFO : token count processed\n",
      "2020-12-23 02:49:56,341 : INFO : frequencies processed\n",
      "2020-12-23 02:49:56,468 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:56,469 : INFO : entropies processed\n",
      "2020-12-23 02:49:56,469 : INFO : extropies processed\n",
      "2020-12-23 02:49:56,471 : INFO : token count processed\n",
      "2020-12-23 02:49:56,472 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:56,473 : INFO : alphabet_target #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:49:56,474 : INFO : vocab #2480\n",
      "2020-12-23 02:49:56,476 : INFO : diff #set()\n",
      "2020-12-23 02:49:56,743 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:56,871 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.21009106761692, 0.45247004281967085], [0.8250656276941299, 0.17493437], [1.0, 1.0], [2.584962500721156, 6.898202761357263, 6.928121260411762, 2.5550440016666567, 4.343158759690606, 0.029918499054499392]]\n",
      "2020-12-23 02:49:56,873 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:56,874 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:56,875 : INFO : built Dictionary(127 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 508 corpus positions)\n",
      "2020-12-23 02:49:56,907 : INFO : token count processed\n",
      "2020-12-23 02:49:56,912 : INFO : frequencies processed\n",
      "2020-12-23 02:49:57,039 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:57,039 : INFO : entropies processed\n",
      "2020-12-23 02:49:57,040 : INFO : extropies processed\n",
      "2020-12-23 02:49:57,041 : INFO : token count processed\n",
      "2020-12-23 02:49:57,041 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:57,042 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:57,043 : INFO : vocab #2480\n",
      "2020-12-23 02:49:57,043 : INFO : diff #set()\n",
      "2020-12-23 02:49:57,309 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:57,438 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.1976555859191398, 0.45503035434998035], [0.8341097682714462, 0.16589023], [0.0, 0.0], [2.584962500721156, 6.388500481644799, 6.424755307916167, 2.5487076744497887, 3.8397928071950105, 0.036254826271367335]]\n",
      "2020-12-23 02:49:57,440 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:57,441 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:57,442 : INFO : built Dictionary(48 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 172 corpus positions)\n",
      "2020-12-23 02:49:57,449 : INFO : token count processed\n",
      "2020-12-23 02:49:57,452 : INFO : frequencies processed\n",
      "2020-12-23 02:49:57,580 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:57,580 : INFO : entropies processed\n",
      "2020-12-23 02:49:57,581 : INFO : extropies processed\n",
      "2020-12-23 02:49:57,582 : INFO : token count processed\n",
      "2020-12-23 02:49:57,583 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:57,584 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:57,585 : INFO : vocab #2480\n",
      "2020-12-23 02:49:57,586 : INFO : diff #set()\n",
      "2020-12-23 02:49:57,842 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:57,970 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.2232599525234655, 0.4497899576992652], [0.9121919870376587, 0.08780801], [0.0, 0.0], [2.584962500721156, 4.8191513650620195, 4.947458638747652, 2.4566552270355233, 2.362496138026496, 0.12830727368563277]]\n",
      "2020-12-23 02:49:57,972 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:57,973 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:57,974 : INFO : built Dictionary(54 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 227 corpus positions)\n",
      "2020-12-23 02:49:57,986 : INFO : token count processed\n",
      "2020-12-23 02:49:57,989 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:57,989 : INFO : frequencies processed\n",
      "2020-12-23 02:49:57,991 : INFO : token count processed\n",
      "2020-12-23 02:49:57,991 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:57,993 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:57,993 : INFO : vocab #2480\n",
      "2020-12-23 02:49:57,995 : INFO : diff #set()\n",
      "2020-12-23 02:49:58,252 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:58,379 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.2255409410219125, 0.4493289615875703], [0.9106945767998695, 0.08930542], [nan, nan], [2.584962500721156, 5.062480936779194, 5.172848109056222, 2.4745953284441278, 2.587885608335066, 0.1103671722770283]]\n",
      "2020-12-23 02:49:58,382 : INFO : Removed 0 and 223 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:58,383 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:58,386 : INFO : built Dictionary(243 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 1781 corpus positions)\n",
      "2020-12-23 02:49:58,438 : INFO : token count processed\n",
      "2020-12-23 02:49:58,446 : INFO : frequencies processed\n",
      "2020-12-23 02:49:58,574 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:58,575 : INFO : entropies processed\n",
      "2020-12-23 02:49:58,575 : INFO : extropies processed\n",
      "2020-12-23 02:49:58,577 : INFO : token count processed\n",
      "2020-12-23 02:49:58,577 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:58,578 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:58,579 : INFO : vocab #2480\n",
      "2020-12-23 02:49:58,580 : INFO : diff #set()\n",
      "2020-12-23 02:49:58,847 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:58,974 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.231126663498772, 0.4482040470225192], [0.8833943456411362, 0.116605654], [1.0, 1.0], [2.584962500721156, 7.185085743102134, 7.197558881679846, 2.5724893621434433, 4.61259638095869, 0.012473138577711929]]\n",
      "2020-12-23 02:49:58,977 : INFO : Removed 0 and 148 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:58,978 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:58,979 : INFO : built Dictionary(161 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 756 corpus positions)\n",
      "2020-12-23 02:49:59,009 : INFO : token count processed\n",
      "2020-12-23 02:49:59,014 : INFO : frequencies processed\n",
      "2020-12-23 02:49:59,142 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:49:59,143 : INFO : entropies processed\n",
      "2020-12-23 02:49:59,143 : INFO : extropies processed\n",
      "2020-12-23 02:49:59,144 : INFO : token count processed\n",
      "2020-12-23 02:49:59,145 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:59,145 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:59,146 : INFO : vocab #2480\n",
      "2020-12-23 02:49:59,147 : INFO : diff #set()\n",
      "2020-12-23 02:49:59,404 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:59,532 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.2101189363232845, 0.4524643373553383], [0.8584270030260086, 0.141573], [0.0, 0.0], [2.584962500721156, 6.591225336124281, 6.6186102802738365, 2.5575775565715997, 4.03364777955268, 0.0273849441495555]]\n",
      "2020-12-23 02:49:59,535 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:59,535 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:49:59,536 : INFO : built Dictionary(40 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 94 corpus positions)\n",
      "2020-12-23 02:49:59,542 : INFO : token count processed\n",
      "2020-12-23 02:49:59,545 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:49:59,545 : INFO : frequencies processed\n",
      "2020-12-23 02:49:59,546 : INFO : token count processed\n",
      "2020-12-23 02:49:59,547 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:49:59,548 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:49:59,549 : INFO : vocab #2480\n",
      "2020-12-23 02:49:59,550 : INFO : diff #set()\n",
      "2020-12-23 02:49:59,809 : INFO : alphabet #2480\n",
      "2020-12-23 02:49:59,937 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.2367441502613088, 0.4470784018293618], [0.9348895773291588, 0.06511042], [nan, nan], [2.584962500721156, 4.7032114441396695, 4.909282378566961, 2.3788915662938646, 2.324319877845805, 0.20607093442729152]]\n",
      "2020-12-23 02:49:59,939 : INFO : Removed 0 and 9 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:49:59,940 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:49:59,941 : INFO : built Dictionary(117 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 334 corpus positions)\n",
      "2020-12-23 02:49:59,965 : INFO : token count processed\n",
      "2020-12-23 02:49:59,967 : INFO : frequencies processed\n",
      "2020-12-23 02:50:00,099 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:00,100 : INFO : entropies processed\n",
      "2020-12-23 02:50:00,101 : INFO : extropies processed\n",
      "2020-12-23 02:50:00,102 : INFO : token count processed\n",
      "2020-12-23 02:50:00,103 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:00,104 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:00,105 : INFO : vocab #2480\n",
      "2020-12-23 02:50:00,106 : INFO : diff #set()\n",
      "2020-12-23 02:50:00,375 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:00,505 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.203776450852871, 0.4537665331767184], [0.8522678762674332, 0.14773212], [0.0, 0.0], [2.584962500721156, 6.14228447828618, 6.196654904510187, 2.5305920744971493, 3.6116924037890312, 0.05437042622400767]]\n",
      "2020-12-23 02:50:00,508 : INFO : Removed 0 and 184 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:00,509 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:00,510 : INFO : built Dictionary(254 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 1124 corpus positions)\n",
      "2020-12-23 02:50:00,563 : INFO : token count processed\n",
      "2020-12-23 02:50:00,569 : INFO : frequencies processed\n",
      "2020-12-23 02:50:00,697 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:00,697 : INFO : entropies processed\n",
      "2020-12-23 02:50:00,698 : INFO : extropies processed\n",
      "2020-12-23 02:50:00,699 : INFO : token count processed\n",
      "2020-12-23 02:50:00,700 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:00,701 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:00,702 : INFO : vocab #2480\n",
      "2020-12-23 02:50:00,703 : INFO : diff #set()\n",
      "2020-12-23 02:50:00,960 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:01,087 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.2147171381723973, 0.45152492964641444], [0.8332061469554901, 0.16679385], [1.0, 1.0], [2.584962500721156, 7.450178124335845, 7.465243554670877, 2.569897070386123, 4.880281053949721, 0.015065430335032381]]\n",
      "2020-12-23 02:50:01,090 : INFO : Removed 0 and 16 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:01,091 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:01,092 : INFO : built Dictionary(57 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 172 corpus positions)\n",
      "2020-12-23 02:50:01,107 : INFO : token count processed\n",
      "2020-12-23 02:50:01,109 : INFO : frequencies processed\n",
      "2020-12-23 02:50:01,241 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:01,242 : INFO : entropies processed\n",
      "2020-12-23 02:50:01,243 : INFO : extropies processed\n",
      "2020-12-23 02:50:01,244 : INFO : token count processed\n",
      "2020-12-23 02:50:01,245 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:01,246 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:01,247 : INFO : vocab #2480\n",
      "2020-12-23 02:50:01,248 : INFO : diff #set()\n",
      "2020-12-23 02:50:01,515 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:01,642 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.2140529474162982, 0.4516603820007809], [0.9011077359318733, 0.098892264], [0.0, 0.0], [2.584962500721156, 5.20665021947654, 5.316246912467891, 2.475365807729805, 2.731284411746735, 0.10959669299135122]]\n",
      "2020-12-23 02:50:01,645 : INFO : Removed 0 and 89 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:01,646 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:01,647 : INFO : built Dictionary(126 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 510 corpus positions)\n",
      "2020-12-23 02:50:01,670 : INFO : token count processed\n",
      "2020-12-23 02:50:01,672 : INFO : frequencies processed\n",
      "2020-12-23 02:50:01,803 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:01,804 : INFO : entropies processed\n",
      "2020-12-23 02:50:01,805 : INFO : extropies processed\n",
      "2020-12-23 02:50:01,806 : INFO : token count processed\n",
      "2020-12-23 02:50:01,807 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:01,808 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:01,809 : INFO : vocab #2480\n",
      "2020-12-23 02:50:01,810 : INFO : diff #set()\n",
      "2020-12-23 02:50:02,071 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:02,199 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.2204502058853384, 0.4503591196728862], [0.8631758987903595, 0.1368241], [0.0, 0.0], [2.584962500721156, 6.524718477352, 6.561559790287605, 2.5481211877855507, 3.976597289566449, 0.03684131293560533]]\n",
      "2020-12-23 02:50:02,201 : INFO : Removed 0 and 8 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:02,202 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:02,203 : INFO : built Dictionary(63 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 217 corpus positions)\n",
      "2020-12-23 02:50:02,213 : INFO : token count processed\n",
      "2020-12-23 02:50:02,215 : INFO : frequencies processed\n",
      "2020-12-23 02:50:02,343 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:02,343 : INFO : entropies processed\n",
      "2020-12-23 02:50:02,344 : INFO : extropies processed\n",
      "2020-12-23 02:50:02,345 : INFO : token count processed\n",
      "2020-12-23 02:50:02,346 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:02,347 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:02,348 : INFO : vocab #2480\n",
      "2020-12-23 02:50:02,349 : INFO : diff #set()\n",
      "2020-12-23 02:50:02,607 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:02,734 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.2251173170263887, 0.4494145060793397], [0.9084241911768913, 0.09157581], [0.0, 0.0], [2.584962500721156, 5.321859380715434, 5.417376028212695, 2.489445853223895, 2.832413527491539, 0.09551664749726108]]\n",
      "2020-12-23 02:50:02,737 : INFO : Removed 0 and 119 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:02,738 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:02,739 : INFO : built Dictionary(141 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 835 corpus positions)\n",
      "2020-12-23 02:50:02,767 : INFO : token count processed\n",
      "2020-12-23 02:50:02,772 : INFO : frequencies processed\n",
      "2020-12-23 02:50:02,900 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:02,900 : INFO : entropies processed\n",
      "2020-12-23 02:50:02,901 : INFO : extropies processed\n",
      "2020-12-23 02:50:02,903 : INFO : token count processed\n",
      "2020-12-23 02:50:02,904 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:02,905 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:02,905 : INFO : vocab #2480\n",
      "2020-12-23 02:50:02,906 : INFO : diff #set()\n",
      "2020-12-23 02:50:03,164 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:03,291 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.2078983480490217, 0.4529194022377144], [0.8263866901397705, 0.17361331], [1.0, 1.0], [2.584962500721156, 6.500767808767801, 6.525680016024892, 2.560050293464065, 3.940717515303736, 0.02491220725709109]]\n",
      "2020-12-23 02:50:03,294 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:03,294 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:03,296 : INFO : built Dictionary(36 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 57 corpus positions)\n",
      "2020-12-23 02:50:03,307 : INFO : token count processed\n",
      "2020-12-23 02:50:03,309 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:03,310 : INFO : frequencies processed\n",
      "2020-12-23 02:50:03,311 : INFO : token count processed\n",
      "2020-12-23 02:50:03,312 : INFO : alphabet_source #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:50:03,313 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:03,314 : INFO : vocab #2480\n",
      "2020-12-23 02:50:03,315 : INFO : diff #set()\n",
      "2020-12-23 02:50:03,573 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:03,702 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.2382155741782763, 0.44678448829359674], [0.9407975077629089, 0.059202492], [nan, nan], [2.584962500721156, 4.736228843383063, 4.993516072930715, 2.3276752711735034, 2.4085535722095592, 0.2572872295476527]]\n",
      "2020-12-23 02:50:03,705 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:03,706 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:03,707 : INFO : built Dictionary(96 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 398 corpus positions)\n",
      "2020-12-23 02:50:03,725 : INFO : token count processed\n",
      "2020-12-23 02:50:03,730 : INFO : frequencies processed\n",
      "2020-12-23 02:50:03,859 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:03,860 : INFO : entropies processed\n",
      "2020-12-23 02:50:03,861 : INFO : extropies processed\n",
      "2020-12-23 02:50:03,862 : INFO : token count processed\n",
      "2020-12-23 02:50:03,863 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:03,864 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:03,865 : INFO : vocab #2480\n",
      "2020-12-23 02:50:03,866 : INFO : diff #set()\n",
      "2020-12-23 02:50:04,129 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:04,257 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.20867032648463, 0.4527610970314536], [0.8632827997207642, 0.1367172], [1.0, 1.0], [2.584962500721156, 5.788442787590127, 5.834262629942076, 2.5391426583692063, 3.24930012922092, 0.045819842351948914]]\n",
      "2020-12-23 02:50:04,259 : INFO : Removed 0 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:04,260 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:04,261 : INFO : built Dictionary(54 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 84 corpus positions)\n",
      "2020-12-23 02:50:04,269 : INFO : token count processed\n",
      "2020-12-23 02:50:04,271 : INFO : frequencies processed\n",
      "2020-12-23 02:50:04,408 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:04,409 : INFO : entropies processed\n",
      "2020-12-23 02:50:04,409 : INFO : extropies processed\n",
      "2020-12-23 02:50:04,411 : INFO : token count processed\n",
      "2020-12-23 02:50:04,412 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:04,413 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:04,413 : INFO : vocab #2480\n",
      "2020-12-23 02:50:04,414 : INFO : diff #set()\n",
      "2020-12-23 02:50:04,685 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:04,814 : INFO : Computed distances or similarities ('284', 'sacp-python-common/setup.py')[[1.2101133287357124, 0.4524654853658779], [0.9020722284913063, 0.09792777], [0.0, 0.0], [2.584962500721156, 5.370004292053436, 5.51547195209715, 2.439494840677442, 2.930509451375994, 0.14546766004371392]]\n",
      "2020-12-23 02:50:04,816 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:04,817 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:04,818 : INFO : built Dictionary(81 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 329 corpus positions)\n",
      "2020-12-23 02:50:04,830 : INFO : token count processed\n",
      "2020-12-23 02:50:04,832 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:04,833 : INFO : frequencies processed\n",
      "2020-12-23 02:50:04,834 : INFO : token count processed\n",
      "2020-12-23 02:50:04,835 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:04,836 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:04,837 : INFO : vocab #2480\n",
      "2020-12-23 02:50:04,838 : INFO : diff #set()\n",
      "2020-12-23 02:50:05,096 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:05,222 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.237304216254112, 0.44696648436764064], [0.9271139651536942, 0.072886035], [nan, nan], [2.584962500721156, 5.695663584743922, 5.76636787801714, 2.5142582074479387, 3.1814053772959836, 0.07070429327321737]]\n",
      "2020-12-23 02:50:05,225 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:05,226 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:05,226 : INFO : built Dictionary(42 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 104 corpus positions)\n",
      "2020-12-23 02:50:05,232 : INFO : token count processed\n",
      "2020-12-23 02:50:05,235 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:05,236 : INFO : frequencies processed\n",
      "2020-12-23 02:50:05,237 : INFO : token count processed\n",
      "2020-12-23 02:50:05,237 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:05,238 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:05,239 : INFO : vocab #2480\n",
      "2020-12-23 02:50:05,240 : INFO : diff #set()\n",
      "2020-12-23 02:50:05,499 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:05,627 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.2336561382161841, 0.44769648420396885], [0.9326469004154205, 0.0673531], [nan, nan], [2.584962500721156, 4.9004417692112465, 5.081347461223232, 2.4040568087091705, 2.496384960502076, 0.18090569201198559]]\n",
      "2020-12-23 02:50:05,630 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:05,631 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:05,632 : INFO : built Dictionary(40 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 111 corpus positions)\n",
      "2020-12-23 02:50:05,649 : INFO : token count processed\n",
      "2020-12-23 02:50:05,652 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:05,652 : INFO : frequencies processed\n",
      "2020-12-23 02:50:05,654 : INFO : token count processed\n",
      "2020-12-23 02:50:05,655 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:05,656 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:05,657 : INFO : vocab #2480\n",
      "2020-12-23 02:50:05,658 : INFO : diff #set()\n",
      "2020-12-23 02:50:05,922 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:06,050 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.2399285279826286, 0.44644281614674597], [0.9477166943252087, 0.052283306], [nan, nan], [2.584962500721156, 4.778624108914332, 4.9606404837181906, 2.402946125917298, 2.3756779829970345, 0.18201637480385813]]\n",
      "2020-12-23 02:50:06,052 : INFO : Removed 0 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:06,053 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:06,054 : INFO : built Dictionary(41 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 142 corpus positions)\n",
      "2020-12-23 02:50:06,060 : INFO : token count processed\n",
      "2020-12-23 02:50:06,062 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:06,063 : INFO : frequencies processed\n",
      "2020-12-23 02:50:06,064 : INFO : token count processed\n",
      "2020-12-23 02:50:06,065 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:06,066 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:06,067 : INFO : vocab #2480\n",
      "2020-12-23 02:50:06,068 : INFO : diff #set()\n",
      "2020-12-23 02:50:06,327 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:06,456 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.238363558636006, 0.4467549501249793], [0.9440350644290447, 0.055964936], [nan, nan], [2.584962500721156, 4.773880192225086, 4.929257556226441, 2.429585136719801, 2.3442950555052846, 0.15537736400135493]]\n",
      "2020-12-23 02:50:06,459 : INFO : Removed 0 and 346 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:06,460 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:06,463 : INFO : built Dictionary(152 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 1967 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:50:06,500 : INFO : token count processed\n",
      "2020-12-23 02:50:06,505 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:06,506 : INFO : frequencies processed\n",
      "2020-12-23 02:50:06,508 : INFO : token count processed\n",
      "2020-12-23 02:50:06,509 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:06,510 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:06,511 : INFO : vocab #2480\n",
      "2020-12-23 02:50:06,512 : INFO : diff #set()\n",
      "2020-12-23 02:50:06,777 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:06,904 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.2558259985490074, 0.44329660206204735], [0.9336234927177429, 0.06637651], [nan, nan], [2.584962500721156, 6.620773041953877, 6.636325827244903, 2.5694097154301296, 4.051363326523747, 0.015552785291026439]]\n",
      "2020-12-23 02:50:06,907 : INFO : Removed 0 and 132 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:06,908 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:06,909 : INFO : built Dictionary(82 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 530 corpus positions)\n",
      "2020-12-23 02:50:06,923 : INFO : token count processed\n",
      "2020-12-23 02:50:06,925 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:06,926 : INFO : frequencies processed\n",
      "2020-12-23 02:50:06,927 : INFO : token count processed\n",
      "2020-12-23 02:50:06,928 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:06,929 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:06,930 : INFO : vocab #2480\n",
      "2020-12-23 02:50:06,931 : INFO : diff #set()\n",
      "2020-12-23 02:50:07,185 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:07,314 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.2291170600257015, 0.44860811391774563], [0.9128947705030441, 0.08710523], [nan, nan], [2.584962500721156, 5.828370634755606, 5.873492575040171, 2.5398405604365912, 3.2885300743190147, 0.04512194028456484]]\n",
      "2020-12-23 02:50:07,316 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:07,317 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:07,318 : INFO : built Dictionary(83 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 293 corpus positions)\n",
      "2020-12-23 02:50:07,331 : INFO : token count processed\n",
      "2020-12-23 02:50:07,333 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:07,334 : INFO : frequencies processed\n",
      "2020-12-23 02:50:07,335 : INFO : token count processed\n",
      "2020-12-23 02:50:07,336 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:07,337 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:07,338 : INFO : vocab #2480\n",
      "2020-12-23 02:50:07,339 : INFO : diff #set()\n",
      "2020-12-23 02:50:07,597 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:07,725 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.2331631164657746, 0.4477953234256392], [0.9163035750389099, 0.083696425], [nan, nan], [2.584962500721156, 5.774409284925443, 5.848676560140939, 2.5106952255056587, 3.263714059419783, 0.07426727521549648]]\n",
      "2020-12-23 02:50:07,727 : INFO : Removed 0 and 51 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:07,728 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:07,729 : INFO : built Dictionary(93 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 342 corpus positions)\n",
      "2020-12-23 02:50:07,752 : INFO : token count processed\n",
      "2020-12-23 02:50:07,757 : INFO : frequencies processed\n",
      "2020-12-23 02:50:07,887 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:07,888 : INFO : entropies processed\n",
      "2020-12-23 02:50:07,889 : INFO : extropies processed\n",
      "2020-12-23 02:50:07,891 : INFO : token count processed\n",
      "2020-12-23 02:50:07,892 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:07,893 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:07,894 : INFO : vocab #2480\n",
      "2020-12-23 02:50:07,895 : INFO : diff #set()\n",
      "2020-12-23 02:50:08,153 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:08,281 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.208501142834073, 0.45279578108650814], [0.8640135377645493, 0.13598646], [0.0, 0.0], [2.584962500721156, 5.977819040873918, 6.028924987359682, 2.533856554235392, 3.443962486638526, 0.051105946485764164]]\n",
      "2020-12-23 02:50:08,286 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:08,291 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:08,292 : INFO : built Dictionary(75 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 289 corpus positions)\n",
      "2020-12-23 02:50:08,304 : INFO : token count processed\n",
      "2020-12-23 02:50:08,306 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:08,307 : INFO : frequencies processed\n",
      "2020-12-23 02:50:08,308 : INFO : token count processed\n",
      "2020-12-23 02:50:08,309 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:08,310 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:08,310 : INFO : vocab #2480\n",
      "2020-12-23 02:50:08,311 : INFO : diff #set()\n",
      "2020-12-23 02:50:08,580 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:08,713 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.2350423131304715, 0.4474188225096142], [0.8888232260942459, 0.111176774], [nan, nan], [2.584962500721156, 5.901812829596593, 5.972345468001015, 2.5144298623167334, 3.3873829672798585, 0.07053263840442181]]\n",
      "2020-12-23 02:50:08,716 : INFO : Removed 0 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:08,717 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:08,718 : INFO : built Dictionary(77 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 289 corpus positions)\n",
      "2020-12-23 02:50:08,730 : INFO : token count processed\n",
      "2020-12-23 02:50:08,732 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:08,733 : INFO : frequencies processed\n",
      "2020-12-23 02:50:08,734 : INFO : token count processed\n",
      "2020-12-23 02:50:08,735 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:08,736 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:08,736 : INFO : vocab #2480\n",
      "2020-12-23 02:50:08,737 : INFO : diff #set()\n",
      "2020-12-23 02:50:08,997 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:09,126 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.238584773865571, 0.4467108021436274], [0.9390228874981403, 0.060977113], [nan, nan], [2.584962500721156, 5.643202320803383, 5.72181324862062, 2.5063515729039185, 3.136850747899464, 0.07861092781723755]]\n",
      "2020-12-23 02:50:09,128 : INFO : Removed 0 and 30 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:09,129 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:09,130 : INFO : built Dictionary(89 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 323 corpus positions)\n",
      "2020-12-23 02:50:09,151 : INFO : token count processed\n",
      "2020-12-23 02:50:09,156 : INFO : frequencies processed\n",
      "2020-12-23 02:50:09,285 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:09,286 : INFO : entropies processed\n",
      "2020-12-23 02:50:09,286 : INFO : extropies processed\n",
      "2020-12-23 02:50:09,288 : INFO : token count processed\n",
      "2020-12-23 02:50:09,289 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:09,291 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:09,291 : INFO : vocab #2480\n",
      "2020-12-23 02:50:09,293 : INFO : diff #set()\n",
      "2020-12-23 02:50:09,553 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:09,682 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.217671876397604, 0.4509233357030276], [0.8897669911384583, 0.11023301], [0.0, 0.0], [2.584962500721156, 5.925214310725336, 5.9809376410529325, 2.5292391703935593, 3.3959751403317764, 0.0557233303275968]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:50:09,685 : INFO : Removed 0 and 293 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:09,686 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:09,689 : INFO : built Dictionary(163 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 1718 corpus positions)\n",
      "2020-12-23 02:50:09,728 : INFO : token count processed\n",
      "2020-12-23 02:50:09,731 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:09,732 : INFO : frequencies processed\n",
      "2020-12-23 02:50:09,735 : INFO : token count processed\n",
      "2020-12-23 02:50:09,736 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:09,737 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:09,738 : INFO : vocab #2480\n",
      "2020-12-23 02:50:09,740 : INFO : diff #set()\n",
      "2020-12-23 02:50:09,995 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:10,125 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.2572683931066553, 0.44301333552263594], [0.9369772896170616, 0.06302271], [nan, nan], [2.584962500721156, 6.551685682764175, 6.56917713689257, 2.567471046592761, 3.984214636171414, 0.017491454128395034]]\n",
      "2020-12-23 02:50:10,128 : INFO : Removed 0 and 88 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:10,129 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:10,130 : INFO : built Dictionary(138 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 559 corpus positions)\n",
      "2020-12-23 02:50:10,155 : INFO : token count processed\n",
      "2020-12-23 02:50:10,158 : INFO : frequencies processed\n",
      "2020-12-23 02:50:10,284 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:10,285 : INFO : entropies processed\n",
      "2020-12-23 02:50:10,286 : INFO : extropies processed\n",
      "2020-12-23 02:50:10,287 : INFO : token count processed\n",
      "2020-12-23 02:50:10,288 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:10,289 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:10,290 : INFO : vocab #2480\n",
      "2020-12-23 02:50:10,291 : INFO : diff #set()\n",
      "2020-12-23 02:50:10,548 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:10,682 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.2223427540322693, 0.4499755936322501], [0.8874091729521751, 0.11259083], [0.0, 0.0], [2.584962500721156, 6.642985062562557, 6.676275042985011, 2.5516725202987027, 4.0913125422638545, 0.03328998042245335]]\n",
      "2020-12-23 02:50:10,685 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:10,686 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:10,686 : INFO : built Dictionary(52 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 154 corpus positions)\n",
      "2020-12-23 02:50:10,696 : INFO : token count processed\n",
      "2020-12-23 02:50:10,700 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:10,701 : INFO : frequencies processed\n",
      "2020-12-23 02:50:10,703 : INFO : token count processed\n",
      "2020-12-23 02:50:10,705 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:10,706 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:10,707 : INFO : vocab #2480\n",
      "2020-12-23 02:50:10,708 : INFO : diff #set()\n",
      "2020-12-23 02:50:10,969 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:11,096 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.2346207859578413, 0.4475032212552175], [0.9156285151839256, 0.084371485], [nan, nan], [2.584962500721156, 5.2461980344571995, 5.373438738938029, 2.4577217962403264, 2.788476238216873, 0.1272407044808297]]\n",
      "2020-12-23 02:50:11,099 : INFO : Removed 0 and 23 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:11,100 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:11,100 : INFO : built Dictionary(74 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 214 corpus positions)\n",
      "2020-12-23 02:50:11,111 : INFO : token count processed\n",
      "2020-12-23 02:50:11,113 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:11,114 : INFO : frequencies processed\n",
      "2020-12-23 02:50:11,115 : INFO : token count processed\n",
      "2020-12-23 02:50:11,116 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:11,117 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:11,118 : INFO : vocab #2480\n",
      "2020-12-23 02:50:11,119 : INFO : diff #set()\n",
      "2020-12-23 02:50:11,375 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:11,504 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/test_auth_utility.py')[[1.2353581180009885, 0.4473556124842623], [0.915933221578598, 0.08406678], [nan, nan], [2.584962500721156, 5.903090303960449, 5.989417644036465, 2.49863516064514, 3.4044551433153085, 0.08632734007601517]]\n",
      "2020-12-23 02:50:11,507 : INFO : Removed 0 and 442 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:11,507 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:11,509 : INFO : built Dictionary(108 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 1213 corpus positions)\n",
      "2020-12-23 02:50:11,534 : INFO : token count processed\n",
      "2020-12-23 02:50:11,537 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:11,537 : INFO : frequencies processed\n",
      "2020-12-23 02:50:11,539 : INFO : token count processed\n",
      "2020-12-23 02:50:11,540 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:11,541 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:11,541 : INFO : vocab #2480\n",
      "2020-12-23 02:50:11,542 : INFO : diff #set()\n",
      "2020-12-23 02:50:11,794 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:11,921 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.242416102072766, 0.44594756480550385], [0.9316817745566368, 0.068318225], [nan, nan], [2.584962500721156, 6.16659449033757, 6.188223847663321, 2.563333143395405, 3.6032613469421646, 0.02162935732575111]]\n",
      "2020-12-23 02:50:11,924 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:11,925 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:11,926 : INFO : built Dictionary(68 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 248 corpus positions)\n",
      "2020-12-23 02:50:11,936 : INFO : token count processed\n",
      "2020-12-23 02:50:11,938 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:11,939 : INFO : frequencies processed\n",
      "2020-12-23 02:50:11,940 : INFO : token count processed\n",
      "2020-12-23 02:50:11,941 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:11,942 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:11,942 : INFO : vocab #2480\n",
      "2020-12-23 02:50:11,943 : INFO : diff #set()\n",
      "2020-12-23 02:50:12,201 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:12,328 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.2330982120729197, 0.4478083384750594], [0.8879689127206802, 0.11203109], [nan, nan], [2.584962500721156, 5.906856253399655, 5.9812529884941545, 2.510565765626656, 3.3962904877729985, 0.07439673509449918]]\n",
      "2020-12-23 02:50:12,330 : INFO : Removed 0 and 68 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:12,331 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:12,332 : INFO : built Dictionary(81 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 247 corpus positions)\n",
      "2020-12-23 02:50:12,354 : INFO : token count processed\n",
      "2020-12-23 02:50:12,356 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:12,357 : INFO : frequencies processed\n",
      "2020-12-23 02:50:12,358 : INFO : token count processed\n",
      "2020-12-23 02:50:12,359 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:12,360 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:12,361 : INFO : vocab #2480\n",
      "2020-12-23 02:50:12,362 : INFO : diff #set()\n",
      "2020-12-23 02:50:12,615 : INFO : alphabet #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:50:12,742 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.2289567735784177, 0.4486403737630934], [0.9290381520986557, 0.07096185], [nan, nan], [2.584962500721156, 5.965115449163356, 6.036790869717516, 2.5132870801669966, 3.4518283689963596, 0.07167542055415943]]\n",
      "2020-12-23 02:50:12,745 : INFO : Removed 0 and 13 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:12,746 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:12,747 : INFO : built Dictionary(88 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 361 corpus positions)\n",
      "2020-12-23 02:50:12,762 : INFO : token count processed\n",
      "2020-12-23 02:50:12,764 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:12,766 : INFO : frequencies processed\n",
      "2020-12-23 02:50:12,768 : INFO : token count processed\n",
      "2020-12-23 02:50:12,770 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:12,771 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:12,774 : INFO : vocab #2480\n",
      "2020-12-23 02:50:12,774 : INFO : diff #set()\n",
      "2020-12-23 02:50:13,029 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:13,155 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.2408408905900827, 0.44626104610964545], [0.9240728542208672, 0.075927146], [nan, nan], [2.584962500721156, 5.791362404253194, 5.858526986455001, 2.5177979185193493, 3.2735644857338446, 0.06716458220180677]]\n",
      "2020-12-23 02:50:13,158 : INFO : Removed 0 and 41 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:13,158 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:13,160 : INFO : built Dictionary(78 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 373 corpus positions)\n",
      "2020-12-23 02:50:13,176 : INFO : token count processed\n",
      "2020-12-23 02:50:13,178 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:13,179 : INFO : frequencies processed\n",
      "2020-12-23 02:50:13,181 : INFO : token count processed\n",
      "2020-12-23 02:50:13,182 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:13,183 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:13,184 : INFO : vocab #2480\n",
      "2020-12-23 02:50:13,185 : INFO : diff #set()\n",
      "2020-12-23 02:50:13,447 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:13,575 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.2328335971591247, 0.4478614086030945], [0.934015080332756, 0.06598492], [nan, nan], [2.584962500721156, 5.651670454631116, 5.716511126497212, 2.5201218288550598, 3.1315486257760563, 0.06484067186609632]]\n",
      "2020-12-23 02:50:13,577 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:13,578 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:13,579 : INFO : built Dictionary(41 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 83 corpus positions)\n",
      "2020-12-23 02:50:13,591 : INFO : token count processed\n",
      "2020-12-23 02:50:13,593 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:13,594 : INFO : frequencies processed\n",
      "2020-12-23 02:50:13,595 : INFO : token count processed\n",
      "2020-12-23 02:50:13,596 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:13,598 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:13,599 : INFO : vocab #2480\n",
      "2020-12-23 02:50:13,600 : INFO : diff #set()\n",
      "2020-12-23 02:50:13,872 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:14,001 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.2393565995735698, 0.4465568369907791], [0.9394261538982391, 0.060573846], [nan, nan], [2.584962500721156, 4.8226207261920235, 5.034020322442121, 2.3735629044710587, 2.449057821720965, 0.2113995962500974]]\n",
      "2020-12-23 02:50:14,003 : INFO : Removed 0 and 50 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:14,004 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:14,005 : INFO : built Dictionary(86 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 253 corpus positions)\n",
      "2020-12-23 02:50:14,022 : INFO : token count processed\n",
      "2020-12-23 02:50:14,025 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:14,026 : INFO : frequencies processed\n",
      "2020-12-23 02:50:14,027 : INFO : token count processed\n",
      "2020-12-23 02:50:14,028 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:14,029 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:14,029 : INFO : vocab #2480\n",
      "2020-12-23 02:50:14,030 : INFO : diff #set()\n",
      "2020-12-23 02:50:14,281 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:14,408 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.235472342704909, 0.44733275419994933], [0.9000740870833397, 0.09992591], [nan, nan], [2.584962500721156, 6.24862851613934, 6.316407945240464, 2.517183071620032, 3.7314454445193075, 0.06777942910112333]]\n",
      "2020-12-23 02:50:14,410 : INFO : Removed 0 and 32 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:14,411 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:14,412 : INFO : built Dictionary(87 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 369 corpus positions)\n",
      "2020-12-23 02:50:14,428 : INFO : token count processed\n",
      "2020-12-23 02:50:14,430 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:14,431 : INFO : frequencies processed\n",
      "2020-12-23 02:50:14,432 : INFO : token count processed\n",
      "2020-12-23 02:50:14,433 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:14,434 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:14,435 : INFO : vocab #2480\n",
      "2020-12-23 02:50:14,436 : INFO : diff #set()\n",
      "2020-12-23 02:50:14,695 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:14,823 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.251270908089877, 0.44419354259255467], [0.9317644909024239, 0.06823551], [nan, nan], [2.584962500721156, 5.850156917433494, 5.913435956070296, 2.5216834620843533, 3.3284734553491395, 0.06327903863680184]]\n",
      "2020-12-23 02:50:14,826 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:14,827 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:14,828 : INFO : built Dictionary(82 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 367 corpus positions)\n",
      "2020-12-23 02:50:14,840 : INFO : token count processed\n",
      "2020-12-23 02:50:14,842 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:14,843 : INFO : frequencies processed\n",
      "2020-12-23 02:50:14,844 : INFO : token count processed\n",
      "2020-12-23 02:50:14,845 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:14,846 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:14,847 : INFO : vocab #2480\n",
      "2020-12-23 02:50:14,848 : INFO : diff #set()\n",
      "2020-12-23 02:50:15,106 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:15,233 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.233390800894046, 0.44774967265007587], [0.9334739074110985, 0.06652609], [nan, nan], [2.584962500721156, 5.6831976040360095, 5.748003009258671, 2.5201570954984938, 3.163040508537515, 0.06480540522266143]]\n",
      "2020-12-23 02:50:15,236 : INFO : Removed 0 and 52 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:15,237 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:15,238 : INFO : built Dictionary(68 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 345 corpus positions)\n",
      "2020-12-23 02:50:15,254 : INFO : token count processed\n",
      "2020-12-23 02:50:15,257 : INFO : frequencies processed\n",
      "2020-12-23 02:50:15,389 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:15,389 : INFO : entropies processed\n",
      "2020-12-23 02:50:15,390 : INFO : extropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:50:15,392 : INFO : token count processed\n",
      "2020-12-23 02:50:15,394 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:15,395 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:15,396 : INFO : vocab #2480\n",
      "2020-12-23 02:50:15,397 : INFO : diff #set()\n",
      "2020-12-23 02:50:15,661 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:15,790 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.232991159875052, 0.44782980692854846], [0.905566394329071, 0.094433606], [0.0, 0.0], [2.584962500721156, 5.749308601266266, 5.807589856704095, 2.526681245283327, 3.2226273559829393, 0.05828125543782914]]\n",
      "2020-12-23 02:50:15,793 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:15,794 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:15,795 : INFO : built Dictionary(62 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 226 corpus positions)\n",
      "2020-12-23 02:50:15,804 : INFO : token count processed\n",
      "2020-12-23 02:50:15,806 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:15,807 : INFO : frequencies processed\n",
      "2020-12-23 02:50:15,808 : INFO : token count processed\n",
      "2020-12-23 02:50:15,809 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:15,810 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:15,811 : INFO : vocab #2480\n",
      "2020-12-23 02:50:15,812 : INFO : diff #set()\n",
      "2020-12-23 02:50:16,072 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:16,200 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.2456265482373259, 0.44531001861593433], [0.9433081299066544, 0.05669187], [nan, nan], [2.584962500721156, 5.015422548793484, 5.127349886664719, 2.4730351628499205, 2.542387385943563, 0.11192733787123554]]\n",
      "2020-12-23 02:50:16,202 : INFO : Removed 0 and 81 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:16,203 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:16,204 : INFO : built Dictionary(91 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 352 corpus positions)\n",
      "2020-12-23 02:50:16,218 : INFO : token count processed\n",
      "2020-12-23 02:50:16,220 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:16,221 : INFO : frequencies processed\n",
      "2020-12-23 02:50:16,222 : INFO : token count processed\n",
      "2020-12-23 02:50:16,223 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:16,224 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:16,225 : INFO : vocab #2480\n",
      "2020-12-23 02:50:16,226 : INFO : diff #set()\n",
      "2020-12-23 02:50:16,483 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:16,611 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.2322488378449021, 0.44797873025905033], [0.9198877066373825, 0.08011229], [nan, nan], [2.584962500721156, 6.030001281822029, 6.087657747013231, 2.5273060355299544, 3.502695246292075, 0.05765646519120171]]\n",
      "2020-12-23 02:50:16,613 : INFO : Removed 0 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:16,614 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:16,615 : INFO : built Dictionary(79 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 315 corpus positions)\n",
      "2020-12-23 02:50:16,627 : INFO : token count processed\n",
      "2020-12-23 02:50:16,629 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:16,630 : INFO : frequencies processed\n",
      "2020-12-23 02:50:16,631 : INFO : token count processed\n",
      "2020-12-23 02:50:16,632 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:16,634 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:16,634 : INFO : vocab #2480\n",
      "2020-12-23 02:50:16,636 : INFO : diff #set()\n",
      "2020-12-23 02:50:16,908 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:17,037 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.238843762166581, 0.44665912686657366], [0.9118843823671341, 0.08811562], [nan, nan], [2.584962500721156, 5.9537092545441395, 6.019855072383131, 2.5188166828821634, 3.434892571661975, 0.06614581783899176]]\n",
      "2020-12-23 02:50:17,040 : INFO : Removed 0 and 90 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:17,040 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:17,041 : INFO : built Dictionary(89 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 316 corpus positions)\n",
      "2020-12-23 02:50:17,061 : INFO : token count processed\n",
      "2020-12-23 02:50:17,065 : INFO : frequencies processed\n",
      "2020-12-23 02:50:17,195 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:17,196 : INFO : entropies processed\n",
      "2020-12-23 02:50:17,197 : INFO : extropies processed\n",
      "2020-12-23 02:50:17,199 : INFO : token count processed\n",
      "2020-12-23 02:50:17,200 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:17,201 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:17,202 : INFO : vocab #2480\n",
      "2020-12-23 02:50:17,203 : INFO : diff #set()\n",
      "2020-12-23 02:50:17,470 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:17,599 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.2167260601263106, 0.4511157323350181], [0.8956128284335136, 0.10438717], [0.0, 0.0], [2.584962500721156, 6.184756445474906, 6.234584660491177, 2.535134285704884, 3.649622159770021, 0.049828215016271216]]\n",
      "2020-12-23 02:50:17,601 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:17,602 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:17,604 : INFO : built Dictionary(99 unique tokens: ['bugfix', 'chang', 'format', 'run', 'string']...) from 2 documents (total 429 corpus positions)\n",
      "2020-12-23 02:50:17,624 : INFO : token count processed\n",
      "2020-12-23 02:50:17,626 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:17,626 : INFO : frequencies processed\n",
      "2020-12-23 02:50:17,628 : INFO : token count processed\n",
      "2020-12-23 02:50:17,629 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:17,629 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:17,630 : INFO : vocab #2480\n",
      "2020-12-23 02:50:17,631 : INFO : diff #set()\n",
      "2020-12-23 02:50:17,889 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:18,018 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.2453429012705906, 0.4453662732022453], [0.9206954464316368, 0.07930455], [nan, nan], [2.584962500721156, 6.212221456585881, 6.2634794393363595, 2.533704517970678, 3.6785169386152035, 0.05125798275047888]]\n",
      "2020-12-23 02:50:18,020 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:18,021 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:18,023 : INFO : built Dictionary(119 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 412 corpus positions)\n",
      "2020-12-23 02:50:18,050 : INFO : token count processed\n",
      "2020-12-23 02:50:18,053 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:18,057 : INFO : frequencies processed\n",
      "2020-12-23 02:50:18,058 : INFO : token count processed\n",
      "2020-12-23 02:50:18,061 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:18,061 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:18,064 : INFO : vocab #2480\n",
      "2020-12-23 02:50:18,065 : INFO : diff #set()\n",
      "2020-12-23 02:50:18,320 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:18,447 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.2758416367680383, 0.43939788421312004], [0.9543319866061211, 0.045668013], [nan, nan], [2.321928094887362, 6.301552355933639, 6.346224519451889, 2.277255931369112, 4.024296424564527, 0.044672163518249874]]\n",
      "2020-12-23 02:50:18,449 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:18,450 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:18,451 : INFO : built Dictionary(156 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 648 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:50:18,478 : INFO : token count processed\n",
      "2020-12-23 02:50:18,483 : INFO : frequencies processed\n",
      "2020-12-23 02:50:18,610 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:18,610 : INFO : entropies processed\n",
      "2020-12-23 02:50:18,611 : INFO : extropies processed\n",
      "2020-12-23 02:50:18,613 : INFO : token count processed\n",
      "2020-12-23 02:50:18,614 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:18,615 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:18,615 : INFO : vocab #2480\n",
      "2020-12-23 02:50:18,616 : INFO : diff #set()\n",
      "2020-12-23 02:50:18,879 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:19,007 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.2626308075598711, 0.44196339794314377], [0.9267336800694466, 0.07326632], [0.0, 0.0], [2.321928094887362, 6.739005504021667, 6.761500292599052, 2.299433306309977, 4.439572197711691, 0.022494788577385627]]\n",
      "2020-12-23 02:50:19,010 : INFO : Removed 0 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:19,011 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:19,013 : INFO : built Dictionary(100 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 499 corpus positions)\n",
      "2020-12-23 02:50:19,035 : INFO : token count processed\n",
      "2020-12-23 02:50:19,038 : INFO : frequencies processed\n",
      "2020-12-23 02:50:19,164 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:19,165 : INFO : entropies processed\n",
      "2020-12-23 02:50:19,165 : INFO : extropies processed\n",
      "2020-12-23 02:50:19,166 : INFO : token count processed\n",
      "2020-12-23 02:50:19,167 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:19,167 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:19,168 : INFO : vocab #2480\n",
      "2020-12-23 02:50:19,169 : INFO : diff #set()\n",
      "2020-12-23 02:50:19,424 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:19,552 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2867583258705422, 0.43730025542568507], [0.9880392011255026, 0.011960799], [0.0, 0.0], [2.321928094887362, 5.870833373337847, 5.905365855037511, 2.287395613187698, 3.5834377601501486, 0.03453248169966372]]\n",
      "2020-12-23 02:50:19,554 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:19,555 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:19,556 : INFO : built Dictionary(60 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 182 corpus positions)\n",
      "2020-12-23 02:50:19,564 : INFO : token count processed\n",
      "2020-12-23 02:50:19,566 : INFO : frequencies processed\n",
      "2020-12-23 02:50:19,704 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:19,704 : INFO : entropies processed\n",
      "2020-12-23 02:50:19,705 : INFO : extropies processed\n",
      "2020-12-23 02:50:19,707 : INFO : token count processed\n",
      "2020-12-23 02:50:19,709 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:19,710 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:19,711 : INFO : vocab #2480\n",
      "2020-12-23 02:50:19,713 : INFO : diff #set()\n",
      "2020-12-23 02:50:19,984 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:20,114 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.2771499164640343, 0.43914543911663195], [0.9886544914916158, 0.0113455085], [0.0, 0.0], [2.321928094887362, 5.371881234145534, 5.455222137824235, 2.238587191208662, 3.1332940429368725, 0.0833409036787005]]\n",
      "2020-12-23 02:50:20,116 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:20,117 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:20,119 : INFO : built Dictionary(51 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 134 corpus positions)\n",
      "2020-12-23 02:50:20,131 : INFO : token count processed\n",
      "2020-12-23 02:50:20,136 : INFO : frequencies processed\n",
      "2020-12-23 02:50:20,271 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:20,272 : INFO : entropies processed\n",
      "2020-12-23 02:50:20,273 : INFO : extropies processed\n",
      "2020-12-23 02:50:20,274 : INFO : token count processed\n",
      "2020-12-23 02:50:20,275 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:20,276 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:20,277 : INFO : vocab #2480\n",
      "2020-12-23 02:50:20,278 : INFO : diff #set()\n",
      "2020-12-23 02:50:20,548 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:20,676 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.2801792429286352, 0.4385620135352219], [0.9860469996929169, 0.013953], [0.0, 0.0], [2.321928094887362, 4.85108279267097, 4.971133725595477, 2.201877161962855, 2.649205630708115, 0.12005093292450741]]\n",
      "2020-12-23 02:50:20,679 : INFO : Removed 0 and 127 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:20,680 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:20,681 : INFO : built Dictionary(91 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 407 corpus positions)\n",
      "2020-12-23 02:50:20,699 : INFO : token count processed\n",
      "2020-12-23 02:50:20,704 : INFO : frequencies processed\n",
      "2020-12-23 02:50:20,831 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:20,831 : INFO : entropies processed\n",
      "2020-12-23 02:50:20,832 : INFO : extropies processed\n",
      "2020-12-23 02:50:20,833 : INFO : token count processed\n",
      "2020-12-23 02:50:20,834 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:20,834 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:20,835 : INFO : vocab #2480\n",
      "2020-12-23 02:50:20,836 : INFO : diff #set()\n",
      "2020-12-23 02:50:21,094 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:21,222 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.2485116687743198, 0.44473863039594863], [0.9295885860919952, 0.070411414], [0.0, 0.0], [2.321928094887362, 6.139571208108155, 6.170634821215498, 2.2908644817800186, 3.848706726328136, 0.03106361310734318]]\n",
      "2020-12-23 02:50:21,225 : INFO : Removed 0 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:21,226 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:21,227 : INFO : built Dictionary(76 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 417 corpus positions)\n",
      "2020-12-23 02:50:21,245 : INFO : token count processed\n",
      "2020-12-23 02:50:21,248 : INFO : frequencies processed\n",
      "2020-12-23 02:50:21,376 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:21,377 : INFO : entropies processed\n",
      "2020-12-23 02:50:21,377 : INFO : extropies processed\n",
      "2020-12-23 02:50:21,378 : INFO : token count processed\n",
      "2020-12-23 02:50:21,379 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:21,380 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:21,380 : INFO : vocab #2480\n",
      "2020-12-23 02:50:21,381 : INFO : diff #set()\n",
      "2020-12-23 02:50:21,642 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:21,770 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.257856105060579, 0.44289802071915896], [0.9482339508831501, 0.05176605], [1.0, 1.0], [2.321928094887362, 5.609710627339259, 5.645796346696447, 2.2858423755301738, 3.323868251809085, 0.03608571935718796]]\n",
      "2020-12-23 02:50:21,773 : INFO : Removed 0 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:21,773 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:21,775 : INFO : built Dictionary(168 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 1077 corpus positions)\n",
      "2020-12-23 02:50:21,809 : INFO : token count processed\n",
      "2020-12-23 02:50:21,812 : INFO : frequencies processed\n",
      "2020-12-23 02:50:21,941 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:21,942 : INFO : entropies processed\n",
      "2020-12-23 02:50:21,942 : INFO : extropies processed\n",
      "2020-12-23 02:50:21,944 : INFO : token count processed\n",
      "2020-12-23 02:50:21,945 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:21,946 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:21,947 : INFO : vocab #2480\n",
      "2020-12-23 02:50:21,948 : INFO : diff #set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:50:22,208 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:22,337 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.2444603124424052, 0.44554140452223334], [0.895370826125145, 0.104629174], [1.0, 1.0], [2.321928094887362, 7.2441902753576075, 7.252936280610662, 2.3131820896343083, 4.9310081857233, 0.008746005253054356]]\n",
      "2020-12-23 02:50:22,339 : INFO : Removed 0 and 136 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:22,340 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:22,342 : INFO : built Dictionary(131 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 684 corpus positions)\n",
      "2020-12-23 02:50:22,370 : INFO : token count processed\n",
      "2020-12-23 02:50:22,373 : INFO : frequencies processed\n",
      "2020-12-23 02:50:22,502 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:22,503 : INFO : entropies processed\n",
      "2020-12-23 02:50:22,504 : INFO : extropies processed\n",
      "2020-12-23 02:50:22,505 : INFO : token count processed\n",
      "2020-12-23 02:50:22,506 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:22,507 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:22,508 : INFO : vocab #2480\n",
      "2020-12-23 02:50:22,509 : INFO : diff #set()\n",
      "2020-12-23 02:50:22,767 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:22,893 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.2721064479708442, 0.44012022451372046], [0.9593022540211678, 0.040697746], [0.0, 0.0], [2.321928094887362, 6.2567074920449475, 6.2815918415239995, 2.2970437454083097, 3.9596637466366373, 0.024884349479052048]]\n",
      "2020-12-23 02:50:22,896 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:22,897 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:22,898 : INFO : built Dictionary(77 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 194 corpus positions)\n",
      "2020-12-23 02:50:22,917 : INFO : token count processed\n",
      "2020-12-23 02:50:22,921 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:22,922 : INFO : frequencies processed\n",
      "2020-12-23 02:50:22,923 : INFO : token count processed\n",
      "2020-12-23 02:50:22,925 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:22,926 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:22,927 : INFO : vocab #2480\n",
      "2020-12-23 02:50:22,929 : INFO : diff #set()\n",
      "2020-12-23 02:50:23,202 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:23,332 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.282992733177521, 0.43802154315584585], [0.9701467379927635, 0.029853262], [nan, nan], [2.321928094887362, 5.7680018917339435, 5.848062606230642, 2.241867380390663, 3.52613451134328, 0.08006071449669871]]\n",
      "2020-12-23 02:50:23,334 : INFO : Removed 0 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:23,336 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:23,338 : INFO : built Dictionary(174 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 735 corpus positions)\n",
      "2020-12-23 02:50:23,371 : INFO : token count processed\n",
      "2020-12-23 02:50:23,377 : INFO : frequencies processed\n",
      "2020-12-23 02:50:23,502 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:23,503 : INFO : entropies processed\n",
      "2020-12-23 02:50:23,504 : INFO : extropies processed\n",
      "2020-12-23 02:50:23,506 : INFO : token count processed\n",
      "2020-12-23 02:50:23,507 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:23,508 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:23,508 : INFO : vocab #2480\n",
      "2020-12-23 02:50:23,510 : INFO : diff #set()\n",
      "2020-12-23 02:50:23,771 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:23,898 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.2249409209692694, 0.4494501362150154], [0.8662787973880768, 0.1337212], [0.0, 0.0], [2.321928094887362, 6.846479111193757, 6.864491027290329, 2.3039161787907894, 4.5425629324029675, 0.018011916096572378]]\n",
      "2020-12-23 02:50:23,900 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:23,901 : INFO : built Dictionary(25 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 38 corpus positions)\n",
      "2020-12-23 02:50:23,905 : INFO : token count processed\n",
      "2020-12-23 02:50:23,907 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:23,908 : INFO : frequencies processed\n",
      "2020-12-23 02:50:23,909 : INFO : token count processed\n",
      "2020-12-23 02:50:23,910 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:23,911 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:23,911 : INFO : vocab #2480\n",
      "2020-12-23 02:50:23,912 : INFO : diff #set()\n",
      "2020-12-23 02:50:24,170 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:24,298 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/fireException.py')[[1.299844523718435, 0.4348120012839737], [0.9697022773325443, 0.030297723], [nan, nan], [2.321928094887362, 4.165013816065912, 4.4842551447948, 2.0026867661584733, 2.162327049907438, 0.31924132872888844]]\n",
      "2020-12-23 02:50:24,300 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:24,301 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:24,302 : INFO : built Dictionary(52 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 145 corpus positions)\n",
      "2020-12-23 02:50:24,308 : INFO : token count processed\n",
      "2020-12-23 02:50:24,311 : INFO : frequencies processed\n",
      "2020-12-23 02:50:24,438 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:24,439 : INFO : entropies processed\n",
      "2020-12-23 02:50:24,440 : INFO : extropies processed\n",
      "2020-12-23 02:50:24,441 : INFO : token count processed\n",
      "2020-12-23 02:50:24,442 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:24,443 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:24,444 : INFO : vocab #2480\n",
      "2020-12-23 02:50:24,445 : INFO : diff #set()\n",
      "2020-12-23 02:50:24,702 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:24,830 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.2582807589772966, 0.4428147368411659], [0.9558019898831844, 0.04419801], [0.0, 0.0], [2.321928094887362, 5.449968864419248, 5.535126340141865, 2.236770619164746, 3.2131982452545027, 0.08515747572261656]]\n",
      "2020-12-23 02:50:24,833 : INFO : Removed 0 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:24,834 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:24,836 : INFO : built Dictionary(145 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 582 corpus positions)\n",
      "2020-12-23 02:50:24,873 : INFO : token count processed\n",
      "2020-12-23 02:50:24,879 : INFO : frequencies processed\n",
      "2020-12-23 02:50:25,005 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:25,006 : INFO : entropies processed\n",
      "2020-12-23 02:50:25,007 : INFO : extropies processed\n",
      "2020-12-23 02:50:25,008 : INFO : token count processed\n",
      "2020-12-23 02:50:25,009 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:25,010 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:25,010 : INFO : vocab #2480\n",
      "2020-12-23 02:50:25,011 : INFO : diff #set()\n",
      "2020-12-23 02:50:25,271 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:25,398 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.2647717671365535, 0.44154559612174177], [0.9521025232970715, 0.047897477], [0.0, 0.0], [2.321928094887362, 6.530294129310484, 6.553958900782922, 2.2982633234149237, 4.23203080589556, 0.023664771472438062]]\n",
      "2020-12-23 02:50:25,401 : INFO : Removed 0 and 142 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:25,402 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:25,404 : INFO : built Dictionary(118 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 598 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:50:25,432 : INFO : token count processed\n",
      "2020-12-23 02:50:25,435 : INFO : frequencies processed\n",
      "2020-12-23 02:50:25,565 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:25,565 : INFO : entropies processed\n",
      "2020-12-23 02:50:25,566 : INFO : extropies processed\n",
      "2020-12-23 02:50:25,567 : INFO : token count processed\n",
      "2020-12-23 02:50:25,567 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:25,568 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:25,568 : INFO : vocab #2480\n",
      "2020-12-23 02:50:25,569 : INFO : diff #set()\n",
      "2020-12-23 02:50:25,826 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:25,954 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.2784146554528257, 0.43890167121544166], [0.9698031656444073, 0.030196834], [0.0, 0.0], [2.321928094887362, 6.470272233491701, 6.497967634056126, 2.294232694322938, 4.176039539168764, 0.027695400564424766]]\n",
      "2020-12-23 02:50:25,956 : INFO : Removed 0 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:25,957 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:25,959 : INFO : built Dictionary(119 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 387 corpus positions)\n",
      "2020-12-23 02:50:25,982 : INFO : token count processed\n",
      "2020-12-23 02:50:25,984 : INFO : frequencies processed\n",
      "2020-12-23 02:50:26,111 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:26,111 : INFO : entropies processed\n",
      "2020-12-23 02:50:26,112 : INFO : extropies processed\n",
      "2020-12-23 02:50:26,113 : INFO : token count processed\n",
      "2020-12-23 02:50:26,114 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:26,115 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:26,115 : INFO : vocab #2480\n",
      "2020-12-23 02:50:26,116 : INFO : diff #set()\n",
      "2020-12-23 02:50:26,375 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:26,513 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.2767050276607195, 0.4392312521167861], [0.9555218145251274, 0.044478185], [0.0, 0.0], [2.321928094887362, 6.550038223589686, 6.584319651691655, 2.2876466667853927, 4.2623915568042925, 0.03428142810196899]]\n",
      "2020-12-23 02:50:26,516 : INFO : Removed 0 and 70 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:26,516 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:26,518 : INFO : built Dictionary(74 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 303 corpus positions)\n",
      "2020-12-23 02:50:26,534 : INFO : token count processed\n",
      "2020-12-23 02:50:26,536 : INFO : frequencies processed\n",
      "2020-12-23 02:50:26,664 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:26,665 : INFO : entropies processed\n",
      "2020-12-23 02:50:26,666 : INFO : extropies processed\n",
      "2020-12-23 02:50:26,667 : INFO : token count processed\n",
      "2020-12-23 02:50:26,668 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:26,669 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:26,670 : INFO : vocab #2480\n",
      "2020-12-23 02:50:26,671 : INFO : diff #set()\n",
      "2020-12-23 02:50:26,927 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:27,053 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.2617985296896173, 0.44212602796997497], [0.9576764889061451, 0.04232351], [0.0, 0.0], [2.321928094887362, 5.860525481261383, 5.905236760591211, 2.2772168155575345, 3.5833086657038486, 0.0447112793298281]]\n",
      "2020-12-23 02:50:27,056 : INFO : Removed 0 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:27,057 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:27,058 : INFO : built Dictionary(46 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 341 corpus positions)\n",
      "2020-12-23 02:50:27,066 : INFO : token count processed\n",
      "2020-12-23 02:50:27,069 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:27,069 : INFO : frequencies processed\n",
      "2020-12-23 02:50:27,071 : INFO : token count processed\n",
      "2020-12-23 02:50:27,073 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:27,074 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:27,076 : INFO : vocab #2480\n",
      "2020-12-23 02:50:27,077 : INFO : diff #set()\n",
      "2020-12-23 02:50:27,332 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:27,460 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.309035932188206, 0.433081177325954], [0.9985538285691291, 0.0014461714], [nan, nan], [2.321928094887362, 5.945464049777852, 5.989488053627913, 2.2779040910373007, 3.667559958740551, 0.044024003850061]]\n",
      "2020-12-23 02:50:27,463 : INFO : Removed 0 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:27,464 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:27,465 : INFO : built Dictionary(193 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 888 corpus positions)\n",
      "2020-12-23 02:50:27,492 : INFO : token count processed\n",
      "2020-12-23 02:50:27,495 : INFO : frequencies processed\n",
      "2020-12-23 02:50:27,623 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:27,623 : INFO : entropies processed\n",
      "2020-12-23 02:50:27,626 : INFO : extropies processed\n",
      "2020-12-23 02:50:27,628 : INFO : token count processed\n",
      "2020-12-23 02:50:27,629 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:27,630 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:27,631 : INFO : vocab #2480\n",
      "2020-12-23 02:50:27,632 : INFO : diff #set()\n",
      "2020-12-23 02:50:27,895 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:28,024 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.2719163012478092, 0.44015706012178707], [0.9506812319159508, 0.049318768], [0.0, 0.0], [2.321928094887362, 6.811563897304216, 6.830080501356432, 2.3034114908351464, 4.508152406469069, 0.018516604052215335]]\n",
      "2020-12-23 02:50:28,027 : INFO : Removed 0 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:28,028 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:28,029 : INFO : built Dictionary(213 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 983 corpus positions)\n",
      "2020-12-23 02:50:28,065 : INFO : token count processed\n",
      "2020-12-23 02:50:28,067 : INFO : frequencies processed\n",
      "2020-12-23 02:50:28,195 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:28,196 : INFO : entropies processed\n",
      "2020-12-23 02:50:28,197 : INFO : extropies processed\n",
      "2020-12-23 02:50:28,198 : INFO : token count processed\n",
      "2020-12-23 02:50:28,199 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:28,200 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:28,201 : INFO : vocab #2480\n",
      "2020-12-23 02:50:28,202 : INFO : diff #set()\n",
      "2020-12-23 02:50:28,472 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:28,601 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.2440523440478823, 0.445622403885719], [0.9006472229957581, 0.09935278], [1.0, 1.0], [2.321928094887362, 7.502034948968415, 7.510319078424919, 2.313643965430858, 5.188390983537557, 0.00828412945650392]]\n",
      "2020-12-23 02:50:28,604 : INFO : Removed 0 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:28,605 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:28,608 : INFO : built Dictionary(257 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 1556 corpus positions)\n",
      "2020-12-23 02:50:28,658 : INFO : token count processed\n",
      "2020-12-23 02:50:28,665 : INFO : frequencies processed\n",
      "2020-12-23 02:50:28,795 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:28,796 : INFO : entropies processed\n",
      "2020-12-23 02:50:28,796 : INFO : extropies processed\n",
      "2020-12-23 02:50:28,798 : INFO : token count processed\n",
      "2020-12-23 02:50:28,798 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:28,799 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:28,800 : INFO : vocab #2480\n",
      "2020-12-23 02:50:28,801 : INFO : diff #set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:50:29,067 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:29,195 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.264938591575883, 0.44151307400534284], [0.9329321011900902, 0.0670679], [1.0, 1.0], [2.321928094887362, 7.39180093901977, 7.400424002910296, 2.313305030996836, 5.0784959080229335, 0.008623063890525806]]\n",
      "2020-12-23 02:50:29,198 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:29,198 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:29,200 : INFO : built Dictionary(43 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 111 corpus positions)\n",
      "2020-12-23 02:50:29,211 : INFO : token count processed\n",
      "2020-12-23 02:50:29,215 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:29,216 : INFO : frequencies processed\n",
      "2020-12-23 02:50:29,218 : INFO : token count processed\n",
      "2020-12-23 02:50:29,219 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:29,221 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:29,222 : INFO : vocab #2480\n",
      "2020-12-23 02:50:29,224 : INFO : diff #set()\n",
      "2020-12-23 02:50:29,481 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:29,609 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.2748876313207724, 0.43958215176518944], [0.9835114721208811, 0.016488528], [nan, nan], [2.321928094887362, 4.927561309677364, 5.07370791118953, 2.1757814933751956, 2.7517798163021676, 0.14614660151216619]]\n",
      "2020-12-23 02:50:29,612 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:29,613 : INFO : built Dictionary(11 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 12 corpus positions)\n",
      "2020-12-23 02:50:29,615 : INFO : token count processed\n",
      "2020-12-23 02:50:29,617 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:29,618 : INFO : frequencies processed\n",
      "2020-12-23 02:50:29,619 : INFO : token count processed\n",
      "2020-12-23 02:50:29,620 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:29,620 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:29,621 : INFO : vocab #2480\n",
      "2020-12-23 02:50:29,622 : INFO : diff #set()\n",
      "2020-12-23 02:50:29,881 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:30,009 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2832936203217449, 0.43796382169152975], [1.0, 0.0], [nan, nan], [2.321928094887362, 2.5216406363433186, 3.4182958340544896, 1.4252728971761912, 1.0963677391671274, 0.896655197711171]]\n",
      "2020-12-23 02:50:30,013 : INFO : Removed 0 and 530 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:30,014 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:30,018 : INFO : built Dictionary(330 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 2884 corpus positions)\n",
      "2020-12-23 02:50:30,094 : INFO : token count processed\n",
      "2020-12-23 02:50:30,096 : INFO : frequencies processed\n",
      "2020-12-23 02:50:30,224 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:30,224 : INFO : entropies processed\n",
      "2020-12-23 02:50:30,225 : INFO : extropies processed\n",
      "2020-12-23 02:50:30,228 : INFO : token count processed\n",
      "2020-12-23 02:50:30,229 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:30,230 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:30,231 : INFO : vocab #2480\n",
      "2020-12-23 02:50:30,232 : INFO : diff #set()\n",
      "2020-12-23 02:50:30,501 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:30,634 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.2668220366239387, 0.44114623196858305], [0.9282896965742111, 0.0717103], [1.584962500721156, 1.1699250014423124], [2.321928094887362, 7.480007711014331, 7.4849743302271605, 2.316961475674532, 5.163046235339799, 0.004966619212829926]]\n",
      "2020-12-23 02:50:30,637 : INFO : Removed 0 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:30,637 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:30,639 : INFO : built Dictionary(207 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 1033 corpus positions)\n",
      "2020-12-23 02:50:30,674 : INFO : token count processed\n",
      "2020-12-23 02:50:30,677 : INFO : frequencies processed\n",
      "2020-12-23 02:50:30,804 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:30,805 : INFO : entropies processed\n",
      "2020-12-23 02:50:30,806 : INFO : extropies processed\n",
      "2020-12-23 02:50:30,808 : INFO : token count processed\n",
      "2020-12-23 02:50:30,809 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:30,810 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:30,811 : INFO : vocab #2480\n",
      "2020-12-23 02:50:30,813 : INFO : diff #set()\n",
      "2020-12-23 02:50:31,072 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:31,200 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.2384662867050253, 0.44673444757212705], [0.8756746202707291, 0.12432538], [1.584962500721156, 1.1699250014423124], [2.321928094887362, 7.131331012509435, 7.138499302989908, 2.314759804406888, 4.8165712081025465, 0.007168290480473516]]\n",
      "2020-12-23 02:50:31,203 : INFO : Removed 0 and 166 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:31,204 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:31,205 : INFO : built Dictionary(197 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 852 corpus positions)\n",
      "2020-12-23 02:50:31,236 : INFO : token count processed\n",
      "2020-12-23 02:50:31,242 : INFO : frequencies processed\n",
      "2020-12-23 02:50:31,367 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:31,367 : INFO : entropies processed\n",
      "2020-12-23 02:50:31,368 : INFO : extropies processed\n",
      "2020-12-23 02:50:31,370 : INFO : token count processed\n",
      "2020-12-23 02:50:31,371 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:31,371 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:31,372 : INFO : vocab #2480\n",
      "2020-12-23 02:50:31,373 : INFO : diff #set()\n",
      "2020-12-23 02:50:31,642 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:31,770 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.2551316921425257, 0.44343308352424116], [0.904457613825798, 0.095542386], [1.0, 1.0], [2.321928094887362, 7.203742744794778, 7.215656639957078, 2.310014199725063, 4.893728545069715, 0.011913895162299681]]\n",
      "2020-12-23 02:50:31,772 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:31,773 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:31,774 : INFO : built Dictionary(56 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 184 corpus positions)\n",
      "2020-12-23 02:50:31,781 : INFO : token count processed\n",
      "2020-12-23 02:50:31,783 : INFO : frequencies processed\n",
      "2020-12-23 02:50:31,912 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:31,913 : INFO : entropies processed\n",
      "2020-12-23 02:50:31,913 : INFO : extropies processed\n",
      "2020-12-23 02:50:31,915 : INFO : token count processed\n",
      "2020-12-23 02:50:31,915 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:31,916 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:31,917 : INFO : vocab #2480\n",
      "2020-12-23 02:50:31,918 : INFO : diff #set()\n",
      "2020-12-23 02:50:32,176 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:32,304 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.2341221154446016, 0.44760310687000876], [0.9138742908835411, 0.08612571], [0.0, 0.0], [2.321928094887362, 5.195502554608948, 5.27470682830114, 2.2427238211951703, 2.9527787334137776, 0.07920427369219141]]\n",
      "2020-12-23 02:50:32,306 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:32,307 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:32,308 : INFO : built Dictionary(61 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 245 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:50:32,316 : INFO : token count processed\n",
      "2020-12-23 02:50:32,318 : INFO : frequencies processed\n",
      "2020-12-23 02:50:32,446 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:32,447 : INFO : entropies processed\n",
      "2020-12-23 02:50:32,447 : INFO : extropies processed\n",
      "2020-12-23 02:50:32,449 : INFO : token count processed\n",
      "2020-12-23 02:50:32,450 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:32,451 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:32,451 : INFO : vocab #2480\n",
      "2020-12-23 02:50:32,452 : INFO : diff #set()\n",
      "2020-12-23 02:50:32,711 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:32,838 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.2272758527878997, 0.44897896178791313], [0.90960992872715, 0.09039007], [1.0, 1.0], [2.321928094887362, 5.32027245610305, 5.376051118658075, 2.2661494323323366, 3.0541230237707127, 0.05577866255502517]]\n",
      "2020-12-23 02:50:32,841 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:32,842 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:32,843 : INFO : built Dictionary(161 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 485 corpus positions)\n",
      "2020-12-23 02:50:32,875 : INFO : token count processed\n",
      "2020-12-23 02:50:32,880 : INFO : frequencies processed\n",
      "2020-12-23 02:50:33,008 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:33,009 : INFO : entropies processed\n",
      "2020-12-23 02:50:33,009 : INFO : extropies processed\n",
      "2020-12-23 02:50:33,010 : INFO : token count processed\n",
      "2020-12-23 02:50:33,012 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:33,013 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:33,013 : INFO : vocab #2480\n",
      "2020-12-23 02:50:33,015 : INFO : diff #set()\n",
      "2020-12-23 02:50:33,287 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:33,418 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.264347965023066, 0.4416282371114342], [0.923521876335144, 0.07647812], [1.0, 1.0], [2.321928094887362, 6.898202761357263, 6.919652418978813, 2.300478437265812, 4.59772432409145, 0.02144965762154971]]\n",
      "2020-12-23 02:50:33,421 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:33,421 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:33,423 : INFO : built Dictionary(125 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 507 corpus positions)\n",
      "2020-12-23 02:50:33,448 : INFO : token count processed\n",
      "2020-12-23 02:50:33,450 : INFO : frequencies processed\n",
      "2020-12-23 02:50:33,577 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:33,578 : INFO : entropies processed\n",
      "2020-12-23 02:50:33,579 : INFO : extropies processed\n",
      "2020-12-23 02:50:33,580 : INFO : token count processed\n",
      "2020-12-23 02:50:33,581 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:33,582 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:33,583 : INFO : vocab #2480\n",
      "2020-12-23 02:50:33,584 : INFO : diff #set()\n",
      "2020-12-23 02:50:33,849 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:33,977 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.226727809282956, 0.4490894647433432], [0.8420824110507965, 0.15791759], [1.0, 1.0], [2.321928094887362, 6.388500481644799, 6.414212725083635, 2.296215851448526, 4.092284630196273, 0.025712243438835714]]\n",
      "2020-12-23 02:50:33,980 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:33,981 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:33,982 : INFO : built Dictionary(47 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 171 corpus positions)\n",
      "2020-12-23 02:50:33,988 : INFO : token count processed\n",
      "2020-12-23 02:50:33,990 : INFO : frequencies processed\n",
      "2020-12-23 02:50:34,119 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:34,120 : INFO : entropies processed\n",
      "2020-12-23 02:50:34,121 : INFO : extropies processed\n",
      "2020-12-23 02:50:34,123 : INFO : token count processed\n",
      "2020-12-23 02:50:34,125 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:34,126 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:34,127 : INFO : vocab #2480\n",
      "2020-12-23 02:50:34,129 : INFO : diff #set()\n",
      "2020-12-23 02:50:34,389 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:34,517 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.2315688022624187, 0.4481152447489747], [0.9266681671142578, 0.07333183], [0.0, 0.0], [2.321928094887362, 4.8191513650620195, 4.9135875837271, 2.227491876222281, 2.591659488839738, 0.09443621866508067]]\n",
      "2020-12-23 02:50:34,520 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:34,520 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:34,521 : INFO : built Dictionary(52 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 226 corpus positions)\n",
      "2020-12-23 02:50:34,528 : INFO : token count processed\n",
      "2020-12-23 02:50:34,531 : INFO : frequencies processed\n",
      "2020-12-23 02:50:34,658 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:34,658 : INFO : entropies processed\n",
      "2020-12-23 02:50:34,659 : INFO : extropies processed\n",
      "2020-12-23 02:50:34,660 : INFO : token count processed\n",
      "2020-12-23 02:50:34,661 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:34,662 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:34,663 : INFO : vocab #2480\n",
      "2020-12-23 02:50:34,664 : INFO : diff #set()\n",
      "2020-12-23 02:50:34,921 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:35,050 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.2198998277549862, 0.4504707768779428], [0.9161458015441895, 0.0838542], [0.0, 0.0], [2.321928094887362, 5.062480936779194, 5.134835362278918, 2.2495736693876376, 2.8129072673915556, 0.07235442549972415]]\n",
      "2020-12-23 02:50:35,053 : INFO : Removed 0 and 223 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:35,053 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:35,055 : INFO : built Dictionary(242 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 1780 corpus positions)\n",
      "2020-12-23 02:50:35,103 : INFO : token count processed\n",
      "2020-12-23 02:50:35,108 : INFO : frequencies processed\n",
      "2020-12-23 02:50:35,239 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:35,239 : INFO : entropies processed\n",
      "2020-12-23 02:50:35,240 : INFO : extropies processed\n",
      "2020-12-23 02:50:35,242 : INFO : token count processed\n",
      "2020-12-23 02:50:35,243 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:35,244 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:35,245 : INFO : vocab #2480\n",
      "2020-12-23 02:50:35,246 : INFO : diff #set()\n",
      "2020-12-23 02:50:35,512 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:35,640 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.2578910819989997, 0.44289115979618504], [0.9007095023989677, 0.0992905], [1.0, 1.0], [2.321928094887362, 7.185085743102134, 7.192824631891441, 2.314189206098056, 4.870896537004079, 0.007738888789306841]]\n",
      "2020-12-23 02:50:35,643 : INFO : Removed 0 and 148 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:35,644 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:35,645 : INFO : built Dictionary(158 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 755 corpus positions)\n",
      "2020-12-23 02:50:35,671 : INFO : token count processed\n",
      "2020-12-23 02:50:35,673 : INFO : frequencies processed\n",
      "2020-12-23 02:50:35,803 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:35,804 : INFO : entropies processed\n",
      "2020-12-23 02:50:35,805 : INFO : extropies processed\n",
      "2020-12-23 02:50:35,806 : INFO : token count processed\n",
      "2020-12-23 02:50:35,807 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:35,807 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:35,808 : INFO : vocab #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:50:35,809 : INFO : diff #set()\n",
      "2020-12-23 02:50:36,065 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:36,193 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.2541721169402362, 0.44362184789925363], [0.9286583662033081, 0.071341634], [1.584962500721156, 1.1699250014423124], [2.321928094887362, 6.591225336124281, 6.605334041684925, 2.3078193893267187, 4.283405946797563, 0.01410870556064392]]\n",
      "2020-12-23 02:50:36,196 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:36,197 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:36,198 : INFO : built Dictionary(39 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 93 corpus positions)\n",
      "2020-12-23 02:50:36,209 : INFO : token count processed\n",
      "2020-12-23 02:50:36,211 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:36,212 : INFO : frequencies processed\n",
      "2020-12-23 02:50:36,213 : INFO : token count processed\n",
      "2020-12-23 02:50:36,214 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:36,215 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:36,215 : INFO : vocab #2480\n",
      "2020-12-23 02:50:36,216 : INFO : diff #set()\n",
      "2020-12-23 02:50:36,475 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:36,602 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.2748897515520452, 0.4395817420680493], [0.9876480139791965, 0.012351986], [nan, nan], [2.321928094887362, 4.7032114441396695, 4.876349949256587, 2.1487895897704448, 2.554421854369225, 0.17313850511691786]]\n",
      "2020-12-23 02:50:36,605 : INFO : Removed 0 and 9 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:36,606 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:36,607 : INFO : built Dictionary(115 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 333 corpus positions)\n",
      "2020-12-23 02:50:36,630 : INFO : token count processed\n",
      "2020-12-23 02:50:36,636 : INFO : frequencies processed\n",
      "2020-12-23 02:50:36,766 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:36,767 : INFO : entropies processed\n",
      "2020-12-23 02:50:36,767 : INFO : extropies processed\n",
      "2020-12-23 02:50:36,768 : INFO : token count processed\n",
      "2020-12-23 02:50:36,769 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:36,770 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:36,771 : INFO : vocab #2480\n",
      "2020-12-23 02:50:36,772 : INFO : diff #set()\n",
      "2020-12-23 02:50:37,033 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:37,160 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.220110052725574, 0.4504281212421541], [0.8798029571771622, 0.12019704], [1.0, 1.0], [2.321928094887362, 6.14228447828618, 6.172222285394163, 2.291990287779379, 3.850294190506801, 0.02993780710798344]]\n",
      "2020-12-23 02:50:37,163 : INFO : Removed 0 and 184 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:37,164 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:37,165 : INFO : built Dictionary(254 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 1123 corpus positions)\n",
      "2020-12-23 02:50:37,211 : INFO : token count processed\n",
      "2020-12-23 02:50:37,217 : INFO : frequencies processed\n",
      "2020-12-23 02:50:37,346 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:37,347 : INFO : entropies processed\n",
      "2020-12-23 02:50:37,347 : INFO : extropies processed\n",
      "2020-12-23 02:50:37,349 : INFO : token count processed\n",
      "2020-12-23 02:50:37,350 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:37,351 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:37,351 : INFO : vocab #2480\n",
      "2020-12-23 02:50:37,352 : INFO : diff #set()\n",
      "2020-12-23 02:50:37,611 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:37,739 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.2831346392028506, 0.43799431835047054], [0.971513919532299, 0.02848608], [0.0, 0.0], [2.321928094887362, 7.450178124335845, 7.4652576854003385, 2.306848533822869, 5.143329590512977, 0.015079561064493596]]\n",
      "2020-12-23 02:50:37,742 : INFO : Removed 0 and 16 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:37,742 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:37,743 : INFO : built Dictionary(56 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 171 corpus positions)\n",
      "2020-12-23 02:50:37,751 : INFO : token count processed\n",
      "2020-12-23 02:50:37,753 : INFO : frequencies processed\n",
      "2020-12-23 02:50:37,880 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:37,881 : INFO : entropies processed\n",
      "2020-12-23 02:50:37,882 : INFO : extropies processed\n",
      "2020-12-23 02:50:37,883 : INFO : token count processed\n",
      "2020-12-23 02:50:37,884 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:37,885 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:37,885 : INFO : vocab #2480\n",
      "2020-12-23 02:50:37,886 : INFO : diff #set()\n",
      "2020-12-23 02:50:38,143 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:38,271 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.228442674537257, 0.44874387455699444], [0.914824053645134, 0.08517595], [0.0, 0.0], [2.321928094887362, 5.20665021947654, 5.285129785975824, 2.243448528388077, 2.963201691088462, 0.07847956649928456]]\n",
      "2020-12-23 02:50:38,274 : INFO : Removed 0 and 89 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:38,274 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:38,276 : INFO : built Dictionary(124 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 509 corpus positions)\n",
      "2020-12-23 02:50:38,298 : INFO : token count processed\n",
      "2020-12-23 02:50:38,303 : INFO : frequencies processed\n",
      "2020-12-23 02:50:38,429 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:38,429 : INFO : entropies processed\n",
      "2020-12-23 02:50:38,430 : INFO : extropies processed\n",
      "2020-12-23 02:50:38,431 : INFO : token count processed\n",
      "2020-12-23 02:50:38,432 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:38,432 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:38,433 : INFO : vocab #2480\n",
      "2020-12-23 02:50:38,433 : INFO : diff #set()\n",
      "2020-12-23 02:50:38,699 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:38,827 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.2755887564662194, 0.4394467133652516], [0.9512371458113194, 0.048762854], [1.0, 1.0], [2.321928094887362, 6.524718477352, 6.549919162374559, 2.2967274098648023, 4.227991067487197, 0.025200685022559455]]\n",
      "2020-12-23 02:50:38,829 : INFO : Removed 0 and 8 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:38,830 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:38,831 : INFO : built Dictionary(62 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 216 corpus positions)\n",
      "2020-12-23 02:50:38,839 : INFO : token count processed\n",
      "2020-12-23 02:50:38,842 : INFO : frequencies processed\n",
      "2020-12-23 02:50:38,969 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:38,970 : INFO : entropies processed\n",
      "2020-12-23 02:50:38,970 : INFO : extropies processed\n",
      "2020-12-23 02:50:38,972 : INFO : token count processed\n",
      "2020-12-23 02:50:38,972 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:38,973 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:38,974 : INFO : vocab #2480\n",
      "2020-12-23 02:50:38,975 : INFO : diff #set()\n",
      "2020-12-23 02:50:39,233 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:39,360 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.2446964752301157, 0.4454945294541368], [0.9315003678202629, 0.06849963], [0.0, 0.0], [2.321928094887362, 5.321859380715434, 5.391768778281412, 2.2520186973213843, 3.0698406833940495, 0.06990939756597747]]\n",
      "2020-12-23 02:50:39,362 : INFO : Removed 0 and 119 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:50:39,363 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:39,364 : INFO : built Dictionary(139 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 834 corpus positions)\n",
      "2020-12-23 02:50:39,387 : INFO : token count processed\n",
      "2020-12-23 02:50:39,390 : INFO : frequencies processed\n",
      "2020-12-23 02:50:39,519 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:39,520 : INFO : entropies processed\n",
      "2020-12-23 02:50:39,521 : INFO : extropies processed\n",
      "2020-12-23 02:50:39,522 : INFO : token count processed\n",
      "2020-12-23 02:50:39,523 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:39,525 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:39,525 : INFO : vocab #2480\n",
      "2020-12-23 02:50:39,527 : INFO : diff #set()\n",
      "2020-12-23 02:50:39,796 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:39,924 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.2481618260714358, 0.44480783740886487], [0.9080621376633644, 0.09193786], [1.584962500721156, 1.1699250014423124], [2.321928094887362, 6.500767808767801, 6.515019908936743, 2.3076759947184193, 4.193091814049382, 0.014252100168942405]]\n",
      "2020-12-23 02:50:39,927 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:39,928 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:39,928 : INFO : built Dictionary(35 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 56 corpus positions)\n",
      "2020-12-23 02:50:39,933 : INFO : token count processed\n",
      "2020-12-23 02:50:39,936 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:39,936 : INFO : frequencies processed\n",
      "2020-12-23 02:50:39,937 : INFO : token count processed\n",
      "2020-12-23 02:50:39,938 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:39,939 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:39,940 : INFO : vocab #2480\n",
      "2020-12-23 02:50:39,941 : INFO : diff #set()\n",
      "2020-12-23 02:50:40,200 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:40,328 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.2851197857274423, 0.4376138206171373], [0.9832451716065407, 0.016754828], [nan, nan], [2.321928094887362, 4.736228843383063, 4.953259040701274, 2.104897897569151, 2.6313309458139122, 0.21703019731821183]]\n",
      "2020-12-23 02:50:40,331 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:40,332 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:40,333 : INFO : built Dictionary(97 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 397 corpus positions)\n",
      "2020-12-23 02:50:40,348 : INFO : token count processed\n",
      "2020-12-23 02:50:40,351 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:40,354 : INFO : frequencies processed\n",
      "2020-12-23 02:50:40,355 : INFO : token count processed\n",
      "2020-12-23 02:50:40,358 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:40,359 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:40,359 : INFO : vocab #2480\n",
      "2020-12-23 02:50:40,360 : INFO : diff #set()\n",
      "2020-12-23 02:50:40,620 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:40,752 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.2857661888737189, 0.4374900656364756], [0.9755022767931223, 0.024497723], [nan, nan], [2.321928094887362, 5.788442787590127, 5.841100153960053, 2.269270728517437, 3.5191720590726905, 0.05265736636992546]]\n",
      "2020-12-23 02:50:40,755 : INFO : Removed 0 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:40,756 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:40,757 : INFO : built Dictionary(53 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 83 corpus positions)\n",
      "2020-12-23 02:50:40,764 : INFO : token count processed\n",
      "2020-12-23 02:50:40,766 : INFO : frequencies processed\n",
      "2020-12-23 02:50:40,894 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:40,895 : INFO : entropies processed\n",
      "2020-12-23 02:50:40,895 : INFO : extropies processed\n",
      "2020-12-23 02:50:40,897 : INFO : token count processed\n",
      "2020-12-23 02:50:40,898 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:40,898 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:40,899 : INFO : vocab #2480\n",
      "2020-12-23 02:50:40,900 : INFO : diff #set()\n",
      "2020-12-23 02:50:41,158 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:41,287 : INFO : Computed distances or similarities ('282', 'sacp-python-common/setup.py')[[1.2765605219060865, 0.4392591325280183], [0.972905470058322, 0.02709453], [0.0, 0.0], [2.321928094887362, 5.370004292053436, 5.488381712079575, 2.2035506748612237, 3.166453617192213, 0.11837742002613894]]\n",
      "2020-12-23 02:50:41,289 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:41,290 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:41,291 : INFO : built Dictionary(79 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 328 corpus positions)\n",
      "2020-12-23 02:50:41,301 : INFO : token count processed\n",
      "2020-12-23 02:50:41,304 : INFO : frequencies processed\n",
      "2020-12-23 02:50:41,432 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:41,433 : INFO : entropies processed\n",
      "2020-12-23 02:50:41,434 : INFO : extropies processed\n",
      "2020-12-23 02:50:41,435 : INFO : token count processed\n",
      "2020-12-23 02:50:41,436 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:41,437 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:41,437 : INFO : vocab #2480\n",
      "2020-12-23 02:50:41,438 : INFO : diff #set()\n",
      "2020-12-23 02:50:41,693 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:41,820 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.2737186816946071, 0.4398081469140668], [0.9709091577678919, 0.029090842], [0.0, 0.0], [2.321928094887362, 5.695663584743922, 5.745630514930207, 2.271961164701077, 3.423702420042845, 0.04996693018628484]]\n",
      "2020-12-23 02:50:41,823 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:41,824 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:41,825 : INFO : built Dictionary(41 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 103 corpus positions)\n",
      "2020-12-23 02:50:41,839 : INFO : token count processed\n",
      "2020-12-23 02:50:41,842 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:41,843 : INFO : frequencies processed\n",
      "2020-12-23 02:50:41,844 : INFO : token count processed\n",
      "2020-12-23 02:50:41,845 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:41,846 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:41,847 : INFO : vocab #2480\n",
      "2020-12-23 02:50:41,848 : INFO : diff #set()\n",
      "2020-12-23 02:50:42,114 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:42,244 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.2808803195643776, 0.43842721225767284], [0.9825186803936958, 0.01748132], [nan, nan], [2.321928094887362, 4.9004417692112465, 5.052286384330629, 2.170083479767979, 2.730358289443267, 0.15184461511938263]]\n",
      "2020-12-23 02:50:42,247 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:42,248 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:42,248 : INFO : built Dictionary(39 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 110 corpus positions)\n",
      "2020-12-23 02:50:42,254 : INFO : token count processed\n",
      "2020-12-23 02:50:42,256 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:42,257 : INFO : frequencies processed\n",
      "2020-12-23 02:50:42,258 : INFO : token count processed\n",
      "2020-12-23 02:50:42,259 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:42,260 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:42,260 : INFO : vocab #2480\n",
      "2020-12-23 02:50:42,261 : INFO : diff #set()\n",
      "2020-12-23 02:50:42,536 : INFO : alphabet #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:50:42,664 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.2845628542596463, 0.43772050225515374], [0.9817988406866789, 0.01820116], [nan, nan], [2.321928094887362, 4.778624108914332, 4.931360852053181, 2.1691913517485135, 2.6094327571658185, 0.15273674313884822]]\n",
      "2020-12-23 02:50:42,666 : INFO : Removed 0 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:42,667 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:42,668 : INFO : built Dictionary(40 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 141 corpus positions)\n",
      "2020-12-23 02:50:42,673 : INFO : token count processed\n",
      "2020-12-23 02:50:42,676 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:42,677 : INFO : frequencies processed\n",
      "2020-12-23 02:50:42,678 : INFO : token count processed\n",
      "2020-12-23 02:50:42,678 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:42,679 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:42,680 : INFO : vocab #2480\n",
      "2020-12-23 02:50:42,681 : INFO : diff #set()\n",
      "2020-12-23 02:50:42,942 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:43,070 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.2799164654584192, 0.43861256109615027], [0.9894991591572762, 0.010500841], [nan, nan], [2.321928094887362, 4.773880192225086, 4.904070030013258, 2.19173825709919, 2.5821419351258954, 0.13018983778817184]]\n",
      "2020-12-23 02:50:43,073 : INFO : Removed 0 and 346 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:43,074 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:43,077 : INFO : built Dictionary(149 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 1966 corpus positions)\n",
      "2020-12-23 02:50:43,109 : INFO : token count processed\n",
      "2020-12-23 02:50:43,113 : INFO : frequencies processed\n",
      "2020-12-23 02:50:43,240 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:43,241 : INFO : entropies processed\n",
      "2020-12-23 02:50:43,241 : INFO : extropies processed\n",
      "2020-12-23 02:50:43,244 : INFO : token count processed\n",
      "2020-12-23 02:50:43,245 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:43,245 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:43,246 : INFO : vocab #2480\n",
      "2020-12-23 02:50:43,247 : INFO : diff #set()\n",
      "2020-12-23 02:50:43,516 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:43,645 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.2666655242405436, 0.4411766929463731], [0.9418208487331867, 0.05817915], [1.0, 1.0], [2.321928094887362, 6.620773041953877, 6.629400584292853, 2.3133005525483856, 4.307472489405491, 0.008627542338976113]]\n",
      "2020-12-23 02:50:43,648 : INFO : Removed 0 and 132 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:43,649 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:43,651 : INFO : built Dictionary(81 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 529 corpus positions)\n",
      "2020-12-23 02:50:43,669 : INFO : token count processed\n",
      "2020-12-23 02:50:43,674 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:43,677 : INFO : frequencies processed\n",
      "2020-12-23 02:50:43,678 : INFO : token count processed\n",
      "2020-12-23 02:50:43,679 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:43,680 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:43,681 : INFO : vocab #2480\n",
      "2020-12-23 02:50:43,682 : INFO : diff #set()\n",
      "2020-12-23 02:50:43,936 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:44,063 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.2787357575613054, 0.43883982453068465], [0.9884943580254912, 0.011505642], [nan, nan], [2.321928094887362, 5.828370634755606, 5.866020866001208, 2.2842778636417593, 3.544092771113846, 0.037650231245602406]]\n",
      "2020-12-23 02:50:44,065 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:44,066 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:44,067 : INFO : built Dictionary(81 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 292 corpus positions)\n",
      "2020-12-23 02:50:44,083 : INFO : token count processed\n",
      "2020-12-23 02:50:44,085 : INFO : frequencies processed\n",
      "2020-12-23 02:50:44,217 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:44,218 : INFO : entropies processed\n",
      "2020-12-23 02:50:44,219 : INFO : extropies processed\n",
      "2020-12-23 02:50:44,220 : INFO : token count processed\n",
      "2020-12-23 02:50:44,221 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:44,222 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:44,223 : INFO : vocab #2480\n",
      "2020-12-23 02:50:44,224 : INFO : diff #set()\n",
      "2020-12-23 02:50:44,490 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:44,617 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.2277940526150852, 0.4488745262723702], [0.8997040539979935, 0.100295946], [0.0, 0.0], [2.321928094887362, 5.774409284925443, 5.821395688855445, 2.2749416909573608, 3.4994675939680824, 0.04698640393000186]]\n",
      "2020-12-23 02:50:44,620 : INFO : Removed 0 and 51 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:44,621 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:44,622 : INFO : built Dictionary(93 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 341 corpus positions)\n",
      "2020-12-23 02:50:44,638 : INFO : token count processed\n",
      "2020-12-23 02:50:44,641 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:44,642 : INFO : frequencies processed\n",
      "2020-12-23 02:50:44,643 : INFO : token count processed\n",
      "2020-12-23 02:50:44,644 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:44,645 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:44,646 : INFO : vocab #2480\n",
      "2020-12-23 02:50:44,647 : INFO : diff #set()\n",
      "2020-12-23 02:50:44,899 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:45,026 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.2835945192001588, 0.43790611318784184], [0.9835572429001331, 0.016442757], [nan, nan], [2.321928094887362, 5.977819040873918, 6.029736714498678, 2.2700104212626018, 3.7078086196113156, 0.051917673624759964]]\n",
      "2020-12-23 02:50:45,029 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:45,030 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:45,031 : INFO : built Dictionary(73 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 288 corpus positions)\n",
      "2020-12-23 02:50:45,047 : INFO : token count processed\n",
      "2020-12-23 02:50:45,049 : INFO : frequencies processed\n",
      "2020-12-23 02:50:45,190 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:45,191 : INFO : entropies processed\n",
      "2020-12-23 02:50:45,192 : INFO : extropies processed\n",
      "2020-12-23 02:50:45,193 : INFO : token count processed\n",
      "2020-12-23 02:50:45,194 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:45,195 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:45,196 : INFO : vocab #2480\n",
      "2020-12-23 02:50:45,197 : INFO : diff #set()\n",
      "2020-12-23 02:50:45,467 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:45,594 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.2797795184227556, 0.4386389086835207], [0.9811606463044882, 0.018839354], [0.0, 0.0], [2.321928094887362, 5.901812829596593, 5.9546922532397195, 2.269048671244236, 3.6327641583523573, 0.0528794236431267]]\n",
      "2020-12-23 02:50:45,597 : INFO : Removed 0 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:45,598 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:45,598 : INFO : built Dictionary(75 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 288 corpus positions)\n",
      "2020-12-23 02:50:45,608 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:50:45,611 : INFO : frequencies processed\n",
      "2020-12-23 02:50:45,739 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:45,740 : INFO : entropies processed\n",
      "2020-12-23 02:50:45,741 : INFO : extropies processed\n",
      "2020-12-23 02:50:45,743 : INFO : token count processed\n",
      "2020-12-23 02:50:45,744 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:45,746 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:45,746 : INFO : vocab #2480\n",
      "2020-12-23 02:50:45,747 : INFO : diff #set()\n",
      "2020-12-23 02:50:46,008 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:46,136 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.2771501604771454, 0.43914539205902164], [0.9780631978064775, 0.021936802], [0.0, 0.0], [2.321928094887362, 5.643202320803383, 5.7024130001936255, 2.262717415497119, 3.3804849053062633, 0.05921067939024294]]\n",
      "2020-12-23 02:50:46,138 : INFO : Removed 0 and 30 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:46,139 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:46,141 : INFO : built Dictionary(89 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 322 corpus positions)\n",
      "2020-12-23 02:50:46,168 : INFO : token count processed\n",
      "2020-12-23 02:50:46,171 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:46,172 : INFO : frequencies processed\n",
      "2020-12-23 02:50:46,174 : INFO : token count processed\n",
      "2020-12-23 02:50:46,175 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:46,176 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:46,177 : INFO : vocab #2480\n",
      "2020-12-23 02:50:46,178 : INFO : diff #set()\n",
      "2020-12-23 02:50:46,430 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:46,555 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.285606388212095, 0.43752065323121775], [0.9852244490757585, 0.014775551], [nan, nan], [2.321928094887362, 5.925214310725336, 5.981558303231146, 2.2655841023815517, 3.6596302083437835, 0.056343992505810014]]\n",
      "2020-12-23 02:50:46,558 : INFO : Removed 0 and 293 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:46,559 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:46,561 : INFO : built Dictionary(160 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 1717 corpus positions)\n",
      "2020-12-23 02:50:46,586 : INFO : token count processed\n",
      "2020-12-23 02:50:46,589 : INFO : frequencies processed\n",
      "2020-12-23 02:50:46,717 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:46,718 : INFO : entropies processed\n",
      "2020-12-23 02:50:46,718 : INFO : extropies processed\n",
      "2020-12-23 02:50:46,720 : INFO : token count processed\n",
      "2020-12-23 02:50:46,720 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:46,721 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:46,722 : INFO : vocab #2480\n",
      "2020-12-23 02:50:46,722 : INFO : diff #set()\n",
      "2020-12-23 02:50:46,980 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:47,108 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.2635260678541844, 0.44178859444194385], [0.9406172148883343, 0.059382785], [1.0, 1.0], [2.321928094887362, 6.551685682764175, 6.561180987614275, 2.3124327900372617, 4.239252892726913, 0.009495304850100084]]\n",
      "2020-12-23 02:50:47,111 : INFO : Removed 0 and 88 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:47,112 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:47,114 : INFO : built Dictionary(137 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 558 corpus positions)\n",
      "2020-12-23 02:50:47,146 : INFO : token count processed\n",
      "2020-12-23 02:50:47,148 : INFO : frequencies processed\n",
      "2020-12-23 02:50:47,279 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:47,279 : INFO : entropies processed\n",
      "2020-12-23 02:50:47,280 : INFO : extropies processed\n",
      "2020-12-23 02:50:47,281 : INFO : token count processed\n",
      "2020-12-23 02:50:47,282 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:47,284 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:47,284 : INFO : vocab #2480\n",
      "2020-12-23 02:50:47,286 : INFO : diff #set()\n",
      "2020-12-23 02:50:47,543 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:47,669 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.2608381935871344, 0.44231382981608286], [0.9382932856678963, 0.061706714], [0.0, 0.0], [2.321928094887362, 6.642985062562557, 6.6672601573508015, 2.2976530000991175, 4.34533206246344, 0.02427509478824419]]\n",
      "2020-12-23 02:50:47,672 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:47,673 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:47,674 : INFO : built Dictionary(51 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 153 corpus positions)\n",
      "2020-12-23 02:50:47,680 : INFO : token count processed\n",
      "2020-12-23 02:50:47,682 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:47,683 : INFO : frequencies processed\n",
      "2020-12-23 02:50:47,684 : INFO : token count processed\n",
      "2020-12-23 02:50:47,685 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:47,686 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:47,687 : INFO : vocab #2480\n",
      "2020-12-23 02:50:47,688 : INFO : diff #set()\n",
      "2020-12-23 02:50:47,948 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:48,077 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.284095125337535, 0.4378101371116161], [0.9887409647926688, 0.011259035], [nan, nan], [2.321928094887362, 5.2461980344571995, 5.3527365147181065, 2.2153896146264547, 3.0308084198307443, 0.10653848026090706]]\n",
      "2020-12-23 02:50:48,079 : INFO : Removed 0 and 23 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:48,080 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:48,081 : INFO : built Dictionary(73 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 213 corpus positions)\n",
      "2020-12-23 02:50:48,094 : INFO : token count processed\n",
      "2020-12-23 02:50:48,096 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:48,097 : INFO : frequencies processed\n",
      "2020-12-23 02:50:48,098 : INFO : token count processed\n",
      "2020-12-23 02:50:48,100 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:48,101 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:48,102 : INFO : vocab #2480\n",
      "2020-12-23 02:50:48,103 : INFO : diff #set()\n",
      "2020-12-23 02:50:48,368 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:48,497 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/test_auth_utility.py')[[1.2921656933965089, 0.436268635762631], [0.9859771300107241, 0.01402287], [nan, nan], [2.321928094887362, 5.903090303960449, 5.975269454648936, 2.249748944198876, 3.653341359761574, 0.07217915068848679]]\n",
      "2020-12-23 02:50:48,500 : INFO : Removed 0 and 442 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:48,501 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:48,503 : INFO : built Dictionary(105 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 1212 corpus positions)\n",
      "2020-12-23 02:50:48,529 : INFO : token count processed\n",
      "2020-12-23 02:50:48,532 : INFO : frequencies processed\n",
      "2020-12-23 02:50:48,657 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:48,658 : INFO : entropies processed\n",
      "2020-12-23 02:50:48,658 : INFO : extropies processed\n",
      "2020-12-23 02:50:48,660 : INFO : token count processed\n",
      "2020-12-23 02:50:48,661 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:48,661 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:48,662 : INFO : vocab #2480\n",
      "2020-12-23 02:50:48,663 : INFO : diff #set()\n",
      "2020-12-23 02:50:48,928 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:49,056 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.2845061108676228, 0.43773137451587474], [0.9699220787733793, 0.030077921], [1.0, 1.0], [2.321928094887362, 6.16659449033757, 6.179620419823097, 2.3089021654018342, 3.857692324935735, 0.013025929485527499]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:50:49,058 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:49,059 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:49,060 : INFO : built Dictionary(64 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 247 corpus positions)\n",
      "2020-12-23 02:50:49,069 : INFO : token count processed\n",
      "2020-12-23 02:50:49,071 : INFO : frequencies processed\n",
      "2020-12-23 02:50:49,199 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:49,200 : INFO : entropies processed\n",
      "2020-12-23 02:50:49,201 : INFO : extropies processed\n",
      "2020-12-23 02:50:49,202 : INFO : token count processed\n",
      "2020-12-23 02:50:49,203 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:49,204 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:49,205 : INFO : vocab #2480\n",
      "2020-12-23 02:50:49,206 : INFO : diff #set()\n",
      "2020-12-23 02:50:49,463 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:49,591 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.1570355547114242, 0.4635992196863828], [0.7798517048358917, 0.2201483], [1.584962500721156, 1.1699250014423124], [2.321928094887362, 5.906856253399655, 5.924236281074303, 2.304548067212715, 3.602308186186941, 0.017380027674647636]]\n",
      "2020-12-23 02:50:49,594 : INFO : Removed 0 and 68 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:49,594 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:49,595 : INFO : built Dictionary(80 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 246 corpus positions)\n",
      "2020-12-23 02:50:49,615 : INFO : token count processed\n",
      "2020-12-23 02:50:49,618 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:49,619 : INFO : frequencies processed\n",
      "2020-12-23 02:50:49,620 : INFO : token count processed\n",
      "2020-12-23 02:50:49,621 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:49,621 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:49,622 : INFO : vocab #2480\n",
      "2020-12-23 02:50:49,623 : INFO : diff #set()\n",
      "2020-12-23 02:50:49,875 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:50,002 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.287594454341276, 0.4371404197550194], [0.9963895531836897, 0.0036104468], [nan, nan], [2.321928094887362, 5.965115449163356, 6.024998450066866, 2.2620450939838523, 3.7030703551795034, 0.0598830009035094]]\n",
      "2020-12-23 02:50:50,004 : INFO : Removed 0 and 13 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:50,005 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:50,006 : INFO : built Dictionary(86 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 360 corpus positions)\n",
      "2020-12-23 02:50:50,017 : INFO : token count processed\n",
      "2020-12-23 02:50:50,020 : INFO : frequencies processed\n",
      "2020-12-23 02:50:50,159 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:50,160 : INFO : entropies processed\n",
      "2020-12-23 02:50:50,161 : INFO : extropies processed\n",
      "2020-12-23 02:50:50,163 : INFO : token count processed\n",
      "2020-12-23 02:50:50,165 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:50,166 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:50,167 : INFO : vocab #2480\n",
      "2020-12-23 02:50:50,168 : INFO : diff #set()\n",
      "2020-12-23 02:50:50,437 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:50,564 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.2646218010527335, 0.44157483582253754], [0.9491625726222992, 0.050837427], [0.0, 0.0], [2.321928094887362, 5.791362404253194, 5.836353148636253, 2.2769373505043022, 3.5144250537488912, 0.0449907443830595]]\n",
      "2020-12-23 02:50:50,567 : INFO : Removed 0 and 41 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:50,568 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:50,570 : INFO : built Dictionary(75 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 372 corpus positions)\n",
      "2020-12-23 02:50:50,586 : INFO : token count processed\n",
      "2020-12-23 02:50:50,588 : INFO : frequencies processed\n",
      "2020-12-23 02:50:50,716 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:50,717 : INFO : entropies processed\n",
      "2020-12-23 02:50:50,718 : INFO : extropies processed\n",
      "2020-12-23 02:50:50,720 : INFO : token count processed\n",
      "2020-12-23 02:50:50,722 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:50,723 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:50,723 : INFO : vocab #2480\n",
      "2020-12-23 02:50:50,725 : INFO : diff #set()\n",
      "2020-12-23 02:50:50,984 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:51,112 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.2723896388498244, 0.4400653756308062], [0.9668154083192348, 0.03318459], [1.0, 1.0], [2.321928094887362, 5.651670454631116, 5.693113936231281, 2.2804846132871965, 3.371185841343919, 0.0414434816001652]]\n",
      "2020-12-23 02:50:51,114 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:51,115 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:51,116 : INFO : built Dictionary(38 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 82 corpus positions)\n",
      "2020-12-23 02:50:51,122 : INFO : token count processed\n",
      "2020-12-23 02:50:51,124 : INFO : frequencies processed\n",
      "2020-12-23 02:50:51,252 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:51,253 : INFO : entropies processed\n",
      "2020-12-23 02:50:51,253 : INFO : extropies processed\n",
      "2020-12-23 02:50:51,255 : INFO : token count processed\n",
      "2020-12-23 02:50:51,255 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:51,256 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:51,257 : INFO : vocab #2480\n",
      "2020-12-23 02:50:51,258 : INFO : diff #set()\n",
      "2020-12-23 02:50:51,516 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:51,644 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.2568226541696064, 0.4431008338880523], [0.9421429447829723, 0.057857055], [1.0, 1.0], [2.321928094887362, 4.8226207261920235, 4.952184534620688, 2.192364286458698, 2.630256439733326, 0.12956380842866455]]\n",
      "2020-12-23 02:50:51,646 : INFO : Removed 0 and 50 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:51,647 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:51,648 : INFO : built Dictionary(83 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 252 corpus positions)\n",
      "2020-12-23 02:50:51,668 : INFO : token count processed\n",
      "2020-12-23 02:50:51,670 : INFO : frequencies processed\n",
      "2020-12-23 02:50:51,797 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:51,798 : INFO : entropies processed\n",
      "2020-12-23 02:50:51,798 : INFO : extropies processed\n",
      "2020-12-23 02:50:51,799 : INFO : token count processed\n",
      "2020-12-23 02:50:51,800 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:51,801 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:51,801 : INFO : vocab #2480\n",
      "2020-12-23 02:50:51,802 : INFO : diff #set()\n",
      "2020-12-23 02:50:52,059 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:52,186 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.2455857056750217, 0.445318117884706], [0.8837382718920708, 0.11626173], [1.0, 1.0], [2.321928094887362, 6.24862851613934, 6.286683712640045, 2.283872898386658, 3.9647556177526826, 0.03805519650070455]]\n",
      "2020-12-23 02:50:52,188 : INFO : Removed 0 and 32 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:52,189 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:52,191 : INFO : built Dictionary(85 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 368 corpus positions)\n",
      "2020-12-23 02:50:52,214 : INFO : token count processed\n",
      "2020-12-23 02:50:52,216 : INFO : frequencies processed\n",
      "2020-12-23 02:50:52,347 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:52,348 : INFO : entropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:50:52,349 : INFO : extropies processed\n",
      "2020-12-23 02:50:52,350 : INFO : token count processed\n",
      "2020-12-23 02:50:52,351 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:52,352 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:52,353 : INFO : vocab #2480\n",
      "2020-12-23 02:50:52,354 : INFO : diff #set()\n",
      "2020-12-23 02:50:52,608 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:52,736 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.2593756264633118, 0.4426001539041734], [0.9511782079935074, 0.048821792], [0.0, 0.0], [2.321928094887362, 5.850156917433494, 5.892644389588948, 2.279440622731908, 3.5707162947015862, 0.042487472155454675]]\n",
      "2020-12-23 02:50:52,738 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:52,739 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:52,740 : INFO : built Dictionary(79 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 366 corpus positions)\n",
      "2020-12-23 02:50:52,750 : INFO : token count processed\n",
      "2020-12-23 02:50:52,753 : INFO : frequencies processed\n",
      "2020-12-23 02:50:52,880 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:52,881 : INFO : entropies processed\n",
      "2020-12-23 02:50:52,882 : INFO : extropies processed\n",
      "2020-12-23 02:50:52,883 : INFO : token count processed\n",
      "2020-12-23 02:50:52,884 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:52,885 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:52,886 : INFO : vocab #2480\n",
      "2020-12-23 02:50:52,887 : INFO : diff #set()\n",
      "2020-12-23 02:50:53,146 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:53,274 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.269135547315014, 0.4406964586947059], [0.972163338214159, 0.027836662], [1.0, 1.0], [2.321928094887362, 5.6831976040360095, 5.725686875231868, 2.279438823691504, 3.4037587803445057, 0.042489271195858436]]\n",
      "2020-12-23 02:50:53,276 : INFO : Removed 0 and 52 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:53,277 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:53,279 : INFO : built Dictionary(65 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 344 corpus positions)\n",
      "2020-12-23 02:50:53,293 : INFO : token count processed\n",
      "2020-12-23 02:50:53,298 : INFO : frequencies processed\n",
      "2020-12-23 02:50:53,437 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:53,438 : INFO : entropies processed\n",
      "2020-12-23 02:50:53,438 : INFO : extropies processed\n",
      "2020-12-23 02:50:53,440 : INFO : token count processed\n",
      "2020-12-23 02:50:53,441 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:53,442 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:53,443 : INFO : vocab #2480\n",
      "2020-12-23 02:50:53,444 : INFO : diff #set()\n",
      "2020-12-23 02:50:53,712 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:53,840 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.176944520975769, 0.45935943261970286], [0.8045304417610168, 0.19546956], [1.584962500721156, 1.1699250014423124], [2.321928094887362, 5.749308601266266, 5.771823211715683, 2.2994134844379452, 3.4498951168283205, 0.022514610449416494]]\n",
      "2020-12-23 02:50:53,842 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:53,843 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:53,844 : INFO : built Dictionary(61 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 225 corpus positions)\n",
      "2020-12-23 02:50:53,852 : INFO : token count processed\n",
      "2020-12-23 02:50:53,854 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:53,855 : INFO : frequencies processed\n",
      "2020-12-23 02:50:53,856 : INFO : token count processed\n",
      "2020-12-23 02:50:53,857 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:53,858 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:53,859 : INFO : vocab #2480\n",
      "2020-12-23 02:50:53,860 : INFO : diff #set()\n",
      "2020-12-23 02:50:54,119 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:54,247 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.282669479379591, 0.4380835723408327], [0.9892464140430093, 0.010753586], [nan, nan], [2.321928094887362, 5.015422548793484, 5.109037005149055, 2.228313638531791, 2.787108910261693, 0.09361445635557164]]\n",
      "2020-12-23 02:50:54,250 : INFO : Removed 0 and 81 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:54,251 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:54,252 : INFO : built Dictionary(89 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 351 corpus positions)\n",
      "2020-12-23 02:50:54,269 : INFO : token count processed\n",
      "2020-12-23 02:50:54,272 : INFO : frequencies processed\n",
      "2020-12-23 02:50:54,403 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:54,404 : INFO : entropies processed\n",
      "2020-12-23 02:50:54,405 : INFO : extropies processed\n",
      "2020-12-23 02:50:54,407 : INFO : token count processed\n",
      "2020-12-23 02:50:54,408 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:54,409 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:54,410 : INFO : vocab #2480\n",
      "2020-12-23 02:50:54,411 : INFO : diff #set()\n",
      "2020-12-23 02:50:54,672 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:54,800 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.2536121099049893, 0.44373208486271365], [0.9394052140414715, 0.060594786], [0.0, 0.0], [2.321928094887362, 6.030001281822029, 6.068074161733978, 2.2838552149754126, 3.746146066846616, 0.03807287991194919]]\n",
      "2020-12-23 02:50:54,803 : INFO : Removed 0 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:54,803 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:54,804 : INFO : built Dictionary(75 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 314 corpus positions)\n",
      "2020-12-23 02:50:54,815 : INFO : token count processed\n",
      "2020-12-23 02:50:54,817 : INFO : frequencies processed\n",
      "2020-12-23 02:50:54,956 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:54,957 : INFO : entropies processed\n",
      "2020-12-23 02:50:54,957 : INFO : extropies processed\n",
      "2020-12-23 02:50:54,959 : INFO : token count processed\n",
      "2020-12-23 02:50:54,960 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:54,961 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:54,962 : INFO : vocab #2480\n",
      "2020-12-23 02:50:54,963 : INFO : diff #set()\n",
      "2020-12-23 02:50:55,230 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:55,358 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.2550299549852066, 0.4434530892990111], [0.9255143105983734, 0.07448569], [1.584962500721156, 1.1699250014423124], [2.321928094887362, 5.9537092545441395, 5.983275773183417, 2.2923615762480853, 3.6613476782960546, 0.029566518639277284]]\n",
      "2020-12-23 02:50:55,360 : INFO : Removed 0 and 90 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:55,361 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:55,362 : INFO : built Dictionary(89 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 315 corpus positions)\n",
      "2020-12-23 02:50:55,373 : INFO : token count processed\n",
      "2020-12-23 02:50:55,375 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:50:55,376 : INFO : frequencies processed\n",
      "2020-12-23 02:50:55,377 : INFO : token count processed\n",
      "2020-12-23 02:50:55,378 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:55,379 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:55,380 : INFO : vocab #2480\n",
      "2020-12-23 02:50:55,381 : INFO : diff #set()\n",
      "2020-12-23 02:50:55,638 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:55,765 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.2872059319717608, 0.4372146757847542], [0.9925747052766383, 0.0074252947], [nan, nan], [2.321928094887362, 6.184756445474906, 6.233037609574635, 2.273646930787634, 3.9111095146872725, 0.04828116409972871]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:50:55,768 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:55,768 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:55,769 : INFO : built Dictionary(97 unique tokens: ['binari', 'debug', 'merg', 'product', 'scan']...) from 2 documents (total 428 corpus positions)\n",
      "2020-12-23 02:50:55,789 : INFO : token count processed\n",
      "2020-12-23 02:50:55,794 : INFO : frequencies processed\n",
      "2020-12-23 02:50:55,920 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:55,921 : INFO : entropies processed\n",
      "2020-12-23 02:50:55,922 : INFO : extropies processed\n",
      "2020-12-23 02:50:55,923 : INFO : token count processed\n",
      "2020-12-23 02:50:55,924 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:55,924 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:55,925 : INFO : vocab #2480\n",
      "2020-12-23 02:50:55,926 : INFO : diff #set()\n",
      "2020-12-23 02:50:56,192 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:56,320 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.2706476433186276, 0.4404029849996746], [0.9501164257526398, 0.049883574], [0.0, 0.0], [2.321928094887362, 6.212221456585881, 6.246951127909133, 2.28719842356411, 3.925023033021771, 0.03472967132325255]]\n",
      "2020-12-23 02:50:56,323 : INFO : Removed 2 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:56,324 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:56,325 : INFO : built Dictionary(131 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 429 corpus positions)\n",
      "2020-12-23 02:50:56,372 : INFO : token count processed\n",
      "2020-12-23 02:50:56,374 : INFO : frequencies processed\n",
      "2020-12-23 02:50:56,502 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:56,502 : INFO : entropies processed\n",
      "2020-12-23 02:50:56,503 : INFO : extropies processed\n",
      "2020-12-23 02:50:56,505 : INFO : token count processed\n",
      "2020-12-23 02:50:56,505 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:56,506 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:56,507 : INFO : vocab #2480\n",
      "2020-12-23 02:50:56,509 : INFO : diff #set()\n",
      "2020-12-23 02:50:56,769 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:56,897 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.1820051598694803, 0.45829405832377423], [0.8245804607868195, 0.17541954], [1.0, 1.0], [4.303508854797678, 6.301552355933639, 6.479907122388317, 4.125154088343001, 2.1763982675906393, 0.17835476645467807]]\n",
      "2020-12-23 02:50:56,899 : INFO : Removed 2 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:56,900 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:56,903 : INFO : built Dictionary(164 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 665 corpus positions)\n",
      "2020-12-23 02:50:56,972 : INFO : token count processed\n",
      "2020-12-23 02:50:56,974 : INFO : frequencies processed\n",
      "2020-12-23 02:50:57,101 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:57,102 : INFO : entropies processed\n",
      "2020-12-23 02:50:57,103 : INFO : extropies processed\n",
      "2020-12-23 02:50:57,105 : INFO : token count processed\n",
      "2020-12-23 02:50:57,106 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:57,107 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:57,108 : INFO : vocab #2480\n",
      "2020-12-23 02:50:57,110 : INFO : diff #set()\n",
      "2020-12-23 02:50:57,380 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:57,510 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.1415572266652407, 0.46694993136240653], [0.7653630077838898, 0.23463699], [2.94770277922009, 1.3393100707180505], [4.303508854797678, 6.739005504021667, 6.827710642325637, 4.214803716493708, 2.524201787527959, 0.08870513830397009]]\n",
      "2020-12-23 02:50:57,513 : INFO : Removed 2 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:57,514 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:57,516 : INFO : built Dictionary(111 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 516 corpus positions)\n",
      "2020-12-23 02:50:57,563 : INFO : token count processed\n",
      "2020-12-23 02:50:57,567 : INFO : frequencies processed\n",
      "2020-12-23 02:50:57,698 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:57,699 : INFO : entropies processed\n",
      "2020-12-23 02:50:57,699 : INFO : extropies processed\n",
      "2020-12-23 02:50:57,700 : INFO : token count processed\n",
      "2020-12-23 02:50:57,701 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:57,702 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:57,702 : INFO : vocab #2480\n",
      "2020-12-23 02:50:57,703 : INFO : diff #set()\n",
      "2020-12-23 02:50:57,962 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:58,089 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2228918847136576, 0.44986443419798405], [0.8984519839286804, 0.101548016], [2.2516291673878226, 1.2667563532600834], [4.303508854797678, 5.870833373337847, 6.020745512004227, 4.1535967161312985, 1.7172366572065494, 0.1499121386663802]]\n",
      "2020-12-23 02:50:58,092 : INFO : Removed 2 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:58,093 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:58,094 : INFO : built Dictionary(72 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 199 corpus positions)\n",
      "2020-12-23 02:50:58,121 : INFO : token count processed\n",
      "2020-12-23 02:50:58,124 : INFO : frequencies processed\n",
      "2020-12-23 02:50:58,252 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:58,253 : INFO : entropies processed\n",
      "2020-12-23 02:50:58,254 : INFO : extropies processed\n",
      "2020-12-23 02:50:58,256 : INFO : token count processed\n",
      "2020-12-23 02:50:58,257 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:58,258 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:58,259 : INFO : vocab #2480\n",
      "2020-12-23 02:50:58,260 : INFO : diff #set()\n",
      "2020-12-23 02:50:58,517 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:58,644 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.1481794263238998, 0.4655104633002017], [0.7834460884332657, 0.21655391], [1.584962500721156, 1.1699250014423124], [4.303508854797678, 5.371881234145534, 5.714245452146024, 3.9611446367971883, 1.4107365973483459, 0.34236421800048955]]\n",
      "2020-12-23 02:50:58,647 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:58,648 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:58,649 : INFO : built Dictionary(63 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 151 corpus positions)\n",
      "2020-12-23 02:50:58,672 : INFO : token count processed\n",
      "2020-12-23 02:50:58,674 : INFO : frequencies processed\n",
      "2020-12-23 02:50:58,801 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:58,802 : INFO : entropies processed\n",
      "2020-12-23 02:50:58,803 : INFO : extropies processed\n",
      "2020-12-23 02:50:58,804 : INFO : token count processed\n",
      "2020-12-23 02:50:58,805 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:58,806 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:58,807 : INFO : vocab #2480\n",
      "2020-12-23 02:50:58,808 : INFO : diff #set()\n",
      "2020-12-23 02:50:59,064 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:59,193 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.1708493879190829, 0.4606491843999241], [0.8522374033927917, 0.1477626], [1.584962500721156, 1.1699250014423124], [4.303508854797678, 4.85108279267097, 5.335705578842715, 3.8188860686259325, 1.0321967240450372, 0.48462278617174537]]\n",
      "2020-12-23 02:50:59,195 : INFO : Removed 2 and 127 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:59,196 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:59,197 : INFO : built Dictionary(102 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 424 corpus positions)\n",
      "2020-12-23 02:50:59,234 : INFO : token count processed\n",
      "2020-12-23 02:50:59,236 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:50:59,367 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:59,368 : INFO : entropies processed\n",
      "2020-12-23 02:50:59,369 : INFO : extropies processed\n",
      "2020-12-23 02:50:59,370 : INFO : token count processed\n",
      "2020-12-23 02:50:59,371 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:59,372 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:59,373 : INFO : vocab #2480\n",
      "2020-12-23 02:50:59,374 : INFO : diff #set()\n",
      "2020-12-23 02:50:59,644 : INFO : alphabet #2480\n",
      "2020-12-23 02:50:59,771 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.1561936948771505, 0.4637802264128108], [0.8643490970134735, 0.1356509], [2.321928094887362, 1.2877123795494492], [4.303508854797678, 6.139571208108155, 6.286864498435325, 4.156215564470507, 1.9833556436376467, 0.14729329032716976]]\n",
      "2020-12-23 02:50:59,774 : INFO : Removed 2 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:50:59,775 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:50:59,776 : INFO : built Dictionary(88 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 434 corpus positions)\n",
      "2020-12-23 02:50:59,813 : INFO : token count processed\n",
      "2020-12-23 02:50:59,816 : INFO : frequencies processed\n",
      "2020-12-23 02:50:59,948 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:50:59,949 : INFO : entropies processed\n",
      "2020-12-23 02:50:59,950 : INFO : extropies processed\n",
      "2020-12-23 02:50:59,951 : INFO : token count processed\n",
      "2020-12-23 02:50:59,952 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:50:59,953 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:50:59,954 : INFO : vocab #2480\n",
      "2020-12-23 02:50:59,955 : INFO : diff #set()\n",
      "2020-12-23 02:51:00,214 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:00,341 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.132002612195967, 0.4690425772837108], [0.7215478718280792, 0.27845213], [2.321928094887362, 1.2877123795494492], [4.303508854797678, 5.609710627339259, 5.803940966785178, 4.10927851535176, 1.5004321119875002, 0.1942303394459186]]\n",
      "2020-12-23 02:51:00,344 : INFO : Removed 2 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:00,345 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:00,347 : INFO : built Dictionary(176 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 1094 corpus positions)\n",
      "2020-12-23 02:51:00,423 : INFO : token count processed\n",
      "2020-12-23 02:51:00,429 : INFO : frequencies processed\n",
      "2020-12-23 02:51:00,557 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:00,558 : INFO : entropies processed\n",
      "2020-12-23 02:51:00,558 : INFO : extropies processed\n",
      "2020-12-23 02:51:00,560 : INFO : token count processed\n",
      "2020-12-23 02:51:00,561 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:00,562 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:00,562 : INFO : vocab #2480\n",
      "2020-12-23 02:51:00,563 : INFO : diff #set()\n",
      "2020-12-23 02:51:00,834 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:00,963 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.1498236738113448, 0.4651544273987531], [0.770351454615593, 0.22964855], [3.0220552088742, 1.3359632893587228], [4.303508854797678, 7.2441902753576075, 7.289767829985605, 4.25793130016968, 2.986258975187927, 0.04557755462799751]]\n",
      "2020-12-23 02:51:00,966 : INFO : Removed 2 and 136 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:00,966 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:00,969 : INFO : built Dictionary(139 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 701 corpus positions)\n",
      "2020-12-23 02:51:01,029 : INFO : token count processed\n",
      "2020-12-23 02:51:01,032 : INFO : frequencies processed\n",
      "2020-12-23 02:51:01,157 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:01,157 : INFO : entropies processed\n",
      "2020-12-23 02:51:01,158 : INFO : extropies processed\n",
      "2020-12-23 02:51:01,159 : INFO : token count processed\n",
      "2020-12-23 02:51:01,160 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:01,161 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:01,162 : INFO : vocab #2480\n",
      "2020-12-23 02:51:01,163 : INFO : diff #set()\n",
      "2020-12-23 02:51:01,423 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:01,550 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.0980817287651539, 0.47662585603305346], [0.6879125237464905, 0.31208748], [2.94770277922009, 1.3393100707180505], [4.303508854797678, 6.2567074920449475, 6.349697487901286, 4.21051885894134, 2.0461886331036085, 0.09298999585633894]]\n",
      "2020-12-23 02:51:01,553 : INFO : Removed 2 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:01,554 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:01,554 : INFO : built Dictionary(88 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 211 corpus positions)\n",
      "2020-12-23 02:51:01,582 : INFO : token count processed\n",
      "2020-12-23 02:51:01,584 : INFO : frequencies processed\n",
      "2020-12-23 02:51:01,711 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:01,712 : INFO : entropies processed\n",
      "2020-12-23 02:51:01,712 : INFO : extropies processed\n",
      "2020-12-23 02:51:01,714 : INFO : token count processed\n",
      "2020-12-23 02:51:01,714 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:01,715 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:01,716 : INFO : vocab #2480\n",
      "2020-12-23 02:51:01,717 : INFO : diff #set()\n",
      "2020-12-23 02:51:01,976 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:02,103 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.1510788912107124, 0.46488299619599743], [0.802095353603363, 0.19790465], [1.9219280948873623, 1.2148067842293933], [4.303508854797678, 5.7680018917339435, 6.044054684069096, 4.027456062462527, 1.7405458292714178, 0.2760527923351521]]\n",
      "2020-12-23 02:51:02,106 : INFO : Removed 2 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:02,107 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:02,109 : INFO : built Dictionary(180 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 752 corpus positions)\n",
      "2020-12-23 02:51:02,184 : INFO : token count processed\n",
      "2020-12-23 02:51:02,186 : INFO : frequencies processed\n",
      "2020-12-23 02:51:02,316 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:02,317 : INFO : entropies processed\n",
      "2020-12-23 02:51:02,318 : INFO : extropies processed\n",
      "2020-12-23 02:51:02,319 : INFO : token count processed\n",
      "2020-12-23 02:51:02,320 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:02,321 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:02,322 : INFO : vocab #2480\n",
      "2020-12-23 02:51:02,323 : INFO : diff #set()\n",
      "2020-12-23 02:51:02,586 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:02,713 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.0944980732818879, 0.47744135588202813], [0.6951919496059418, 0.30480805], [3.2516291673878226, 1.3589504783379556], [4.303508854797678, 6.846479111193757, 6.917627329850854, 4.232360636140582, 2.614118475053176, 0.07114821865709686]]\n",
      "2020-12-23 02:51:02,716 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:02,717 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:02,718 : INFO : built Dictionary(38 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 55 corpus positions)\n",
      "2020-12-23 02:51:02,733 : INFO : token count processed\n",
      "2020-12-23 02:51:02,735 : INFO : frequencies processed\n",
      "2020-12-23 02:51:02,863 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:02,864 : INFO : entropies processed\n",
      "2020-12-23 02:51:02,865 : INFO : extropies processed\n",
      "2020-12-23 02:51:02,866 : INFO : token count processed\n",
      "2020-12-23 02:51:02,867 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:02,868 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:02,869 : INFO : vocab #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:51:02,870 : INFO : diff #set()\n",
      "2020-12-23 02:51:03,128 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:03,254 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/fireException.py')[[1.1749081345941566, 0.45978953505850145], [0.8578294217586517, 0.14217058], [0.0, 0.0], [4.303508854797678, 4.165013816065912, 5.170180584150402, 3.2983420867131867, 0.8666717293527242, 1.0051667680844902]]\n",
      "2020-12-23 02:51:03,257 : INFO : Removed 2 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:03,258 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:03,259 : INFO : built Dictionary(64 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 162 corpus positions)\n",
      "2020-12-23 02:51:03,283 : INFO : token count processed\n",
      "2020-12-23 02:51:03,286 : INFO : frequencies processed\n",
      "2020-12-23 02:51:03,416 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:03,417 : INFO : entropies processed\n",
      "2020-12-23 02:51:03,418 : INFO : extropies processed\n",
      "2020-12-23 02:51:03,420 : INFO : token count processed\n",
      "2020-12-23 02:51:03,421 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:03,422 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:03,423 : INFO : vocab #2480\n",
      "2020-12-23 02:51:03,424 : INFO : diff #set()\n",
      "2020-12-23 02:51:03,689 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:03,816 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.138042285284783, 0.4677175970197437], [0.7465085089206696, 0.2534915], [2.0, 1.2451124978365313], [4.303508854797678, 5.449968864419248, 5.809928713775723, 3.943549005441203, 1.5064198589780453, 0.35995984935647485]]\n",
      "2020-12-23 02:51:03,818 : INFO : Removed 2 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:03,819 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:03,822 : INFO : built Dictionary(153 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 599 corpus positions)\n",
      "2020-12-23 02:51:03,887 : INFO : token count processed\n",
      "2020-12-23 02:51:03,890 : INFO : frequencies processed\n",
      "2020-12-23 02:51:04,016 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:04,017 : INFO : entropies processed\n",
      "2020-12-23 02:51:04,018 : INFO : extropies processed\n",
      "2020-12-23 02:51:04,019 : INFO : token count processed\n",
      "2020-12-23 02:51:04,020 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:04,021 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:04,022 : INFO : vocab #2480\n",
      "2020-12-23 02:51:04,023 : INFO : diff #set()\n",
      "2020-12-23 02:51:04,280 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:04,409 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.1551782047877126, 0.46399875322537476], [0.7977451235055923, 0.20225488], [2.8453509366224363, 1.321020357168122], [4.303508854797678, 6.530294129310484, 6.617354116424717, 4.216448867683444, 2.313845261627039, 0.08705998711423302]]\n",
      "2020-12-23 02:51:04,411 : INFO : Removed 2 and 142 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:04,412 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:04,413 : INFO : built Dictionary(128 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 615 corpus positions)\n",
      "2020-12-23 02:51:04,460 : INFO : token count processed\n",
      "2020-12-23 02:51:04,463 : INFO : frequencies processed\n",
      "2020-12-23 02:51:04,596 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:04,597 : INFO : entropies processed\n",
      "2020-12-23 02:51:04,598 : INFO : extropies processed\n",
      "2020-12-23 02:51:04,599 : INFO : token count processed\n",
      "2020-12-23 02:51:04,600 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:04,601 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:04,602 : INFO : vocab #2480\n",
      "2020-12-23 02:51:04,603 : INFO : diff #set()\n",
      "2020-12-23 02:51:04,863 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:04,990 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.20269276232459, 0.4539897788308252], [0.8737199753522873, 0.12628002], [2.5216406363433186, 1.2998438251349493], [4.303508854797678, 6.470272233491701, 6.583132466610852, 4.190648621678527, 2.2796236118131743, 0.11286023311915105]]\n",
      "2020-12-23 02:51:04,993 : INFO : Removed 2 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:04,994 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:04,995 : INFO : built Dictionary(128 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 404 corpus positions)\n",
      "2020-12-23 02:51:05,045 : INFO : token count processed\n",
      "2020-12-23 02:51:05,050 : INFO : frequencies processed\n",
      "2020-12-23 02:51:05,184 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:05,184 : INFO : entropies processed\n",
      "2020-12-23 02:51:05,186 : INFO : extropies processed\n",
      "2020-12-23 02:51:05,187 : INFO : token count processed\n",
      "2020-12-23 02:51:05,187 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:05,188 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:05,188 : INFO : vocab #2480\n",
      "2020-12-23 02:51:05,189 : INFO : diff #set()\n",
      "2020-12-23 02:51:05,447 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:05,574 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.1531420296492298, 0.4644375457957648], [0.7604081183671951, 0.23959188], [2.75, 1.3226647836567114], [4.303508854797678, 6.550038223589686, 6.679090085975792, 4.174456992411573, 2.3755812311781144, 0.1290518623861061]]\n",
      "2020-12-23 02:51:05,577 : INFO : Removed 2 and 70 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:05,578 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:05,579 : INFO : built Dictionary(86 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 320 corpus positions)\n",
      "2020-12-23 02:51:05,613 : INFO : token count processed\n",
      "2020-12-23 02:51:05,615 : INFO : frequencies processed\n",
      "2020-12-23 02:51:05,743 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:05,744 : INFO : entropies processed\n",
      "2020-12-23 02:51:05,745 : INFO : extropies processed\n",
      "2020-12-23 02:51:05,746 : INFO : token count processed\n",
      "2020-12-23 02:51:05,747 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:05,748 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:05,749 : INFO : vocab #2480\n",
      "2020-12-23 02:51:05,751 : INFO : diff #set()\n",
      "2020-12-23 02:51:06,013 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:06,140 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.1722922100774655, 0.4603432242498993], [0.8470669090747833, 0.15293309], [2.0, 1.2451124978365313], [4.303508854797678, 5.860525481261383, 6.069052810302242, 4.094981525756818, 1.7655439555045644, 0.20852732904085958]]\n",
      "2020-12-23 02:51:06,143 : INFO : Removed 2 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:06,144 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:06,145 : INFO : built Dictionary(59 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 358 corpus positions)\n",
      "2020-12-23 02:51:06,169 : INFO : token count processed\n",
      "2020-12-23 02:51:06,171 : INFO : frequencies processed\n",
      "2020-12-23 02:51:06,299 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:06,300 : INFO : entropies processed\n",
      "2020-12-23 02:51:06,301 : INFO : extropies processed\n",
      "2020-12-23 02:51:06,303 : INFO : token count processed\n",
      "2020-12-23 02:51:06,304 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:06,305 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:06,306 : INFO : vocab #2480\n",
      "2020-12-23 02:51:06,307 : INFO : diff #set()\n",
      "2020-12-23 02:51:06,563 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:06,691 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.2290618628582117, 0.4486192225808176], [0.8886451795697212, 0.11135482], [0.0, 0.0], [4.303508854797678, 5.945464049777852, 6.124617338007003, 4.1243555665685285, 1.8211084832093247, 0.17915328822915022]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:51:06,694 : INFO : Removed 2 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:06,695 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:06,696 : INFO : built Dictionary(201 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 905 corpus positions)\n",
      "2020-12-23 02:51:06,777 : INFO : token count processed\n",
      "2020-12-23 02:51:06,779 : INFO : frequencies processed\n",
      "2020-12-23 02:51:06,907 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:06,908 : INFO : entropies processed\n",
      "2020-12-23 02:51:06,909 : INFO : extropies processed\n",
      "2020-12-23 02:51:06,911 : INFO : token count processed\n",
      "2020-12-23 02:51:06,912 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:06,913 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:06,914 : INFO : vocab #2480\n",
      "2020-12-23 02:51:06,915 : INFO : diff #set()\n",
      "2020-12-23 02:51:07,175 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:07,302 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.1049327122970953, 0.4750745684923621], [0.6744969189167023, 0.32550308], [2.8464393446710154, 1.3178207096846455], [4.303508854797678, 6.811563897304216, 6.877899557060594, 4.237173195041301, 2.5743907022629164, 0.06633565975637801]]\n",
      "2020-12-23 02:51:07,305 : INFO : Removed 2 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:07,306 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:07,307 : INFO : built Dictionary(220 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 1000 corpus positions)\n",
      "2020-12-23 02:51:07,405 : INFO : token count processed\n",
      "2020-12-23 02:51:07,408 : INFO : frequencies processed\n",
      "2020-12-23 02:51:07,538 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:07,538 : INFO : entropies processed\n",
      "2020-12-23 02:51:07,539 : INFO : extropies processed\n",
      "2020-12-23 02:51:07,541 : INFO : token count processed\n",
      "2020-12-23 02:51:07,543 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:07,544 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:07,545 : INFO : vocab #2480\n",
      "2020-12-23 02:51:07,546 : INFO : diff #set()\n",
      "2020-12-23 02:51:07,815 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:07,943 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.1116056296732597, 0.4735732780532203], [0.6603016555309296, 0.33969834], [3.180832987205441, 1.3478475537994532], [4.303508854797678, 7.502034948968415, 7.5392144702928245, 4.2663293334732675, 3.2357056154951467, 0.03717952132440949]]\n",
      "2020-12-23 02:51:07,946 : INFO : Removed 2 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:07,947 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:07,948 : INFO : built Dictionary(263 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 1573 corpus positions)\n",
      "2020-12-23 02:51:08,082 : INFO : token count processed\n",
      "2020-12-23 02:51:08,084 : INFO : frequencies processed\n",
      "2020-12-23 02:51:08,211 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:08,212 : INFO : entropies processed\n",
      "2020-12-23 02:51:08,213 : INFO : extropies processed\n",
      "2020-12-23 02:51:08,214 : INFO : token count processed\n",
      "2020-12-23 02:51:08,215 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:08,216 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:08,217 : INFO : vocab #2480\n",
      "2020-12-23 02:51:08,218 : INFO : diff #set()\n",
      "2020-12-23 02:51:08,479 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:08,605 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.1045320927277706, 0.4751650038768755], [0.6460598409175873, 0.35394016], [3.3248629576173565, 1.3574960179923319], [4.303508854797678, 7.39180093901977, 7.421342082264823, 4.273967711552625, 3.1178332274671448, 0.029541143245052304]]\n",
      "2020-12-23 02:51:08,608 : INFO : Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:08,609 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:08,609 : INFO : built Dictionary(55 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 128 corpus positions)\n",
      "2020-12-23 02:51:08,625 : INFO : token count processed\n",
      "2020-12-23 02:51:08,627 : INFO : frequencies processed\n",
      "2020-12-23 02:51:08,755 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:08,755 : INFO : entropies processed\n",
      "2020-12-23 02:51:08,756 : INFO : extropies processed\n",
      "2020-12-23 02:51:08,757 : INFO : token count processed\n",
      "2020-12-23 02:51:08,758 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:08,759 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:08,760 : INFO : vocab #2480\n",
      "2020-12-23 02:51:08,761 : INFO : diff #set()\n",
      "2020-12-23 02:51:09,019 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:09,147 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.1244412499988203, 0.4707120048627164], [0.7341150939464569, 0.2658849], [1.3709505944546687, 1.0438561897747245], [4.303508854797678, 4.927561309677364, 5.409988147282221, 3.821082017192821, 1.1064792924845435, 0.4824268376048577]]\n",
      "2020-12-23 02:51:09,150 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:09,151 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:09,151 : INFO : built Dictionary(24 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 29 corpus positions)\n",
      "2020-12-23 02:51:09,156 : INFO : token count processed\n",
      "2020-12-23 02:51:09,158 : INFO : frequencies processed\n",
      "2020-12-23 02:51:09,293 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:09,294 : INFO : entropies processed\n",
      "2020-12-23 02:51:09,295 : INFO : extropies processed\n",
      "2020-12-23 02:51:09,297 : INFO : token count processed\n",
      "2020-12-23 02:51:09,299 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:09,300 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:09,301 : INFO : vocab #2480\n",
      "2020-12-23 02:51:09,303 : INFO : diff #set()\n",
      "2020-12-23 02:51:09,576 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:09,709 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.1335728403153613, 0.46869737986174914], [0.5997955799102783, 0.40020442], [0.0, 0.0], [4.303508854797678, 2.5216406363433186, 4.582913245731168, 2.2422362454098277, 0.27940439093349045, 2.0612726093878497]]\n",
      "2020-12-23 02:51:09,713 : INFO : Removed 2 and 530 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:09,714 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:09,716 : INFO : built Dictionary(336 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 2901 corpus positions)\n",
      "2020-12-23 02:51:09,903 : INFO : token count processed\n",
      "2020-12-23 02:51:09,906 : INFO : frequencies processed\n",
      "2020-12-23 02:51:10,034 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:10,034 : INFO : entropies processed\n",
      "2020-12-23 02:51:10,035 : INFO : extropies processed\n",
      "2020-12-23 02:51:10,037 : INFO : token count processed\n",
      "2020-12-23 02:51:10,038 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:10,039 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:10,039 : INFO : vocab #2480\n",
      "2020-12-23 02:51:10,040 : INFO : diff #set()\n",
      "2020-12-23 02:51:10,299 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:10,428 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.142022716018991, 0.46684845707823674], [0.7410262227058411, 0.25897378], [3.4565647621309536, 1.3654661895235272], [4.303508854797678, 7.480007711014331, 7.5016938393640435, 4.281822726447964, 3.1981849845663657, 0.021686128349712952]]\n",
      "2020-12-23 02:51:10,431 : INFO : Removed 2 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:10,432 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:10,434 : INFO : built Dictionary(214 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 1050 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:51:10,527 : INFO : token count processed\n",
      "2020-12-23 02:51:10,535 : INFO : frequencies processed\n",
      "2020-12-23 02:51:10,664 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:10,665 : INFO : entropies processed\n",
      "2020-12-23 02:51:10,665 : INFO : extropies processed\n",
      "2020-12-23 02:51:10,667 : INFO : token count processed\n",
      "2020-12-23 02:51:10,668 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:10,669 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:10,670 : INFO : vocab #2480\n",
      "2020-12-23 02:51:10,671 : INFO : diff #set()\n",
      "2020-12-23 02:51:10,945 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:11,075 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.1367216108157998, 0.4680066860081975], [0.7462237775325775, 0.25377622], [3.3927474104487847, 1.3672090515720436], [4.303508854797678, 7.131331012509435, 7.182311673618219, 4.252528193688892, 2.8788028188205415, 0.05098066110878463]]\n",
      "2020-12-23 02:51:11,078 : INFO : Removed 2 and 166 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:11,079 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:11,081 : INFO : built Dictionary(204 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 869 corpus positions)\n",
      "2020-12-23 02:51:11,170 : INFO : token count processed\n",
      "2020-12-23 02:51:11,173 : INFO : frequencies processed\n",
      "2020-12-23 02:51:11,300 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:11,301 : INFO : entropies processed\n",
      "2020-12-23 02:51:11,301 : INFO : extropies processed\n",
      "2020-12-23 02:51:11,303 : INFO : token count processed\n",
      "2020-12-23 02:51:11,303 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:11,304 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:11,305 : INFO : vocab #2480\n",
      "2020-12-23 02:51:11,306 : INFO : diff #set()\n",
      "2020-12-23 02:51:11,563 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:11,691 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.1332610523880788, 0.46876588258176377], [0.7285430729389191, 0.27145693], [3.2516291673878226, 1.3589504783379556], [4.303508854797678, 7.203742744794778, 7.25782218859424, 4.249429410998216, 2.954313333796562, 0.054079443799461835]]\n",
      "2020-12-23 02:51:11,694 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:11,695 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:11,696 : INFO : built Dictionary(68 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 201 corpus positions)\n",
      "2020-12-23 02:51:11,722 : INFO : token count processed\n",
      "2020-12-23 02:51:11,725 : INFO : frequencies processed\n",
      "2020-12-23 02:51:11,852 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:11,853 : INFO : entropies processed\n",
      "2020-12-23 02:51:11,854 : INFO : extropies processed\n",
      "2020-12-23 02:51:11,855 : INFO : token count processed\n",
      "2020-12-23 02:51:11,856 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:11,857 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:11,857 : INFO : vocab #2480\n",
      "2020-12-23 02:51:11,858 : INFO : diff #set()\n",
      "2020-12-23 02:51:12,117 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:12,245 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.154679413839926, 0.4641061652034197], [0.7526417821645737, 0.24735822], [1.7924812503605778, 1.1575860145844845], [4.303508854797678, 5.195502554608948, 5.549902881362996, 3.949108528043631, 1.2463940265653184, 0.35440032675404787]]\n",
      "2020-12-23 02:51:12,248 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:12,249 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:12,250 : INFO : built Dictionary(74 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 262 corpus positions)\n",
      "2020-12-23 02:51:12,279 : INFO : token count processed\n",
      "2020-12-23 02:51:12,281 : INFO : frequencies processed\n",
      "2020-12-23 02:51:12,413 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:12,414 : INFO : entropies processed\n",
      "2020-12-23 02:51:12,415 : INFO : extropies processed\n",
      "2020-12-23 02:51:12,416 : INFO : token count processed\n",
      "2020-12-23 02:51:12,417 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:12,418 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:12,419 : INFO : vocab #2480\n",
      "2020-12-23 02:51:12,420 : INFO : diff #set()\n",
      "2020-12-23 02:51:12,688 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:12,817 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.1033215496773998, 0.4754384797480806], [0.7277411818504333, 0.27225882], [2.0, 1.2451124978365313], [4.303508854797678, 5.32027245610305, 5.616792313006094, 4.006988997894634, 1.3132834582084163, 0.2965198569030445]]\n",
      "2020-12-23 02:51:12,819 : INFO : Removed 2 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:12,820 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:12,822 : INFO : built Dictionary(167 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 502 corpus positions)\n",
      "2020-12-23 02:51:12,900 : INFO : token count processed\n",
      "2020-12-23 02:51:12,902 : INFO : frequencies processed\n",
      "2020-12-23 02:51:13,030 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:13,031 : INFO : entropies processed\n",
      "2020-12-23 02:51:13,032 : INFO : extropies processed\n",
      "2020-12-23 02:51:13,033 : INFO : token count processed\n",
      "2020-12-23 02:51:13,034 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:13,035 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:13,036 : INFO : vocab #2480\n",
      "2020-12-23 02:51:13,037 : INFO : diff #set()\n",
      "2020-12-23 02:51:13,294 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:13,421 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.1302975383990963, 0.4694179953620436], [0.7002322375774384, 0.29976776], [3.3927474104487847, 1.3672090515720436], [4.303508854797678, 6.898202761357263, 6.978507183737776, 4.223204432417166, 2.674998328940098, 0.08030442238051272]]\n",
      "2020-12-23 02:51:13,423 : INFO : Removed 2 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:13,424 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:13,425 : INFO : built Dictionary(138 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 524 corpus positions)\n",
      "2020-12-23 02:51:13,481 : INFO : token count processed\n",
      "2020-12-23 02:51:13,483 : INFO : frequencies processed\n",
      "2020-12-23 02:51:13,611 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:13,612 : INFO : entropies processed\n",
      "2020-12-23 02:51:13,612 : INFO : extropies processed\n",
      "2020-12-23 02:51:13,613 : INFO : token count processed\n",
      "2020-12-23 02:51:13,614 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:13,615 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:13,615 : INFO : vocab #2480\n",
      "2020-12-23 02:51:13,616 : INFO : diff #set()\n",
      "2020-12-23 02:51:13,873 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:14,001 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.1684230882549802, 0.461164615621549], [0.8268292099237442, 0.17317079], [2.0, 1.2451124978365313], [4.303508854797678, 6.388500481644799, 6.536391315387306, 4.155618021055172, 2.232882460589628, 0.14789083374250644]]\n",
      "2020-12-23 02:51:14,003 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:14,004 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:14,005 : INFO : built Dictionary(59 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 188 corpus positions)\n",
      "2020-12-23 02:51:14,022 : INFO : token count processed\n",
      "2020-12-23 02:51:14,025 : INFO : frequencies processed\n",
      "2020-12-23 02:51:14,152 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:14,153 : INFO : entropies processed\n",
      "2020-12-23 02:51:14,154 : INFO : extropies processed\n",
      "2020-12-23 02:51:14,155 : INFO : token count processed\n",
      "2020-12-23 02:51:14,156 : INFO : alphabet_source #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:51:14,157 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:14,158 : INFO : vocab #2480\n",
      "2020-12-23 02:51:14,159 : INFO : diff #set()\n",
      "2020-12-23 02:51:14,419 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:14,547 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.160027623749287, 0.46295704231052426], [0.7854734510183334, 0.21452655], [1.7924812503605778, 1.1575860145844845], [4.303508854797678, 4.8191513650620195, 5.2322360357205, 3.8904241841391967, 0.9287271809228219, 0.4130846706584803]]\n",
      "2020-12-23 02:51:14,549 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:14,550 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:14,551 : INFO : built Dictionary(64 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 243 corpus positions)\n",
      "2020-12-23 02:51:14,576 : INFO : token count processed\n",
      "2020-12-23 02:51:14,580 : INFO : frequencies processed\n",
      "2020-12-23 02:51:14,710 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:14,711 : INFO : entropies processed\n",
      "2020-12-23 02:51:14,712 : INFO : extropies processed\n",
      "2020-12-23 02:51:14,713 : INFO : token count processed\n",
      "2020-12-23 02:51:14,714 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:14,715 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:14,715 : INFO : vocab #2480\n",
      "2020-12-23 02:51:14,717 : INFO : diff #set()\n",
      "2020-12-23 02:51:14,983 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:15,112 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.100093895883267, 0.4761691855589226], [0.7445785105228424, 0.2554215], [2.0, 1.2451124978365313], [4.303508854797678, 5.062480936779194, 5.3960560138272875, 3.969933777749585, 1.0925471590296096, 0.33357507704809386]]\n",
      "2020-12-23 02:51:15,115 : INFO : Removed 2 and 223 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:15,116 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:15,119 : INFO : built Dictionary(250 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 1797 corpus positions)\n",
      "2020-12-23 02:51:15,242 : INFO : token count processed\n",
      "2020-12-23 02:51:15,249 : INFO : frequencies processed\n",
      "2020-12-23 02:51:15,378 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:15,379 : INFO : entropies processed\n",
      "2020-12-23 02:51:15,379 : INFO : extropies processed\n",
      "2020-12-23 02:51:15,381 : INFO : token count processed\n",
      "2020-12-23 02:51:15,381 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:15,382 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:15,383 : INFO : vocab #2480\n",
      "2020-12-23 02:51:15,383 : INFO : diff #set()\n",
      "2020-12-23 02:51:15,642 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:15,770 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.1692927073497184, 0.4609797454312775], [0.7901281714439392, 0.20987183], [3.121928094887362, 1.3519647487142497], [4.303508854797678, 7.185085743102134, 7.227759811800449, 4.260834786099363, 2.924250957002771, 0.04267406869831447]]\n",
      "2020-12-23 02:51:15,773 : INFO : Removed 2 and 148 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:15,774 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:15,776 : INFO : built Dictionary(167 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 772 corpus positions)\n",
      "2020-12-23 02:51:15,847 : INFO : token count processed\n",
      "2020-12-23 02:51:15,850 : INFO : frequencies processed\n",
      "2020-12-23 02:51:15,978 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:15,978 : INFO : entropies processed\n",
      "2020-12-23 02:51:15,979 : INFO : extropies processed\n",
      "2020-12-23 02:51:15,981 : INFO : token count processed\n",
      "2020-12-23 02:51:15,982 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:15,983 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:15,984 : INFO : vocab #2480\n",
      "2020-12-23 02:51:15,985 : INFO : diff #set()\n",
      "2020-12-23 02:51:16,245 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:16,373 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.1053588581647094, 0.47497840860808094], [0.7091281712055206, 0.29087183], [3.121928094887362, 1.3519647487142497], [4.303508854797678, 6.591225336124281, 6.671608317812995, 4.223125873108964, 2.3680994630153167, 0.0803829816887136]]\n",
      "2020-12-23 02:51:16,376 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:16,377 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:16,378 : INFO : built Dictionary(51 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 110 corpus positions)\n",
      "2020-12-23 02:51:16,399 : INFO : token count processed\n",
      "2020-12-23 02:51:16,401 : INFO : frequencies processed\n",
      "2020-12-23 02:51:16,535 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:16,536 : INFO : entropies processed\n",
      "2020-12-23 02:51:16,536 : INFO : extropies processed\n",
      "2020-12-23 02:51:16,538 : INFO : token count processed\n",
      "2020-12-23 02:51:16,538 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:16,539 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:16,540 : INFO : vocab #2480\n",
      "2020-12-23 02:51:16,541 : INFO : diff #set()\n",
      "2020-12-23 02:51:16,806 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:16,935 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.169629905651068, 0.460908101144521], [0.8209235221147537, 0.17907648], [1.584962500721156, 1.1699250014423124], [4.303508854797678, 4.7032114441396695, 5.311246711709506, 3.6954735872278413, 1.0077378569118283, 0.6080352675698366]]\n",
      "2020-12-23 02:51:16,937 : INFO : Removed 2 and 9 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:16,938 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:16,940 : INFO : built Dictionary(122 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 350 corpus positions)\n",
      "2020-12-23 02:51:16,989 : INFO : token count processed\n",
      "2020-12-23 02:51:16,991 : INFO : frequencies processed\n",
      "2020-12-23 02:51:17,122 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:17,123 : INFO : entropies processed\n",
      "2020-12-23 02:51:17,123 : INFO : extropies processed\n",
      "2020-12-23 02:51:17,125 : INFO : token count processed\n",
      "2020-12-23 02:51:17,126 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:17,127 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:17,127 : INFO : vocab #2480\n",
      "2020-12-23 02:51:17,129 : INFO : diff #set()\n",
      "2020-12-23 02:51:17,387 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:17,514 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.097295446603412, 0.4768045444524798], [0.7667950391769409, 0.23320496], [3.169925001442312, 1.3594000115384994], [4.303508854797678, 6.14228447828618, 6.300924094857328, 4.144869238226529, 1.9974152400596505, 0.15863961657114878]]\n",
      "2020-12-23 02:51:17,517 : INFO : Removed 2 and 184 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:17,518 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:17,520 : INFO : built Dictionary(262 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 1140 corpus positions)\n",
      "2020-12-23 02:51:17,650 : INFO : token count processed\n",
      "2020-12-23 02:51:17,656 : INFO : frequencies processed\n",
      "2020-12-23 02:51:17,785 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:17,786 : INFO : entropies processed\n",
      "2020-12-23 02:51:17,787 : INFO : extropies processed\n",
      "2020-12-23 02:51:17,788 : INFO : token count processed\n",
      "2020-12-23 02:51:17,789 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:17,790 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:17,790 : INFO : vocab #2480\n",
      "2020-12-23 02:51:17,791 : INFO : diff #set()\n",
      "2020-12-23 02:51:18,050 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:18,175 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.1677018958432517, 0.46131804466176046], [0.7499415576457977, 0.25005844], [2.94770277922009, 1.3393100707180505], [4.303508854797678, 7.450178124335845, 7.503605178384967, 4.250081800748556, 3.200096323587289, 0.053427054049121736]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:51:18,178 : INFO : Removed 2 and 16 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:18,178 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:18,179 : INFO : built Dictionary(68 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 188 corpus positions)\n",
      "2020-12-23 02:51:18,199 : INFO : token count processed\n",
      "2020-12-23 02:51:18,202 : INFO : frequencies processed\n",
      "2020-12-23 02:51:18,330 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:18,331 : INFO : entropies processed\n",
      "2020-12-23 02:51:18,332 : INFO : extropies processed\n",
      "2020-12-23 02:51:18,334 : INFO : token count processed\n",
      "2020-12-23 02:51:18,336 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:18,337 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:18,337 : INFO : vocab #2480\n",
      "2020-12-23 02:51:18,339 : INFO : diff #set()\n",
      "2020-12-23 02:51:18,603 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:18,732 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.1475330381908104, 0.46565057776361385], [0.7537060379981995, 0.24629396], [1.7924812503605778, 1.1575860145844845], [4.303508854797678, 5.20665021947654, 5.558055046475277, 3.9521040277989394, 1.2545461916775995, 0.35140482699873754]]\n",
      "2020-12-23 02:51:18,734 : INFO : Removed 2 and 89 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:18,735 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:18,736 : INFO : built Dictionary(132 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 526 corpus positions)\n",
      "2020-12-23 02:51:18,787 : INFO : token count processed\n",
      "2020-12-23 02:51:18,792 : INFO : frequencies processed\n",
      "2020-12-23 02:51:18,920 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:18,921 : INFO : entropies processed\n",
      "2020-12-23 02:51:18,922 : INFO : extropies processed\n",
      "2020-12-23 02:51:18,924 : INFO : token count processed\n",
      "2020-12-23 02:51:18,925 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:18,926 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:18,927 : INFO : vocab #2480\n",
      "2020-12-23 02:51:18,928 : INFO : diff #set()\n",
      "2020-12-23 02:51:19,186 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:19,314 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.1617103516679455, 0.4625966652879346], [0.7930163443088531, 0.20698366], [3.169925001442312, 1.3594000115384994], [4.303508854797678, 6.524718477352, 6.633760347530252, 4.1944669846194245, 2.3302514927325744, 0.10904187017825251]]\n",
      "2020-12-23 02:51:19,317 : INFO : Removed 2 and 8 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:19,317 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:19,318 : INFO : built Dictionary(75 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 233 corpus positions)\n",
      "2020-12-23 02:51:19,344 : INFO : token count processed\n",
      "2020-12-23 02:51:19,349 : INFO : frequencies processed\n",
      "2020-12-23 02:51:19,486 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:19,487 : INFO : entropies processed\n",
      "2020-12-23 02:51:19,488 : INFO : extropies processed\n",
      "2020-12-23 02:51:19,490 : INFO : token count processed\n",
      "2020-12-23 02:51:19,491 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:19,493 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:19,494 : INFO : vocab #2480\n",
      "2020-12-23 02:51:19,495 : INFO : diff #set()\n",
      "2020-12-23 02:51:19,759 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:19,887 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.1860458514245498, 0.45744694666323865], [0.8276946097612381, 0.17230539], [1.0, 1.0], [4.303508854797678, 5.321859380715434, 5.666783957596301, 3.9585842779168106, 1.3632751027986227, 0.3449245768808664]]\n",
      "2020-12-23 02:51:19,889 : INFO : Removed 2 and 119 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:19,890 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:19,893 : INFO : built Dictionary(147 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 851 corpus positions)\n",
      "2020-12-23 02:51:19,955 : INFO : token count processed\n",
      "2020-12-23 02:51:19,961 : INFO : frequencies processed\n",
      "2020-12-23 02:51:20,088 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:20,088 : INFO : entropies processed\n",
      "2020-12-23 02:51:20,089 : INFO : extropies processed\n",
      "2020-12-23 02:51:20,090 : INFO : token count processed\n",
      "2020-12-23 02:51:20,091 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:20,091 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:20,092 : INFO : vocab #2480\n",
      "2020-12-23 02:51:20,093 : INFO : diff #set()\n",
      "2020-12-23 02:51:20,358 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:20,487 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.1103516891699547, 0.4738546684573323], [0.7137651443481445, 0.28623486], [3.180832987205441, 1.3478475537994532], [4.303508854797678, 6.500767808767801, 6.573023985990174, 4.231252677575304, 2.269515131192496, 0.0722561772223731]]\n",
      "2020-12-23 02:51:20,489 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:20,490 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:20,491 : INFO : built Dictionary(49 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 73 corpus positions)\n",
      "2020-12-23 02:51:20,509 : INFO : token count processed\n",
      "2020-12-23 02:51:20,511 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:51:20,512 : INFO : frequencies processed\n",
      "2020-12-23 02:51:20,513 : INFO : token count processed\n",
      "2020-12-23 02:51:20,514 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:20,515 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:20,516 : INFO : vocab #2480\n",
      "2020-12-23 02:51:20,517 : INFO : diff #set()\n",
      "2020-12-23 02:51:20,776 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:20,905 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.2160231397693184, 0.4512588258009332], [0.9284964948892593, 0.071503505], [nan, nan], [4.303508854797678, 4.736228843383063, 5.499324184633309, 3.5404135135474313, 1.1958153298356313, 0.7630953412502466]]\n",
      "2020-12-23 02:51:20,907 : INFO : Removed 2 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:20,908 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:20,909 : INFO : built Dictionary(107 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 414 corpus positions)\n",
      "2020-12-23 02:51:20,950 : INFO : token count processed\n",
      "2020-12-23 02:51:20,953 : INFO : frequencies processed\n",
      "2020-12-23 02:51:21,083 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:21,084 : INFO : entropies processed\n",
      "2020-12-23 02:51:21,085 : INFO : extropies processed\n",
      "2020-12-23 02:51:21,086 : INFO : token count processed\n",
      "2020-12-23 02:51:21,087 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:21,089 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:21,090 : INFO : vocab #2480\n",
      "2020-12-23 02:51:21,091 : INFO : diff #set()\n",
      "2020-12-23 02:51:21,359 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:21,486 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.1930794458938536, 0.45597983323053803], [0.8459969907999039, 0.15400301], [2.2516291673878226, 1.2667563532600834], [4.303508854797678, 5.788442787590127, 5.982755579602592, 4.109196062785213, 1.6792467248049139, 0.19431279201246454]]\n",
      "2020-12-23 02:51:21,488 : INFO : Removed 2 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:21,489 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:21,490 : INFO : built Dictionary(66 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 100 corpus positions)\n",
      "2020-12-23 02:51:21,515 : INFO : token count processed\n",
      "2020-12-23 02:51:21,518 : INFO : frequencies processed\n",
      "2020-12-23 02:51:21,651 : INFO : scalar_distribution processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:51:21,652 : INFO : entropies processed\n",
      "2020-12-23 02:51:21,653 : INFO : extropies processed\n",
      "2020-12-23 02:51:21,655 : INFO : token count processed\n",
      "2020-12-23 02:51:21,657 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:21,658 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:21,659 : INFO : vocab #2480\n",
      "2020-12-23 02:51:21,661 : INFO : diff #set()\n",
      "2020-12-23 02:51:21,922 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:22,050 : INFO : Computed distances or similarities ('281', 'sacp-python-common/setup.py')[[1.1658518189906797, 0.4617121038622187], [0.8283920288085938, 0.17160797], [1.584962500721156, 1.1699250014423124], [4.303508854797678, 5.370004292053436, 5.826750178482657, 3.8467629683684574, 1.5232413236849789, 0.4567458864292204]]\n",
      "2020-12-23 02:51:22,052 : INFO : Removed 2 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:22,053 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:22,054 : INFO : built Dictionary(91 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 345 corpus positions)\n",
      "2020-12-23 02:51:22,088 : INFO : token count processed\n",
      "2020-12-23 02:51:22,093 : INFO : frequencies processed\n",
      "2020-12-23 02:51:22,223 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:22,223 : INFO : entropies processed\n",
      "2020-12-23 02:51:22,224 : INFO : extropies processed\n",
      "2020-12-23 02:51:22,226 : INFO : token count processed\n",
      "2020-12-23 02:51:22,227 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:22,228 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:22,229 : INFO : vocab #2480\n",
      "2020-12-23 02:51:22,230 : INFO : diff #set()\n",
      "2020-12-23 02:51:22,487 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:22,615 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.16003823790519, 0.4629547673979153], [0.8081010729074478, 0.19189893], [2.0, 1.2451124978365313], [4.303508854797678, 5.695663584743922, 5.9138284067722084, 4.085344032769391, 1.6103195519745306, 0.21816482202828613]]\n",
      "2020-12-23 02:51:22,618 : INFO : Removed 2 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:22,619 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:22,619 : INFO : built Dictionary(54 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 120 corpus positions)\n",
      "2020-12-23 02:51:22,634 : INFO : token count processed\n",
      "2020-12-23 02:51:22,636 : INFO : frequencies processed\n",
      "2020-12-23 02:51:22,764 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:22,765 : INFO : entropies processed\n",
      "2020-12-23 02:51:22,766 : INFO : extropies processed\n",
      "2020-12-23 02:51:22,768 : INFO : token count processed\n",
      "2020-12-23 02:51:22,769 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:22,770 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:22,771 : INFO : vocab #2480\n",
      "2020-12-23 02:51:22,772 : INFO : diff #set()\n",
      "2020-12-23 02:51:23,032 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:23,159 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.2077663864187278, 0.4529464739347376], [0.8896030709147453, 0.11039693], [1.0, 1.0], [4.303508854797678, 4.9004417692112465, 5.457460781248259, 3.746489842760666, 1.1539519264505813, 0.5570190120370127]]\n",
      "2020-12-23 02:51:23,161 : INFO : Removed 2 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:23,162 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:23,164 : INFO : built Dictionary(53 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 127 corpus positions)\n",
      "2020-12-23 02:51:23,185 : INFO : token count processed\n",
      "2020-12-23 02:51:23,190 : INFO : frequencies processed\n",
      "2020-12-23 02:51:23,320 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:23,321 : INFO : entropies processed\n",
      "2020-12-23 02:51:23,322 : INFO : extropies processed\n",
      "2020-12-23 02:51:23,323 : INFO : token count processed\n",
      "2020-12-23 02:51:23,324 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:23,325 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:23,325 : INFO : vocab #2480\n",
      "2020-12-23 02:51:23,326 : INFO : diff #set()\n",
      "2020-12-23 02:51:23,587 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:23,715 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.2212887233208642, 0.450189112969062], [0.9002227410674095, 0.09977726], [0.0, 0.0], [4.303508854797678, 4.778624108914332, 5.361126437744286, 3.7210065259677236, 1.057617582946608, 0.5825023288299533]]\n",
      "2020-12-23 02:51:23,718 : INFO : Removed 2 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:23,719 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:23,720 : INFO : built Dictionary(54 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 158 corpus positions)\n",
      "2020-12-23 02:51:23,739 : INFO : token count processed\n",
      "2020-12-23 02:51:23,742 : INFO : frequencies processed\n",
      "2020-12-23 02:51:23,874 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:23,875 : INFO : entropies processed\n",
      "2020-12-23 02:51:23,876 : INFO : extropies processed\n",
      "2020-12-23 02:51:23,878 : INFO : token count processed\n",
      "2020-12-23 02:51:23,879 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:23,880 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:23,881 : INFO : vocab #2480\n",
      "2020-12-23 02:51:23,882 : INFO : diff #set()\n",
      "2020-12-23 02:51:24,150 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:24,279 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.2212379157959246, 0.45019941037773753], [0.9138624593615532, 0.08613754], [0.0, 0.0], [4.303508854797678, 4.773880192225086, 5.288185602813656, 3.7892034442091065, 0.9846767480159784, 0.5143054105885705]]\n",
      "2020-12-23 02:51:24,282 : INFO : Removed 2 and 346 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:24,283 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:24,285 : INFO : built Dictionary(160 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 1983 corpus positions)\n",
      "2020-12-23 02:51:24,350 : INFO : token count processed\n",
      "2020-12-23 02:51:24,358 : INFO : frequencies processed\n",
      "2020-12-23 02:51:24,486 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:24,487 : INFO : entropies processed\n",
      "2020-12-23 02:51:24,487 : INFO : extropies processed\n",
      "2020-12-23 02:51:24,489 : INFO : token count processed\n",
      "2020-12-23 02:51:24,490 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:24,491 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:24,492 : INFO : vocab #2480\n",
      "2020-12-23 02:51:24,493 : INFO : diff #set()\n",
      "2020-12-23 02:51:24,759 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:24,886 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.2114380322754532, 0.4521944478684093], [0.898559920489788, 0.10144008], [2.584962500721156, 1.315172029168969], [4.303508854797678, 6.620773041953877, 6.670013723274998, 4.254268173476557, 2.3665048684773202, 0.04924068132112147]]\n",
      "2020-12-23 02:51:24,889 : INFO : Removed 2 and 132 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:24,890 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:24,891 : INFO : built Dictionary(91 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 546 corpus positions)\n",
      "2020-12-23 02:51:24,924 : INFO : token count processed\n",
      "2020-12-23 02:51:24,933 : INFO : frequencies processed\n",
      "2020-12-23 02:51:25,059 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:25,059 : INFO : entropies processed\n",
      "2020-12-23 02:51:25,060 : INFO : extropies processed\n",
      "2020-12-23 02:51:25,061 : INFO : token count processed\n",
      "2020-12-23 02:51:25,062 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:25,063 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:25,064 : INFO : vocab #2480\n",
      "2020-12-23 02:51:25,065 : INFO : diff #set()\n",
      "2020-12-23 02:51:25,323 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:25,450 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.1739137442649565, 0.459999851713583], [0.8092974126338959, 0.19070259], [2.321928094887362, 1.2877123795494492], [4.303508854797678, 5.828370634755606, 5.970916629316468, 4.1609628602368165, 1.6674077745187903, 0.14254599456086225]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:51:25,453 : INFO : Removed 2 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:25,454 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:25,455 : INFO : built Dictionary(92 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 309 corpus positions)\n",
      "2020-12-23 02:51:25,491 : INFO : token count processed\n",
      "2020-12-23 02:51:25,497 : INFO : frequencies processed\n",
      "2020-12-23 02:51:25,626 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:25,627 : INFO : entropies processed\n",
      "2020-12-23 02:51:25,627 : INFO : extropies processed\n",
      "2020-12-23 02:51:25,628 : INFO : token count processed\n",
      "2020-12-23 02:51:25,629 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:25,630 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:25,630 : INFO : vocab #2480\n",
      "2020-12-23 02:51:25,632 : INFO : diff #set()\n",
      "2020-12-23 02:51:25,894 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:26,022 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.103260518439003, 0.47545227575620524], [0.745198667049408, 0.25480133], [2.321928094887362, 1.2877123795494492], [4.303508854797678, 5.774409284925443, 5.9885954429434545, 4.089322696779666, 1.6850865881457766, 0.21418615801801177]]\n",
      "2020-12-23 02:51:26,025 : INFO : Removed 2 and 51 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:26,026 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:26,027 : INFO : built Dictionary(101 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 358 corpus positions)\n",
      "2020-12-23 02:51:26,068 : INFO : token count processed\n",
      "2020-12-23 02:51:26,072 : INFO : frequencies processed\n",
      "2020-12-23 02:51:26,200 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:26,200 : INFO : entropies processed\n",
      "2020-12-23 02:51:26,201 : INFO : extropies processed\n",
      "2020-12-23 02:51:26,202 : INFO : token count processed\n",
      "2020-12-23 02:51:26,202 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:26,203 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:26,204 : INFO : vocab #2480\n",
      "2020-12-23 02:51:26,205 : INFO : diff #set()\n",
      "2020-12-23 02:51:26,460 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:26,588 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.077053173146129, 0.4814513238894564], [0.6574404835700989, 0.34255952], [2.6464393446710153, 1.3017576173934458], [4.303508854797678, 5.977819040873918, 6.115535669398048, 4.165792226273547, 1.8120268146003697, 0.1377166285241298]]\n",
      "2020-12-23 02:51:26,590 : INFO : Removed 2 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:26,591 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:26,592 : INFO : built Dictionary(84 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 305 corpus positions)\n",
      "2020-12-23 02:51:26,623 : INFO : token count processed\n",
      "2020-12-23 02:51:26,626 : INFO : frequencies processed\n",
      "2020-12-23 02:51:26,756 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:26,756 : INFO : entropies processed\n",
      "2020-12-23 02:51:26,757 : INFO : extropies processed\n",
      "2020-12-23 02:51:26,758 : INFO : token count processed\n",
      "2020-12-23 02:51:26,759 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:26,760 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:26,761 : INFO : vocab #2480\n",
      "2020-12-23 02:51:26,762 : INFO : diff #set()\n",
      "2020-12-23 02:51:27,020 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:27,147 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.151570456850382, 0.46477678516922444], [0.7386524379253387, 0.26134756], [2.1280852788913944, 1.2238339714721664], [4.303508854797678, 5.901812829596593, 6.099769854989617, 4.105551829404654, 1.7962610001919392, 0.1979570253930243]]\n",
      "2020-12-23 02:51:27,150 : INFO : Removed 2 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:27,151 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:27,152 : INFO : built Dictionary(87 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 305 corpus positions)\n",
      "2020-12-23 02:51:27,190 : INFO : token count processed\n",
      "2020-12-23 02:51:27,192 : INFO : frequencies processed\n",
      "2020-12-23 02:51:27,322 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:27,322 : INFO : entropies processed\n",
      "2020-12-23 02:51:27,323 : INFO : extropies processed\n",
      "2020-12-23 02:51:27,324 : INFO : token count processed\n",
      "2020-12-23 02:51:27,325 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:27,325 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:27,326 : INFO : vocab #2480\n",
      "2020-12-23 02:51:27,327 : INFO : diff #set()\n",
      "2020-12-23 02:51:27,585 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:27,715 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.1624018221611787, 0.46244874091003385], [0.7721804529428482, 0.22781955], [2.0, 1.2451124978365313], [4.303508854797678, 5.643202320803383, 5.886995004650711, 4.059716170950349, 1.5834861498530328, 0.2437926838473281]]\n",
      "2020-12-23 02:51:27,717 : INFO : Removed 2 and 30 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:27,718 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:27,719 : INFO : built Dictionary(99 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 339 corpus positions)\n",
      "2020-12-23 02:51:27,755 : INFO : token count processed\n",
      "2020-12-23 02:51:27,760 : INFO : frequencies processed\n",
      "2020-12-23 02:51:27,886 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:27,886 : INFO : entropies processed\n",
      "2020-12-23 02:51:27,887 : INFO : extropies processed\n",
      "2020-12-23 02:51:27,888 : INFO : token count processed\n",
      "2020-12-23 02:51:27,889 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:27,889 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:27,890 : INFO : vocab #2480\n",
      "2020-12-23 02:51:27,891 : INFO : diff #set()\n",
      "2020-12-23 02:51:28,147 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:28,276 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.1296083787754807, 0.4695699030706282], [0.7573085874319077, 0.24269141], [2.2516291673878226, 1.2667563532600834], [4.303508854797678, 5.925214310725336, 6.1090987147532685, 4.119624450769746, 1.8055898599555906, 0.18388440402793282]]\n",
      "2020-12-23 02:51:28,279 : INFO : Removed 2 and 293 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:28,280 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:28,282 : INFO : built Dictionary(171 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 1734 corpus positions)\n",
      "2020-12-23 02:51:28,361 : INFO : token count processed\n",
      "2020-12-23 02:51:28,364 : INFO : frequencies processed\n",
      "2020-12-23 02:51:28,491 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:28,492 : INFO : entropies processed\n",
      "2020-12-23 02:51:28,493 : INFO : extropies processed\n",
      "2020-12-23 02:51:28,494 : INFO : token count processed\n",
      "2020-12-23 02:51:28,495 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:28,496 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:28,497 : INFO : vocab #2480\n",
      "2020-12-23 02:51:28,497 : INFO : diff #set()\n",
      "2020-12-23 02:51:28,755 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:28,883 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.2107688459907706, 0.45233132437771595], [0.9027211368083954, 0.09727886], [2.584962500721156, 1.315172029168969], [4.303508854797678, 6.551685682764175, 6.606595108437083, 4.248599429124771, 2.303086253639405, 0.05490942567290791]]\n",
      "2020-12-23 02:51:28,886 : INFO : Removed 2 and 88 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:28,887 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:28,888 : INFO : built Dictionary(145 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 575 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:51:28,944 : INFO : token count processed\n",
      "2020-12-23 02:51:28,946 : INFO : frequencies processed\n",
      "2020-12-23 02:51:29,072 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:29,073 : INFO : entropies processed\n",
      "2020-12-23 02:51:29,074 : INFO : extropies processed\n",
      "2020-12-23 02:51:29,075 : INFO : token count processed\n",
      "2020-12-23 02:51:29,076 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:29,077 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:29,078 : INFO : vocab #2480\n",
      "2020-12-23 02:51:29,079 : INFO : diff #set()\n",
      "2020-12-23 02:51:29,337 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:29,465 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.075573881069989, 0.48179446133928283], [0.6035399734973907, 0.39646003], [2.8464393446710154, 1.3178207096846455], [4.303508854797678, 6.642985062562557, 6.728482892437684, 4.218011024922552, 2.4249740376400064, 0.085497829875127]]\n",
      "2020-12-23 02:51:29,468 : INFO : Removed 2 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:29,468 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:29,469 : INFO : built Dictionary(65 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 170 corpus positions)\n",
      "2020-12-23 02:51:29,488 : INFO : token count processed\n",
      "2020-12-23 02:51:29,491 : INFO : frequencies processed\n",
      "2020-12-23 02:51:29,618 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:29,619 : INFO : entropies processed\n",
      "2020-12-23 02:51:29,620 : INFO : extropies processed\n",
      "2020-12-23 02:51:29,621 : INFO : token count processed\n",
      "2020-12-23 02:51:29,622 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:29,623 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:29,623 : INFO : vocab #2480\n",
      "2020-12-23 02:51:29,624 : INFO : diff #set()\n",
      "2020-12-23 02:51:29,882 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:30,009 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.1809884944504994, 0.4585076916015324], [0.7880035489797592, 0.21199645], [0.0, 0.0], [4.303508854797678, 5.2461980344571995, 5.661892042907802, 3.8878148463470765, 1.3583831881101238, 0.4156940084506022]]\n",
      "2020-12-23 02:51:30,011 : INFO : Removed 2 and 23 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:30,012 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:30,013 : INFO : built Dictionary(85 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 230 corpus positions)\n",
      "2020-12-23 02:51:30,048 : INFO : token count processed\n",
      "2020-12-23 02:51:30,051 : INFO : frequencies processed\n",
      "2020-12-23 02:51:30,178 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:30,179 : INFO : entropies processed\n",
      "2020-12-23 02:51:30,180 : INFO : extropies processed\n",
      "2020-12-23 02:51:30,181 : INFO : token count processed\n",
      "2020-12-23 02:51:30,182 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:30,183 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:30,184 : INFO : vocab #2480\n",
      "2020-12-23 02:51:30,185 : INFO : diff #set()\n",
      "2020-12-23 02:51:30,441 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:30,569 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/test_auth_utility.py')[[1.2010096596185538, 0.4543369428797987], [0.858915701508522, 0.1410843], [1.584962500721156, 1.1699250014423124], [4.303508854797678, 5.903090303960449, 6.176118364872301, 4.030480793885826, 1.8726095100746232, 0.2730280609118516]]\n",
      "2020-12-23 02:51:30,572 : INFO : Removed 2 and 442 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:30,573 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:30,576 : INFO : built Dictionary(116 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 1229 corpus positions)\n",
      "2020-12-23 02:51:30,624 : INFO : token count processed\n",
      "2020-12-23 02:51:30,628 : INFO : frequencies processed\n",
      "2020-12-23 02:51:30,759 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:30,760 : INFO : entropies processed\n",
      "2020-12-23 02:51:30,761 : INFO : extropies processed\n",
      "2020-12-23 02:51:30,762 : INFO : token count processed\n",
      "2020-12-23 02:51:30,763 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:30,764 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:30,764 : INFO : vocab #2480\n",
      "2020-12-23 02:51:30,765 : INFO : diff #set()\n",
      "2020-12-23 02:51:31,032 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:31,170 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.1844806695284353, 0.4577747077138799], [0.8666612207889557, 0.13333878], [2.584962500721156, 1.315172029168969], [4.303508854797678, 6.16659449033757, 6.233507630653752, 4.236595714481495, 1.929998775856074, 0.06691314031618223]]\n",
      "2020-12-23 02:51:31,173 : INFO : Removed 2 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:31,174 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:31,174 : INFO : built Dictionary(78 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 264 corpus positions)\n",
      "2020-12-23 02:51:31,206 : INFO : token count processed\n",
      "2020-12-23 02:51:31,209 : INFO : frequencies processed\n",
      "2020-12-23 02:51:31,337 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:31,337 : INFO : entropies processed\n",
      "2020-12-23 02:51:31,338 : INFO : extropies processed\n",
      "2020-12-23 02:51:31,339 : INFO : token count processed\n",
      "2020-12-23 02:51:31,340 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:31,341 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:31,342 : INFO : vocab #2480\n",
      "2020-12-23 02:51:31,343 : INFO : diff #set()\n",
      "2020-12-23 02:51:31,603 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:31,730 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.0882504465412006, 0.47886976471443565], [0.7693727016448975, 0.2306273], [2.0, 1.2451124978365313], [4.303508854797678, 5.906856253399655, 6.127522850788331, 4.082842257409002, 1.8240139959906534, 0.2206665973886759]]\n",
      "2020-12-23 02:51:31,733 : INFO : Removed 2 and 68 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:31,734 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:31,735 : INFO : built Dictionary(93 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 263 corpus positions)\n",
      "2020-12-23 02:51:31,767 : INFO : token count processed\n",
      "2020-12-23 02:51:31,772 : INFO : frequencies processed\n",
      "2020-12-23 02:51:31,904 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:31,904 : INFO : entropies processed\n",
      "2020-12-23 02:51:31,905 : INFO : extropies processed\n",
      "2020-12-23 02:51:31,907 : INFO : token count processed\n",
      "2020-12-23 02:51:31,907 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:31,908 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:31,909 : INFO : vocab #2480\n",
      "2020-12-23 02:51:31,910 : INFO : diff #set()\n",
      "2020-12-23 02:51:32,175 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:32,303 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.1729153251451203, 0.4602112141361118], [0.8253817111253738, 0.17461829], [1.0, 1.0], [4.303508854797678, 5.965115449163356, 6.204690148346633, 4.0639341556144, 1.9011812935489552, 0.23957469918327678]]\n",
      "2020-12-23 02:51:32,305 : INFO : Removed 2 and 13 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:32,306 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:32,307 : INFO : built Dictionary(98 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 377 corpus positions)\n",
      "2020-12-23 02:51:32,346 : INFO : token count processed\n",
      "2020-12-23 02:51:32,349 : INFO : frequencies processed\n",
      "2020-12-23 02:51:32,589 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:32,590 : INFO : entropies processed\n",
      "2020-12-23 02:51:32,591 : INFO : extropies processed\n",
      "2020-12-23 02:51:32,592 : INFO : token count processed\n",
      "2020-12-23 02:51:32,593 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:32,594 : INFO : alphabet_target #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:51:32,595 : INFO : vocab #2480\n",
      "2020-12-23 02:51:32,596 : INFO : diff #set()\n",
      "2020-12-23 02:51:32,853 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:32,981 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.1757618658453832, 0.45960912161288114], [0.777042418718338, 0.22295758], [1.9219280948873623, 1.2148067842293933], [4.303508854797678, 5.791362404253194, 5.999401719145007, 4.095469539905865, 1.6958928643473294, 0.20803931489181338]]\n",
      "2020-12-23 02:51:32,983 : INFO : Removed 2 and 41 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:32,984 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:32,985 : INFO : built Dictionary(88 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 389 corpus positions)\n",
      "2020-12-23 02:51:33,014 : INFO : token count processed\n",
      "2020-12-23 02:51:33,016 : INFO : frequencies processed\n",
      "2020-12-23 02:51:33,144 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:33,145 : INFO : entropies processed\n",
      "2020-12-23 02:51:33,145 : INFO : extropies processed\n",
      "2020-12-23 02:51:33,147 : INFO : token count processed\n",
      "2020-12-23 02:51:33,148 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:33,149 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:33,149 : INFO : vocab #2480\n",
      "2020-12-23 02:51:33,150 : INFO : diff #set()\n",
      "2020-12-23 02:51:33,408 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:33,534 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.1680084013964258, 0.4612528251070866], [0.8034028559923172, 0.19659714], [2.0, 1.2451124978365313], [4.303508854797678, 5.651670454631116, 5.8599576329853384, 4.095221676443455, 1.5564487781876606, 0.2082871783542224]]\n",
      "2020-12-23 02:51:33,536 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:33,537 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:33,538 : INFO : built Dictionary(51 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 99 corpus positions)\n",
      "2020-12-23 02:51:33,552 : INFO : token count processed\n",
      "2020-12-23 02:51:33,555 : INFO : frequencies processed\n",
      "2020-12-23 02:51:33,682 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:33,683 : INFO : entropies processed\n",
      "2020-12-23 02:51:33,684 : INFO : extropies processed\n",
      "2020-12-23 02:51:33,685 : INFO : token count processed\n",
      "2020-12-23 02:51:33,686 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:33,687 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:33,687 : INFO : vocab #2480\n",
      "2020-12-23 02:51:33,689 : INFO : diff #set()\n",
      "2020-12-23 02:51:33,947 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:34,075 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.1705899950117216, 0.46070423354853796], [0.843631774187088, 0.15636823], [2.0, 1.2451124978365313], [4.303508854797678, 4.8226207261920235, 5.409171970104426, 3.7169576108852755, 1.105663115306748, 0.5865512439124023]]\n",
      "2020-12-23 02:51:34,078 : INFO : Removed 2 and 50 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:34,079 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:34,080 : INFO : built Dictionary(97 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 269 corpus positions)\n",
      "2020-12-23 02:51:34,111 : INFO : token count processed\n",
      "2020-12-23 02:51:34,115 : INFO : frequencies processed\n",
      "2020-12-23 02:51:34,245 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:34,246 : INFO : entropies processed\n",
      "2020-12-23 02:51:34,246 : INFO : extropies processed\n",
      "2020-12-23 02:51:34,248 : INFO : token count processed\n",
      "2020-12-23 02:51:34,249 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:34,250 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:34,251 : INFO : vocab #2480\n",
      "2020-12-23 02:51:34,252 : INFO : diff #set()\n",
      "2020-12-23 02:51:34,520 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:34,651 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.1924720173857066, 0.4561061633034639], [0.8262918442487717, 0.17370816], [1.584962500721156, 1.1699250014423124], [4.303508854797678, 6.24862851613934, 6.465619877811493, 4.086517493125525, 2.162111023013815, 0.21699136167215283]]\n",
      "2020-12-23 02:51:34,653 : INFO : Removed 2 and 32 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:34,654 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:34,656 : INFO : built Dictionary(97 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 385 corpus positions)\n",
      "2020-12-23 02:51:34,698 : INFO : token count processed\n",
      "2020-12-23 02:51:34,700 : INFO : frequencies processed\n",
      "2020-12-23 02:51:34,831 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:34,832 : INFO : entropies processed\n",
      "2020-12-23 02:51:34,833 : INFO : extropies processed\n",
      "2020-12-23 02:51:34,834 : INFO : token count processed\n",
      "2020-12-23 02:51:34,835 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:34,836 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:34,836 : INFO : vocab #2480\n",
      "2020-12-23 02:51:34,837 : INFO : diff #set()\n",
      "2020-12-23 02:51:35,106 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:35,236 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.1827762780946658, 0.458132154923772], [0.7977984398603439, 0.20220156], [1.9219280948873623, 1.2148067842293933], [4.303508854797678, 5.850156917433494, 6.047096742652221, 4.10656902957895, 1.7435878878545434, 0.19693982521872755]]\n",
      "2020-12-23 02:51:35,238 : INFO : Removed 2 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:35,239 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:35,240 : INFO : built Dictionary(92 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 383 corpus positions)\n",
      "2020-12-23 02:51:35,277 : INFO : token count processed\n",
      "2020-12-23 02:51:35,282 : INFO : frequencies processed\n",
      "2020-12-23 02:51:35,412 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:35,412 : INFO : entropies processed\n",
      "2020-12-23 02:51:35,413 : INFO : extropies processed\n",
      "2020-12-23 02:51:35,414 : INFO : token count processed\n",
      "2020-12-23 02:51:35,415 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:35,416 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:35,416 : INFO : vocab #2480\n",
      "2020-12-23 02:51:35,418 : INFO : diff #set()\n",
      "2020-12-23 02:51:35,679 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:35,806 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.1709364926736814, 0.4606307017154703], [0.7987854480743408, 0.20121455], [2.0, 1.2451124978365313], [4.303508854797678, 5.6831976040360095, 5.892034529729228, 4.094671929104459, 1.5885256749315504, 0.20883692569321877]]\n",
      "2020-12-23 02:51:35,808 : INFO : Removed 2 and 52 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:35,809 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:35,810 : INFO : built Dictionary(78 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 361 corpus positions)\n",
      "2020-12-23 02:51:35,834 : INFO : token count processed\n",
      "2020-12-23 02:51:35,837 : INFO : frequencies processed\n",
      "2020-12-23 02:51:35,964 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:35,965 : INFO : entropies processed\n",
      "2020-12-23 02:51:35,966 : INFO : extropies processed\n",
      "2020-12-23 02:51:35,967 : INFO : token count processed\n",
      "2020-12-23 02:51:35,968 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:35,969 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:35,970 : INFO : vocab #2480\n",
      "2020-12-23 02:51:35,971 : INFO : diff #set()\n",
      "2020-12-23 02:51:36,228 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:36,357 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.1203586727075596, 0.47161832234876816], [0.7638384401798248, 0.23616156], [2.321928094887362, 1.2877123795494492], [4.303508854797678, 5.749308601266266, 5.943226536689737, 4.109590919374208, 1.6397176818920594, 0.19391793542347102]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:51:36,359 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:36,360 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:36,361 : INFO : built Dictionary(73 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 242 corpus positions)\n",
      "2020-12-23 02:51:36,389 : INFO : token count processed\n",
      "2020-12-23 02:51:36,392 : INFO : frequencies processed\n",
      "2020-12-23 02:51:36,524 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:36,525 : INFO : entropies processed\n",
      "2020-12-23 02:51:36,526 : INFO : extropies processed\n",
      "2020-12-23 02:51:36,528 : INFO : token count processed\n",
      "2020-12-23 02:51:36,530 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:36,531 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:36,532 : INFO : vocab #2480\n",
      "2020-12-23 02:51:36,534 : INFO : diff #set()\n",
      "2020-12-23 02:51:36,805 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:36,938 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.1631099938107459, 0.46229734172616077], [0.7858483493328094, 0.21415165], [1.584962500721156, 1.1699250014423124], [4.303508854797678, 5.015422548793484, 5.374945370454056, 3.9439860331371053, 1.0714365156563783, 0.3595228216605726]]\n",
      "2020-12-23 02:51:36,940 : INFO : Removed 2 and 81 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:36,941 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:36,942 : INFO : built Dictionary(101 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 368 corpus positions)\n",
      "2020-12-23 02:51:36,983 : INFO : token count processed\n",
      "2020-12-23 02:51:36,985 : INFO : frequencies processed\n",
      "2020-12-23 02:51:37,113 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:37,114 : INFO : entropies processed\n",
      "2020-12-23 02:51:37,115 : INFO : extropies processed\n",
      "2020-12-23 02:51:37,116 : INFO : token count processed\n",
      "2020-12-23 02:51:37,117 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:37,118 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:37,118 : INFO : vocab #2480\n",
      "2020-12-23 02:51:37,119 : INFO : diff #set()\n",
      "2020-12-23 02:51:37,384 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:37,513 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.1699556119046304, 0.460838919705953], [0.7827271521091461, 0.21727285], [1.9219280948873623, 1.2148067842293933], [4.303508854797678, 6.030001281822029, 6.209209349542618, 4.12430078707709, 1.9057004947449405, 0.17920806772058917]]\n",
      "2020-12-23 02:51:37,515 : INFO : Removed 2 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:37,516 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:37,518 : INFO : built Dictionary(88 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 331 corpus positions)\n",
      "2020-12-23 02:51:37,562 : INFO : token count processed\n",
      "2020-12-23 02:51:37,564 : INFO : frequencies processed\n",
      "2020-12-23 02:51:37,690 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:37,691 : INFO : entropies processed\n",
      "2020-12-23 02:51:37,692 : INFO : extropies processed\n",
      "2020-12-23 02:51:37,694 : INFO : token count processed\n",
      "2020-12-23 02:51:37,695 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:37,696 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:37,696 : INFO : vocab #2480\n",
      "2020-12-23 02:51:37,697 : INFO : diff #set()\n",
      "2020-12-23 02:51:37,954 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:38,081 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.1326413465640583, 0.4689020972096974], [0.7710757255554199, 0.22892427], [2.321928094887362, 1.2877123795494492], [4.303508854797678, 5.9537092545441395, 6.142973779941772, 4.114244329400045, 1.8394649251440942, 0.1892645253976326]]\n",
      "2020-12-23 02:51:38,084 : INFO : Removed 2 and 90 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:38,084 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:38,085 : INFO : built Dictionary(100 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 332 corpus positions)\n",
      "2020-12-23 02:51:38,123 : INFO : token count processed\n",
      "2020-12-23 02:51:38,125 : INFO : frequencies processed\n",
      "2020-12-23 02:51:38,252 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:38,252 : INFO : entropies processed\n",
      "2020-12-23 02:51:38,253 : INFO : extropies processed\n",
      "2020-12-23 02:51:38,254 : INFO : token count processed\n",
      "2020-12-23 02:51:38,255 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:38,256 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:38,256 : INFO : vocab #2480\n",
      "2020-12-23 02:51:38,257 : INFO : diff #set()\n",
      "2020-12-23 02:51:38,513 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:38,640 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.1606885225939019, 0.4628154357017189], [0.8056579530239105, 0.19434205], [2.0, 1.2451124978365313], [4.303508854797678, 6.184756445474906, 6.368498791303551, 4.1197665089690325, 2.0649899365058735, 0.18374234582864535]]\n",
      "2020-12-23 02:51:38,643 : INFO : Removed 2 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:38,644 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:38,646 : INFO : built Dictionary(110 unique tokens: ['allow', 'binari', 'creat', 'csb', 'debug']...) from 2 documents (total 445 corpus positions)\n",
      "2020-12-23 02:51:38,689 : INFO : token count processed\n",
      "2020-12-23 02:51:38,691 : INFO : frequencies processed\n",
      "2020-12-23 02:51:38,820 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:38,821 : INFO : entropies processed\n",
      "2020-12-23 02:51:38,824 : INFO : extropies processed\n",
      "2020-12-23 02:51:38,825 : INFO : token count processed\n",
      "2020-12-23 02:51:38,825 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:38,826 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:38,827 : INFO : vocab #2480\n",
      "2020-12-23 02:51:38,828 : INFO : diff #set()\n",
      "2020-12-23 02:51:39,086 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:39,214 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.1938326151188499, 0.45582328984831216], [0.8063565343618393, 0.19364347], [1.584962500721156, 1.1699250014423124], [4.303508854797678, 6.212221456585881, 6.381567823691743, 4.134162487691816, 2.078058968894065, 0.16934636710586215]]\n",
      "2020-12-23 02:51:39,216 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:39,217 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:39,219 : INFO : built Dictionary(123 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 417 corpus positions)\n",
      "2020-12-23 02:51:39,257 : INFO : token count processed\n",
      "2020-12-23 02:51:39,261 : INFO : frequencies processed\n",
      "2020-12-23 02:51:39,389 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:39,390 : INFO : entropies processed\n",
      "2020-12-23 02:51:39,390 : INFO : extropies processed\n",
      "2020-12-23 02:51:39,391 : INFO : token count processed\n",
      "2020-12-23 02:51:39,392 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:39,393 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:39,394 : INFO : vocab #2480\n",
      "2020-12-23 02:51:39,395 : INFO : diff #set()\n",
      "2020-12-23 02:51:39,655 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:39,782 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.231274157576945, 0.44817441935774993], [0.9300634860992432, 0.069936514], [0.0, 0.0], [3.321928094887362, 6.301552355933639, 6.3807023498408375, 3.2427781009801633, 3.0587742549534753, 0.07914999390719846]]\n",
      "2020-12-23 02:51:39,785 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:39,786 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:39,787 : INFO : built Dictionary(158 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 653 corpus positions)\n",
      "2020-12-23 02:51:39,828 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:51:39,834 : INFO : frequencies processed\n",
      "2020-12-23 02:51:39,960 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:39,960 : INFO : entropies processed\n",
      "2020-12-23 02:51:39,961 : INFO : extropies processed\n",
      "2020-12-23 02:51:39,962 : INFO : token count processed\n",
      "2020-12-23 02:51:39,963 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:39,964 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:39,964 : INFO : vocab #2480\n",
      "2020-12-23 02:51:39,965 : INFO : diff #set()\n",
      "2020-12-23 02:51:40,222 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:40,351 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.201776185054925, 0.4541787702073153], [0.8912068828940392, 0.10879312], [2.0, 1.2451124978365313], [3.321928094887362, 6.739005504021667, 6.773215898239361, 3.2877177006696687, 3.4512878033519985, 0.034210394217693896]]\n",
      "2020-12-23 02:51:40,353 : INFO : Removed 0 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:40,354 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:40,355 : INFO : built Dictionary(103 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 504 corpus positions)\n",
      "2020-12-23 02:51:40,379 : INFO : token count processed\n",
      "2020-12-23 02:51:40,381 : INFO : frequencies processed\n",
      "2020-12-23 02:51:40,508 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:40,509 : INFO : entropies processed\n",
      "2020-12-23 02:51:40,512 : INFO : extropies processed\n",
      "2020-12-23 02:51:40,513 : INFO : token count processed\n",
      "2020-12-23 02:51:40,514 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:40,514 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:40,515 : INFO : vocab #2480\n",
      "2020-12-23 02:51:40,516 : INFO : diff #set()\n",
      "2020-12-23 02:51:40,774 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:40,902 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.254805806746277, 0.4434971725760352], [0.962341520935297, 0.03765848], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 5.870833373337847, 5.932834465114411, 3.259927003110798, 2.6109063702270485, 0.062001091776563655]]\n",
      "2020-12-23 02:51:40,905 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:40,906 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:40,907 : INFO : built Dictionary(64 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 187 corpus positions)\n",
      "2020-12-23 02:51:40,925 : INFO : token count processed\n",
      "2020-12-23 02:51:40,930 : INFO : frequencies processed\n",
      "2020-12-23 02:51:41,065 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:41,066 : INFO : entropies processed\n",
      "2020-12-23 02:51:41,067 : INFO : extropies processed\n",
      "2020-12-23 02:51:41,068 : INFO : token count processed\n",
      "2020-12-23 02:51:41,069 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:41,070 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:41,071 : INFO : vocab #2480\n",
      "2020-12-23 02:51:41,072 : INFO : diff #set()\n",
      "2020-12-23 02:51:41,329 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:41,457 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.231927277887638, 0.4480432717980084], [0.9430785328149796, 0.056921467], [1.0, 1.0], [3.321928094887362, 5.371881234145534, 5.529137746249241, 3.1646715827836562, 2.2072096513618784, 0.1572565121037064]]\n",
      "2020-12-23 02:51:41,460 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:41,460 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:41,461 : INFO : built Dictionary(55 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 139 corpus positions)\n",
      "2020-12-23 02:51:41,472 : INFO : token count processed\n",
      "2020-12-23 02:51:41,475 : INFO : frequencies processed\n",
      "2020-12-23 02:51:41,602 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:41,603 : INFO : entropies processed\n",
      "2020-12-23 02:51:41,604 : INFO : extropies processed\n",
      "2020-12-23 02:51:41,605 : INFO : token count processed\n",
      "2020-12-23 02:51:41,606 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:41,607 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:41,608 : INFO : vocab #2480\n",
      "2020-12-23 02:51:41,609 : INFO : diff #set()\n",
      "2020-12-23 02:51:41,867 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:41,996 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.2318720927299183, 0.4480543500935344], [0.9420859031379223, 0.057914097], [1.0, 1.0], [3.321928094887362, 4.85108279267097, 5.0756246944856125, 3.09738619307272, 1.7536965995982503, 0.22454190181464284]]\n",
      "2020-12-23 02:51:41,998 : INFO : Removed 0 and 127 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:41,999 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:42,000 : INFO : built Dictionary(95 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 412 corpus positions)\n",
      "2020-12-23 02:51:42,030 : INFO : token count processed\n",
      "2020-12-23 02:51:42,034 : INFO : frequencies processed\n",
      "2020-12-23 02:51:42,163 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:42,163 : INFO : entropies processed\n",
      "2020-12-23 02:51:42,164 : INFO : extropies processed\n",
      "2020-12-23 02:51:42,165 : INFO : token count processed\n",
      "2020-12-23 02:51:42,165 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:42,166 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:42,166 : INFO : vocab #2480\n",
      "2020-12-23 02:51:42,167 : INFO : diff #set()\n",
      "2020-12-23 02:51:42,428 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:42,556 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.210016792450552, 0.45248524962163816], [0.9100892618298531, 0.08991074], [1.0, 1.0], [3.321928094887362, 6.139571208108155, 6.2024866241531385, 3.259012678842378, 2.8805585292657763, 0.06291541604498363]]\n",
      "2020-12-23 02:51:42,559 : INFO : Removed 0 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:42,560 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:42,561 : INFO : built Dictionary(82 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 422 corpus positions)\n",
      "2020-12-23 02:51:42,584 : INFO : token count processed\n",
      "2020-12-23 02:51:42,587 : INFO : frequencies processed\n",
      "2020-12-23 02:51:42,715 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:42,716 : INFO : entropies processed\n",
      "2020-12-23 02:51:42,717 : INFO : extropies processed\n",
      "2020-12-23 02:51:42,718 : INFO : token count processed\n",
      "2020-12-23 02:51:42,719 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:42,720 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:42,721 : INFO : vocab #2480\n",
      "2020-12-23 02:51:42,722 : INFO : diff #set()\n",
      "2020-12-23 02:51:42,987 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:43,115 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.2430299045481146, 0.44582553178285067], [0.9581877738237381, 0.041812226], [0.0, 0.0], [3.321928094887362, 5.609710627339259, 5.703226150467339, 3.228412571759282, 2.3812980555799768, 0.09351552312807954]]\n",
      "2020-12-23 02:51:43,118 : INFO : Removed 0 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:43,119 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:43,120 : INFO : built Dictionary(172 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 1082 corpus positions)\n",
      "2020-12-23 02:51:43,159 : INFO : token count processed\n",
      "2020-12-23 02:51:43,161 : INFO : frequencies processed\n",
      "2020-12-23 02:51:43,291 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:43,292 : INFO : entropies processed\n",
      "2020-12-23 02:51:43,293 : INFO : extropies processed\n",
      "2020-12-23 02:51:43,295 : INFO : token count processed\n",
      "2020-12-23 02:51:43,296 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:43,297 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:43,297 : INFO : vocab #2480\n",
      "2020-12-23 02:51:43,298 : INFO : diff #set()\n",
      "2020-12-23 02:51:43,557 : INFO : alphabet #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:51:43,685 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.2103240804173954, 0.4524223433385212], [0.8739563673734665, 0.12604363], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 7.2441902753576075, 7.2658902127146945, 3.3002281575302757, 3.9439621178273323, 0.021699937357086974]]\n",
      "2020-12-23 02:51:43,687 : INFO : Removed 0 and 136 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:43,688 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:43,689 : INFO : built Dictionary(134 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 689 corpus positions)\n",
      "2020-12-23 02:51:43,723 : INFO : token count processed\n",
      "2020-12-23 02:51:43,726 : INFO : frequencies processed\n",
      "2020-12-23 02:51:43,853 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:43,854 : INFO : entropies processed\n",
      "2020-12-23 02:51:43,855 : INFO : extropies processed\n",
      "2020-12-23 02:51:43,857 : INFO : token count processed\n",
      "2020-12-23 02:51:43,859 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:43,860 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:43,861 : INFO : vocab #2480\n",
      "2020-12-23 02:51:43,863 : INFO : diff #set()\n",
      "2020-12-23 02:51:44,134 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:44,262 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.237598401623443, 0.4469077200245007], [0.9458478651940823, 0.054152135], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 6.2567074920449475, 6.305191813318693, 3.273443773613616, 2.983263718431331, 0.04848432127374558]]\n",
      "2020-12-23 02:51:44,265 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:44,266 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:44,267 : INFO : built Dictionary(82 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 199 corpus positions)\n",
      "2020-12-23 02:51:44,289 : INFO : token count processed\n",
      "2020-12-23 02:51:44,291 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:51:44,292 : INFO : frequencies processed\n",
      "2020-12-23 02:51:44,293 : INFO : token count processed\n",
      "2020-12-23 02:51:44,294 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:44,295 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:44,295 : INFO : vocab #2480\n",
      "2020-12-23 02:51:44,296 : INFO : diff #set()\n",
      "2020-12-23 02:51:44,566 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:44,694 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.2665263349628606, 0.44120378597603455], [0.9895644634962082, 0.0104355365], [nan, nan], [3.321928094887362, 5.7680018917339435, 5.925208247984992, 3.164721738636313, 2.60328015309763, 0.15720635625104862]]\n",
      "2020-12-23 02:51:44,697 : INFO : Removed 0 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:44,698 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:44,700 : INFO : built Dictionary(178 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 740 corpus positions)\n",
      "2020-12-23 02:51:44,746 : INFO : token count processed\n",
      "2020-12-23 02:51:44,748 : INFO : frequencies processed\n",
      "2020-12-23 02:51:44,876 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:44,877 : INFO : entropies processed\n",
      "2020-12-23 02:51:44,878 : INFO : extropies processed\n",
      "2020-12-23 02:51:44,881 : INFO : token count processed\n",
      "2020-12-23 02:51:44,882 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:44,884 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:44,885 : INFO : vocab #2480\n",
      "2020-12-23 02:51:44,887 : INFO : diff #set()\n",
      "2020-12-23 02:51:45,145 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:45,272 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.1975146788702387, 0.4550595313948522], [0.8848906680941582, 0.11510933], [1.0, 1.0], [3.321928094887362, 6.846479111193757, 6.884258777434013, 3.2841484286471054, 3.562330682546651, 0.037779666240256304]]\n",
      "2020-12-23 02:51:45,274 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:45,275 : INFO : built Dictionary(30 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 43 corpus positions)\n",
      "2020-12-23 02:51:45,287 : INFO : token count processed\n",
      "2020-12-23 02:51:45,291 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:51:45,292 : INFO : frequencies processed\n",
      "2020-12-23 02:51:45,293 : INFO : token count processed\n",
      "2020-12-23 02:51:45,294 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:45,295 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:45,295 : INFO : vocab #2480\n",
      "2020-12-23 02:51:45,296 : INFO : diff #set()\n",
      "2020-12-23 02:51:45,565 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:45,696 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2786801570439223, 0.4388505323613632], [0.9890908114612103, 0.010909189], [nan, nan], [3.321928094887362, 4.165013816065912, 4.751391498686891, 2.7355504122663827, 1.4294634037995286, 0.586377682620979]]\n",
      "2020-12-23 02:51:45,699 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:45,700 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:45,700 : INFO : built Dictionary(57 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 150 corpus positions)\n",
      "2020-12-23 02:51:45,712 : INFO : token count processed\n",
      "2020-12-23 02:51:45,714 : INFO : frequencies processed\n",
      "2020-12-23 02:51:45,842 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:45,843 : INFO : entropies processed\n",
      "2020-12-23 02:51:45,844 : INFO : extropies processed\n",
      "2020-12-23 02:51:45,845 : INFO : token count processed\n",
      "2020-12-23 02:51:45,846 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:45,847 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:45,848 : INFO : vocab #2480\n",
      "2020-12-23 02:51:45,849 : INFO : diff #set()\n",
      "2020-12-23 02:51:46,110 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:46,239 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.2336706928468832, 0.44769356700717094], [0.9483850710093975, 0.05161493], [0.0, 0.0], [3.321928094887362, 5.449968864419248, 5.63301427232838, 3.1388826869782314, 2.3110861774410174, 0.18304540790913126]]\n",
      "2020-12-23 02:51:46,242 : INFO : Removed 0 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:46,243 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:46,245 : INFO : built Dictionary(148 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 587 corpus positions)\n",
      "2020-12-23 02:51:46,285 : INFO : token count processed\n",
      "2020-12-23 02:51:46,287 : INFO : frequencies processed\n",
      "2020-12-23 02:51:46,416 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:46,417 : INFO : entropies processed\n",
      "2020-12-23 02:51:46,418 : INFO : extropies processed\n",
      "2020-12-23 02:51:46,419 : INFO : token count processed\n",
      "2020-12-23 02:51:46,420 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:46,421 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:46,422 : INFO : vocab #2480\n",
      "2020-12-23 02:51:46,423 : INFO : diff #set()\n",
      "2020-12-23 02:51:46,688 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:46,816 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.2339032338264673, 0.44764696377966806], [0.9360674992203712, 0.0639325], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 6.530294129310484, 6.575780749459577, 3.2764414747382684, 3.253852654572215, 0.045486620149093326]]\n",
      "2020-12-23 02:51:46,819 : INFO : Removed 0 and 142 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:46,819 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:46,821 : INFO : built Dictionary(122 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 603 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:51:46,855 : INFO : token count processed\n",
      "2020-12-23 02:51:46,857 : INFO : frequencies processed\n",
      "2020-12-23 02:51:46,985 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:46,986 : INFO : entropies processed\n",
      "2020-12-23 02:51:46,987 : INFO : extropies processed\n",
      "2020-12-23 02:51:46,988 : INFO : token count processed\n",
      "2020-12-23 02:51:46,989 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:46,989 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:46,990 : INFO : vocab #2480\n",
      "2020-12-23 02:51:46,991 : INFO : diff #set()\n",
      "2020-12-23 02:51:47,248 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:47,377 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.189378030445694, 0.45675072376442405], [0.8756842613220215, 0.12431574], [1.0, 1.0], [3.321928094887362, 6.470272233491701, 6.519035309839041, 3.2731650185400225, 3.197107214951679, 0.04876307634734012]]\n",
      "2020-12-23 02:51:47,380 : INFO : Removed 0 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:47,381 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:47,383 : INFO : built Dictionary(123 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 392 corpus positions)\n",
      "2020-12-23 02:51:47,419 : INFO : token count processed\n",
      "2020-12-23 02:51:47,424 : INFO : frequencies processed\n",
      "2020-12-23 02:51:47,552 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:47,552 : INFO : entropies processed\n",
      "2020-12-23 02:51:47,553 : INFO : extropies processed\n",
      "2020-12-23 02:51:47,554 : INFO : token count processed\n",
      "2020-12-23 02:51:47,555 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:47,556 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:47,556 : INFO : vocab #2480\n",
      "2020-12-23 02:51:47,557 : INFO : diff #set()\n",
      "2020-12-23 02:51:47,821 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:47,949 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.2500698387599103, 0.44443064956203043], [0.958522904664278, 0.041477095], [1.0, 1.0], [3.321928094887362, 6.550038223589686, 6.619116548980312, 3.252849769496736, 3.2971884540929497, 0.06907832539062575]]\n",
      "2020-12-23 02:51:47,951 : INFO : Removed 0 and 70 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:47,952 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:47,953 : INFO : built Dictionary(78 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 308 corpus positions)\n",
      "2020-12-23 02:51:47,969 : INFO : token count processed\n",
      "2020-12-23 02:51:47,972 : INFO : frequencies processed\n",
      "2020-12-23 02:51:48,107 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:48,108 : INFO : entropies processed\n",
      "2020-12-23 02:51:48,109 : INFO : extropies processed\n",
      "2020-12-23 02:51:48,110 : INFO : token count processed\n",
      "2020-12-23 02:51:48,111 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:48,112 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:48,113 : INFO : vocab #2480\n",
      "2020-12-23 02:51:48,114 : INFO : diff #set()\n",
      "2020-12-23 02:51:48,372 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:48,500 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.2076159098521204, 0.4529773478879241], [0.9058429151773453, 0.094157085], [1.0, 1.0], [3.321928094887362, 5.860525481261383, 5.946549238742715, 3.235904337406031, 2.6246211438553524, 0.08602375748133184]]\n",
      "2020-12-23 02:51:48,502 : INFO : Removed 0 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:48,503 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:48,505 : INFO : built Dictionary(51 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 346 corpus positions)\n",
      "2020-12-23 02:51:48,524 : INFO : token count processed\n",
      "2020-12-23 02:51:48,526 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:51:48,529 : INFO : frequencies processed\n",
      "2020-12-23 02:51:48,530 : INFO : token count processed\n",
      "2020-12-23 02:51:48,532 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:48,533 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:48,535 : INFO : vocab #2480\n",
      "2020-12-23 02:51:48,536 : INFO : diff #set()\n",
      "2020-12-23 02:51:48,791 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:48,918 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.3133186134778314, 0.43227940767597295], [1.0, 0.0], [nan, nan], [3.321928094887362, 5.945464049777852, 6.032793420096757, 3.234598724568457, 2.710865325209395, 0.08732937031890486]]\n",
      "2020-12-23 02:51:48,921 : INFO : Removed 0 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:48,922 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:48,924 : INFO : built Dictionary(196 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 893 corpus positions)\n",
      "2020-12-23 02:51:48,980 : INFO : token count processed\n",
      "2020-12-23 02:51:48,982 : INFO : frequencies processed\n",
      "2020-12-23 02:51:49,115 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:49,116 : INFO : entropies processed\n",
      "2020-12-23 02:51:49,117 : INFO : extropies processed\n",
      "2020-12-23 02:51:49,118 : INFO : token count processed\n",
      "2020-12-23 02:51:49,119 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:49,120 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:49,121 : INFO : vocab #2480\n",
      "2020-12-23 02:51:49,122 : INFO : diff #set()\n",
      "2020-12-23 02:51:49,381 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:49,507 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.2419952113362152, 0.4460312827358833], [0.9481084868311882, 0.051891513], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 6.811563897304216, 6.847642211346827, 3.2858497808447513, 3.5257141164594645, 0.036078314042610415]]\n",
      "2020-12-23 02:51:49,510 : INFO : Removed 0 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:49,511 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:49,514 : INFO : built Dictionary(216 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 988 corpus positions)\n",
      "2020-12-23 02:51:49,573 : INFO : token count processed\n",
      "2020-12-23 02:51:49,576 : INFO : frequencies processed\n",
      "2020-12-23 02:51:49,706 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:49,707 : INFO : entropies processed\n",
      "2020-12-23 02:51:49,708 : INFO : extropies processed\n",
      "2020-12-23 02:51:49,710 : INFO : token count processed\n",
      "2020-12-23 02:51:49,711 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:49,712 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:49,712 : INFO : vocab #2480\n",
      "2020-12-23 02:51:49,714 : INFO : diff #set()\n",
      "2020-12-23 02:51:49,984 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:50,113 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.2194661414320798, 0.4505587994033393], [0.8899662122130394, 0.11003379], [2.0, 1.2451124978365313], [3.321928094887362, 7.502034948968415, 7.5231807927016465, 3.3007822511541303, 4.201252697814285, 0.021145843733231473]]\n",
      "2020-12-23 02:51:50,116 : INFO : Removed 0 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:50,117 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:50,119 : INFO : built Dictionary(260 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 1561 corpus positions)\n",
      "2020-12-23 02:51:50,196 : INFO : token count processed\n",
      "2020-12-23 02:51:50,201 : INFO : frequencies processed\n",
      "2020-12-23 02:51:50,330 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:50,330 : INFO : entropies processed\n",
      "2020-12-23 02:51:50,331 : INFO : extropies processed\n",
      "2020-12-23 02:51:50,333 : INFO : token count processed\n",
      "2020-12-23 02:51:50,334 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:50,335 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:50,336 : INFO : vocab #2480\n",
      "2020-12-23 02:51:50,337 : INFO : diff #set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:51:50,598 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:50,726 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.19111280537355, 0.4563890994327496], [0.848093718290329, 0.15190628], [2.0, 1.2451124978365313], [3.321928094887362, 7.39180093901977, 7.408035449441803, 3.3056935844653292, 4.086107354554441, 0.0162345104220325]]\n",
      "2020-12-23 02:51:50,729 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:50,730 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:50,731 : INFO : built Dictionary(47 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 116 corpus positions)\n",
      "2020-12-23 02:51:50,746 : INFO : token count processed\n",
      "2020-12-23 02:51:50,749 : INFO : frequencies processed\n",
      "2020-12-23 02:51:50,879 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:50,880 : INFO : entropies processed\n",
      "2020-12-23 02:51:50,881 : INFO : extropies processed\n",
      "2020-12-23 02:51:50,882 : INFO : token count processed\n",
      "2020-12-23 02:51:50,883 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:50,884 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:50,885 : INFO : vocab #2480\n",
      "2020-12-23 02:51:50,886 : INFO : diff #set()\n",
      "2020-12-23 02:51:51,151 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:51,280 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.2337322733512242, 0.4476812247958973], [0.959528736770153, 0.040471263], [0.0, 0.0], [3.321928094887362, 4.927561309677364, 5.186828504525716, 3.0626609000390097, 1.8649004096383535, 0.25926719484835203]]\n",
      "2020-12-23 02:51:51,282 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:51,283 : INFO : built Dictionary(16 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 17 corpus positions)\n",
      "2020-12-23 02:51:51,286 : INFO : token count processed\n",
      "2020-12-23 02:51:51,288 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:51:51,289 : INFO : frequencies processed\n",
      "2020-12-23 02:51:51,290 : INFO : token count processed\n",
      "2020-12-23 02:51:51,291 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:51,292 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:51,292 : INFO : vocab #2480\n",
      "2020-12-23 02:51:51,293 : INFO : diff #set()\n",
      "2020-12-23 02:51:51,553 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:51,682 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2650912088176989, 0.4414833257517988], [0.9443178027868271, 0.055682197], [nan, nan], [3.321928094887362, 2.5216406363433186, 3.9698157824268097, 1.873752948803871, 0.6478876875394475, 1.448175146083491]]\n",
      "2020-12-23 02:51:51,685 : INFO : Removed 0 and 530 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:51,686 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:51,688 : INFO : built Dictionary(334 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 2889 corpus positions)\n",
      "2020-12-23 02:51:51,791 : INFO : token count processed\n",
      "2020-12-23 02:51:51,793 : INFO : frequencies processed\n",
      "2020-12-23 02:51:51,923 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:51,924 : INFO : entropies processed\n",
      "2020-12-23 02:51:51,925 : INFO : extropies processed\n",
      "2020-12-23 02:51:51,928 : INFO : token count processed\n",
      "2020-12-23 02:51:51,929 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:51,930 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:51,931 : INFO : vocab #2480\n",
      "2020-12-23 02:51:51,932 : INFO : diff #set()\n",
      "2020-12-23 02:51:52,192 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:52,321 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.2442199029520231, 0.44558913263562566], [0.9395995326340199, 0.060400467], [2.0, 1.2451124978365313], [3.321928094887362, 7.480007711014331, 7.491669494195761, 3.3102663117059317, 4.169741399308398, 0.011661783181430074]]\n",
      "2020-12-23 02:51:52,324 : INFO : Removed 0 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:52,325 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:52,326 : INFO : built Dictionary(211 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 1038 corpus positions)\n",
      "2020-12-23 02:51:52,381 : INFO : token count processed\n",
      "2020-12-23 02:51:52,386 : INFO : frequencies processed\n",
      "2020-12-23 02:51:52,515 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:52,516 : INFO : entropies processed\n",
      "2020-12-23 02:51:52,517 : INFO : extropies processed\n",
      "2020-12-23 02:51:52,519 : INFO : token count processed\n",
      "2020-12-23 02:51:52,520 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:52,520 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:52,521 : INFO : vocab #2480\n",
      "2020-12-23 02:51:52,522 : INFO : diff #set()\n",
      "2020-12-23 02:51:52,783 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:52,912 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.2242497618335666, 0.44958979749452566], [0.921849399805069, 0.0781506], [2.0, 1.2451124978365313], [3.321928094887362, 7.131331012509435, 7.154036749565869, 3.299222357830927, 3.8321086546785073, 0.02270573705643475]]\n",
      "2020-12-23 02:51:52,915 : INFO : Removed 0 and 166 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:52,916 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:52,917 : INFO : built Dictionary(202 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 857 corpus positions)\n",
      "2020-12-23 02:51:52,966 : INFO : token count processed\n",
      "2020-12-23 02:51:52,971 : INFO : frequencies processed\n",
      "2020-12-23 02:51:53,102 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:53,102 : INFO : entropies processed\n",
      "2020-12-23 02:51:53,103 : INFO : extropies processed\n",
      "2020-12-23 02:51:53,105 : INFO : token count processed\n",
      "2020-12-23 02:51:53,106 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:53,107 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:53,107 : INFO : vocab #2480\n",
      "2020-12-23 02:51:53,108 : INFO : diff #set()\n",
      "2020-12-23 02:51:53,370 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:53,498 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.2432929162574649, 0.44577326159809844], [0.9462375640869141, 0.053762436], [1.0, 1.0], [3.321928094887362, 7.203742744794778, 7.237128037901265, 3.288542801780876, 3.9151999430139024, 0.033385293106486635]]\n",
      "2020-12-23 02:51:53,500 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:53,500 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:53,501 : INFO : built Dictionary(61 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 189 corpus positions)\n",
      "2020-12-23 02:51:53,523 : INFO : token count processed\n",
      "2020-12-23 02:51:53,525 : INFO : frequencies processed\n",
      "2020-12-23 02:51:53,660 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:53,660 : INFO : entropies processed\n",
      "2020-12-23 02:51:53,661 : INFO : extropies processed\n",
      "2020-12-23 02:51:53,663 : INFO : token count processed\n",
      "2020-12-23 02:51:53,665 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:53,666 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:53,667 : INFO : vocab #2480\n",
      "2020-12-23 02:51:53,669 : INFO : diff #set()\n",
      "2020-12-23 02:51:53,935 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:54,063 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.2201336736920827, 0.4504233289417208], [0.9433185160160065, 0.056681484], [0.0, 0.0], [3.321928094887362, 5.195502554608948, 5.372567150647753, 3.144863498848557, 2.050639055760391, 0.17706459603880464]]\n",
      "2020-12-23 02:51:54,066 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:54,067 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:54,068 : INFO : built Dictionary(66 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 250 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:51:54,086 : INFO : token count processed\n",
      "2020-12-23 02:51:54,091 : INFO : frequencies processed\n",
      "2020-12-23 02:51:54,225 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:54,226 : INFO : entropies processed\n",
      "2020-12-23 02:51:54,227 : INFO : extropies processed\n",
      "2020-12-23 02:51:54,228 : INFO : token count processed\n",
      "2020-12-23 02:51:54,229 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:54,230 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:54,230 : INFO : vocab #2480\n",
      "2020-12-23 02:51:54,232 : INFO : diff #set()\n",
      "2020-12-23 02:51:54,500 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:54,628 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.2115499213571423, 0.45217156996679453], [0.9321879968047142, 0.067812], [1.0, 1.0], [3.321928094887362, 5.32027245610305, 5.456204935928593, 3.1859956150618185, 2.1342768410412307, 0.13593247982554324]]\n",
      "2020-12-23 02:51:54,631 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:54,632 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:54,634 : INFO : built Dictionary(166 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 490 corpus positions)\n",
      "2020-12-23 02:51:54,682 : INFO : token count processed\n",
      "2020-12-23 02:51:54,685 : INFO : frequencies processed\n",
      "2020-12-23 02:51:54,814 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:54,814 : INFO : entropies processed\n",
      "2020-12-23 02:51:54,815 : INFO : extropies processed\n",
      "2020-12-23 02:51:54,816 : INFO : token count processed\n",
      "2020-12-23 02:51:54,817 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:54,818 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:54,819 : INFO : vocab #2480\n",
      "2020-12-23 02:51:54,820 : INFO : diff #set()\n",
      "2020-12-23 02:51:55,086 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:55,214 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.2482456819212924, 0.4447912468113475], [0.9606434479355812, 0.039356552], [1.0, 1.0], [3.321928094887362, 6.898202761357263, 6.95207087079183, 3.268059985452795, 3.6301427759044675, 0.05386810943456677]]\n",
      "2020-12-23 02:51:55,216 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:55,217 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:55,219 : INFO : built Dictionary(130 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 512 corpus positions)\n",
      "2020-12-23 02:51:55,257 : INFO : token count processed\n",
      "2020-12-23 02:51:55,259 : INFO : frequencies processed\n",
      "2020-12-23 02:51:55,389 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:55,390 : INFO : entropies processed\n",
      "2020-12-23 02:51:55,390 : INFO : extropies processed\n",
      "2020-12-23 02:51:55,391 : INFO : token count processed\n",
      "2020-12-23 02:51:55,392 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:55,394 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:55,394 : INFO : vocab #2480\n",
      "2020-12-23 02:51:55,395 : INFO : diff #set()\n",
      "2020-12-23 02:51:55,659 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:55,787 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.1850065285391682, 0.45766453643896926], [0.8557832837104797, 0.14421672], [1.0, 1.0], [3.321928094887362, 6.388500481644799, 6.44485600429801, 3.2655725722341513, 3.1229279094106475, 0.05635552265321042]]\n",
      "2020-12-23 02:51:55,789 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:55,790 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:55,791 : INFO : built Dictionary(52 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 176 corpus positions)\n",
      "2020-12-23 02:51:55,801 : INFO : token count processed\n",
      "2020-12-23 02:51:55,803 : INFO : frequencies processed\n",
      "2020-12-23 02:51:55,931 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:55,932 : INFO : entropies processed\n",
      "2020-12-23 02:51:55,933 : INFO : extropies processed\n",
      "2020-12-23 02:51:55,934 : INFO : token count processed\n",
      "2020-12-23 02:51:55,935 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:55,935 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:55,936 : INFO : vocab #2480\n",
      "2020-12-23 02:51:55,937 : INFO : diff #set()\n",
      "2020-12-23 02:51:56,200 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:56,328 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.2238789453112864, 0.44966476350178564], [0.9522388316690922, 0.04776117], [0.0, 0.0], [3.321928094887362, 4.8191513650620195, 5.025907960208988, 3.115171499740393, 1.7039798653216258, 0.20675659514696854]]\n",
      "2020-12-23 02:51:56,331 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:56,332 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:56,332 : INFO : built Dictionary(57 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 231 corpus positions)\n",
      "2020-12-23 02:51:56,343 : INFO : token count processed\n",
      "2020-12-23 02:51:56,346 : INFO : frequencies processed\n",
      "2020-12-23 02:51:56,475 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:56,476 : INFO : entropies processed\n",
      "2020-12-23 02:51:56,476 : INFO : extropies processed\n",
      "2020-12-23 02:51:56,478 : INFO : token count processed\n",
      "2020-12-23 02:51:56,479 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:56,480 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:56,480 : INFO : vocab #2480\n",
      "2020-12-23 02:51:56,481 : INFO : diff #set()\n",
      "2020-12-23 02:51:56,739 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:56,868 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.210562257162407, 0.4523735971515464], [0.9367354512214661, 0.06326455], [0.0, 0.0], [3.321928094887362, 5.062480936779194, 5.2242789288442815, 3.160130102822274, 1.9023508339569193, 0.16179799206508783]]\n",
      "2020-12-23 02:51:56,871 : INFO : Removed 0 and 223 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:56,872 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:56,873 : INFO : built Dictionary(246 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 1785 corpus positions)\n",
      "2020-12-23 02:51:56,945 : INFO : token count processed\n",
      "2020-12-23 02:51:56,947 : INFO : frequencies processed\n",
      "2020-12-23 02:51:57,074 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:57,075 : INFO : entropies processed\n",
      "2020-12-23 02:51:57,076 : INFO : extropies processed\n",
      "2020-12-23 02:51:57,077 : INFO : token count processed\n",
      "2020-12-23 02:51:57,078 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:57,079 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:57,079 : INFO : vocab #2480\n",
      "2020-12-23 02:51:57,081 : INFO : diff #set()\n",
      "2020-12-23 02:51:57,343 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:57,470 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.2427259151432892, 0.445885961029754], [0.9385647438466549, 0.061435256], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 7.185085743102134, 7.204225704474975, 3.3027881335145217, 3.882297609587613, 0.019139961372840908]]\n",
      "2020-12-23 02:51:57,473 : INFO : Removed 0 and 148 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:57,474 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:57,476 : INFO : built Dictionary(163 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 760 corpus positions)\n",
      "2020-12-23 02:51:57,521 : INFO : token count processed\n",
      "2020-12-23 02:51:57,523 : INFO : frequencies processed\n",
      "2020-12-23 02:51:57,650 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:57,651 : INFO : entropies processed\n",
      "2020-12-23 02:51:57,652 : INFO : extropies processed\n",
      "2020-12-23 02:51:57,653 : INFO : token count processed\n",
      "2020-12-23 02:51:57,654 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:57,655 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:57,656 : INFO : vocab #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:51:57,657 : INFO : diff #set()\n",
      "2020-12-23 02:51:57,915 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:58,043 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.2269422578955298, 0.4490462186231107], [0.9327217191457748, 0.06727828], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 6.591225336124281, 6.630163075257994, 3.2829903557536495, 3.308234980370632, 0.03893773913371312]]\n",
      "2020-12-23 02:51:58,045 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:58,046 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:58,048 : INFO : built Dictionary(44 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 98 corpus positions)\n",
      "2020-12-23 02:51:58,061 : INFO : token count processed\n",
      "2020-12-23 02:51:58,063 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:51:58,064 : INFO : frequencies processed\n",
      "2020-12-23 02:51:58,065 : INFO : token count processed\n",
      "2020-12-23 02:51:58,066 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:58,067 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:58,068 : INFO : vocab #2480\n",
      "2020-12-23 02:51:58,069 : INFO : diff #set()\n",
      "2020-12-23 02:51:58,336 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:58,466 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.266678349458975, 0.44117419670006824], [0.997346448013559, 0.002653552], [nan, nan], [3.321928094887362, 4.7032114441396695, 5.035877258184877, 2.989262280842155, 1.713949163297515, 0.3326658140452077]]\n",
      "2020-12-23 02:51:58,468 : INFO : Removed 0 and 9 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:58,469 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:58,470 : INFO : built Dictionary(121 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 338 corpus positions)\n",
      "2020-12-23 02:51:58,498 : INFO : token count processed\n",
      "2020-12-23 02:51:58,500 : INFO : frequencies processed\n",
      "2020-12-23 02:51:58,638 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:58,639 : INFO : entropies processed\n",
      "2020-12-23 02:51:58,640 : INFO : extropies processed\n",
      "2020-12-23 02:51:58,641 : INFO : token count processed\n",
      "2020-12-23 02:51:58,642 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:58,644 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:58,645 : INFO : vocab #2480\n",
      "2020-12-23 02:51:58,646 : INFO : diff #set()\n",
      "2020-12-23 02:51:58,916 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:59,046 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.2467854447645739, 0.44508032679764115], [0.9572965800762177, 0.04270342], [0.0, 0.0], [3.321928094887362, 6.14228447828618, 6.239035130531343, 3.225177442642199, 2.917107035643981, 0.09675065224516377]]\n",
      "2020-12-23 02:51:59,049 : INFO : Removed 0 and 184 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:59,049 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:59,051 : INFO : built Dictionary(260 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 1128 corpus positions)\n",
      "2020-12-23 02:51:59,129 : INFO : token count processed\n",
      "2020-12-23 02:51:59,136 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:51:59,140 : INFO : frequencies processed\n",
      "2020-12-23 02:51:59,141 : INFO : token count processed\n",
      "2020-12-23 02:51:59,144 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:59,145 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:59,146 : INFO : vocab #2480\n",
      "2020-12-23 02:51:59,146 : INFO : diff #set()\n",
      "2020-12-23 02:51:59,399 : INFO : alphabet #2480\n",
      "2020-12-23 02:51:59,526 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.2690182782389867, 0.4407192350940921], [0.9838270246982574, 0.016172975], [nan, nan], [3.321928094887362, 7.450178124335845, 7.483292124514129, 3.2888140947090783, 4.1613640296267675, 0.033114000178284364]]\n",
      "2020-12-23 02:51:59,528 : INFO : Removed 0 and 16 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:51:59,528 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:51:59,529 : INFO : built Dictionary(61 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 176 corpus positions)\n",
      "2020-12-23 02:51:59,542 : INFO : token count processed\n",
      "2020-12-23 02:51:59,546 : INFO : frequencies processed\n",
      "2020-12-23 02:51:59,677 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:51:59,678 : INFO : entropies processed\n",
      "2020-12-23 02:51:59,678 : INFO : extropies processed\n",
      "2020-12-23 02:51:59,680 : INFO : token count processed\n",
      "2020-12-23 02:51:59,680 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:51:59,681 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:51:59,682 : INFO : vocab #2480\n",
      "2020-12-23 02:51:59,683 : INFO : diff #set()\n",
      "2020-12-23 02:51:59,941 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:00,070 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.2153089966664496, 0.4514042968745123], [0.9439435787498951, 0.05605642], [0.0, 0.0], [3.321928094887362, 5.20665021947654, 5.382097948525798, 3.1464803658381033, 2.060169853638436, 0.17544772904925843]]\n",
      "2020-12-23 02:52:00,072 : INFO : Removed 0 and 89 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:00,073 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:00,074 : INFO : built Dictionary(129 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 514 corpus positions)\n",
      "2020-12-23 02:52:00,110 : INFO : token count processed\n",
      "2020-12-23 02:52:00,113 : INFO : frequencies processed\n",
      "2020-12-23 02:52:00,240 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:00,241 : INFO : entropies processed\n",
      "2020-12-23 02:52:00,242 : INFO : extropies processed\n",
      "2020-12-23 02:52:00,243 : INFO : token count processed\n",
      "2020-12-23 02:52:00,244 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:00,244 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:00,245 : INFO : vocab #2480\n",
      "2020-12-23 02:52:00,246 : INFO : diff #set()\n",
      "2020-12-23 02:52:00,512 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:00,639 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.2517589685917785, 0.44409726527053084], [0.9686291627585888, 0.031370837], [1.0, 1.0], [3.321928094887362, 6.524718477352, 6.58203997205213, 3.264606600187231, 3.260111877164768, 0.05732149470013059]]\n",
      "2020-12-23 02:52:00,642 : INFO : Removed 0 and 8 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:00,642 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:00,643 : INFO : built Dictionary(67 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 221 corpus positions)\n",
      "2020-12-23 02:52:00,657 : INFO : token count processed\n",
      "2020-12-23 02:52:00,659 : INFO : frequencies processed\n",
      "2020-12-23 02:52:00,787 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:00,788 : INFO : entropies processed\n",
      "2020-12-23 02:52:00,788 : INFO : extropies processed\n",
      "2020-12-23 02:52:00,790 : INFO : token count processed\n",
      "2020-12-23 02:52:00,790 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:00,791 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:00,792 : INFO : vocab #2480\n",
      "2020-12-23 02:52:00,793 : INFO : diff #set()\n",
      "2020-12-23 02:52:01,051 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:01,180 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.2404190991790343, 0.446345061228247], [0.9625675454735756, 0.037432455], [0.0, 0.0], [3.321928094887362, 5.321859380715434, 5.476359699861718, 3.1674277757410776, 2.154431604974356, 0.15450031914628415]]\n",
      "2020-12-23 02:52:01,182 : INFO : Removed 0 and 119 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:01,183 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:52:01,186 : INFO : built Dictionary(144 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 839 corpus positions)\n",
      "2020-12-23 02:52:01,225 : INFO : token count processed\n",
      "2020-12-23 02:52:01,227 : INFO : frequencies processed\n",
      "2020-12-23 02:52:01,355 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:01,355 : INFO : entropies processed\n",
      "2020-12-23 02:52:01,356 : INFO : extropies processed\n",
      "2020-12-23 02:52:01,358 : INFO : token count processed\n",
      "2020-12-23 02:52:01,359 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:01,360 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:01,360 : INFO : vocab #2480\n",
      "2020-12-23 02:52:01,361 : INFO : diff #set()\n",
      "2020-12-23 02:52:01,623 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:01,751 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.2402985265384314, 0.4463690834743962], [0.9498302042484283, 0.050169796], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 6.500767808767801, 6.539429136747014, 3.283266766908149, 3.2175010418596517, 0.038661327979212956]]\n",
      "2020-12-23 02:52:01,753 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:01,754 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:01,755 : INFO : built Dictionary(39 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 61 corpus positions)\n",
      "2020-12-23 02:52:01,768 : INFO : token count processed\n",
      "2020-12-23 02:52:01,771 : INFO : frequencies processed\n",
      "2020-12-23 02:52:01,901 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:01,902 : INFO : entropies processed\n",
      "2020-12-23 02:52:01,903 : INFO : extropies processed\n",
      "2020-12-23 02:52:01,904 : INFO : token count processed\n",
      "2020-12-23 02:52:01,905 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:01,906 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:01,907 : INFO : vocab #2480\n",
      "2020-12-23 02:52:01,908 : INFO : diff #set()\n",
      "2020-12-23 02:52:02,174 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:02,302 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.2336306839872784, 0.44770158610773075], [0.935846246778965, 0.06415375], [0.0, 0.0], [3.321928094887362, 4.736228843383063, 5.113245254138203, 2.9449116841322223, 1.7913171592508408, 0.37701641075514036]]\n",
      "2020-12-23 02:52:02,304 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:02,305 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:02,306 : INFO : built Dictionary(102 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 402 corpus positions)\n",
      "2020-12-23 02:52:02,334 : INFO : token count processed\n",
      "2020-12-23 02:52:02,336 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:52:02,337 : INFO : frequencies processed\n",
      "2020-12-23 02:52:02,338 : INFO : token count processed\n",
      "2020-12-23 02:52:02,339 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:02,340 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:02,341 : INFO : vocab #2480\n",
      "2020-12-23 02:52:02,342 : INFO : diff #set()\n",
      "2020-12-23 02:52:02,595 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:02,721 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.273076823989394, 0.43993233728235226], [0.986612412147224, 0.013387588], [nan, nan], [3.321928094887362, 5.788442787590127, 5.892702673392078, 3.217668209085412, 2.570774578504716, 0.10425988580195078]]\n",
      "2020-12-23 02:52:02,724 : INFO : Removed 0 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:02,725 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:02,725 : INFO : built Dictionary(58 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 88 corpus positions)\n",
      "2020-12-23 02:52:02,736 : INFO : token count processed\n",
      "2020-12-23 02:52:02,739 : INFO : frequencies processed\n",
      "2020-12-23 02:52:02,868 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:02,869 : INFO : entropies processed\n",
      "2020-12-23 02:52:02,870 : INFO : extropies processed\n",
      "2020-12-23 02:52:02,871 : INFO : token count processed\n",
      "2020-12-23 02:52:02,872 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:02,873 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:02,874 : INFO : vocab #2480\n",
      "2020-12-23 02:52:02,875 : INFO : diff #set()\n",
      "2020-12-23 02:52:03,132 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:03,260 : INFO : Computed distances or similarities ('280', 'sacp-python-common/setup.py')[[1.2450316394828076, 0.4454280208854304], [0.9724751729518175, 0.027524827], [0.0, 0.0], [3.321928094887362, 5.370004292053436, 5.619755841242241, 3.072176545698558, 2.2978277463548786, 0.2497515491888045]]\n",
      "2020-12-23 02:52:03,263 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:03,263 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:03,264 : INFO : built Dictionary(82 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 333 corpus positions)\n",
      "2020-12-23 02:52:03,283 : INFO : token count processed\n",
      "2020-12-23 02:52:03,286 : INFO : frequencies processed\n",
      "2020-12-23 02:52:03,417 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:03,418 : INFO : entropies processed\n",
      "2020-12-23 02:52:03,419 : INFO : extropies processed\n",
      "2020-12-23 02:52:03,420 : INFO : token count processed\n",
      "2020-12-23 02:52:03,421 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:03,422 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:03,423 : INFO : vocab #2480\n",
      "2020-12-23 02:52:03,424 : INFO : diff #set()\n",
      "2020-12-23 02:52:03,691 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:03,823 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.225989745653408, 0.4492383677654652], [0.9394196830689907, 0.060580317], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 5.695663584743922, 5.785076598518681, 3.232515081112603, 2.4631485036313188, 0.08941301377475863]]\n",
      "2020-12-23 02:52:03,826 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:03,827 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:03,827 : INFO : built Dictionary(45 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 108 corpus positions)\n",
      "2020-12-23 02:52:03,836 : INFO : token count processed\n",
      "2020-12-23 02:52:03,839 : INFO : frequencies processed\n",
      "2020-12-23 02:52:03,966 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:03,967 : INFO : entropies processed\n",
      "2020-12-23 02:52:03,968 : INFO : extropies processed\n",
      "2020-12-23 02:52:03,970 : INFO : token count processed\n",
      "2020-12-23 02:52:03,971 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:03,973 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:03,973 : INFO : vocab #2480\n",
      "2020-12-23 02:52:03,976 : INFO : diff #set()\n",
      "2020-12-23 02:52:04,234 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:04,362 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.2285218701977698, 0.44872792740923617], [0.9427164979279041, 0.057283502], [0.0, 0.0], [3.321928094887362, 4.9004417692112465, 5.161355235486335, 3.0610146286122735, 1.8394271405989726, 0.26091346627508827]]\n",
      "2020-12-23 02:52:04,364 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:04,365 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:04,366 : INFO : built Dictionary(43 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 115 corpus positions)\n",
      "2020-12-23 02:52:04,374 : INFO : token count processed\n",
      "2020-12-23 02:52:04,376 : INFO : frequencies processed\n",
      "2020-12-23 02:52:04,504 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:04,505 : INFO : entropies processed\n",
      "2020-12-23 02:52:04,506 : INFO : extropies processed\n",
      "2020-12-23 02:52:04,507 : INFO : token count processed\n",
      "2020-12-23 02:52:04,508 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:04,509 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:04,509 : INFO : vocab #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:52:04,510 : INFO : diff #set()\n",
      "2020-12-23 02:52:04,770 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:04,899 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.2564940440542538, 0.44316536205134194], [0.9734923131763935, 0.026507687], [0.0, 0.0], [3.321928094887362, 4.778624108914332, 5.050513100437517, 3.0500391033641776, 1.7285850055501544, 0.27188899152318413]]\n",
      "2020-12-23 02:52:04,902 : INFO : Removed 0 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:04,902 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:04,903 : INFO : built Dictionary(44 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 146 corpus positions)\n",
      "2020-12-23 02:52:04,911 : INFO : token count processed\n",
      "2020-12-23 02:52:04,914 : INFO : frequencies processed\n",
      "2020-12-23 02:52:05,042 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:05,042 : INFO : entropies processed\n",
      "2020-12-23 02:52:05,043 : INFO : extropies processed\n",
      "2020-12-23 02:52:05,045 : INFO : token count processed\n",
      "2020-12-23 02:52:05,047 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:05,048 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:05,049 : INFO : vocab #2480\n",
      "2020-12-23 02:52:05,050 : INFO : diff #set()\n",
      "2020-12-23 02:52:05,314 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:05,442 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.2530701243628473, 0.4438388264913828], [0.9789031650871038, 0.021096835], [0.0, 0.0], [3.321928094887362, 4.773880192225086, 5.009338031102545, 3.0864702560099024, 1.6874099362151829, 0.2354578388774593]]\n",
      "2020-12-23 02:52:05,446 : INFO : Removed 0 and 346 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:05,446 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:05,448 : INFO : built Dictionary(153 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 1971 corpus positions)\n",
      "2020-12-23 02:52:05,484 : INFO : token count processed\n",
      "2020-12-23 02:52:05,487 : INFO : frequencies processed\n",
      "2020-12-23 02:52:05,615 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:05,616 : INFO : entropies processed\n",
      "2020-12-23 02:52:05,616 : INFO : extropies processed\n",
      "2020-12-23 02:52:05,620 : INFO : token count processed\n",
      "2020-12-23 02:52:05,621 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:05,622 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:05,623 : INFO : vocab #2480\n",
      "2020-12-23 02:52:05,624 : INFO : diff #set()\n",
      "2020-12-23 02:52:05,887 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:06,015 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.2364470013038118, 0.4471378035862314], [0.9249761030077934, 0.0750239], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 6.620773041953877, 6.639008253925084, 3.3036928829161543, 3.317080159037722, 0.018235211971207477]]\n",
      "2020-12-23 02:52:06,017 : INFO : Removed 0 and 132 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:06,018 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:06,020 : INFO : built Dictionary(86 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 534 corpus positions)\n",
      "2020-12-23 02:52:06,047 : INFO : token count processed\n",
      "2020-12-23 02:52:06,049 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:52:06,051 : INFO : frequencies processed\n",
      "2020-12-23 02:52:06,052 : INFO : token count processed\n",
      "2020-12-23 02:52:06,054 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:06,056 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:06,058 : INFO : vocab #2480\n",
      "2020-12-23 02:52:06,059 : INFO : diff #set()\n",
      "2020-12-23 02:52:06,316 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:06,445 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.2685365844146919, 0.44081281601108113], [0.9922157539986074, 0.007784246], [nan, nan], [3.321928094887362, 5.828370634755606, 5.90318770846798, 3.2471110211749874, 2.581259613580618, 0.07481707371237434]]\n",
      "2020-12-23 02:52:06,447 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:06,448 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:06,450 : INFO : built Dictionary(85 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 297 corpus positions)\n",
      "2020-12-23 02:52:06,474 : INFO : token count processed\n",
      "2020-12-23 02:52:06,477 : INFO : frequencies processed\n",
      "2020-12-23 02:52:06,604 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:06,605 : INFO : entropies processed\n",
      "2020-12-23 02:52:06,605 : INFO : extropies processed\n",
      "2020-12-23 02:52:06,607 : INFO : token count processed\n",
      "2020-12-23 02:52:06,608 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:06,610 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:06,610 : INFO : vocab #2480\n",
      "2020-12-23 02:52:06,612 : INFO : diff #set()\n",
      "2020-12-23 02:52:06,875 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:07,004 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.2057353074175456, 0.45336355483686336], [0.9085961803793907, 0.09140382], [1.0, 1.0], [3.321928094887362, 5.774409284925443, 5.873664544581309, 3.2226728352314966, 2.5517364496939465, 0.09925525965586601]]\n",
      "2020-12-23 02:52:07,006 : INFO : Removed 0 and 51 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:07,007 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:07,008 : INFO : built Dictionary(97 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 346 corpus positions)\n",
      "2020-12-23 02:52:07,033 : INFO : token count processed\n",
      "2020-12-23 02:52:07,035 : INFO : frequencies processed\n",
      "2020-12-23 02:52:07,163 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:07,164 : INFO : entropies processed\n",
      "2020-12-23 02:52:07,165 : INFO : extropies processed\n",
      "2020-12-23 02:52:07,166 : INFO : token count processed\n",
      "2020-12-23 02:52:07,167 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:07,168 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:07,169 : INFO : vocab #2480\n",
      "2020-12-23 02:52:07,170 : INFO : diff #set()\n",
      "2020-12-23 02:52:07,442 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:07,571 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.2495668853600468, 0.44453001442539836], [0.9602096937596798, 0.039790306], [0.0, 0.0], [3.321928094887362, 5.977819040873918, 6.0736391409418395, 3.22610799481944, 2.7517110460544774, 0.09582010006792174]]\n",
      "2020-12-23 02:52:07,574 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:07,574 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:07,576 : INFO : built Dictionary(78 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 293 corpus positions)\n",
      "2020-12-23 02:52:07,598 : INFO : token count processed\n",
      "2020-12-23 02:52:07,600 : INFO : frequencies processed\n",
      "2020-12-23 02:52:07,731 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:07,732 : INFO : entropies processed\n",
      "2020-12-23 02:52:07,733 : INFO : extropies processed\n",
      "2020-12-23 02:52:07,735 : INFO : token count processed\n",
      "2020-12-23 02:52:07,736 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:07,737 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:07,738 : INFO : vocab #2480\n",
      "2020-12-23 02:52:07,740 : INFO : diff #set()\n",
      "2020-12-23 02:52:08,014 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:08,142 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.262186558927472, 0.4420501908004047], [0.9851758293807507, 0.014824171], [0.0, 0.0], [3.321928094887362, 5.901812829596593, 6.012274578044112, 3.2114663464398436, 2.6903464831567496, 0.11046174844751899]]\n",
      "2020-12-23 02:52:08,145 : INFO : Removed 0 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:08,146 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:52:08,147 : INFO : built Dictionary(79 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 293 corpus positions)\n",
      "2020-12-23 02:52:08,165 : INFO : token count processed\n",
      "2020-12-23 02:52:08,170 : INFO : frequencies processed\n",
      "2020-12-23 02:52:08,301 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:08,302 : INFO : entropies processed\n",
      "2020-12-23 02:52:08,302 : INFO : extropies processed\n",
      "2020-12-23 02:52:08,303 : INFO : token count processed\n",
      "2020-12-23 02:52:08,304 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:08,305 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:08,306 : INFO : vocab #2480\n",
      "2020-12-23 02:52:08,306 : INFO : diff #set()\n",
      "2020-12-23 02:52:08,560 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:08,687 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.2276374675670887, 0.4489060785515287], [0.9366175457835197, 0.063382454], [1.0, 1.0], [3.321928094887362, 5.643202320803383, 5.753289003628258, 3.211841412062486, 2.431360908740896, 0.11008668282487566]]\n",
      "2020-12-23 02:52:08,689 : INFO : Removed 0 and 30 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:08,690 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:08,692 : INFO : built Dictionary(94 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 327 corpus positions)\n",
      "2020-12-23 02:52:08,717 : INFO : token count processed\n",
      "2020-12-23 02:52:08,719 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:52:08,721 : INFO : frequencies processed\n",
      "2020-12-23 02:52:08,723 : INFO : token count processed\n",
      "2020-12-23 02:52:08,725 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:08,726 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:08,728 : INFO : vocab #2480\n",
      "2020-12-23 02:52:08,728 : INFO : diff #set()\n",
      "2020-12-23 02:52:08,982 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:09,108 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.2596217273977754, 0.44255194923781316], [0.9765259269624949, 0.023474073], [nan, nan], [3.321928094887362, 5.925214310725336, 6.036611056681861, 3.2105313489308367, 2.7146829617944985, 0.11139674595652505]]\n",
      "2020-12-23 02:52:09,112 : INFO : Removed 0 and 293 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:09,112 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:09,114 : INFO : built Dictionary(164 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 1722 corpus positions)\n",
      "2020-12-23 02:52:09,159 : INFO : token count processed\n",
      "2020-12-23 02:52:09,161 : INFO : frequencies processed\n",
      "2020-12-23 02:52:09,298 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:09,299 : INFO : entropies processed\n",
      "2020-12-23 02:52:09,299 : INFO : extropies processed\n",
      "2020-12-23 02:52:09,301 : INFO : token count processed\n",
      "2020-12-23 02:52:09,302 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:09,303 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:09,304 : INFO : vocab #2480\n",
      "2020-12-23 02:52:09,305 : INFO : diff #set()\n",
      "2020-12-23 02:52:09,564 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:09,691 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.231448152978221, 0.44813947331258475], [0.9178919717669487, 0.08210803], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 6.551685682764175, 6.571948990071452, 3.3016647875800844, 3.25002089518409, 0.020263307307277323]]\n",
      "2020-12-23 02:52:09,694 : INFO : Removed 0 and 88 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:09,695 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:09,697 : INFO : built Dictionary(140 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 563 corpus positions)\n",
      "2020-12-23 02:52:09,733 : INFO : token count processed\n",
      "2020-12-23 02:52:09,736 : INFO : frequencies processed\n",
      "2020-12-23 02:52:09,865 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:09,866 : INFO : entropies processed\n",
      "2020-12-23 02:52:09,866 : INFO : extropies processed\n",
      "2020-12-23 02:52:09,867 : INFO : token count processed\n",
      "2020-12-23 02:52:09,868 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:09,869 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:09,870 : INFO : vocab #2480\n",
      "2020-12-23 02:52:09,871 : INFO : diff #set()\n",
      "2020-12-23 02:52:10,128 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:10,254 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.2147135499279786, 0.4515256612000769], [0.9096782952547073, 0.090321705], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 6.642985062562557, 6.689099546590739, 3.2758136108591804, 3.3671714517033764, 0.0461144840281813]]\n",
      "2020-12-23 02:52:10,257 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:10,258 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:10,259 : INFO : built Dictionary(55 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 158 corpus positions)\n",
      "2020-12-23 02:52:10,275 : INFO : token count processed\n",
      "2020-12-23 02:52:10,280 : INFO : frequencies processed\n",
      "2020-12-23 02:52:10,415 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:10,416 : INFO : entropies processed\n",
      "2020-12-23 02:52:10,417 : INFO : extropies processed\n",
      "2020-12-23 02:52:10,419 : INFO : token count processed\n",
      "2020-12-23 02:52:10,421 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:10,422 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:10,423 : INFO : vocab #2480\n",
      "2020-12-23 02:52:10,424 : INFO : diff #set()\n",
      "2020-12-23 02:52:10,682 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:10,810 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.248849469332063, 0.4446718260324524], [0.9704517144709826, 0.029548286], [0.0, 0.0], [3.321928094887362, 5.2461980344571995, 5.4383199501241535, 3.1298061792204077, 2.1163918552367913, 0.19212191566695402]]\n",
      "2020-12-23 02:52:10,813 : INFO : Removed 0 and 23 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:10,813 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:10,814 : INFO : built Dictionary(77 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 218 corpus positions)\n",
      "2020-12-23 02:52:10,833 : INFO : token count processed\n",
      "2020-12-23 02:52:10,837 : INFO : frequencies processed\n",
      "2020-12-23 02:52:10,976 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:10,977 : INFO : entropies processed\n",
      "2020-12-23 02:52:10,977 : INFO : extropies processed\n",
      "2020-12-23 02:52:10,979 : INFO : token count processed\n",
      "2020-12-23 02:52:10,980 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:10,981 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:10,982 : INFO : vocab #2480\n",
      "2020-12-23 02:52:10,983 : INFO : diff #set()\n",
      "2020-12-23 02:52:11,253 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:11,381 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/test_auth_utility.py')[[1.2339751621795936, 0.44763255067900715], [0.9396333545446396, 0.060366645], [0.0, 0.0], [3.321928094887362, 5.903090303960449, 6.028904676967989, 3.196113721879823, 2.706976582080627, 0.12581437300753961]]\n",
      "2020-12-23 02:52:11,384 : INFO : Removed 0 and 442 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:11,385 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:11,387 : INFO : built Dictionary(110 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 1217 corpus positions)\n",
      "2020-12-23 02:52:11,419 : INFO : token count processed\n",
      "2020-12-23 02:52:11,427 : INFO : frequencies processed\n",
      "2020-12-23 02:52:11,556 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:11,556 : INFO : entropies processed\n",
      "2020-12-23 02:52:11,557 : INFO : extropies processed\n",
      "2020-12-23 02:52:11,558 : INFO : token count processed\n",
      "2020-12-23 02:52:11,559 : INFO : alphabet_source #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:52:11,560 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:11,561 : INFO : vocab #2480\n",
      "2020-12-23 02:52:11,562 : INFO : diff #set()\n",
      "2020-12-23 02:52:11,828 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:11,957 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.266326795950498, 0.4412426318158586], [0.9790785554796457, 0.020921445], [1.0, 1.0], [3.321928094887362, 6.16659449033757, 6.198409934443523, 3.290112650781408, 2.876481839556161, 0.031815444105953716]]\n",
      "2020-12-23 02:52:11,960 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:11,961 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:11,962 : INFO : built Dictionary(70 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 252 corpus positions)\n",
      "2020-12-23 02:52:11,997 : INFO : token count processed\n",
      "2020-12-23 02:52:12,002 : INFO : frequencies processed\n",
      "2020-12-23 02:52:12,128 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:12,129 : INFO : entropies processed\n",
      "2020-12-23 02:52:12,130 : INFO : extropies processed\n",
      "2020-12-23 02:52:12,132 : INFO : token count processed\n",
      "2020-12-23 02:52:12,133 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:12,134 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:12,135 : INFO : vocab #2480\n",
      "2020-12-23 02:52:12,136 : INFO : diff #set()\n",
      "2020-12-23 02:52:12,397 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:12,525 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.1959318090484172, 0.45538754704470485], [0.9017251133918762, 0.09827489], [1.0, 1.0], [3.321928094887362, 5.906856253399655, 6.001190251829143, 3.2275940964578753, 2.6792621569417805, 0.09433399842948731]]\n",
      "2020-12-23 02:52:12,528 : INFO : Removed 0 and 68 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:12,529 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:12,530 : INFO : built Dictionary(85 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 251 corpus positions)\n",
      "2020-12-23 02:52:12,561 : INFO : token count processed\n",
      "2020-12-23 02:52:12,563 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:52:12,564 : INFO : frequencies processed\n",
      "2020-12-23 02:52:12,565 : INFO : token count processed\n",
      "2020-12-23 02:52:12,566 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:12,567 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:12,568 : INFO : vocab #2480\n",
      "2020-12-23 02:52:12,569 : INFO : diff #set()\n",
      "2020-12-23 02:52:12,822 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:12,950 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.258702807205519, 0.4427319950238191], [0.9828587267547846, 0.017141273], [nan, nan], [3.321928094887362, 5.965115449163356, 6.083364332163069, 3.203679211887649, 2.761436237275707, 0.11824888299971281]]\n",
      "2020-12-23 02:52:12,952 : INFO : Removed 0 and 13 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:12,953 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:12,954 : INFO : built Dictionary(91 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 365 corpus positions)\n",
      "2020-12-23 02:52:12,974 : INFO : token count processed\n",
      "2020-12-23 02:52:12,976 : INFO : frequencies processed\n",
      "2020-12-23 02:52:13,104 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:13,105 : INFO : entropies processed\n",
      "2020-12-23 02:52:13,105 : INFO : extropies processed\n",
      "2020-12-23 02:52:13,106 : INFO : token count processed\n",
      "2020-12-23 02:52:13,107 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:13,108 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:13,108 : INFO : vocab #2480\n",
      "2020-12-23 02:52:13,109 : INFO : diff #set()\n",
      "2020-12-23 02:52:13,370 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:13,498 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.246650853756643, 0.4451069904021321], [0.9566319137811661, 0.043368086], [0.0, 0.0], [3.321928094887362, 5.791362404253194, 5.891366380533553, 3.221924118607003, 2.5694382856461906, 0.10000397628035884]]\n",
      "2020-12-23 02:52:13,500 : INFO : Removed 0 and 41 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:13,501 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:13,502 : INFO : built Dictionary(80 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 377 corpus positions)\n",
      "2020-12-23 02:52:13,523 : INFO : token count processed\n",
      "2020-12-23 02:52:13,526 : INFO : frequencies processed\n",
      "2020-12-23 02:52:13,654 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:13,654 : INFO : entropies processed\n",
      "2020-12-23 02:52:13,655 : INFO : extropies processed\n",
      "2020-12-23 02:52:13,657 : INFO : token count processed\n",
      "2020-12-23 02:52:13,658 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:13,659 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:13,660 : INFO : vocab #2480\n",
      "2020-12-23 02:52:13,661 : INFO : diff #set()\n",
      "2020-12-23 02:52:13,922 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:14,050 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.2552131020819177, 0.44341707622966636], [0.9734225440770388, 0.026577456], [1.0, 1.0], [3.321928094887362, 5.651670454631116, 5.746322992050473, 3.2272755574680048, 2.424394897163111, 0.09465253741935697]]\n",
      "2020-12-23 02:52:14,052 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:14,053 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:14,054 : INFO : built Dictionary(44 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 87 corpus positions)\n",
      "2020-12-23 02:52:14,062 : INFO : token count processed\n",
      "2020-12-23 02:52:14,065 : INFO : frequencies processed\n",
      "2020-12-23 02:52:14,193 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:14,193 : INFO : entropies processed\n",
      "2020-12-23 02:52:14,194 : INFO : extropies processed\n",
      "2020-12-23 02:52:14,196 : INFO : token count processed\n",
      "2020-12-23 02:52:14,198 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:14,199 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:14,200 : INFO : vocab #2480\n",
      "2020-12-23 02:52:14,202 : INFO : diff #set()\n",
      "2020-12-23 02:52:14,468 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:14,598 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.2666728899383972, 0.4411752593146237], [0.982393853366375, 0.017606147], [0.0, 0.0], [3.321928094887362, 4.8226207261920235, 5.140148022861415, 3.0044007982179712, 1.8182199279740527, 0.3175272966693914]]\n",
      "2020-12-23 02:52:14,600 : INFO : Removed 0 and 50 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:14,601 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:14,602 : INFO : built Dictionary(87 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 257 corpus positions)\n",
      "2020-12-23 02:52:14,625 : INFO : token count processed\n",
      "2020-12-23 02:52:14,627 : INFO : frequencies processed\n",
      "2020-12-23 02:52:14,755 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:14,756 : INFO : entropies processed\n",
      "2020-12-23 02:52:14,756 : INFO : extropies processed\n",
      "2020-12-23 02:52:14,758 : INFO : token count processed\n",
      "2020-12-23 02:52:14,759 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:14,760 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:14,761 : INFO : vocab #2480\n",
      "2020-12-23 02:52:14,762 : INFO : diff #set()\n",
      "2020-12-23 02:52:15,032 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:15,161 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.2227626928924211, 0.4498905813012036], [0.9217603355646133, 0.078239664], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 6.24862851613934, 6.330403019528757, 3.240153591497946, 3.0084749246413947, 0.08177450338941661]]\n",
      "2020-12-23 02:52:15,164 : INFO : Removed 0 and 32 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:52:15,165 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:15,167 : INFO : built Dictionary(90 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 373 corpus positions)\n",
      "2020-12-23 02:52:15,192 : INFO : token count processed\n",
      "2020-12-23 02:52:15,195 : INFO : frequencies processed\n",
      "2020-12-23 02:52:15,322 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:15,323 : INFO : entropies processed\n",
      "2020-12-23 02:52:15,324 : INFO : extropies processed\n",
      "2020-12-23 02:52:15,325 : INFO : token count processed\n",
      "2020-12-23 02:52:15,326 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:15,327 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:15,328 : INFO : vocab #2480\n",
      "2020-12-23 02:52:15,329 : INFO : diff #set()\n",
      "2020-12-23 02:52:15,588 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:15,714 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.24315744264357, 0.44580018370065727], [0.9583514034748077, 0.041648597], [0.0, 0.0], [3.321928094887362, 5.850156917433494, 5.94453187536628, 3.227553136954576, 2.622603780478918, 0.09437495793278661]]\n",
      "2020-12-23 02:52:15,717 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:15,717 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:15,718 : INFO : built Dictionary(84 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 371 corpus positions)\n",
      "2020-12-23 02:52:15,737 : INFO : token count processed\n",
      "2020-12-23 02:52:15,745 : INFO : frequencies processed\n",
      "2020-12-23 02:52:15,877 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:15,877 : INFO : entropies processed\n",
      "2020-12-23 02:52:15,878 : INFO : extropies processed\n",
      "2020-12-23 02:52:15,879 : INFO : token count processed\n",
      "2020-12-23 02:52:15,880 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:15,880 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:15,881 : INFO : vocab #2480\n",
      "2020-12-23 02:52:15,882 : INFO : diff #set()\n",
      "2020-12-23 02:52:16,144 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:16,272 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.2527604823914495, 0.4438998321465742], [0.9771532844752073, 0.022846716], [1.0, 1.0], [3.321928094887362, 5.6831976040360095, 5.77884808188739, 3.226277617035982, 2.456919987000028, 0.0956504778513807]]\n",
      "2020-12-23 02:52:16,274 : INFO : Removed 0 and 52 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:16,275 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:16,276 : INFO : built Dictionary(71 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 349 corpus positions)\n",
      "2020-12-23 02:52:16,290 : INFO : token count processed\n",
      "2020-12-23 02:52:16,292 : INFO : frequencies processed\n",
      "2020-12-23 02:52:16,419 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:16,420 : INFO : entropies processed\n",
      "2020-12-23 02:52:16,421 : INFO : extropies processed\n",
      "2020-12-23 02:52:16,423 : INFO : token count processed\n",
      "2020-12-23 02:52:16,424 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:16,425 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:16,426 : INFO : vocab #2480\n",
      "2020-12-23 02:52:16,428 : INFO : diff #set()\n",
      "2020-12-23 02:52:16,688 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:16,817 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.1621832574991895, 0.4624954876195894], [0.8581682443618774, 0.14183176], [1.0, 1.0], [3.321928094887362, 5.749308601266266, 5.8324251541109, 3.238811542042728, 2.5104970592235376, 0.0831165528446336]]\n",
      "2020-12-23 02:52:16,819 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:16,820 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:16,821 : INFO : built Dictionary(66 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 230 corpus positions)\n",
      "2020-12-23 02:52:16,834 : INFO : token count processed\n",
      "2020-12-23 02:52:16,836 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:52:16,837 : INFO : frequencies processed\n",
      "2020-12-23 02:52:16,838 : INFO : token count processed\n",
      "2020-12-23 02:52:16,839 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:16,840 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:16,841 : INFO : vocab #2480\n",
      "2020-12-23 02:52:16,842 : INFO : diff #set()\n",
      "2020-12-23 02:52:17,099 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:17,227 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.2681381346930174, 0.440890254744271], [0.9864658145233989, 0.0135341855], [nan, nan], [3.321928094887362, 5.015422548793484, 5.199289810497675, 3.1380608331831716, 1.8773617156103124, 0.183867261704191]]\n",
      "2020-12-23 02:52:17,229 : INFO : Removed 0 and 81 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:17,230 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:17,231 : INFO : built Dictionary(93 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 356 corpus positions)\n",
      "2020-12-23 02:52:17,260 : INFO : token count processed\n",
      "2020-12-23 02:52:17,264 : INFO : frequencies processed\n",
      "2020-12-23 02:52:17,392 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:17,392 : INFO : entropies processed\n",
      "2020-12-23 02:52:17,393 : INFO : extropies processed\n",
      "2020-12-23 02:52:17,393 : INFO : token count processed\n",
      "2020-12-23 02:52:17,394 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:17,395 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:17,396 : INFO : vocab #2480\n",
      "2020-12-23 02:52:17,397 : INFO : diff #set()\n",
      "2020-12-23 02:52:17,653 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:17,780 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.2090714340537987, 0.45267888787323224], [0.9161539152264595, 0.083846085], [1.0, 1.0], [3.321928094887362, 6.030001281822029, 6.105466863983123, 3.246462512726268, 2.783538769095761, 0.07546558216109389]]\n",
      "2020-12-23 02:52:17,782 : INFO : Removed 0 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:17,783 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:17,785 : INFO : built Dictionary(81 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 319 corpus positions)\n",
      "2020-12-23 02:52:17,806 : INFO : token count processed\n",
      "2020-12-23 02:52:17,809 : INFO : frequencies processed\n",
      "2020-12-23 02:52:18,051 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:18,052 : INFO : entropies processed\n",
      "2020-12-23 02:52:18,053 : INFO : extropies processed\n",
      "2020-12-23 02:52:18,054 : INFO : token count processed\n",
      "2020-12-23 02:52:18,055 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:18,057 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:18,058 : INFO : vocab #2480\n",
      "2020-12-23 02:52:18,059 : INFO : diff #set()\n",
      "2020-12-23 02:52:18,321 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:18,449 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.251813143556841, 0.4440865810119816], [0.9624276496469975, 0.03757235], [1.0, 1.0], [3.321928094887362, 5.9537092545441395, 6.047550329497619, 3.2280870199338834, 2.7256222346102565, 0.09384107495347926]]\n",
      "2020-12-23 02:52:18,451 : INFO : Removed 0 and 90 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:18,452 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:18,453 : INFO : built Dictionary(94 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 320 corpus positions)\n",
      "2020-12-23 02:52:18,473 : INFO : token count processed\n",
      "2020-12-23 02:52:18,475 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:52:18,477 : INFO : frequencies processed\n",
      "2020-12-23 02:52:18,479 : INFO : token count processed\n",
      "2020-12-23 02:52:18,481 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:18,482 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:18,483 : INFO : vocab #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:52:18,484 : INFO : diff #set()\n",
      "2020-12-23 02:52:18,738 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:18,866 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.2637240663715228, 0.44174995303331277], [0.9870346253737807, 0.012965375], [nan, nan], [3.321928094887362, 6.184756445474906, 6.280358397749528, 3.2263261426127405, 2.958430302862166, 0.09560195227462209]]\n",
      "2020-12-23 02:52:18,868 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:18,869 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:18,870 : INFO : built Dictionary(102 unique tokens: ['assist', 'bug', 'merg', 'mkdir', 'nlp']...) from 2 documents (total 433 corpus positions)\n",
      "2020-12-23 02:52:18,893 : INFO : token count processed\n",
      "2020-12-23 02:52:18,895 : INFO : frequencies processed\n",
      "2020-12-23 02:52:19,022 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:19,022 : INFO : entropies processed\n",
      "2020-12-23 02:52:19,023 : INFO : extropies processed\n",
      "2020-12-23 02:52:19,023 : INFO : token count processed\n",
      "2020-12-23 02:52:19,024 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:19,025 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:19,025 : INFO : vocab #2480\n",
      "2020-12-23 02:52:19,026 : INFO : diff #set()\n",
      "2020-12-23 02:52:19,283 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:19,410 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.2533111299100537, 0.4437913551866748], [0.9591374881565571, 0.04086251], [0.0, 0.0], [3.321928094887362, 6.212221456585881, 6.28909871030421, 3.245050841169033, 2.967170615416848, 0.07687725371832954]]\n",
      "2020-12-23 02:52:19,413 : INFO : Removed 5 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:19,414 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:19,416 : INFO : built Dictionary(129 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 433 corpus positions)\n",
      "2020-12-23 02:52:19,471 : INFO : token count processed\n",
      "2020-12-23 02:52:19,474 : INFO : frequencies processed\n",
      "2020-12-23 02:52:19,604 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:19,604 : INFO : entropies processed\n",
      "2020-12-23 02:52:19,605 : INFO : extropies processed\n",
      "2020-12-23 02:52:19,606 : INFO : token count processed\n",
      "2020-12-23 02:52:19,607 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:19,608 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:19,609 : INFO : vocab #2480\n",
      "2020-12-23 02:52:19,610 : INFO : diff #set()\n",
      "2020-12-23 02:52:19,873 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:19,999 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.193656838767276, 0.4558598146836628], [0.9272702038288116, 0.072729796], [2.321928094887362, 1.2877123795494492], [4.386143391856655, 6.301552355933639, 6.4928484681800995, 4.194847279610194, 2.1067050763234443, 0.19129611224646048]]\n",
      "2020-12-23 02:52:20,002 : INFO : Removed 5 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:20,003 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:20,004 : INFO : built Dictionary(164 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 669 corpus positions)\n",
      "2020-12-23 02:52:20,071 : INFO : token count processed\n",
      "2020-12-23 02:52:20,074 : INFO : frequencies processed\n",
      "2020-12-23 02:52:20,202 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:20,203 : INFO : entropies processed\n",
      "2020-12-23 02:52:20,203 : INFO : extropies processed\n",
      "2020-12-23 02:52:20,205 : INFO : token count processed\n",
      "2020-12-23 02:52:20,206 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:20,207 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:20,208 : INFO : vocab #2480\n",
      "2020-12-23 02:52:20,209 : INFO : diff #set()\n",
      "2020-12-23 02:52:20,468 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:20,597 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.2003055558933, 0.454482331929581], [0.9097787216305733, 0.09022128], [3.085055102756477, 1.348691494104856], [4.386143391856655, 6.739005504021667, 6.842135013092299, 4.283013882786023, 2.4559916212356434, 0.1031295090706319]]\n",
      "2020-12-23 02:52:20,600 : INFO : Removed 5 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:20,601 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:20,602 : INFO : built Dictionary(110 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 520 corpus positions)\n",
      "2020-12-23 02:52:20,651 : INFO : token count processed\n",
      "2020-12-23 02:52:20,654 : INFO : frequencies processed\n",
      "2020-12-23 02:52:20,781 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:20,782 : INFO : entropies processed\n",
      "2020-12-23 02:52:20,782 : INFO : extropies processed\n",
      "2020-12-23 02:52:20,783 : INFO : token count processed\n",
      "2020-12-23 02:52:20,784 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:20,785 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:20,785 : INFO : vocab #2480\n",
      "2020-12-23 02:52:20,786 : INFO : diff #set()\n",
      "2020-12-23 02:52:21,043 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:21,171 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2056872046581293, 0.45337344202211805], [0.9458465240895748, 0.054153476], [2.725480556997868, 1.3192201298976014], [4.386143391856655, 5.870833373337847, 6.037156028411328, 4.2198207367831735, 1.6510126365546727, 0.16632265507348087]]\n",
      "2020-12-23 02:52:21,173 : INFO : Removed 5 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:21,174 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:21,175 : INFO : built Dictionary(74 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 203 corpus positions)\n",
      "2020-12-23 02:52:21,200 : INFO : token count processed\n",
      "2020-12-23 02:52:21,205 : INFO : frequencies processed\n",
      "2020-12-23 02:52:21,341 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:21,342 : INFO : entropies processed\n",
      "2020-12-23 02:52:21,342 : INFO : extropies processed\n",
      "2020-12-23 02:52:21,344 : INFO : token count processed\n",
      "2020-12-23 02:52:21,344 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:21,346 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:21,347 : INFO : vocab #2480\n",
      "2020-12-23 02:52:21,348 : INFO : diff #set()\n",
      "2020-12-23 02:52:21,612 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:21,743 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.1923078351583078, 0.45614032115512165], [0.9562857821583748, 0.043714218], [1.0, 1.0], [4.386143391856655, 5.371881234145534, 5.7889305219669325, 3.969094104035257, 1.4027871301102772, 0.4170492878213983]]\n",
      "2020-12-23 02:52:21,745 : INFO : Removed 5 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:21,746 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:21,748 : INFO : built Dictionary(65 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 155 corpus positions)\n",
      "2020-12-23 02:52:21,772 : INFO : token count processed\n",
      "2020-12-23 02:52:21,774 : INFO : frequencies processed\n",
      "2020-12-23 02:52:21,901 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:21,902 : INFO : entropies processed\n",
      "2020-12-23 02:52:21,903 : INFO : extropies processed\n",
      "2020-12-23 02:52:21,904 : INFO : token count processed\n",
      "2020-12-23 02:52:21,905 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:21,906 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:21,907 : INFO : vocab #2480\n",
      "2020-12-23 02:52:21,908 : INFO : diff #set()\n",
      "2020-12-23 02:52:22,167 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:22,295 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.2016218056730712, 0.45421061756530157], [0.952582448720932, 0.04741755], [1.0, 1.0], [4.386143391856655, 4.85108279267097, 5.4277739083388985, 3.8094522761887264, 1.0416305164822433, 0.5766911156679289]]\n",
      "2020-12-23 02:52:22,297 : INFO : Removed 5 and 127 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:52:22,298 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:22,299 : INFO : built Dictionary(106 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 428 corpus positions)\n",
      "2020-12-23 02:52:22,336 : INFO : token count processed\n",
      "2020-12-23 02:52:22,338 : INFO : frequencies processed\n",
      "2020-12-23 02:52:22,466 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:22,467 : INFO : entropies processed\n",
      "2020-12-23 02:52:22,470 : INFO : extropies processed\n",
      "2020-12-23 02:52:22,472 : INFO : token count processed\n",
      "2020-12-23 02:52:22,472 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:22,473 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:22,473 : INFO : vocab #2480\n",
      "2020-12-23 02:52:22,474 : INFO : diff #set()\n",
      "2020-12-23 02:52:22,731 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:22,858 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.252243968907016, 0.4440016329515522], [0.9803983625024557, 0.019601637], [1.0, 1.0], [4.386143391856655, 6.139571208108155, 6.341399452841628, 4.184315147123183, 1.955256060984973, 0.20182824473347338]]\n",
      "2020-12-23 02:52:22,861 : INFO : Removed 5 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:22,862 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:22,863 : INFO : built Dictionary(91 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 438 corpus positions)\n",
      "2020-12-23 02:52:22,893 : INFO : token count processed\n",
      "2020-12-23 02:52:22,896 : INFO : frequencies processed\n",
      "2020-12-23 02:52:23,025 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:23,026 : INFO : entropies processed\n",
      "2020-12-23 02:52:23,027 : INFO : extropies processed\n",
      "2020-12-23 02:52:23,028 : INFO : token count processed\n",
      "2020-12-23 02:52:23,029 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:23,030 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:23,031 : INFO : vocab #2480\n",
      "2020-12-23 02:52:23,032 : INFO : diff #set()\n",
      "2020-12-23 02:52:23,299 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:23,429 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.2480522923944872, 0.44482951014224914], [0.9645249620079994, 0.035475038], [1.584962500721156, 1.1699250014423124], [4.386143391856655, 5.609710627339259, 5.8600985531593075, 4.135755466036606, 1.4739551613026523, 0.2503879258200481]]\n",
      "2020-12-23 02:52:23,432 : INFO : Removed 5 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:23,433 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:23,434 : INFO : built Dictionary(176 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 1098 corpus positions)\n",
      "2020-12-23 02:52:23,515 : INFO : token count processed\n",
      "2020-12-23 02:52:23,519 : INFO : frequencies processed\n",
      "2020-12-23 02:52:23,649 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:23,649 : INFO : entropies processed\n",
      "2020-12-23 02:52:23,650 : INFO : extropies processed\n",
      "2020-12-23 02:52:23,651 : INFO : token count processed\n",
      "2020-12-23 02:52:23,652 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:23,653 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:23,655 : INFO : vocab #2480\n",
      "2020-12-23 02:52:23,656 : INFO : diff #set()\n",
      "2020-12-23 02:52:23,917 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:24,044 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.1984467685050224, 0.4548665968746723], [0.9178735762834549, 0.08212642], [2.9995812306460645, 1.3120987901327013], [4.386143391856655, 7.2441902753576075, 7.301003813063996, 4.3293298541502665, 2.914860421207341, 0.05681353770638875]]\n",
      "2020-12-23 02:52:24,046 : INFO : Removed 5 and 136 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:24,047 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:24,049 : INFO : built Dictionary(139 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 705 corpus positions)\n",
      "2020-12-23 02:52:24,115 : INFO : token count processed\n",
      "2020-12-23 02:52:24,121 : INFO : frequencies processed\n",
      "2020-12-23 02:52:24,249 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:24,250 : INFO : entropies processed\n",
      "2020-12-23 02:52:24,251 : INFO : extropies processed\n",
      "2020-12-23 02:52:24,252 : INFO : token count processed\n",
      "2020-12-23 02:52:24,253 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:24,253 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:24,254 : INFO : vocab #2480\n",
      "2020-12-23 02:52:24,255 : INFO : diff #set()\n",
      "2020-12-23 02:52:24,513 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:24,641 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.2198898793894601, 0.4504727956483281], [0.9474338218569756, 0.05256618], [3.084962500721156, 1.3480058660457088], [4.386143391856655, 6.2567074920449475, 6.374562527485663, 4.268288356415939, 1.988419135629008, 0.11785503544071574]]\n",
      "2020-12-23 02:52:24,643 : INFO : Removed 5 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:24,644 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:24,645 : INFO : built Dictionary(92 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 215 corpus positions)\n",
      "2020-12-23 02:52:24,678 : INFO : token count processed\n",
      "2020-12-23 02:52:24,683 : INFO : frequencies processed\n",
      "2020-12-23 02:52:24,813 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:24,814 : INFO : entropies processed\n",
      "2020-12-23 02:52:24,815 : INFO : extropies processed\n",
      "2020-12-23 02:52:24,816 : INFO : token count processed\n",
      "2020-12-23 02:52:24,817 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:24,818 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:24,819 : INFO : vocab #2480\n",
      "2020-12-23 02:52:24,820 : INFO : diff #set()\n",
      "2020-12-23 02:52:25,081 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:25,209 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.2519354816525807, 0.4440624556730866], [0.9639735743403435, 0.036026426], [0.0, 0.0], [4.386143391856655, 5.7680018917339435, 6.130007555324474, 4.0241377282661235, 1.7438641634678191, 0.3620056635905309]]\n",
      "2020-12-23 02:52:25,212 : INFO : Removed 5 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:25,213 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:25,214 : INFO : built Dictionary(184 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 756 corpus positions)\n",
      "2020-12-23 02:52:25,290 : INFO : token count processed\n",
      "2020-12-23 02:52:25,292 : INFO : frequencies processed\n",
      "2020-12-23 02:52:25,422 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:25,423 : INFO : entropies processed\n",
      "2020-12-23 02:52:25,424 : INFO : extropies processed\n",
      "2020-12-23 02:52:25,426 : INFO : token count processed\n",
      "2020-12-23 02:52:25,427 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:25,428 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:25,429 : INFO : vocab #2480\n",
      "2020-12-23 02:52:25,430 : INFO : diff #set()\n",
      "2020-12-23 02:52:25,690 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:25,818 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.2335566284342567, 0.4477164300512985], [0.9497893154621124, 0.050210685], [2.721928094887362, 1.3198385641318495], [4.386143391856655, 6.846479111193757, 6.951959426852536, 4.280663076197875, 2.5658160349958807, 0.10548031565877913]]\n",
      "2020-12-23 02:52:25,821 : INFO : Removed 5 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:25,822 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:25,823 : INFO : built Dictionary(40 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 59 corpus positions)\n",
      "2020-12-23 02:52:25,840 : INFO : token count processed\n",
      "2020-12-23 02:52:25,842 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:52:25,843 : INFO : frequencies processed\n",
      "2020-12-23 02:52:25,844 : INFO : token count processed\n",
      "2020-12-23 02:52:25,845 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:25,847 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:25,847 : INFO : vocab #2480\n",
      "2020-12-23 02:52:25,848 : INFO : diff #set()\n",
      "2020-12-23 02:52:26,115 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:26,244 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2582680554708636, 0.4428172278208547], [0.9909963309764862, 0.009003669], [nan, nan], [4.386143391856655, 4.165013816065912, 5.271418898701708, 3.2797383092208596, 0.8852755068450531, 1.1064050826357965]]\n",
      "2020-12-23 02:52:26,247 : INFO : Removed 5 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:26,248 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:26,249 : INFO : built Dictionary(68 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 166 corpus positions)\n",
      "2020-12-23 02:52:26,275 : INFO : token count processed\n",
      "2020-12-23 02:52:26,278 : INFO : frequencies processed\n",
      "2020-12-23 02:52:26,406 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:26,406 : INFO : entropies processed\n",
      "2020-12-23 02:52:26,407 : INFO : extropies processed\n",
      "2020-12-23 02:52:26,408 : INFO : token count processed\n",
      "2020-12-23 02:52:26,409 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:26,410 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:26,411 : INFO : vocab #2480\n",
      "2020-12-23 02:52:26,412 : INFO : diff #set()\n",
      "2020-12-23 02:52:26,669 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:26,797 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.2394864681382591, 0.4465309410113672], [0.9517553374171257, 0.048244663], [0.0, 0.0], [4.386143391856655, 5.449968864419248, 5.905786331258682, 3.9303259250172218, 1.5196429394020265, 0.45581746683943347]]\n",
      "2020-12-23 02:52:26,800 : INFO : Removed 5 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:26,801 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:26,802 : INFO : built Dictionary(157 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 603 corpus positions)\n",
      "2020-12-23 02:52:26,863 : INFO : token count processed\n",
      "2020-12-23 02:52:26,866 : INFO : frequencies processed\n",
      "2020-12-23 02:52:26,994 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:26,994 : INFO : entropies processed\n",
      "2020-12-23 02:52:26,995 : INFO : extropies processed\n",
      "2020-12-23 02:52:26,996 : INFO : token count processed\n",
      "2020-12-23 02:52:26,997 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:26,998 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:26,999 : INFO : vocab #2480\n",
      "2020-12-23 02:52:27,000 : INFO : diff #set()\n",
      "2020-12-23 02:52:27,259 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:27,386 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.219970749480418, 0.4504563856231209], [0.946607518941164, 0.05339248], [2.2359263506290326, 1.2653331222512114], [4.386143391856655, 6.530294129310484, 6.662956681316032, 4.2534808398511075, 2.276813289459377, 0.13266255200554866]]\n",
      "2020-12-23 02:52:27,389 : INFO : Removed 5 and 142 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:27,390 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:27,392 : INFO : built Dictionary(131 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 619 corpus positions)\n",
      "2020-12-23 02:52:27,452 : INFO : token count processed\n",
      "2020-12-23 02:52:27,455 : INFO : frequencies processed\n",
      "2020-12-23 02:52:27,582 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:27,582 : INFO : entropies processed\n",
      "2020-12-23 02:52:27,583 : INFO : extropies processed\n",
      "2020-12-23 02:52:27,584 : INFO : token count processed\n",
      "2020-12-23 02:52:27,585 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:27,586 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:27,586 : INFO : vocab #2480\n",
      "2020-12-23 02:52:27,587 : INFO : diff #set()\n",
      "2020-12-23 02:52:27,846 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:27,975 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.188861582037184, 0.456858491284449], [0.9147014245390892, 0.085298575], [2.0, 1.2451124978365313], [4.386143391856655, 6.470272233491701, 6.609826449278267, 4.246589176070089, 2.2236830574216118, 0.1395542157865659]]\n",
      "2020-12-23 02:52:27,977 : INFO : Removed 5 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:27,978 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:27,980 : INFO : built Dictionary(130 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 408 corpus positions)\n",
      "2020-12-23 02:52:28,033 : INFO : token count processed\n",
      "2020-12-23 02:52:28,035 : INFO : frequencies processed\n",
      "2020-12-23 02:52:28,162 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:28,162 : INFO : entropies processed\n",
      "2020-12-23 02:52:28,163 : INFO : extropies processed\n",
      "2020-12-23 02:52:28,164 : INFO : token count processed\n",
      "2020-12-23 02:52:28,165 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:28,166 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:28,167 : INFO : vocab #2480\n",
      "2020-12-23 02:52:28,168 : INFO : diff #set()\n",
      "2020-12-23 02:52:28,430 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:28,557 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.2198798003769973, 0.45047484094867307], [0.9224218651652336, 0.077578135], [2.5032583347756456, 1.2991301890771523], [4.386143391856655, 6.550038223589686, 6.707794649645898, 4.228386965800443, 2.3216512577892425, 0.15775642605621165]]\n",
      "2020-12-23 02:52:28,560 : INFO : Removed 5 and 70 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:28,561 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:28,562 : INFO : built Dictionary(89 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 324 corpus positions)\n",
      "2020-12-23 02:52:28,597 : INFO : token count processed\n",
      "2020-12-23 02:52:28,600 : INFO : frequencies processed\n",
      "2020-12-23 02:52:28,727 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:28,728 : INFO : entropies processed\n",
      "2020-12-23 02:52:28,728 : INFO : extropies processed\n",
      "2020-12-23 02:52:28,729 : INFO : token count processed\n",
      "2020-12-23 02:52:28,730 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:28,731 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:28,731 : INFO : vocab #2480\n",
      "2020-12-23 02:52:28,732 : INFO : diff #set()\n",
      "2020-12-23 02:52:28,997 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:29,125 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.2237812056892914, 0.4496845271655385], [0.9639971517026424, 0.03600285], [1.0, 1.0], [4.386143391856655, 5.860525481261383, 6.1240254094658795, 4.1226434636521585, 1.7378820176092242, 0.26349992820449675]]\n",
      "2020-12-23 02:52:29,128 : INFO : Removed 5 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:29,128 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:29,129 : INFO : built Dictionary(61 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 362 corpus positions)\n",
      "2020-12-23 02:52:29,153 : INFO : token count processed\n",
      "2020-12-23 02:52:29,155 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:52:29,158 : INFO : frequencies processed\n",
      "2020-12-23 02:52:29,159 : INFO : token count processed\n",
      "2020-12-23 02:52:29,161 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:29,162 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:29,162 : INFO : vocab #2480\n",
      "2020-12-23 02:52:29,163 : INFO : diff #set()\n",
      "2020-12-23 02:52:29,420 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:29,547 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.2928472171744163, 0.43613895967841554], [0.9893959183245897, 0.010604082], [nan, nan], [4.386143391856655, 5.945464049777852, 6.174339304035504, 4.157268137599003, 1.7881959121788489, 0.22887525425765176]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:52:29,550 : INFO : Removed 5 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:29,551 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:29,552 : INFO : built Dictionary(197 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 909 corpus positions)\n",
      "2020-12-23 02:52:29,635 : INFO : token count processed\n",
      "2020-12-23 02:52:29,637 : INFO : frequencies processed\n",
      "2020-12-23 02:52:29,764 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:29,765 : INFO : entropies processed\n",
      "2020-12-23 02:52:29,766 : INFO : extropies processed\n",
      "2020-12-23 02:52:29,767 : INFO : token count processed\n",
      "2020-12-23 02:52:29,768 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:29,769 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:29,770 : INFO : vocab #2480\n",
      "2020-12-23 02:52:29,771 : INFO : diff #set()\n",
      "2020-12-23 02:52:30,029 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:30,157 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.224759879764207, 0.4494867104965889], [0.9501712024211884, 0.049828798], [3.625, 1.3785939957689286], [4.386143391856655, 6.811563897304216, 6.892164486982125, 4.305542802178746, 2.5060210951254698, 0.08060058967790873]]\n",
      "2020-12-23 02:52:30,160 : INFO : Removed 5 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:30,160 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:30,162 : INFO : built Dictionary(220 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 1004 corpus positions)\n",
      "2020-12-23 02:52:30,260 : INFO : token count processed\n",
      "2020-12-23 02:52:30,262 : INFO : frequencies processed\n",
      "2020-12-23 02:52:30,391 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:30,392 : INFO : entropies processed\n",
      "2020-12-23 02:52:30,393 : INFO : extropies processed\n",
      "2020-12-23 02:52:30,394 : INFO : token count processed\n",
      "2020-12-23 02:52:30,395 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:30,396 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:30,397 : INFO : vocab #2480\n",
      "2020-12-23 02:52:30,398 : INFO : diff #set()\n",
      "2020-12-23 02:52:30,657 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:30,785 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.1896305647518766, 0.4566980458246012], [0.8928973376750946, 0.10710266], [3.149397470347699, 1.3258158049395798], [4.386143391856655, 7.502034948968415, 7.552747153714089, 4.3354311871109825, 3.1666037618574334, 0.05071220474567362]]\n",
      "2020-12-23 02:52:30,788 : INFO : Removed 5 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:30,789 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:30,792 : INFO : built Dictionary(262 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 1577 corpus positions)\n",
      "2020-12-23 02:52:30,937 : INFO : token count processed\n",
      "2020-12-23 02:52:30,939 : INFO : frequencies processed\n",
      "2020-12-23 02:52:31,065 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:31,065 : INFO : entropies processed\n",
      "2020-12-23 02:52:31,066 : INFO : extropies processed\n",
      "2020-12-23 02:52:31,068 : INFO : token count processed\n",
      "2020-12-23 02:52:31,069 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:31,070 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:31,071 : INFO : vocab #2480\n",
      "2020-12-23 02:52:31,072 : INFO : diff #set()\n",
      "2020-12-23 02:52:31,340 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:31,469 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.166578302755749, 0.4615572853877767], [0.8678531050682068, 0.1321469], [3.6168746059562222, 1.3781755222681253], [4.386143391856655, 7.39180093901977, 7.433076856726487, 4.34486747414994, 3.0469334648698316, 0.04127591770671657]]\n",
      "2020-12-23 02:52:31,471 : INFO : Removed 5 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:31,472 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:31,474 : INFO : built Dictionary(55 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 132 corpus positions)\n",
      "2020-12-23 02:52:31,497 : INFO : token count processed\n",
      "2020-12-23 02:52:31,499 : INFO : frequencies processed\n",
      "2020-12-23 02:52:31,627 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:31,628 : INFO : entropies processed\n",
      "2020-12-23 02:52:31,629 : INFO : extropies processed\n",
      "2020-12-23 02:52:31,631 : INFO : token count processed\n",
      "2020-12-23 02:52:31,632 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:31,633 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:31,634 : INFO : vocab #2480\n",
      "2020-12-23 02:52:31,635 : INFO : diff #set()\n",
      "2020-12-23 02:52:31,891 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:32,019 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.191029333202561, 0.4564064865979363], [0.925683856010437, 0.074316144], [1.9219280948873623, 1.2148067842293933], [4.386143391856655, 4.927561309677364, 5.495086707506964, 3.818617994027054, 1.1089433156503086, 0.5675253978296002]]\n",
      "2020-12-23 02:52:32,021 : INFO : Removed 5 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:32,022 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:32,023 : INFO : built Dictionary(26 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 33 corpus positions)\n",
      "2020-12-23 02:52:32,032 : INFO : token count processed\n",
      "2020-12-23 02:52:32,035 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:52:32,035 : INFO : frequencies processed\n",
      "2020-12-23 02:52:32,036 : INFO : token count processed\n",
      "2020-12-23 02:52:32,038 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:32,039 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:32,040 : INFO : vocab #2480\n",
      "2020-12-23 02:52:32,042 : INFO : diff #set()\n",
      "2020-12-23 02:52:32,303 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:32,433 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.273708062540341, 0.43981020099947754], [1.0, 0.0], [nan, nan], [4.386143391856655, 2.5216406363433186, 4.731884343063671, 2.1758996851363026, 0.34574095120701553, 2.210243706720352]]\n",
      "2020-12-23 02:52:32,437 : INFO : Removed 5 and 530 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:32,438 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:32,440 : INFO : built Dictionary(342 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 2905 corpus positions)\n",
      "2020-12-23 02:52:32,627 : INFO : token count processed\n",
      "2020-12-23 02:52:32,629 : INFO : frequencies processed\n",
      "2020-12-23 02:52:32,755 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:32,755 : INFO : entropies processed\n",
      "2020-12-23 02:52:32,756 : INFO : extropies processed\n",
      "2020-12-23 02:52:32,758 : INFO : token count processed\n",
      "2020-12-23 02:52:32,759 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:32,760 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:32,760 : INFO : vocab #2480\n",
      "2020-12-23 02:52:32,761 : INFO : diff #set()\n",
      "2020-12-23 02:52:33,032 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:33,162 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.2446351266928437, 0.44550670534741227], [0.9639236629009247, 0.036076337], [2.75, 1.3226647836567116], [4.386143391856655, 7.480007711014331, 7.519602470034718, 4.346548632836269, 3.1334590781780625, 0.039594759020387116]]\n",
      "2020-12-23 02:52:33,165 : INFO : Removed 5 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:33,166 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:33,168 : INFO : built Dictionary(218 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 1054 corpus positions)\n",
      "2020-12-23 02:52:33,260 : INFO : token count processed\n",
      "2020-12-23 02:52:33,262 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:52:33,390 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:33,390 : INFO : entropies processed\n",
      "2020-12-23 02:52:33,391 : INFO : extropies processed\n",
      "2020-12-23 02:52:33,393 : INFO : token count processed\n",
      "2020-12-23 02:52:33,394 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:33,395 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:33,395 : INFO : vocab #2480\n",
      "2020-12-23 02:52:33,397 : INFO : diff #set()\n",
      "2020-12-23 02:52:33,655 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:33,782 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.2356828371489161, 0.4472906368397331], [0.9466508887708187, 0.05334911], [2.9139770731827523, 1.3356231683419404], [4.386143391856655, 7.131331012509435, 7.207221506011959, 4.310252898354132, 2.821078114155304, 0.07589049350252441]]\n",
      "2020-12-23 02:52:33,785 : INFO : Removed 5 and 166 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:33,786 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:33,788 : INFO : built Dictionary(210 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 873 corpus positions)\n",
      "2020-12-23 02:52:33,891 : INFO : token count processed\n",
      "2020-12-23 02:52:33,897 : INFO : frequencies processed\n",
      "2020-12-23 02:52:34,026 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:34,026 : INFO : entropies processed\n",
      "2020-12-23 02:52:34,027 : INFO : extropies processed\n",
      "2020-12-23 02:52:34,028 : INFO : token count processed\n",
      "2020-12-23 02:52:34,029 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:34,030 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:34,030 : INFO : vocab #2480\n",
      "2020-12-23 02:52:34,031 : INFO : diff #set()\n",
      "2020-12-23 02:52:34,295 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:34,423 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.2446864623534455, 0.44549651667233214], [0.9461656287312508, 0.05383437], [2.25, 1.2709632597765914], [4.386143391856655, 7.203742744794778, 7.29224737422588, 4.297638762425553, 2.906103982369225, 0.08850462943110227]]\n",
      "2020-12-23 02:52:34,426 : INFO : Removed 5 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:34,427 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:34,428 : INFO : built Dictionary(69 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 205 corpus positions)\n",
      "2020-12-23 02:52:34,454 : INFO : token count processed\n",
      "2020-12-23 02:52:34,458 : INFO : frequencies processed\n",
      "2020-12-23 02:52:34,589 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:34,590 : INFO : entropies processed\n",
      "2020-12-23 02:52:34,590 : INFO : extropies processed\n",
      "2020-12-23 02:52:34,592 : INFO : token count processed\n",
      "2020-12-23 02:52:34,593 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:34,594 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:34,595 : INFO : vocab #2480\n",
      "2020-12-23 02:52:34,596 : INFO : diff #set()\n",
      "2020-12-23 02:52:34,852 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:34,979 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.222704003446499, 0.4499024604488099], [0.9491536021232605, 0.050846398], [2.0, 1.2451124978365313], [4.386143391856655, 5.195502554608948, 5.63717679303737, 3.9444691534282326, 1.251033401180715, 0.4416742384284218]]\n",
      "2020-12-23 02:52:34,982 : INFO : Removed 5 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:34,983 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:34,984 : INFO : built Dictionary(78 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 266 corpus positions)\n",
      "2020-12-23 02:52:35,007 : INFO : token count processed\n",
      "2020-12-23 02:52:35,009 : INFO : frequencies processed\n",
      "2020-12-23 02:52:35,136 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:35,137 : INFO : entropies processed\n",
      "2020-12-23 02:52:35,138 : INFO : extropies processed\n",
      "2020-12-23 02:52:35,140 : INFO : token count processed\n",
      "2020-12-23 02:52:35,142 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:35,143 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:35,144 : INFO : vocab #2480\n",
      "2020-12-23 02:52:35,146 : INFO : diff #set()\n",
      "2020-12-23 02:52:35,406 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:35,535 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.2381040400917558, 0.4468067534335906], [0.9620382748544216, 0.037961725], [0.0, 0.0], [4.386143391856655, 5.32027245610305, 5.715458443870979, 3.990957404088725, 1.3293150520143238, 0.39518598776792935]]\n",
      "2020-12-23 02:52:35,537 : INFO : Removed 5 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:35,538 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:35,539 : INFO : built Dictionary(172 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 506 corpus positions)\n",
      "2020-12-23 02:52:35,607 : INFO : token count processed\n",
      "2020-12-23 02:52:35,610 : INFO : frequencies processed\n",
      "2020-12-23 02:52:35,740 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:35,741 : INFO : entropies processed\n",
      "2020-12-23 02:52:35,742 : INFO : extropies processed\n",
      "2020-12-23 02:52:35,743 : INFO : token count processed\n",
      "2020-12-23 02:52:35,744 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:35,745 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:35,746 : INFO : vocab #2480\n",
      "2020-12-23 02:52:35,747 : INFO : diff #set()\n",
      "2020-12-23 02:52:36,016 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:36,146 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.2271784598516737, 0.44899859531983727], [0.9312408864498138, 0.06875911], [2.721928094887362, 1.3198385641318495], [4.386143391856655, 6.898202761357263, 7.0238209573297254, 4.260525195884192, 2.63767756547307, 0.1256181959724625]]\n",
      "2020-12-23 02:52:36,149 : INFO : Removed 5 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:36,150 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:36,151 : INFO : built Dictionary(140 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 528 corpus positions)\n",
      "2020-12-23 02:52:36,203 : INFO : token count processed\n",
      "2020-12-23 02:52:36,205 : INFO : frequencies processed\n",
      "2020-12-23 02:52:36,336 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:36,336 : INFO : entropies processed\n",
      "2020-12-23 02:52:36,337 : INFO : extropies processed\n",
      "2020-12-23 02:52:36,339 : INFO : token count processed\n",
      "2020-12-23 02:52:36,340 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:36,341 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:36,342 : INFO : vocab #2480\n",
      "2020-12-23 02:52:36,343 : INFO : diff #set()\n",
      "2020-12-23 02:52:36,606 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:36,733 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.237091845887101, 0.4470089155429637], [0.9628795608878136, 0.03712044], [1.584962500721156, 1.1699250014423124], [4.386143391856655, 6.388500481644799, 6.572464932688614, 4.202178940812839, 2.186321540831959, 0.1839644510438152]]\n",
      "2020-12-23 02:52:36,736 : INFO : Removed 5 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:36,737 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:36,738 : INFO : built Dictionary(60 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 192 corpus positions)\n",
      "2020-12-23 02:52:36,755 : INFO : token count processed\n",
      "2020-12-23 02:52:36,758 : INFO : frequencies processed\n",
      "2020-12-23 02:52:36,885 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:36,886 : INFO : entropies processed\n",
      "2020-12-23 02:52:36,886 : INFO : extropies processed\n",
      "2020-12-23 02:52:36,887 : INFO : token count processed\n",
      "2020-12-23 02:52:36,888 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:36,889 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:36,890 : INFO : vocab #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:52:36,891 : INFO : diff #set()\n",
      "2020-12-23 02:52:37,148 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:37,276 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.2195854424784727, 0.4505345822071], [0.9462319575250149, 0.053768042], [2.0, 1.2451124978365313], [4.386143391856655, 4.8191513650620195, 5.333170423845218, 3.872124333073458, 0.9470270319885623, 0.5140190587831981]]\n",
      "2020-12-23 02:52:37,278 : INFO : Removed 5 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:37,279 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:37,280 : INFO : built Dictionary(68 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 247 corpus positions)\n",
      "2020-12-23 02:52:37,300 : INFO : token count processed\n",
      "2020-12-23 02:52:37,303 : INFO : frequencies processed\n",
      "2020-12-23 02:52:37,431 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:37,432 : INFO : entropies processed\n",
      "2020-12-23 02:52:37,432 : INFO : extropies processed\n",
      "2020-12-23 02:52:37,434 : INFO : token count processed\n",
      "2020-12-23 02:52:37,434 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:37,435 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:37,436 : INFO : vocab #2480\n",
      "2020-12-23 02:52:37,437 : INFO : diff #set()\n",
      "2020-12-23 02:52:37,697 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:37,826 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.2360336135807448, 0.44722046838938956], [0.9583084061741829, 0.041691594], [0.0, 0.0], [4.386143391856655, 5.062480936779194, 5.505309454768055, 3.9433148738677932, 1.1191660629113995, 0.44282851798886114]]\n",
      "2020-12-23 02:52:37,829 : INFO : Removed 5 and 223 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:37,829 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:37,832 : INFO : built Dictionary(255 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 1801 corpus positions)\n",
      "2020-12-23 02:52:37,955 : INFO : token count processed\n",
      "2020-12-23 02:52:37,958 : INFO : frequencies processed\n",
      "2020-12-23 02:52:38,089 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:38,090 : INFO : entropies processed\n",
      "2020-12-23 02:52:38,091 : INFO : extropies processed\n",
      "2020-12-23 02:52:38,093 : INFO : token count processed\n",
      "2020-12-23 02:52:38,094 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:38,095 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:38,095 : INFO : vocab #2480\n",
      "2020-12-23 02:52:38,096 : INFO : diff #set()\n",
      "2020-12-23 02:52:38,356 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:38,483 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.2441791314157804, 0.4455972279579715], [0.9577321633696556, 0.042267837], [2.321928094887362, 1.2877123795494492], [4.386143391856655, 7.185085743102134, 7.24915020737681, 4.3220789275819795, 2.863006815520155, 0.06406446427467571]]\n",
      "2020-12-23 02:52:38,486 : INFO : Removed 5 and 148 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:38,486 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:38,488 : INFO : built Dictionary(172 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 776 corpus positions)\n",
      "2020-12-23 02:52:38,558 : INFO : token count processed\n",
      "2020-12-23 02:52:38,561 : INFO : frequencies processed\n",
      "2020-12-23 02:52:38,688 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:38,689 : INFO : entropies processed\n",
      "2020-12-23 02:52:38,690 : INFO : extropies processed\n",
      "2020-12-23 02:52:38,691 : INFO : token count processed\n",
      "2020-12-23 02:52:38,693 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:38,694 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:38,694 : INFO : vocab #2480\n",
      "2020-12-23 02:52:38,696 : INFO : diff #set()\n",
      "2020-12-23 02:52:38,965 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:39,092 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.246139436504563, 0.44520833557697453], [0.9577360562980175, 0.042263944], [2.25, 1.2709632597765914], [4.386143391856655, 6.591225336124281, 6.705989026833672, 4.271379701147264, 2.3198456349770167, 0.11476369070939096]]\n",
      "2020-12-23 02:52:39,095 : INFO : Removed 5 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:39,096 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:39,097 : INFO : built Dictionary(54 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 114 corpus positions)\n",
      "2020-12-23 02:52:39,117 : INFO : token count processed\n",
      "2020-12-23 02:52:39,119 : INFO : frequencies processed\n",
      "2020-12-23 02:52:39,247 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:39,248 : INFO : entropies processed\n",
      "2020-12-23 02:52:39,249 : INFO : extropies processed\n",
      "2020-12-23 02:52:39,250 : INFO : token count processed\n",
      "2020-12-23 02:52:39,251 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:39,252 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:39,252 : INFO : vocab #2480\n",
      "2020-12-23 02:52:39,253 : INFO : diff #set()\n",
      "2020-12-23 02:52:39,511 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:39,640 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.2383682368776443, 0.4467540163967503], [0.9592546001076698, 0.0407454], [0.0, 0.0], [4.386143391856655, 4.7032114441396695, 5.4225657222403605, 3.6667891137559643, 1.0364223303837052, 0.719354278100691]]\n",
      "2020-12-23 02:52:39,643 : INFO : Removed 5 and 9 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:39,644 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:39,645 : INFO : built Dictionary(129 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 354 corpus positions)\n",
      "2020-12-23 02:52:39,690 : INFO : token count processed\n",
      "2020-12-23 02:52:39,693 : INFO : frequencies processed\n",
      "2020-12-23 02:52:39,821 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:39,822 : INFO : entropies processed\n",
      "2020-12-23 02:52:39,822 : INFO : extropies processed\n",
      "2020-12-23 02:52:39,824 : INFO : token count processed\n",
      "2020-12-23 02:52:39,825 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:39,826 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:39,827 : INFO : vocab #2480\n",
      "2020-12-23 02:52:39,828 : INFO : diff #set()\n",
      "2020-12-23 02:52:40,086 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:40,215 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.2361712898750965, 0.4471929339795146], [0.9637923128902912, 0.036207687], [1.584962500721156, 1.1699250014423124], [4.386143391856655, 6.14228447828618, 6.3949870034747285, 4.133440866668106, 2.0088436116180732, 0.25270252518854885]]\n",
      "2020-12-23 02:52:40,217 : INFO : Removed 5 and 184 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:40,218 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:40,220 : INFO : built Dictionary(263 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 1144 corpus positions)\n",
      "2020-12-23 02:52:40,349 : INFO : token count processed\n",
      "2020-12-23 02:52:40,352 : INFO : frequencies processed\n",
      "2020-12-23 02:52:40,482 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:40,483 : INFO : entropies processed\n",
      "2020-12-23 02:52:40,484 : INFO : extropies processed\n",
      "2020-12-23 02:52:40,485 : INFO : token count processed\n",
      "2020-12-23 02:52:40,486 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:40,487 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:40,488 : INFO : vocab #2480\n",
      "2020-12-23 02:52:40,490 : INFO : diff #set()\n",
      "2020-12-23 02:52:40,758 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:40,886 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.2371114105426992, 0.447005006226941], [0.9523100815713406, 0.04768992], [2.94770277922009, 1.3393100707180505], [4.386143391856655, 7.450178124335845, 7.52055990706552, 4.31576160912698, 3.1344165152088648, 0.0703817827296751]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:52:40,889 : INFO : Removed 5 and 16 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:40,890 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:40,891 : INFO : built Dictionary(69 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 192 corpus positions)\n",
      "2020-12-23 02:52:40,918 : INFO : token count processed\n",
      "2020-12-23 02:52:40,920 : INFO : frequencies processed\n",
      "2020-12-23 02:52:41,048 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:41,049 : INFO : entropies processed\n",
      "2020-12-23 02:52:41,050 : INFO : extropies processed\n",
      "2020-12-23 02:52:41,051 : INFO : token count processed\n",
      "2020-12-23 02:52:41,052 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:41,053 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:41,054 : INFO : vocab #2480\n",
      "2020-12-23 02:52:41,055 : INFO : diff #set()\n",
      "2020-12-23 02:52:41,312 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:41,440 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.2203426794246923, 0.45038092960457243], [0.94968181848526, 0.05031818], [2.0, 1.2451124978365313], [4.386143391856655, 5.20665021947654, 5.64469641577498, 3.9480971955582156, 1.2585530239183251, 0.43804619629844055]]\n",
      "2020-12-23 02:52:41,443 : INFO : Removed 5 and 89 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:41,444 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:41,445 : INFO : built Dictionary(137 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 530 corpus positions)\n",
      "2020-12-23 02:52:41,502 : INFO : token count processed\n",
      "2020-12-23 02:52:41,507 : INFO : frequencies processed\n",
      "2020-12-23 02:52:41,633 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:41,634 : INFO : entropies processed\n",
      "2020-12-23 02:52:41,634 : INFO : extropies processed\n",
      "2020-12-23 02:52:41,636 : INFO : token count processed\n",
      "2020-12-23 02:52:41,637 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:41,638 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:41,638 : INFO : vocab #2480\n",
      "2020-12-23 02:52:41,639 : INFO : diff #set()\n",
      "2020-12-23 02:52:41,900 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:42,027 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.2345070911163962, 0.44752599084408534], [0.9368336275219917, 0.06316637], [2.25, 1.2709632597765914], [4.386143391856655, 6.524718477352, 6.668904133870678, 4.241957735337978, 2.2827607420140223, 0.14418565651867787]]\n",
      "2020-12-23 02:52:42,029 : INFO : Removed 5 and 8 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:42,030 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:42,032 : INFO : built Dictionary(75 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 237 corpus positions)\n",
      "2020-12-23 02:52:42,061 : INFO : token count processed\n",
      "2020-12-23 02:52:42,063 : INFO : frequencies processed\n",
      "2020-12-23 02:52:42,191 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:42,192 : INFO : entropies processed\n",
      "2020-12-23 02:52:42,193 : INFO : extropies processed\n",
      "2020-12-23 02:52:42,194 : INFO : token count processed\n",
      "2020-12-23 02:52:42,195 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:42,196 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:42,197 : INFO : vocab #2480\n",
      "2020-12-23 02:52:42,198 : INFO : diff #set()\n",
      "2020-12-23 02:52:42,466 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:42,594 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.2304550753864811, 0.4483390008770858], [0.9567002952098846, 0.043299705], [1.584962500721156, 1.1699250014423124], [4.386143391856655, 5.321859380715434, 5.722581076060433, 3.985421696511657, 1.3364376842037782, 0.40072169534499924]]\n",
      "2020-12-23 02:52:42,597 : INFO : Removed 5 and 119 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:42,598 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:42,600 : INFO : built Dictionary(152 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 855 corpus positions)\n",
      "2020-12-23 02:52:42,672 : INFO : token count processed\n",
      "2020-12-23 02:52:42,674 : INFO : frequencies processed\n",
      "2020-12-23 02:52:42,800 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:42,801 : INFO : entropies processed\n",
      "2020-12-23 02:52:42,802 : INFO : extropies processed\n",
      "2020-12-23 02:52:42,803 : INFO : token count processed\n",
      "2020-12-23 02:52:42,804 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:42,805 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:42,806 : INFO : vocab #2480\n",
      "2020-12-23 02:52:42,807 : INFO : diff #set()\n",
      "2020-12-23 02:52:43,074 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:43,203 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.2460942613736472, 0.4452172899406405], [0.9586474522948265, 0.041352548], [2.5032583347756456, 1.2991301890771525], [4.386143391856655, 6.500767808767801, 6.613595356371134, 4.2733158442533234, 2.2274519645144784, 0.11282754760333269]]\n",
      "2020-12-23 02:52:43,205 : INFO : Removed 5 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:43,206 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:43,207 : INFO : built Dictionary(48 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 77 corpus positions)\n",
      "2020-12-23 02:52:43,219 : INFO : token count processed\n",
      "2020-12-23 02:52:43,222 : INFO : frequencies processed\n",
      "2020-12-23 02:52:43,349 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:43,350 : INFO : entropies processed\n",
      "2020-12-23 02:52:43,350 : INFO : extropies processed\n",
      "2020-12-23 02:52:43,352 : INFO : token count processed\n",
      "2020-12-23 02:52:43,353 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:43,353 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:43,354 : INFO : vocab #2480\n",
      "2020-12-23 02:52:43,355 : INFO : diff #set()\n",
      "2020-12-23 02:52:43,615 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:43,743 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.209289309381786, 0.4526342456614814], [0.9552310295403004, 0.04476897], [1.0, 1.0], [4.386143391856655, 4.736228843383063, 5.5105983956619315, 3.6117738395777863, 1.1244550038052763, 0.7743695522788689]]\n",
      "2020-12-23 02:52:43,745 : INFO : Removed 5 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:43,746 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:43,747 : INFO : built Dictionary(111 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 418 corpus positions)\n",
      "2020-12-23 02:52:43,797 : INFO : token count processed\n",
      "2020-12-23 02:52:43,800 : INFO : frequencies processed\n",
      "2020-12-23 02:52:43,926 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:43,927 : INFO : entropies processed\n",
      "2020-12-23 02:52:43,927 : INFO : extropies processed\n",
      "2020-12-23 02:52:43,928 : INFO : token count processed\n",
      "2020-12-23 02:52:43,929 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:43,930 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:43,930 : INFO : vocab #2480\n",
      "2020-12-23 02:52:43,931 : INFO : diff #set()\n",
      "2020-12-23 02:52:44,189 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:44,317 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.2615546829885191, 0.4421736991468875], [0.9719649832695723, 0.028035017], [0.9182958340544896, 0.9182958340544896], [4.386143391856655, 5.788442787590127, 6.0446513663197585, 4.129934813127024, 1.6585079744631033, 0.2562085787296313]]\n",
      "2020-12-23 02:52:44,320 : INFO : Removed 5 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:44,320 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:44,321 : INFO : built Dictionary(65 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 104 corpus positions)\n",
      "2020-12-23 02:52:44,341 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:52:44,343 : INFO : frequencies processed\n",
      "2020-12-23 02:52:44,472 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:44,472 : INFO : entropies processed\n",
      "2020-12-23 02:52:44,473 : INFO : extropies processed\n",
      "2020-12-23 02:52:44,474 : INFO : token count processed\n",
      "2020-12-23 02:52:44,475 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:44,476 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:44,477 : INFO : vocab #2480\n",
      "2020-12-23 02:52:44,478 : INFO : diff #set()\n",
      "2020-12-23 02:52:44,735 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:44,863 : INFO : Computed distances or similarities ('278', 'sacp-python-common/setup.py')[[1.1563944696352249, 0.4637370453696071], [0.8429513722658157, 0.15704863], [2.251629167387823, 1.2667563532600834], [4.386143391856655, 5.370004292053436, 5.821589129536948, 3.934558554373144, 1.4354457376802925, 0.4515848374835114]]\n",
      "2020-12-23 02:52:44,865 : INFO : Removed 5 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:44,866 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:44,867 : INFO : built Dictionary(94 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 349 corpus positions)\n",
      "2020-12-23 02:52:44,907 : INFO : token count processed\n",
      "2020-12-23 02:52:44,910 : INFO : frequencies processed\n",
      "2020-12-23 02:52:45,038 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:45,038 : INFO : entropies processed\n",
      "2020-12-23 02:52:45,039 : INFO : extropies processed\n",
      "2020-12-23 02:52:45,040 : INFO : token count processed\n",
      "2020-12-23 02:52:45,041 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:45,041 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:45,042 : INFO : vocab #2480\n",
      "2020-12-23 02:52:45,043 : INFO : diff #set()\n",
      "2020-12-23 02:52:45,300 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:45,427 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.2041372755792852, 0.45369225006059705], [0.9451194256544113, 0.054880574], [0.9182958340544896, 0.9182958340544896], [4.386143391856655, 5.695663584743922, 5.968151992686225, 4.113654983914353, 1.5820086008295702, 0.2724884079423031]]\n",
      "2020-12-23 02:52:45,430 : INFO : Removed 5 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:45,431 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:45,432 : INFO : built Dictionary(55 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 124 corpus positions)\n",
      "2020-12-23 02:52:45,453 : INFO : token count processed\n",
      "2020-12-23 02:52:45,456 : INFO : frequencies processed\n",
      "2020-12-23 02:52:45,587 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:45,587 : INFO : entropies processed\n",
      "2020-12-23 02:52:45,588 : INFO : extropies processed\n",
      "2020-12-23 02:52:45,590 : INFO : token count processed\n",
      "2020-12-23 02:52:45,590 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:45,591 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:45,592 : INFO : vocab #2480\n",
      "2020-12-23 02:52:45,593 : INFO : diff #set()\n",
      "2020-12-23 02:52:45,968 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:46,096 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.2015568001316759, 0.45422402907805504], [0.9386070631444454, 0.061392937], [1.0, 1.0], [4.386143391856655, 4.9004417692112465, 5.513876518749041, 3.77270864231886, 1.1277331268923856, 0.6134347495377943]]\n",
      "2020-12-23 02:52:46,099 : INFO : Removed 5 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:46,100 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:46,101 : INFO : built Dictionary(53 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 131 corpus positions)\n",
      "2020-12-23 02:52:46,122 : INFO : token count processed\n",
      "2020-12-23 02:52:46,124 : INFO : frequencies processed\n",
      "2020-12-23 02:52:46,256 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:46,256 : INFO : entropies processed\n",
      "2020-12-23 02:52:46,257 : INFO : extropies processed\n",
      "2020-12-23 02:52:46,258 : INFO : token count processed\n",
      "2020-12-23 02:52:46,260 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:46,260 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:46,261 : INFO : vocab #2480\n",
      "2020-12-23 02:52:46,263 : INFO : diff #set()\n",
      "2020-12-23 02:52:46,523 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:46,652 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.208037605449797, 0.45289083733530483], [0.9440510384738445, 0.05594896], [1.0, 1.0], [4.386143391856655, 4.778624108914332, 5.408861152022809, 3.755906348748179, 1.0227177601661541, 0.630237043108477]]\n",
      "2020-12-23 02:52:46,655 : INFO : Removed 5 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:46,656 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:46,657 : INFO : built Dictionary(54 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 162 corpus positions)\n",
      "2020-12-23 02:52:46,672 : INFO : token count processed\n",
      "2020-12-23 02:52:46,675 : INFO : frequencies processed\n",
      "2020-12-23 02:52:46,804 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:46,805 : INFO : entropies processed\n",
      "2020-12-23 02:52:46,806 : INFO : extropies processed\n",
      "2020-12-23 02:52:46,809 : INFO : token count processed\n",
      "2020-12-23 02:52:46,810 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:46,812 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:46,813 : INFO : vocab #2480\n",
      "2020-12-23 02:52:46,814 : INFO : diff #set()\n",
      "2020-12-23 02:52:47,075 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:47,201 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.2017022550912244, 0.4541940208707132], [0.944744810461998, 0.05525519], [1.0, 1.0], [4.386143391856655, 4.773880192225086, 5.338974606994108, 3.821048977087634, 0.9528312151374525, 0.565094414769022]]\n",
      "2020-12-23 02:52:47,205 : INFO : Removed 5 and 346 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:47,206 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:47,207 : INFO : built Dictionary(156 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 1987 corpus positions)\n",
      "2020-12-23 02:52:47,272 : INFO : token count processed\n",
      "2020-12-23 02:52:47,278 : INFO : frequencies processed\n",
      "2020-12-23 02:52:47,405 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:47,406 : INFO : entropies processed\n",
      "2020-12-23 02:52:47,407 : INFO : extropies processed\n",
      "2020-12-23 02:52:47,409 : INFO : token count processed\n",
      "2020-12-23 02:52:47,410 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:47,411 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:47,412 : INFO : vocab #2480\n",
      "2020-12-23 02:52:47,413 : INFO : diff #set()\n",
      "2020-12-23 02:52:47,670 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:47,798 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.1782201559915715, 0.4590904171230475], [0.9252372533082962, 0.07476275], [3.3927474104487847, 1.3672090515720436], [4.386143391856655, 6.620773041953877, 6.672423203968703, 4.334493229841828, 2.2862798121120473, 0.05165016201482597]]\n",
      "2020-12-23 02:52:47,801 : INFO : Removed 5 and 132 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:47,802 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:47,803 : INFO : built Dictionary(93 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 550 corpus positions)\n",
      "2020-12-23 02:52:47,835 : INFO : token count processed\n",
      "2020-12-23 02:52:47,838 : INFO : frequencies processed\n",
      "2020-12-23 02:52:47,964 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:47,965 : INFO : entropies processed\n",
      "2020-12-23 02:52:47,966 : INFO : extropies processed\n",
      "2020-12-23 02:52:47,967 : INFO : token count processed\n",
      "2020-12-23 02:52:47,968 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:47,969 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:47,970 : INFO : vocab #2480\n",
      "2020-12-23 02:52:47,971 : INFO : diff #set()\n",
      "2020-12-23 02:52:48,231 : INFO : alphabet #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:52:48,359 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.1658015732288518, 0.4617228154050906], [0.9195019453763962, 0.080498055], [1.9219280948873623, 1.2148067842293933], [4.386143391856655, 5.828370634755606, 5.999517113060985, 4.214996913551276, 1.6133737212043293, 0.17114647830537866]]\n",
      "2020-12-23 02:52:48,361 : INFO : Removed 5 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:48,362 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:48,366 : INFO : built Dictionary(96 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 313 corpus positions)\n",
      "2020-12-23 02:52:48,415 : INFO : token count processed\n",
      "2020-12-23 02:52:48,418 : INFO : frequencies processed\n",
      "2020-12-23 02:52:48,547 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:48,548 : INFO : entropies processed\n",
      "2020-12-23 02:52:48,549 : INFO : extropies processed\n",
      "2020-12-23 02:52:48,550 : INFO : token count processed\n",
      "2020-12-23 02:52:48,551 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:48,552 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:48,553 : INFO : vocab #2480\n",
      "2020-12-23 02:52:48,554 : INFO : diff #set()\n",
      "2020-12-23 02:52:48,813 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:48,941 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.20461497244553, 0.45359394383987267], [0.9502938240766525, 0.049706176], [1.0, 1.0], [4.386143391856655, 5.774409284925443, 6.062389222699966, 4.098163454082132, 1.6762458308433104, 0.2879799377745229]]\n",
      "2020-12-23 02:52:48,943 : INFO : Removed 5 and 51 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:48,944 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:48,946 : INFO : built Dictionary(107 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 362 corpus positions)\n",
      "2020-12-23 02:52:48,989 : INFO : token count processed\n",
      "2020-12-23 02:52:48,991 : INFO : frequencies processed\n",
      "2020-12-23 02:52:49,124 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:49,125 : INFO : entropies processed\n",
      "2020-12-23 02:52:49,126 : INFO : extropies processed\n",
      "2020-12-23 02:52:49,128 : INFO : token count processed\n",
      "2020-12-23 02:52:49,128 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:49,129 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:49,130 : INFO : vocab #2480\n",
      "2020-12-23 02:52:49,131 : INFO : diff #set()\n",
      "2020-12-23 02:52:49,391 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:49,519 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.1989859128702618, 0.4547550733031909], [0.9156422093510628, 0.08435779], [0.9182958340544896, 0.9182958340544896], [4.386143391856655, 5.977819040873918, 6.21260799967499, 4.151354433055584, 1.8264646078183349, 0.2347889588010723]]\n",
      "2020-12-23 02:52:49,521 : INFO : Removed 5 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:49,522 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:49,523 : INFO : built Dictionary(87 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 309 corpus positions)\n",
      "2020-12-23 02:52:49,556 : INFO : token count processed\n",
      "2020-12-23 02:52:49,562 : INFO : frequencies processed\n",
      "2020-12-23 02:52:49,690 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:49,691 : INFO : entropies processed\n",
      "2020-12-23 02:52:49,692 : INFO : extropies processed\n",
      "2020-12-23 02:52:49,694 : INFO : token count processed\n",
      "2020-12-23 02:52:49,695 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:49,696 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:49,697 : INFO : vocab #2480\n",
      "2020-12-23 02:52:49,698 : INFO : diff #set()\n",
      "2020-12-23 02:52:49,958 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:50,087 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.1888519834880689, 0.45686049469934426], [0.9087687358260155, 0.091231264], [1.5, 1.1225562489182657], [4.386143391856655, 5.901812829596593, 6.158931152381434, 4.129025069071814, 1.7727877605247784, 0.2571183227848408]]\n",
      "2020-12-23 02:52:50,089 : INFO : Removed 5 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:50,090 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:50,091 : INFO : built Dictionary(90 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 309 corpus positions)\n",
      "2020-12-23 02:52:50,124 : INFO : token count processed\n",
      "2020-12-23 02:52:50,129 : INFO : frequencies processed\n",
      "2020-12-23 02:52:50,261 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:50,262 : INFO : entropies processed\n",
      "2020-12-23 02:52:50,263 : INFO : extropies processed\n",
      "2020-12-23 02:52:50,264 : INFO : token count processed\n",
      "2020-12-23 02:52:50,266 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:50,267 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:50,267 : INFO : vocab #2480\n",
      "2020-12-23 02:52:50,269 : INFO : diff #set()\n",
      "2020-12-23 02:52:50,532 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:50,660 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.2149771220951067, 0.4514719317074111], [0.9402923546731472, 0.059707645], [0.9182958340544896, 0.9182958340544896], [4.386143391856655, 5.643202320803383, 5.941792810555983, 4.087552902104056, 1.5556494186993275, 0.29859048975260016]]\n",
      "2020-12-23 02:52:50,662 : INFO : Removed 5 and 30 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:50,663 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:50,664 : INFO : built Dictionary(103 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 343 corpus positions)\n",
      "2020-12-23 02:52:50,698 : INFO : token count processed\n",
      "2020-12-23 02:52:50,703 : INFO : frequencies processed\n",
      "2020-12-23 02:52:50,830 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:50,831 : INFO : entropies processed\n",
      "2020-12-23 02:52:50,831 : INFO : extropies processed\n",
      "2020-12-23 02:52:50,833 : INFO : token count processed\n",
      "2020-12-23 02:52:50,833 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:50,834 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:50,835 : INFO : vocab #2480\n",
      "2020-12-23 02:52:50,836 : INFO : diff #set()\n",
      "2020-12-23 02:52:51,099 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:51,231 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.224957547882154, 0.4494467775135122], [0.9396199509501457, 0.06038005], [0.9182958340544896, 0.9182958340544896], [4.386143391856655, 5.925214310725336, 6.181277871502965, 4.130079831079025, 1.7951344796463093, 0.25606356077762893]]\n",
      "2020-12-23 02:52:51,234 : INFO : Removed 5 and 293 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:51,235 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:51,236 : INFO : built Dictionary(167 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 1738 corpus positions)\n",
      "2020-12-23 02:52:51,309 : INFO : token count processed\n",
      "2020-12-23 02:52:51,312 : INFO : frequencies processed\n",
      "2020-12-23 02:52:51,443 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:51,443 : INFO : entropies processed\n",
      "2020-12-23 02:52:51,444 : INFO : extropies processed\n",
      "2020-12-23 02:52:51,445 : INFO : token count processed\n",
      "2020-12-23 02:52:51,446 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:51,447 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:51,447 : INFO : vocab #2480\n",
      "2020-12-23 02:52:51,448 : INFO : diff #set()\n",
      "2020-12-23 02:52:51,707 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:51,836 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.1755418908189126, 0.4596555939557579], [0.9231141805648804, 0.07688582], [3.3927474104487847, 1.3672090515720436], [4.386143391856655, 6.551685682764175, 6.608926185142797, 4.328902889478032, 2.222782793286142, 0.05724050237862244]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:52:51,838 : INFO : Removed 5 and 88 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:51,839 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:51,841 : INFO : built Dictionary(148 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 579 corpus positions)\n",
      "2020-12-23 02:52:51,904 : INFO : token count processed\n",
      "2020-12-23 02:52:51,906 : INFO : frequencies processed\n",
      "2020-12-23 02:52:52,034 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:52,035 : INFO : entropies processed\n",
      "2020-12-23 02:52:52,036 : INFO : extropies processed\n",
      "2020-12-23 02:52:52,038 : INFO : token count processed\n",
      "2020-12-23 02:52:52,039 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:52,040 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:52,041 : INFO : vocab #2480\n",
      "2020-12-23 02:52:52,043 : INFO : diff #set()\n",
      "2020-12-23 02:52:52,314 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:52,444 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.18925479885714, 0.4567764339363474], [0.9288355857133865, 0.071164414], [2.5, 1.2968140217166515], [4.386143391856655, 6.642985062562557, 6.7725060985082965, 4.256622355910915, 2.3863627066516413, 0.12952103594573927]]\n",
      "2020-12-23 02:52:52,446 : INFO : Removed 5 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:52,447 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:52,449 : INFO : built Dictionary(62 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 174 corpus positions)\n",
      "2020-12-23 02:52:52,472 : INFO : token count processed\n",
      "2020-12-23 02:52:52,474 : INFO : frequencies processed\n",
      "2020-12-23 02:52:52,602 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:52,603 : INFO : entropies processed\n",
      "2020-12-23 02:52:52,604 : INFO : extropies processed\n",
      "2020-12-23 02:52:52,605 : INFO : token count processed\n",
      "2020-12-23 02:52:52,606 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:52,607 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:52,607 : INFO : vocab #2480\n",
      "2020-12-23 02:52:52,608 : INFO : diff #set()\n",
      "2020-12-23 02:52:52,866 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:52,993 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.1564263589500468, 0.4637301876085836], [0.8900606259703636, 0.109939374], [2.2359263506290326, 1.2653331222512112], [4.386143391856655, 5.2461980344571995, 5.654713369508755, 3.9776280568050986, 1.2685699776521, 0.4085153350515558]]\n",
      "2020-12-23 02:52:52,996 : INFO : Removed 5 and 23 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:52,997 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:52,998 : INFO : built Dictionary(83 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 234 corpus positions)\n",
      "2020-12-23 02:52:53,033 : INFO : token count processed\n",
      "2020-12-23 02:52:53,036 : INFO : frequencies processed\n",
      "2020-12-23 02:52:53,166 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:53,167 : INFO : entropies processed\n",
      "2020-12-23 02:52:53,168 : INFO : extropies processed\n",
      "2020-12-23 02:52:53,169 : INFO : token count processed\n",
      "2020-12-23 02:52:53,170 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:53,171 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:53,171 : INFO : vocab #2480\n",
      "2020-12-23 02:52:53,172 : INFO : diff #set()\n",
      "2020-12-23 02:52:53,443 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:53,575 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/test_auth_utility.py')[[1.172677483506847, 0.4602615931684131], [0.9185213074088097, 0.08147869], [2.521640636343318, 1.2998438251349493], [4.386143391856655, 5.903090303960449, 6.173752191300632, 4.115481504516473, 1.7876087994439764, 0.27066188734018226]]\n",
      "2020-12-23 02:52:53,578 : INFO : Removed 5 and 442 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:53,579 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:53,580 : INFO : built Dictionary(121 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 1233 corpus positions)\n",
      "2020-12-23 02:52:53,623 : INFO : token count processed\n",
      "2020-12-23 02:52:53,626 : INFO : frequencies processed\n",
      "2020-12-23 02:52:53,758 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:53,759 : INFO : entropies processed\n",
      "2020-12-23 02:52:53,760 : INFO : extropies processed\n",
      "2020-12-23 02:52:53,762 : INFO : token count processed\n",
      "2020-12-23 02:52:53,763 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:53,764 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:53,765 : INFO : vocab #2480\n",
      "2020-12-23 02:52:53,766 : INFO : diff #set()\n",
      "2020-12-23 02:52:54,032 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:54,159 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.2452710384182792, 0.4453805277355146], [0.9746443890035152, 0.025355611], [0.9182958340544896, 0.9182958340544896], [4.386143391856655, 6.16659449033757, 6.259395749202756, 4.293342132991469, 1.873252357346101, 0.09280125886518675]]\n",
      "2020-12-23 02:52:54,162 : INFO : Removed 5 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:54,162 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:54,163 : INFO : built Dictionary(79 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 268 corpus positions)\n",
      "2020-12-23 02:52:54,195 : INFO : token count processed\n",
      "2020-12-23 02:52:54,200 : INFO : frequencies processed\n",
      "2020-12-23 02:52:54,333 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:54,334 : INFO : entropies processed\n",
      "2020-12-23 02:52:54,335 : INFO : extropies processed\n",
      "2020-12-23 02:52:54,336 : INFO : token count processed\n",
      "2020-12-23 02:52:54,337 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:54,338 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:54,338 : INFO : vocab #2480\n",
      "2020-12-23 02:52:54,339 : INFO : diff #set()\n",
      "2020-12-23 02:52:54,609 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:54,741 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.1996389308721962, 0.45462006785062764], [0.9442200623452663, 0.055779938], [2.0, 1.2451124978365313], [4.386143391856655, 5.906856253399655, 6.175646284035695, 4.117353361220616, 1.7895028921790397, 0.26879003063603957]]\n",
      "2020-12-23 02:52:54,744 : INFO : Removed 5 and 68 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:54,744 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:54,745 : INFO : built Dictionary(91 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 267 corpus positions)\n",
      "2020-12-23 02:52:54,778 : INFO : token count processed\n",
      "2020-12-23 02:52:54,783 : INFO : frequencies processed\n",
      "2020-12-23 02:52:54,914 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:54,915 : INFO : entropies processed\n",
      "2020-12-23 02:52:54,916 : INFO : extropies processed\n",
      "2020-12-23 02:52:54,917 : INFO : token count processed\n",
      "2020-12-23 02:52:54,918 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:54,919 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:54,919 : INFO : vocab #2480\n",
      "2020-12-23 02:52:54,920 : INFO : diff #set()\n",
      "2020-12-23 02:52:55,179 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:55,308 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.1739280186760763, 0.4599968312699703], [0.9139823168516159, 0.08601768], [2.321928094887362, 1.2877123795494492], [4.386143391856655, 5.965115449163356, 6.219161811028755, 4.132097029991257, 1.8330184191720997, 0.25404636186539875]]\n",
      "2020-12-23 02:52:55,310 : INFO : Removed 5 and 13 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:55,311 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:55,312 : INFO : built Dictionary(98 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 381 corpus positions)\n",
      "2020-12-23 02:52:55,353 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:52:55,358 : INFO : frequencies processed\n",
      "2020-12-23 02:52:55,484 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:55,484 : INFO : entropies processed\n",
      "2020-12-23 02:52:55,485 : INFO : extropies processed\n",
      "2020-12-23 02:52:55,486 : INFO : token count processed\n",
      "2020-12-23 02:52:55,487 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:55,487 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:55,488 : INFO : vocab #2480\n",
      "2020-12-23 02:52:55,490 : INFO : diff #set()\n",
      "2020-12-23 02:52:55,749 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:55,875 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.1763279094906105, 0.459489581344412], [0.9206266775727272, 0.07937332], [2.321928094887362, 1.2877123795494492], [4.386143391856655, 5.791362404253194, 6.032135996720852, 4.145369799388996, 1.6459926048641966, 0.24077359246765795]]\n",
      "2020-12-23 02:52:55,877 : INFO : Removed 5 and 41 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:55,878 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:55,880 : INFO : built Dictionary(88 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 393 corpus positions)\n",
      "2020-12-23 02:52:55,916 : INFO : token count processed\n",
      "2020-12-23 02:52:55,918 : INFO : frequencies processed\n",
      "2020-12-23 02:52:56,045 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:56,045 : INFO : entropies processed\n",
      "2020-12-23 02:52:56,046 : INFO : extropies processed\n",
      "2020-12-23 02:52:56,047 : INFO : token count processed\n",
      "2020-12-23 02:52:56,048 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:56,049 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:56,049 : INFO : vocab #2480\n",
      "2020-12-23 02:52:56,050 : INFO : diff #set()\n",
      "2020-12-23 02:52:56,310 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:56,438 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.1831648446898908, 0.4580506151114968], [0.9073952659964561, 0.092604734], [2.321928094887362, 1.2877123795494492], [4.386143391856655, 5.651670454631116, 5.8956541431981435, 4.142159703289629, 1.5095107513414883, 0.24398368856702746]]\n",
      "2020-12-23 02:52:56,440 : INFO : Removed 5 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:56,441 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:56,442 : INFO : built Dictionary(54 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 103 corpus positions)\n",
      "2020-12-23 02:52:56,457 : INFO : token count processed\n",
      "2020-12-23 02:52:56,459 : INFO : frequencies processed\n",
      "2020-12-23 02:52:56,587 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:56,588 : INFO : entropies processed\n",
      "2020-12-23 02:52:56,588 : INFO : extropies processed\n",
      "2020-12-23 02:52:56,590 : INFO : token count processed\n",
      "2020-12-23 02:52:56,590 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:56,591 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:56,592 : INFO : vocab #2480\n",
      "2020-12-23 02:52:56,593 : INFO : diff #set()\n",
      "2020-12-23 02:52:56,851 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:56,979 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.2092836912848564, 0.45263539668752484], [0.9344890490174294, 0.06551095], [1.0, 1.0], [4.386143391856655, 4.8226207261920235, 5.501476757076791, 3.707287360971888, 1.1153333652201356, 0.6788560308847673]]\n",
      "2020-12-23 02:52:56,982 : INFO : Removed 5 and 50 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:56,983 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:56,984 : INFO : built Dictionary(97 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 273 corpus positions)\n",
      "2020-12-23 02:52:57,020 : INFO : token count processed\n",
      "2020-12-23 02:52:57,024 : INFO : frequencies processed\n",
      "2020-12-23 02:52:57,152 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:57,152 : INFO : entropies processed\n",
      "2020-12-23 02:52:57,153 : INFO : extropies processed\n",
      "2020-12-23 02:52:57,154 : INFO : token count processed\n",
      "2020-12-23 02:52:57,155 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:57,156 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:57,157 : INFO : vocab #2480\n",
      "2020-12-23 02:52:57,157 : INFO : diff #set()\n",
      "2020-12-23 02:52:57,418 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:57,545 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.2037162557130263, 0.4537789279393611], [0.921143189072609, 0.07885681], [2.251629167387823, 1.2667563532600834], [4.386143391856655, 6.24862851613934, 6.473631311064557, 4.161140596931438, 2.087487919207902, 0.22500279492521713]]\n",
      "2020-12-23 02:52:57,547 : INFO : Removed 5 and 32 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:57,548 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:57,550 : INFO : built Dictionary(97 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 389 corpus positions)\n",
      "2020-12-23 02:52:57,588 : INFO : token count processed\n",
      "2020-12-23 02:52:57,591 : INFO : frequencies processed\n",
      "2020-12-23 02:52:57,717 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:57,718 : INFO : entropies processed\n",
      "2020-12-23 02:52:57,718 : INFO : extropies processed\n",
      "2020-12-23 02:52:57,720 : INFO : token count processed\n",
      "2020-12-23 02:52:57,720 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:57,721 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:57,722 : INFO : vocab #2480\n",
      "2020-12-23 02:52:57,723 : INFO : diff #set()\n",
      "2020-12-23 02:52:57,986 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:58,114 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.17867645579429, 0.4589942656884432], [0.9224276319146156, 0.07757237], [2.321928094887362, 1.2877123795494492], [4.386143391856655, 5.850156917433494, 6.0785381591424485, 4.1577621501477005, 1.6923947672857933, 0.2283812417089548]]\n",
      "2020-12-23 02:52:58,116 : INFO : Removed 5 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:58,117 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:58,119 : INFO : built Dictionary(92 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 387 corpus positions)\n",
      "2020-12-23 02:52:58,159 : INFO : token count processed\n",
      "2020-12-23 02:52:58,162 : INFO : frequencies processed\n",
      "2020-12-23 02:52:58,292 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:58,292 : INFO : entropies processed\n",
      "2020-12-23 02:52:58,293 : INFO : extropies processed\n",
      "2020-12-23 02:52:58,294 : INFO : token count processed\n",
      "2020-12-23 02:52:58,295 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:58,296 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:58,297 : INFO : vocab #2480\n",
      "2020-12-23 02:52:58,298 : INFO : diff #set()\n",
      "2020-12-23 02:52:58,555 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:58,682 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.1814656917215665, 0.4584073926969813], [0.9053924307227135, 0.09460757], [2.321928094887362, 1.2877123795494492], [4.386143391856655, 5.6831976040360095, 5.926299485813191, 4.143041510079474, 1.5401560939565355, 0.24310188177718128]]\n",
      "2020-12-23 02:52:58,685 : INFO : Removed 5 and 52 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:58,686 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:58,687 : INFO : built Dictionary(82 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 365 corpus positions)\n",
      "2020-12-23 02:52:58,719 : INFO : token count processed\n",
      "2020-12-23 02:52:58,724 : INFO : frequencies processed\n",
      "2020-12-23 02:52:58,852 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:58,853 : INFO : entropies processed\n",
      "2020-12-23 02:52:58,854 : INFO : extropies processed\n",
      "2020-12-23 02:52:58,854 : INFO : token count processed\n",
      "2020-12-23 02:52:58,855 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:58,856 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:58,856 : INFO : vocab #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:52:58,857 : INFO : diff #set()\n",
      "2020-12-23 02:52:59,115 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:59,243 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.2107181299367902, 0.45234170130435963], [0.9282956421375275, 0.07170436], [0.9182958340544896, 0.9182958340544896], [4.386143391856655, 5.749308601266266, 6.000911326473783, 4.134540666649138, 1.6147679346171273, 0.25160272520751636]]\n",
      "2020-12-23 02:52:59,246 : INFO : Removed 5 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:59,246 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:59,247 : INFO : built Dictionary(75 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 246 corpus positions)\n",
      "2020-12-23 02:52:59,270 : INFO : token count processed\n",
      "2020-12-23 02:52:59,273 : INFO : frequencies processed\n",
      "2020-12-23 02:52:59,401 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:59,402 : INFO : entropies processed\n",
      "2020-12-23 02:52:59,402 : INFO : extropies processed\n",
      "2020-12-23 02:52:59,404 : INFO : token count processed\n",
      "2020-12-23 02:52:59,404 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:59,405 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:59,406 : INFO : vocab #2480\n",
      "2020-12-23 02:52:59,407 : INFO : diff #set()\n",
      "2020-12-23 02:52:59,666 : INFO : alphabet #2480\n",
      "2020-12-23 02:52:59,793 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.1942329174660622, 0.4557401322530596], [0.9328539595007896, 0.06714604], [1.0, 1.0], [4.386143391856655, 5.015422548793484, 5.444014049118695, 3.957551891531444, 1.0578706572620398, 0.4285915003252114]]\n",
      "2020-12-23 02:52:59,795 : INFO : Removed 5 and 81 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:52:59,796 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:52:59,797 : INFO : built Dictionary(101 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 372 corpus positions)\n",
      "2020-12-23 02:52:59,837 : INFO : token count processed\n",
      "2020-12-23 02:52:59,840 : INFO : frequencies processed\n",
      "2020-12-23 02:52:59,967 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:52:59,968 : INFO : entropies processed\n",
      "2020-12-23 02:52:59,969 : INFO : extropies processed\n",
      "2020-12-23 02:52:59,970 : INFO : token count processed\n",
      "2020-12-23 02:52:59,971 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:52:59,971 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:52:59,972 : INFO : vocab #2480\n",
      "2020-12-23 02:52:59,973 : INFO : diff #set()\n",
      "2020-12-23 02:53:00,226 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:00,353 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.1739537628935979, 0.45999138393310157], [0.9238907620310783, 0.07610924], [2.321928094887362, 1.2877123795494492], [4.386143391856655, 6.030001281822029, 6.237848951429484, 4.178295722249199, 1.851705559572829, 0.20784766960745493]]\n",
      "2020-12-23 02:53:00,355 : INFO : Removed 5 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:00,356 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:00,358 : INFO : built Dictionary(92 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 335 corpus positions)\n",
      "2020-12-23 02:53:00,400 : INFO : token count processed\n",
      "2020-12-23 02:53:00,402 : INFO : frequencies processed\n",
      "2020-12-23 02:53:00,531 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:00,531 : INFO : entropies processed\n",
      "2020-12-23 02:53:00,532 : INFO : extropies processed\n",
      "2020-12-23 02:53:00,533 : INFO : token count processed\n",
      "2020-12-23 02:53:00,533 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:00,534 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:00,534 : INFO : vocab #2480\n",
      "2020-12-23 02:53:00,535 : INFO : diff #set()\n",
      "2020-12-23 02:53:00,804 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:00,933 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.2026299869986783, 0.4540027176160478], [0.934911385178566, 0.065088615], [0.9182958340544896, 0.9182958340544896], [4.386143391856655, 5.9537092545441395, 6.202592715056709, 4.137259931344086, 1.8164493232000538, 0.2488834605125696]]\n",
      "2020-12-23 02:53:00,935 : INFO : Removed 5 and 90 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:00,936 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:00,937 : INFO : built Dictionary(102 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 336 corpus positions)\n",
      "2020-12-23 02:53:00,973 : INFO : token count processed\n",
      "2020-12-23 02:53:00,978 : INFO : frequencies processed\n",
      "2020-12-23 02:53:01,109 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:01,110 : INFO : entropies processed\n",
      "2020-12-23 02:53:01,111 : INFO : extropies processed\n",
      "2020-12-23 02:53:01,112 : INFO : token count processed\n",
      "2020-12-23 02:53:01,113 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:01,114 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:01,114 : INFO : vocab #2480\n",
      "2020-12-23 02:53:01,116 : INFO : diff #set()\n",
      "2020-12-23 02:53:01,375 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:01,503 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.1794881879719965, 0.4588233171066164], [0.9107418730854988, 0.08925813], [1.584962500721156, 1.1699250014423124], [4.386143391856655, 6.184756445474906, 6.402058808670634, 4.168841028660927, 2.0159154168139786, 0.21730236319572782]]\n",
      "2020-12-23 02:53:01,505 : INFO : Removed 5 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:01,506 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:01,507 : INFO : built Dictionary(109 unique tokens: ['analysi', 'bug', 'build', 'cisco', 'close']...) from 2 documents (total 449 corpus positions)\n",
      "2020-12-23 02:53:01,546 : INFO : token count processed\n",
      "2020-12-23 02:53:01,549 : INFO : frequencies processed\n",
      "2020-12-23 02:53:01,676 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:01,677 : INFO : entropies processed\n",
      "2020-12-23 02:53:01,678 : INFO : extropies processed\n",
      "2020-12-23 02:53:01,679 : INFO : token count processed\n",
      "2020-12-23 02:53:01,680 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:01,681 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:01,684 : INFO : vocab #2480\n",
      "2020-12-23 02:53:01,686 : INFO : diff #set()\n",
      "2020-12-23 02:53:01,941 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:02,069 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.183510821293109, 0.4579780371355267], [0.918310709297657, 0.08168929], [2.321928094887362, 1.2877123795494492], [4.386143391856655, 6.212221456585881, 6.397311558034772, 4.2010532904077635, 2.011168166178117, 0.1850901014488917]]\n",
      "2020-12-23 02:53:02,072 : INFO : Removed 2 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:02,072 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:02,074 : INFO : built Dictionary(131 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 428 corpus positions)\n",
      "2020-12-23 02:53:02,124 : INFO : token count processed\n",
      "2020-12-23 02:53:02,127 : INFO : frequencies processed\n",
      "2020-12-23 02:53:02,255 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:02,255 : INFO : entropies processed\n",
      "2020-12-23 02:53:02,256 : INFO : extropies processed\n",
      "2020-12-23 02:53:02,258 : INFO : token count processed\n",
      "2020-12-23 02:53:02,259 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:02,260 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:02,261 : INFO : vocab #2480\n",
      "2020-12-23 02:53:02,262 : INFO : diff #set()\n",
      "2020-12-23 02:53:02,523 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:02,650 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.2195237249510578, 0.45054711006617004], [0.9417950250208378, 0.058204975], [1.0, 1.0], [4.349648912578752, 6.301552355933639, 6.481160821518037, 4.170040446994355, 2.1315119089392844, 0.1796084655843977]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:53:02,653 : INFO : Removed 2 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:02,654 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:02,656 : INFO : built Dictionary(165 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 664 corpus positions)\n",
      "2020-12-23 02:53:02,731 : INFO : token count processed\n",
      "2020-12-23 02:53:02,737 : INFO : frequencies processed\n",
      "2020-12-23 02:53:02,864 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:02,865 : INFO : entropies processed\n",
      "2020-12-23 02:53:02,866 : INFO : extropies processed\n",
      "2020-12-23 02:53:02,867 : INFO : token count processed\n",
      "2020-12-23 02:53:02,868 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:02,868 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:02,869 : INFO : vocab #2480\n",
      "2020-12-23 02:53:02,870 : INFO : diff #set()\n",
      "2020-12-23 02:53:03,136 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:03,264 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.1715994251272674, 0.46049008322121593], [0.8484843969345093, 0.1515156], [2.807354922057604, 1.3343545280186873], [4.349648912578752, 6.739005504021667, 6.8335254835259285, 4.2551289330744915, 2.483876570947176, 0.09451997950426172]]\n",
      "2020-12-23 02:53:03,266 : INFO : Removed 2 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:03,267 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:03,268 : INFO : built Dictionary(113 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 515 corpus positions)\n",
      "2020-12-23 02:53:03,314 : INFO : token count processed\n",
      "2020-12-23 02:53:03,317 : INFO : frequencies processed\n",
      "2020-12-23 02:53:03,443 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:03,444 : INFO : entropies processed\n",
      "2020-12-23 02:53:03,445 : INFO : extropies processed\n",
      "2020-12-23 02:53:03,446 : INFO : token count processed\n",
      "2020-12-23 02:53:03,446 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:03,447 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:03,448 : INFO : vocab #2480\n",
      "2020-12-23 02:53:03,448 : INFO : diff #set()\n",
      "2020-12-23 02:53:03,707 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:03,835 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2372755679249654, 0.4469722077765695], [0.9499278627336025, 0.050072137], [1.584962500721156, 1.1699250014423124], [4.349648912578752, 5.870833373337847, 6.025831443031263, 4.194650842885336, 1.676182530452511, 0.15499806969341634]]\n",
      "2020-12-23 02:53:03,837 : INFO : Removed 2 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:03,838 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:03,839 : INFO : built Dictionary(73 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 198 corpus positions)\n",
      "2020-12-23 02:53:03,860 : INFO : token count processed\n",
      "2020-12-23 02:53:03,863 : INFO : frequencies processed\n",
      "2020-12-23 02:53:03,990 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:03,991 : INFO : entropies processed\n",
      "2020-12-23 02:53:03,992 : INFO : extropies processed\n",
      "2020-12-23 02:53:03,993 : INFO : token count processed\n",
      "2020-12-23 02:53:03,994 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:03,995 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:03,995 : INFO : vocab #2480\n",
      "2020-12-23 02:53:03,996 : INFO : diff #set()\n",
      "2020-12-23 02:53:04,255 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:04,383 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.1809285489020651, 0.4585202942588035], [0.8994844630360603, 0.10051554], [1.0, 1.0], [4.349648912578752, 5.371881234145534, 5.72453185364586, 3.996998293078428, 1.3748829410671073, 0.3526506195003254]]\n",
      "2020-12-23 02:53:04,385 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:04,386 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:04,387 : INFO : built Dictionary(64 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 150 corpus positions)\n",
      "2020-12-23 02:53:04,405 : INFO : token count processed\n",
      "2020-12-23 02:53:04,407 : INFO : frequencies processed\n",
      "2020-12-23 02:53:04,535 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:04,535 : INFO : entropies processed\n",
      "2020-12-23 02:53:04,536 : INFO : extropies processed\n",
      "2020-12-23 02:53:04,537 : INFO : token count processed\n",
      "2020-12-23 02:53:04,538 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:04,539 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:04,540 : INFO : vocab #2480\n",
      "2020-12-23 02:53:04,541 : INFO : diff #set()\n",
      "2020-12-23 02:53:04,799 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:04,928 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.1745719652231728, 0.45986061440710774], [0.9130508378148079, 0.08694916], [1.0, 1.0], [4.349648912578752, 4.85108279267097, 5.347798384023611, 3.8529333212261117, 0.9981494714448589, 0.49671559135264154]]\n",
      "2020-12-23 02:53:04,930 : INFO : Removed 2 and 127 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:04,931 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:04,932 : INFO : built Dictionary(104 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 423 corpus positions)\n",
      "2020-12-23 02:53:04,975 : INFO : token count processed\n",
      "2020-12-23 02:53:04,978 : INFO : frequencies processed\n",
      "2020-12-23 02:53:05,104 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:05,104 : INFO : entropies processed\n",
      "2020-12-23 02:53:05,105 : INFO : extropies processed\n",
      "2020-12-23 02:53:05,106 : INFO : token count processed\n",
      "2020-12-23 02:53:05,107 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:05,107 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:05,108 : INFO : vocab #2480\n",
      "2020-12-23 02:53:05,109 : INFO : diff #set()\n",
      "2020-12-23 02:53:05,480 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:05,608 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.2223313147206938, 0.4499779098535007], [0.9380668178200722, 0.061933182], [1.584962500721156, 1.1699250014423124], [4.349648912578752, 6.139571208108155, 6.299194017555054, 4.190026103131853, 1.949545104976302, 0.15962280944689944]]\n",
      "2020-12-23 02:53:05,610 : INFO : Removed 2 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:05,611 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:05,612 : INFO : built Dictionary(91 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 433 corpus positions)\n",
      "2020-12-23 02:53:05,647 : INFO : token count processed\n",
      "2020-12-23 02:53:05,649 : INFO : frequencies processed\n",
      "2020-12-23 02:53:05,776 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:05,777 : INFO : entropies processed\n",
      "2020-12-23 02:53:05,778 : INFO : extropies processed\n",
      "2020-12-23 02:53:05,779 : INFO : token count processed\n",
      "2020-12-23 02:53:05,780 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:05,781 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:05,782 : INFO : vocab #2480\n",
      "2020-12-23 02:53:05,783 : INFO : diff #set()\n",
      "2020-12-23 02:53:06,040 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:06,168 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.1957124166089244, 0.4554330487161009], [0.870756208896637, 0.12924379], [1.0, 1.0], [4.349648912578752, 5.609710627339259, 5.818917004952158, 4.140442534965854, 1.4692680923734054, 0.2092063776128983]]\n",
      "2020-12-23 02:53:06,171 : INFO : Removed 2 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:06,172 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:06,173 : INFO : built Dictionary(179 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 1093 corpus positions)\n",
      "2020-12-23 02:53:06,242 : INFO : token count processed\n",
      "2020-12-23 02:53:06,244 : INFO : frequencies processed\n",
      "2020-12-23 02:53:06,374 : INFO : scalar_distribution processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:53:06,375 : INFO : entropies processed\n",
      "2020-12-23 02:53:06,375 : INFO : extropies processed\n",
      "2020-12-23 02:53:06,377 : INFO : token count processed\n",
      "2020-12-23 02:53:06,378 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:06,379 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:06,380 : INFO : vocab #2480\n",
      "2020-12-23 02:53:06,381 : INFO : diff #set()\n",
      "2020-12-23 02:53:06,649 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:06,779 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.2115475464153769, 0.45217205554584], [0.8977648764848709, 0.10223512], [2.584962500721156, 1.315172029168969], [4.349648912578752, 7.2441902753576075, 7.301601455235693, 4.292237732700667, 2.951952542656941, 0.05741117987808586]]\n",
      "2020-12-23 02:53:06,781 : INFO : Removed 2 and 136 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:06,782 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:06,783 : INFO : built Dictionary(141 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 700 corpus positions)\n",
      "2020-12-23 02:53:06,840 : INFO : token count processed\n",
      "2020-12-23 02:53:06,846 : INFO : frequencies processed\n",
      "2020-12-23 02:53:06,974 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:06,975 : INFO : entropies processed\n",
      "2020-12-23 02:53:06,976 : INFO : extropies processed\n",
      "2020-12-23 02:53:06,977 : INFO : token count processed\n",
      "2020-12-23 02:53:06,978 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:06,979 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:06,979 : INFO : vocab #2480\n",
      "2020-12-23 02:53:06,980 : INFO : diff #set()\n",
      "2020-12-23 02:53:07,245 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:07,372 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.1419704925938783, 0.4668598393197389], [0.8032775521278381, 0.19672245], [2.584962500721156, 1.315172029168969], [4.349648912578752, 6.2567074920449475, 6.358620588118358, 4.247735816505342, 2.0089716755396054, 0.10191309607341026]]\n",
      "2020-12-23 02:53:07,375 : INFO : Removed 2 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:07,376 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:07,377 : INFO : built Dictionary(89 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 210 corpus positions)\n",
      "2020-12-23 02:53:07,403 : INFO : token count processed\n",
      "2020-12-23 02:53:07,406 : INFO : frequencies processed\n",
      "2020-12-23 02:53:07,533 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:07,533 : INFO : entropies processed\n",
      "2020-12-23 02:53:07,534 : INFO : extropies processed\n",
      "2020-12-23 02:53:07,535 : INFO : token count processed\n",
      "2020-12-23 02:53:07,536 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:07,537 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:07,538 : INFO : vocab #2480\n",
      "2020-12-23 02:53:07,539 : INFO : diff #set()\n",
      "2020-12-23 02:53:07,797 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:07,925 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.1920238215996435, 0.45619942180657674], [0.882161058485508, 0.11783894], [1.584962500721156, 1.1699250014423124], [4.349648912578752, 5.7680018917339435, 6.05939921657227, 4.0582515877404255, 1.709750303993518, 0.2913973248383268]]\n",
      "2020-12-23 02:53:07,928 : INFO : Removed 2 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:07,928 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:07,930 : INFO : built Dictionary(184 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 751 corpus positions)\n",
      "2020-12-23 02:53:08,005 : INFO : token count processed\n",
      "2020-12-23 02:53:08,007 : INFO : frequencies processed\n",
      "2020-12-23 02:53:08,133 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:08,134 : INFO : entropies processed\n",
      "2020-12-23 02:53:08,135 : INFO : extropies processed\n",
      "2020-12-23 02:53:08,136 : INFO : token count processed\n",
      "2020-12-23 02:53:08,137 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:08,138 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:08,138 : INFO : vocab #2480\n",
      "2020-12-23 02:53:08,139 : INFO : diff #set()\n",
      "2020-12-23 02:53:08,405 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:08,533 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.170281200477496, 0.46076978401692104], [0.8368529975414276, 0.163147], [2.584962500721156, 1.315172029168969], [4.349648912578752, 6.846479111193757, 6.9317787692860495, 4.26434925448646, 2.582129856707297, 0.08529965809229267]]\n",
      "2020-12-23 02:53:08,536 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:08,537 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:08,538 : INFO : built Dictionary(39 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 54 corpus positions)\n",
      "2020-12-23 02:53:08,546 : INFO : token count processed\n",
      "2020-12-23 02:53:08,551 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:53:08,552 : INFO : frequencies processed\n",
      "2020-12-23 02:53:08,553 : INFO : token count processed\n",
      "2020-12-23 02:53:08,555 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:08,556 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:08,557 : INFO : vocab #2480\n",
      "2020-12-23 02:53:08,559 : INFO : diff #set()\n",
      "2020-12-23 02:53:08,825 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:08,952 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2323215856543412, 0.4479641313448477], [0.9716418497264385, 0.02835815], [nan, nan], [4.349648912578752, 4.165013816065912, 5.217720100474499, 3.2969426281701653, 0.8680711878957466, 1.052706284408587]]\n",
      "2020-12-23 02:53:08,954 : INFO : Removed 2 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:08,955 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:08,956 : INFO : built Dictionary(65 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 161 corpus positions)\n",
      "2020-12-23 02:53:08,975 : INFO : token count processed\n",
      "2020-12-23 02:53:08,980 : INFO : frequencies processed\n",
      "2020-12-23 02:53:09,108 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:09,109 : INFO : entropies processed\n",
      "2020-12-23 02:53:09,110 : INFO : extropies processed\n",
      "2020-12-23 02:53:09,111 : INFO : token count processed\n",
      "2020-12-23 02:53:09,112 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:09,113 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:09,114 : INFO : vocab #2480\n",
      "2020-12-23 02:53:09,114 : INFO : diff #set()\n",
      "2020-12-23 02:53:09,373 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:09,501 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.1513161171760216, 0.4648317334751691], [0.8632092177867889, 0.13679078], [1.584962500721156, 1.1699250014423124], [4.349648912578752, 5.449968864419248, 5.812601052472912, 3.9870167245250894, 1.4629521398941598, 0.3626321880536638]]\n",
      "2020-12-23 02:53:09,504 : INFO : Removed 2 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:09,505 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:09,506 : INFO : built Dictionary(154 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 598 corpus positions)\n",
      "2020-12-23 02:53:09,577 : INFO : token count processed\n",
      "2020-12-23 02:53:09,579 : INFO : frequencies processed\n",
      "2020-12-23 02:53:09,707 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:09,708 : INFO : entropies processed\n",
      "2020-12-23 02:53:09,708 : INFO : extropies processed\n",
      "2020-12-23 02:53:09,709 : INFO : token count processed\n",
      "2020-12-23 02:53:09,710 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:09,711 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:09,711 : INFO : vocab #2480\n",
      "2020-12-23 02:53:09,712 : INFO : diff #set()\n",
      "2020-12-23 02:53:09,980 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:10,108 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.181877509243731, 0.4583208707928861], [0.8841275274753571, 0.11587247], [2.807354922057604, 1.3343545280186873], [4.349648912578752, 6.530294129310484, 6.630166309093912, 4.249776732795324, 2.28051739651516, 0.09987217978342855]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:53:10,111 : INFO : Removed 2 and 142 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:10,112 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:10,113 : INFO : built Dictionary(132 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 614 corpus positions)\n",
      "2020-12-23 02:53:10,164 : INFO : token count processed\n",
      "2020-12-23 02:53:10,166 : INFO : frequencies processed\n",
      "2020-12-23 02:53:10,296 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:10,297 : INFO : entropies processed\n",
      "2020-12-23 02:53:10,298 : INFO : extropies processed\n",
      "2020-12-23 02:53:10,299 : INFO : token count processed\n",
      "2020-12-23 02:53:10,300 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:10,301 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:10,302 : INFO : vocab #2480\n",
      "2020-12-23 02:53:10,304 : INFO : diff #set()\n",
      "2020-12-23 02:53:10,572 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:10,703 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.2208427161148183, 0.45027952350872363], [0.9449097216129303, 0.05509028], [1.0, 1.0], [4.349648912578752, 6.470272233491701, 6.593659912674871, 4.226261233395581, 2.244011000096119, 0.12338767918317028]]\n",
      "2020-12-23 02:53:10,706 : INFO : Removed 2 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:10,707 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:10,708 : INFO : built Dictionary(129 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 403 corpus positions)\n",
      "2020-12-23 02:53:10,758 : INFO : token count processed\n",
      "2020-12-23 02:53:10,761 : INFO : frequencies processed\n",
      "2020-12-23 02:53:10,886 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:10,887 : INFO : entropies processed\n",
      "2020-12-23 02:53:10,887 : INFO : extropies processed\n",
      "2020-12-23 02:53:10,888 : INFO : token count processed\n",
      "2020-12-23 02:53:10,889 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:10,890 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:10,890 : INFO : vocab #2480\n",
      "2020-12-23 02:53:10,891 : INFO : diff #set()\n",
      "2020-12-23 02:53:11,148 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:11,276 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.1699897366368657, 0.460831672664885], [0.8408266305923462, 0.15917337], [2.584962500721156, 1.315172029168969], [4.349648912578752, 6.550038223589686, 6.684178919690131, 4.215508216478307, 2.334530007111379, 0.1341406961004452]]\n",
      "2020-12-23 02:53:11,279 : INFO : Removed 2 and 70 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:11,280 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:11,281 : INFO : built Dictionary(87 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 319 corpus positions)\n",
      "2020-12-23 02:53:11,318 : INFO : token count processed\n",
      "2020-12-23 02:53:11,322 : INFO : frequencies processed\n",
      "2020-12-23 02:53:11,452 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:11,453 : INFO : entropies processed\n",
      "2020-12-23 02:53:11,453 : INFO : extropies processed\n",
      "2020-12-23 02:53:11,455 : INFO : token count processed\n",
      "2020-12-23 02:53:11,455 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:11,456 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:11,457 : INFO : vocab #2480\n",
      "2020-12-23 02:53:11,458 : INFO : diff #set()\n",
      "2020-12-23 02:53:11,724 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:11,851 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.2031653515308083, 0.45389239591353314], [0.926719181239605, 0.07328082], [1.584962500721156, 1.1699250014423124], [4.349648912578752, 5.860525481261383, 6.072271741663246, 4.137902652176888, 1.7226228290844938, 0.2117462604018634]]\n",
      "2020-12-23 02:53:11,854 : INFO : Removed 2 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:11,855 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:11,856 : INFO : built Dictionary(60 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 357 corpus positions)\n",
      "2020-12-23 02:53:11,881 : INFO : token count processed\n",
      "2020-12-23 02:53:11,883 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:53:11,884 : INFO : frequencies processed\n",
      "2020-12-23 02:53:11,885 : INFO : token count processed\n",
      "2020-12-23 02:53:11,886 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:11,887 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:11,887 : INFO : vocab #2480\n",
      "2020-12-23 02:53:11,888 : INFO : diff #set()\n",
      "2020-12-23 02:53:12,140 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:12,268 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.2842040681231615, 0.4377892562032165], [0.9681638404726982, 0.03183616], [nan, nan], [4.349648912578752, 5.945464049777852, 6.134571484927687, 4.160541477428918, 1.7849225723489344, 0.1891074351498343]]\n",
      "2020-12-23 02:53:12,271 : INFO : Removed 2 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:12,272 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:12,273 : INFO : built Dictionary(204 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 904 corpus positions)\n",
      "2020-12-23 02:53:12,362 : INFO : token count processed\n",
      "2020-12-23 02:53:12,365 : INFO : frequencies processed\n",
      "2020-12-23 02:53:12,498 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:12,499 : INFO : entropies processed\n",
      "2020-12-23 02:53:12,500 : INFO : extropies processed\n",
      "2020-12-23 02:53:12,501 : INFO : token count processed\n",
      "2020-12-23 02:53:12,502 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:12,503 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:12,503 : INFO : vocab #2480\n",
      "2020-12-23 02:53:12,504 : INFO : diff #set()\n",
      "2020-12-23 02:53:12,773 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:12,900 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.1650870483477378, 0.46187519377714575], [0.8168571591377258, 0.18314284], [2.321928094887362, 1.2877123795494492], [4.349648912578752, 6.811563897304216, 6.890886644597819, 4.27032616528515, 2.5412377320190664, 0.07932274729360245]]\n",
      "2020-12-23 02:53:12,903 : INFO : Removed 2 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:12,904 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:12,905 : INFO : built Dictionary(221 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 999 corpus positions)\n",
      "2020-12-23 02:53:13,003 : INFO : token count processed\n",
      "2020-12-23 02:53:13,005 : INFO : frequencies processed\n",
      "2020-12-23 02:53:13,135 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:13,136 : INFO : entropies processed\n",
      "2020-12-23 02:53:13,136 : INFO : extropies processed\n",
      "2020-12-23 02:53:13,138 : INFO : token count processed\n",
      "2020-12-23 02:53:13,139 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:13,140 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:13,141 : INFO : vocab #2480\n",
      "2020-12-23 02:53:13,142 : INFO : diff #set()\n",
      "2020-12-23 02:53:13,412 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:13,541 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.1723275512713673, 0.46033573501139097], [0.8322755545377731, 0.16772445], [3.169925001442312, 1.3594000115384994], [4.349648912578752, 7.502034948968415, 7.550247593588217, 4.30143626795895, 3.200598681009465, 0.04821264461980235]]\n",
      "2020-12-23 02:53:13,545 : INFO : Removed 2 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:13,545 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:13,547 : INFO : built Dictionary(268 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 1572 corpus positions)\n",
      "2020-12-23 02:53:13,681 : INFO : token count processed\n",
      "2020-12-23 02:53:13,684 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:53:13,812 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:13,813 : INFO : entropies processed\n",
      "2020-12-23 02:53:13,813 : INFO : extropies processed\n",
      "2020-12-23 02:53:13,815 : INFO : token count processed\n",
      "2020-12-23 02:53:13,816 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:13,816 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:13,817 : INFO : vocab #2480\n",
      "2020-12-23 02:53:13,818 : INFO : diff #set()\n",
      "2020-12-23 02:53:14,078 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:14,204 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.1739478147080948, 0.4599926425254482], [0.8233599215745926, 0.17664008], [2.584962500721156, 1.315172029168969], [4.349648912578752, 7.39180093901977, 7.435961264147814, 4.305488587450709, 3.0863123515690614, 0.044160325128043354]]\n",
      "2020-12-23 02:53:14,207 : INFO : Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:14,208 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:14,209 : INFO : built Dictionary(55 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 127 corpus positions)\n",
      "2020-12-23 02:53:14,230 : INFO : token count processed\n",
      "2020-12-23 02:53:14,232 : INFO : frequencies processed\n",
      "2020-12-23 02:53:14,360 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:14,360 : INFO : entropies processed\n",
      "2020-12-23 02:53:14,361 : INFO : extropies processed\n",
      "2020-12-23 02:53:14,363 : INFO : token count processed\n",
      "2020-12-23 02:53:14,363 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:14,364 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:14,365 : INFO : vocab #2480\n",
      "2020-12-23 02:53:14,366 : INFO : diff #set()\n",
      "2020-12-23 02:53:14,624 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:14,750 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.1560552900976167, 0.4638099980982976], [0.8852033913135529, 0.11479661], [1.584962500721156, 1.1699250014423124], [4.349648912578752, 4.927561309677364, 5.430321850235519, 3.8468883720205973, 1.0806729376567663, 0.502760540558155]]\n",
      "2020-12-23 02:53:14,753 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:14,754 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:14,755 : INFO : built Dictionary(25 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 28 corpus positions)\n",
      "2020-12-23 02:53:14,763 : INFO : token count processed\n",
      "2020-12-23 02:53:14,766 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:53:14,767 : INFO : frequencies processed\n",
      "2020-12-23 02:53:14,769 : INFO : token count processed\n",
      "2020-12-23 02:53:14,770 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:14,771 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:14,772 : INFO : vocab #2480\n",
      "2020-12-23 02:53:14,773 : INFO : diff #set()\n",
      "2020-12-23 02:53:15,040 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:15,171 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2181344367107299, 0.4508293020701213], [0.8335715532302856, 0.16642845], [nan, nan], [4.349648912578752, 2.5216406363433186, 4.7068905956085185, 2.164398953313553, 0.3572416830297662, 2.1852499592652]]\n",
      "2020-12-23 02:53:15,175 : INFO : Removed 2 and 530 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:15,176 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:15,180 : INFO : built Dictionary(344 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 2900 corpus positions)\n",
      "2020-12-23 02:53:15,373 : INFO : token count processed\n",
      "2020-12-23 02:53:15,376 : INFO : frequencies processed\n",
      "2020-12-23 02:53:15,505 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:15,505 : INFO : entropies processed\n",
      "2020-12-23 02:53:15,506 : INFO : extropies processed\n",
      "2020-12-23 02:53:15,508 : INFO : token count processed\n",
      "2020-12-23 02:53:15,509 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:15,510 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:15,510 : INFO : vocab #2480\n",
      "2020-12-23 02:53:15,511 : INFO : diff #set()\n",
      "2020-12-23 02:53:15,770 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:15,899 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.2158596531249233, 0.4512921197828332], [0.9093630313873291, 0.09063697], [2.0, 1.2451124978365313], [4.349648912578752, 7.480007711014331, 7.513023234413809, 4.316633389179274, 3.1633743218350565, 0.03301552339947822]]\n",
      "2020-12-23 02:53:15,902 : INFO : Removed 2 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:15,902 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:15,904 : INFO : built Dictionary(218 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 1049 corpus positions)\n",
      "2020-12-23 02:53:15,995 : INFO : token count processed\n",
      "2020-12-23 02:53:16,001 : INFO : frequencies processed\n",
      "2020-12-23 02:53:16,132 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:16,132 : INFO : entropies processed\n",
      "2020-12-23 02:53:16,133 : INFO : extropies processed\n",
      "2020-12-23 02:53:16,134 : INFO : token count processed\n",
      "2020-12-23 02:53:16,135 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:16,136 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:16,137 : INFO : vocab #2480\n",
      "2020-12-23 02:53:16,138 : INFO : diff #set()\n",
      "2020-12-23 02:53:16,396 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:16,523 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.2066951307013043, 0.45316635999563404], [0.8839540332555771, 0.11604597], [2.75, 1.3226647836567114], [4.349648912578752, 7.131331012509435, 7.193963240082792, 4.287016685005395, 2.8443143275040397, 0.06263222757335729]]\n",
      "2020-12-23 02:53:16,526 : INFO : Removed 2 and 166 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:16,527 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:16,528 : INFO : built Dictionary(206 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 868 corpus positions)\n",
      "2020-12-23 02:53:16,619 : INFO : token count processed\n",
      "2020-12-23 02:53:16,621 : INFO : frequencies processed\n",
      "2020-12-23 02:53:16,750 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:16,750 : INFO : entropies processed\n",
      "2020-12-23 02:53:16,751 : INFO : extropies processed\n",
      "2020-12-23 02:53:16,752 : INFO : token count processed\n",
      "2020-12-23 02:53:16,753 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:16,754 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:16,754 : INFO : vocab #2480\n",
      "2020-12-23 02:53:16,755 : INFO : diff #set()\n",
      "2020-12-23 02:53:17,013 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:17,140 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.1998078327171624, 0.45458516199790877], [0.8711904883384705, 0.12880951], [3.0, 1.3485155455967714], [4.349648912578752, 7.203742744794778, 7.2701043913001335, 4.283287266073398, 2.9204554787213812, 0.06636164650535559]]\n",
      "2020-12-23 02:53:17,142 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:17,143 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:17,144 : INFO : built Dictionary(69 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 200 corpus positions)\n",
      "2020-12-23 02:53:17,165 : INFO : token count processed\n",
      "2020-12-23 02:53:17,167 : INFO : frequencies processed\n",
      "2020-12-23 02:53:17,293 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:17,294 : INFO : entropies processed\n",
      "2020-12-23 02:53:17,295 : INFO : extropies processed\n",
      "2020-12-23 02:53:17,297 : INFO : token count processed\n",
      "2020-12-23 02:53:17,299 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:17,300 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:17,301 : INFO : vocab #2480\n",
      "2020-12-23 02:53:17,303 : INFO : diff #set()\n",
      "2020-12-23 02:53:17,570 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:17,699 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.1908326315010864, 0.4564474645946974], [0.9150559827685356, 0.08494402], [1.584962500721156, 1.1699250014423124], [4.349648912578752, 5.195502554608948, 5.568254018237734, 3.976897448949967, 1.2186051056589813, 0.3727514636287852]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:53:17,701 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:17,702 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:17,704 : INFO : built Dictionary(76 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 261 corpus positions)\n",
      "2020-12-23 02:53:17,731 : INFO : token count processed\n",
      "2020-12-23 02:53:17,733 : INFO : frequencies processed\n",
      "2020-12-23 02:53:17,864 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:17,865 : INFO : entropies processed\n",
      "2020-12-23 02:53:17,866 : INFO : extropies processed\n",
      "2020-12-23 02:53:17,867 : INFO : token count processed\n",
      "2020-12-23 02:53:17,868 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:17,870 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:17,870 : INFO : vocab #2480\n",
      "2020-12-23 02:53:17,872 : INFO : diff #set()\n",
      "2020-12-23 02:53:18,140 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:18,268 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.206700109877985, 0.4531653374754637], [0.9141997620463371, 0.08580024], [1.0, 1.0], [4.349648912578752, 5.32027245610305, 5.6444867117344035, 4.025434656947398, 1.2948377991556512, 0.32421425563135386]]\n",
      "2020-12-23 02:53:18,270 : INFO : Removed 2 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:18,271 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:18,272 : INFO : built Dictionary(172 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 501 corpus positions)\n",
      "2020-12-23 02:53:18,346 : INFO : token count processed\n",
      "2020-12-23 02:53:18,349 : INFO : frequencies processed\n",
      "2020-12-23 02:53:18,478 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:18,478 : INFO : entropies processed\n",
      "2020-12-23 02:53:18,479 : INFO : extropies processed\n",
      "2020-12-23 02:53:18,480 : INFO : token count processed\n",
      "2020-12-23 02:53:18,482 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:18,483 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:18,483 : INFO : vocab #2480\n",
      "2020-12-23 02:53:18,484 : INFO : diff #set()\n",
      "2020-12-23 02:53:18,746 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:18,873 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.1674935979297645, 0.46136237770442723], [0.841015174984932, 0.15898483], [2.584962500721156, 1.315172029168969], [4.349648912578752, 6.898202761357263, 7.005275861888123, 4.2425758120478925, 2.6556269493093705, 0.10707310053085983]]\n",
      "2020-12-23 02:53:18,876 : INFO : Removed 2 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:18,877 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:18,878 : INFO : built Dictionary(141 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 523 corpus positions)\n",
      "2020-12-23 02:53:18,932 : INFO : token count processed\n",
      "2020-12-23 02:53:18,937 : INFO : frequencies processed\n",
      "2020-12-23 02:53:19,071 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:19,072 : INFO : entropies processed\n",
      "2020-12-23 02:53:19,072 : INFO : extropies processed\n",
      "2020-12-23 02:53:19,074 : INFO : token count processed\n",
      "2020-12-23 02:53:19,075 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:19,076 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:19,076 : INFO : vocab #2480\n",
      "2020-12-23 02:53:19,078 : INFO : diff #set()\n",
      "2020-12-23 02:53:19,336 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:19,464 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.2402277972041897, 0.44638317641090014], [0.967748187482357, 0.032251813], [0.0, 0.0], [4.349648912578752, 6.388500481644799, 6.550170319164746, 4.187979075058806, 2.2005214065859935, 0.16166983751994657]]\n",
      "2020-12-23 02:53:19,467 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:19,468 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:19,468 : INFO : built Dictionary(60 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 187 corpus positions)\n",
      "2020-12-23 02:53:19,486 : INFO : token count processed\n",
      "2020-12-23 02:53:19,488 : INFO : frequencies processed\n",
      "2020-12-23 02:53:19,625 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:19,626 : INFO : entropies processed\n",
      "2020-12-23 02:53:19,627 : INFO : extropies processed\n",
      "2020-12-23 02:53:19,629 : INFO : token count processed\n",
      "2020-12-23 02:53:19,631 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:19,632 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:19,633 : INFO : vocab #2480\n",
      "2020-12-23 02:53:19,635 : INFO : diff #set()\n",
      "2020-12-23 02:53:19,903 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:20,032 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.195378962626466, 0.45550222400037893], [0.9197289273142815, 0.08027107], [1.584962500721156, 1.1699250014423124], [4.349648912578752, 4.8191513650620195, 5.250672098570898, 3.918128179069874, 0.9010231859921456, 0.43152073350887843]]\n",
      "2020-12-23 02:53:20,034 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:20,035 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:20,036 : INFO : built Dictionary(66 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 242 corpus positions)\n",
      "2020-12-23 02:53:20,054 : INFO : token count processed\n",
      "2020-12-23 02:53:20,057 : INFO : frequencies processed\n",
      "2020-12-23 02:53:20,192 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:20,192 : INFO : entropies processed\n",
      "2020-12-23 02:53:20,193 : INFO : extropies processed\n",
      "2020-12-23 02:53:20,193 : INFO : token count processed\n",
      "2020-12-23 02:53:20,194 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:20,195 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:20,195 : INFO : vocab #2480\n",
      "2020-12-23 02:53:20,197 : INFO : diff #set()\n",
      "2020-12-23 02:53:20,450 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:20,577 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.2085636458041833, 0.45278296683901065], [0.9154537245631218, 0.084546275], [1.0, 1.0], [4.349648912578752, 5.062480936779194, 5.425436135464179, 3.986693713893767, 1.0757872228854266, 0.36295519868498527]]\n",
      "2020-12-23 02:53:20,580 : INFO : Removed 2 and 223 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:20,581 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:20,584 : INFO : built Dictionary(253 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 1796 corpus positions)\n",
      "2020-12-23 02:53:20,708 : INFO : token count processed\n",
      "2020-12-23 02:53:20,714 : INFO : frequencies processed\n",
      "2020-12-23 02:53:20,842 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:20,843 : INFO : entropies processed\n",
      "2020-12-23 02:53:20,843 : INFO : extropies processed\n",
      "2020-12-23 02:53:20,846 : INFO : token count processed\n",
      "2020-12-23 02:53:20,847 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:20,848 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:20,848 : INFO : vocab #2480\n",
      "2020-12-23 02:53:20,850 : INFO : diff #set()\n",
      "2020-12-23 02:53:21,111 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:21,239 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.2296215514911277, 0.4485066083664375], [0.9406598433852196, 0.059340157], [2.584962500721156, 1.315172029168969], [4.349648912578752, 7.185085743102134, 7.2355708742482765, 4.299163781432609, 2.885921961669524, 0.05048513114614206]]\n",
      "2020-12-23 02:53:21,242 : INFO : Removed 2 and 148 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:21,243 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:21,245 : INFO : built Dictionary(170 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 771 corpus positions)\n",
      "2020-12-23 02:53:21,318 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:53:21,320 : INFO : frequencies processed\n",
      "2020-12-23 02:53:21,449 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:21,450 : INFO : entropies processed\n",
      "2020-12-23 02:53:21,450 : INFO : extropies processed\n",
      "2020-12-23 02:53:21,452 : INFO : token count processed\n",
      "2020-12-23 02:53:21,453 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:21,454 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:21,455 : INFO : vocab #2480\n",
      "2020-12-23 02:53:21,456 : INFO : diff #set()\n",
      "2020-12-23 02:53:21,715 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:21,844 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.1841351963266655, 0.4578471157288366], [0.8493369668722153, 0.15066303], [2.5216406363433186, 1.2998438251349493], [4.349648912578752, 6.591225336124281, 6.680987115704112, 4.25988713299892, 2.3313382031253598, 0.08976177957983111]]\n",
      "2020-12-23 02:53:21,846 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:21,847 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:21,849 : INFO : built Dictionary(52 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 109 corpus positions)\n",
      "2020-12-23 02:53:21,872 : INFO : token count processed\n",
      "2020-12-23 02:53:21,874 : INFO : frequencies processed\n",
      "2020-12-23 02:53:22,002 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:22,003 : INFO : entropies processed\n",
      "2020-12-23 02:53:22,003 : INFO : extropies processed\n",
      "2020-12-23 02:53:22,005 : INFO : token count processed\n",
      "2020-12-23 02:53:22,005 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:22,006 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:22,007 : INFO : vocab #2480\n",
      "2020-12-23 02:53:22,008 : INFO : diff #set()\n",
      "2020-12-23 02:53:22,266 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:22,394 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.205335612409771, 0.4534457224437145], [0.9366128370165825, 0.06338716], [1.0, 1.0], [4.349648912578752, 4.7032114441396695, 5.320664347684904, 3.732196009033519, 0.9710154351061515, 0.6174529035452343]]\n",
      "2020-12-23 02:53:22,396 : INFO : Removed 2 and 9 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:22,397 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:22,399 : INFO : built Dictionary(127 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 349 corpus positions)\n",
      "2020-12-23 02:53:22,448 : INFO : token count processed\n",
      "2020-12-23 02:53:22,450 : INFO : frequencies processed\n",
      "2020-12-23 02:53:22,580 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:22,581 : INFO : entropies processed\n",
      "2020-12-23 02:53:22,582 : INFO : extropies processed\n",
      "2020-12-23 02:53:22,583 : INFO : token count processed\n",
      "2020-12-23 02:53:22,584 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:22,585 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:22,586 : INFO : vocab #2480\n",
      "2020-12-23 02:53:22,588 : INFO : diff #set()\n",
      "2020-12-23 02:53:22,851 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:22,979 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.1868679607089638, 0.4572749786300809], [0.9050377532839775, 0.09496225], [2.0, 1.2451124978365313], [4.349648912578752, 6.14228447828618, 6.342276286535452, 4.1496571043294805, 1.9926273739567, 0.19999180824927265]]\n",
      "2020-12-23 02:53:22,981 : INFO : Removed 2 and 184 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:22,982 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:22,984 : INFO : built Dictionary(262 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 1139 corpus positions)\n",
      "2020-12-23 02:53:23,101 : INFO : token count processed\n",
      "2020-12-23 02:53:23,104 : INFO : frequencies processed\n",
      "2020-12-23 02:53:23,233 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:23,234 : INFO : entropies processed\n",
      "2020-12-23 02:53:23,235 : INFO : extropies processed\n",
      "2020-12-23 02:53:23,237 : INFO : token count processed\n",
      "2020-12-23 02:53:23,238 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:23,238 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:23,239 : INFO : vocab #2480\n",
      "2020-12-23 02:53:23,240 : INFO : diff #set()\n",
      "2020-12-23 02:53:23,498 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:23,626 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.1886475501316651, 0.45690316832412864], [0.8563031107187271, 0.14369689], [2.94770277922009, 1.3393100707180505], [4.349648912578752, 7.450178124335845, 7.5032624014463725, 4.296564635468224, 3.15361348886762, 0.053084277110527545]]\n",
      "2020-12-23 02:53:23,629 : INFO : Removed 2 and 16 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:23,630 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:23,631 : INFO : built Dictionary(69 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 187 corpus positions)\n",
      "2020-12-23 02:53:23,651 : INFO : token count processed\n",
      "2020-12-23 02:53:23,653 : INFO : frequencies processed\n",
      "2020-12-23 02:53:23,780 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:23,781 : INFO : entropies processed\n",
      "2020-12-23 02:53:23,782 : INFO : extropies processed\n",
      "2020-12-23 02:53:23,783 : INFO : token count processed\n",
      "2020-12-23 02:53:23,784 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:23,785 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:23,786 : INFO : vocab #2480\n",
      "2020-12-23 02:53:23,787 : INFO : diff #set()\n",
      "2020-12-23 02:53:24,046 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:24,174 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.1851675768860575, 0.45763080624921043], [0.9159927293658257, 0.08400727], [1.584962500721156, 1.1699250014423124], [4.349648912578752, 5.20665021947654, 5.576198087076017, 3.980101044979275, 1.226549174497265, 0.36954786759947744]]\n",
      "2020-12-23 02:53:24,176 : INFO : Removed 2 and 89 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:24,177 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:24,179 : INFO : built Dictionary(134 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 525 corpus positions)\n",
      "2020-12-23 02:53:24,232 : INFO : token count processed\n",
      "2020-12-23 02:53:24,235 : INFO : frequencies processed\n",
      "2020-12-23 02:53:24,363 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:24,364 : INFO : entropies processed\n",
      "2020-12-23 02:53:24,365 : INFO : extropies processed\n",
      "2020-12-23 02:53:24,366 : INFO : token count processed\n",
      "2020-12-23 02:53:24,367 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:24,368 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:24,369 : INFO : vocab #2480\n",
      "2020-12-23 02:53:24,370 : INFO : diff #set()\n",
      "2020-12-23 02:53:24,642 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:24,769 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.1886617863222977, 0.45690019638911084], [0.8767047300934792, 0.12329527], [2.807354922057604, 1.3343545280186873], [4.349648912578752, 6.524718477352, 6.63857032237634, 4.235797067554412, 2.288921409797588, 0.11385184502434065]]\n",
      "2020-12-23 02:53:24,772 : INFO : Removed 2 and 8 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:24,773 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:24,773 : INFO : built Dictionary(76 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 232 corpus positions)\n",
      "2020-12-23 02:53:24,796 : INFO : token count processed\n",
      "2020-12-23 02:53:24,799 : INFO : frequencies processed\n",
      "2020-12-23 02:53:24,926 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:24,927 : INFO : entropies processed\n",
      "2020-12-23 02:53:24,927 : INFO : extropies processed\n",
      "2020-12-23 02:53:24,929 : INFO : token count processed\n",
      "2020-12-23 02:53:24,929 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:24,930 : INFO : alphabet_target #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:53:24,931 : INFO : vocab #2480\n",
      "2020-12-23 02:53:24,932 : INFO : diff #set()\n",
      "2020-12-23 02:53:25,189 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:25,318 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.2245790525660463, 0.4495232474865313], [0.945342157036066, 0.054657843], [0.0, 0.0], [4.349648912578752, 5.321859380715434, 5.6711522902574885, 4.000356003036698, 1.3215033776787362, 0.3492929095420543]]\n",
      "2020-12-23 02:53:25,320 : INFO : Removed 2 and 119 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:25,321 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:25,324 : INFO : built Dictionary(151 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 850 corpus positions)\n",
      "2020-12-23 02:53:25,386 : INFO : token count processed\n",
      "2020-12-23 02:53:25,388 : INFO : frequencies processed\n",
      "2020-12-23 02:53:25,517 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:25,518 : INFO : entropies processed\n",
      "2020-12-23 02:53:25,519 : INFO : extropies processed\n",
      "2020-12-23 02:53:25,521 : INFO : token count processed\n",
      "2020-12-23 02:53:25,522 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:25,523 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:25,524 : INFO : vocab #2480\n",
      "2020-12-23 02:53:25,525 : INFO : diff #set()\n",
      "2020-12-23 02:53:25,785 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:25,913 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.191702615879236, 0.4562662802676057], [0.8697097450494766, 0.13029025], [2.584962500721156, 1.315172029168969], [4.349648912578752, 6.500767808767801, 6.590282651202585, 4.260134070143968, 2.240633738623833, 0.08951484243478447]]\n",
      "2020-12-23 02:53:25,916 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:25,917 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:25,918 : INFO : built Dictionary(49 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 72 corpus positions)\n",
      "2020-12-23 02:53:25,930 : INFO : token count processed\n",
      "2020-12-23 02:53:25,932 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:53:25,933 : INFO : frequencies processed\n",
      "2020-12-23 02:53:25,934 : INFO : token count processed\n",
      "2020-12-23 02:53:25,935 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:25,935 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:25,936 : INFO : vocab #2480\n",
      "2020-12-23 02:53:25,937 : INFO : diff #set()\n",
      "2020-12-23 02:53:26,196 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:26,324 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.2348755550134807, 0.4474522072411177], [0.9635048247873783, 0.036495175], [nan, nan], [4.349648912578752, 4.736228843383063, 5.506965817330313, 3.578911938631501, 1.1573169047515606, 0.7707369739472503]]\n",
      "2020-12-23 02:53:26,326 : INFO : Removed 2 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:26,327 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:26,328 : INFO : built Dictionary(109 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 413 corpus positions)\n",
      "2020-12-23 02:53:26,369 : INFO : token count processed\n",
      "2020-12-23 02:53:26,374 : INFO : frequencies processed\n",
      "2020-12-23 02:53:26,503 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:26,503 : INFO : entropies processed\n",
      "2020-12-23 02:53:26,504 : INFO : extropies processed\n",
      "2020-12-23 02:53:26,505 : INFO : token count processed\n",
      "2020-12-23 02:53:26,506 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:26,508 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:26,508 : INFO : vocab #2480\n",
      "2020-12-23 02:53:26,509 : INFO : diff #set()\n",
      "2020-12-23 02:53:26,766 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:26,893 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.217938828464753, 0.4508690623772502], [0.9146677777171135, 0.08533222], [1.584962500721156, 1.1699250014423124], [4.349648912578752, 5.788442787590127, 5.992117735942833, 4.145973964226045, 1.642468823364081, 0.20367494835270605]]\n",
      "2020-12-23 02:53:26,895 : INFO : Removed 2 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:26,896 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:26,898 : INFO : built Dictionary(67 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 99 corpus positions)\n",
      "2020-12-23 02:53:26,922 : INFO : token count processed\n",
      "2020-12-23 02:53:26,925 : INFO : frequencies processed\n",
      "2020-12-23 02:53:27,066 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:27,067 : INFO : entropies processed\n",
      "2020-12-23 02:53:27,067 : INFO : extropies processed\n",
      "2020-12-23 02:53:27,069 : INFO : token count processed\n",
      "2020-12-23 02:53:27,070 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:27,071 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:27,071 : INFO : vocab #2480\n",
      "2020-12-23 02:53:27,073 : INFO : diff #set()\n",
      "2020-12-23 02:53:27,338 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:27,466 : INFO : Computed distances or similarities ('277', 'sacp-python-common/setup.py')[[1.2031358827579846, 0.4538984671014277], [0.9002812206745148, 0.09971878], [1.0, 1.0], [4.349648912578752, 5.370004292053436, 5.849093683079986, 3.8705595215522015, 1.499444770501234, 0.4790893910265499]]\n",
      "2020-12-23 02:53:27,468 : INFO : Removed 2 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:27,469 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:27,471 : INFO : built Dictionary(93 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 344 corpus positions)\n",
      "2020-12-23 02:53:27,506 : INFO : token count processed\n",
      "2020-12-23 02:53:27,509 : INFO : frequencies processed\n",
      "2020-12-23 02:53:27,639 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:27,640 : INFO : entropies processed\n",
      "2020-12-23 02:53:27,641 : INFO : extropies processed\n",
      "2020-12-23 02:53:27,642 : INFO : token count processed\n",
      "2020-12-23 02:53:27,643 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:27,644 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:27,645 : INFO : vocab #2480\n",
      "2020-12-23 02:53:27,646 : INFO : diff #set()\n",
      "2020-12-23 02:53:27,904 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:28,033 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.2182478385789894, 0.45080625465214047], [0.9214229360222816, 0.078577064], [1.0, 1.0], [4.349648912578752, 5.695663584743922, 5.9285159242468675, 4.116796573075807, 1.5788670116681152, 0.23285233950294515]]\n",
      "2020-12-23 02:53:28,035 : INFO : Removed 2 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:28,036 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:28,037 : INFO : built Dictionary(53 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 119 corpus positions)\n",
      "2020-12-23 02:53:28,051 : INFO : token count processed\n",
      "2020-12-23 02:53:28,054 : INFO : frequencies processed\n",
      "2020-12-23 02:53:28,181 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:28,182 : INFO : entropies processed\n",
      "2020-12-23 02:53:28,183 : INFO : extropies processed\n",
      "2020-12-23 02:53:28,184 : INFO : token count processed\n",
      "2020-12-23 02:53:28,185 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:28,186 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:28,186 : INFO : vocab #2480\n",
      "2020-12-23 02:53:28,187 : INFO : diff #set()\n",
      "2020-12-23 02:53:28,446 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:28,575 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.224729931761541, 0.4494927612216734], [0.9520415961742401, 0.047958404], [1.584962500721156, 1.1699250014423124], [4.349648912578752, 4.9004417692112465, 5.439845689289725, 3.810244992500274, 1.0901967767109726, 0.5394039200784784]]\n",
      "2020-12-23 02:53:28,577 : INFO : Removed 2 and 3 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:53:28,578 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:28,579 : INFO : built Dictionary(51 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 126 corpus positions)\n",
      "2020-12-23 02:53:28,592 : INFO : token count processed\n",
      "2020-12-23 02:53:28,594 : INFO : frequencies processed\n",
      "2020-12-23 02:53:28,722 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:28,723 : INFO : entropies processed\n",
      "2020-12-23 02:53:28,723 : INFO : extropies processed\n",
      "2020-12-23 02:53:28,724 : INFO : token count processed\n",
      "2020-12-23 02:53:28,725 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:28,726 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:28,727 : INFO : vocab #2480\n",
      "2020-12-23 02:53:28,728 : INFO : diff #set()\n",
      "2020-12-23 02:53:28,985 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:29,113 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.2164389324181724, 0.45117417194480663], [0.9476092308759689, 0.05239077], [1.584962500721156, 1.1699250014423124], [4.349648912578752, 4.778624108914332, 5.322038772735388, 3.806234248757697, 0.9723898601566354, 0.5434146638210553]]\n",
      "2020-12-23 02:53:29,115 : INFO : Removed 2 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:29,116 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:29,117 : INFO : built Dictionary(53 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 157 corpus positions)\n",
      "2020-12-23 02:53:29,131 : INFO : token count processed\n",
      "2020-12-23 02:53:29,134 : INFO : frequencies processed\n",
      "2020-12-23 02:53:29,262 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:29,263 : INFO : entropies processed\n",
      "2020-12-23 02:53:29,264 : INFO : extropies processed\n",
      "2020-12-23 02:53:29,266 : INFO : token count processed\n",
      "2020-12-23 02:53:29,267 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:29,269 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:29,270 : INFO : vocab #2480\n",
      "2020-12-23 02:53:29,271 : INFO : diff #set()\n",
      "2020-12-23 02:53:29,552 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:29,684 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.2360758221613066, 0.4472120265731587], [0.9683185629546642, 0.031681437], [1.0, 1.0], [4.349648912578752, 4.773880192225086, 5.26689713512231, 3.856631969681528, 0.9172482225435576, 0.4930169428972242]]\n",
      "2020-12-23 02:53:29,688 : INFO : Removed 2 and 346 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:29,688 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:29,690 : INFO : built Dictionary(160 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 1982 corpus positions)\n",
      "2020-12-23 02:53:29,752 : INFO : token count processed\n",
      "2020-12-23 02:53:29,757 : INFO : frequencies processed\n",
      "2020-12-23 02:53:29,886 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:29,886 : INFO : entropies processed\n",
      "2020-12-23 02:53:29,887 : INFO : extropies processed\n",
      "2020-12-23 02:53:29,890 : INFO : token count processed\n",
      "2020-12-23 02:53:29,891 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:29,892 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:29,893 : INFO : vocab #2480\n",
      "2020-12-23 02:53:29,894 : INFO : diff #set()\n",
      "2020-12-23 02:53:30,153 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:30,280 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.215762273635082, 0.4513119534070973], [0.9232985824346542, 0.07670142], [2.5216406363433186, 1.2998438251349493], [4.349648912578752, 6.620773041953877, 6.6674824623122575, 4.302939492220371, 2.317833549733505, 0.04670942035838088]]\n",
      "2020-12-23 02:53:30,282 : INFO : Removed 2 and 132 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:30,283 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:30,284 : INFO : built Dictionary(93 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 545 corpus positions)\n",
      "2020-12-23 02:53:30,319 : INFO : token count processed\n",
      "2020-12-23 02:53:30,324 : INFO : frequencies processed\n",
      "2020-12-23 02:53:30,452 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:30,452 : INFO : entropies processed\n",
      "2020-12-23 02:53:30,453 : INFO : extropies processed\n",
      "2020-12-23 02:53:30,454 : INFO : token count processed\n",
      "2020-12-23 02:53:30,455 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:30,455 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:30,456 : INFO : vocab #2480\n",
      "2020-12-23 02:53:30,457 : INFO : diff #set()\n",
      "2020-12-23 02:53:30,724 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:30,852 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.1966326122534796, 0.4552422623709118], [0.9139387980103493, 0.0860612], [1.584962500721156, 1.1699250014423124], [4.349648912578752, 5.828370634755606, 5.974974602808738, 4.203044944525621, 1.6253256902299853, 0.1466039680531317]]\n",
      "2020-12-23 02:53:30,855 : INFO : Removed 2 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:30,856 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:30,858 : INFO : built Dictionary(94 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 308 corpus positions)\n",
      "2020-12-23 02:53:30,899 : INFO : token count processed\n",
      "2020-12-23 02:53:30,904 : INFO : frequencies processed\n",
      "2020-12-23 02:53:31,031 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:31,032 : INFO : entropies processed\n",
      "2020-12-23 02:53:31,032 : INFO : extropies processed\n",
      "2020-12-23 02:53:31,033 : INFO : token count processed\n",
      "2020-12-23 02:53:31,034 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:31,035 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:31,035 : INFO : vocab #2480\n",
      "2020-12-23 02:53:31,036 : INFO : diff #set()\n",
      "2020-12-23 02:53:31,302 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:31,430 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.1884809105024485, 0.45693795874619353], [0.8944693580269814, 0.10553064], [1.584962500721156, 1.1699250014423124], [4.349648912578752, 5.774409284925443, 6.0093362143464795, 4.114721983157715, 1.6596873017677272, 0.23492692942103677]]\n",
      "2020-12-23 02:53:31,433 : INFO : Removed 2 and 51 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:31,434 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:31,435 : INFO : built Dictionary(104 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 357 corpus positions)\n",
      "2020-12-23 02:53:31,472 : INFO : token count processed\n",
      "2020-12-23 02:53:31,474 : INFO : frequencies processed\n",
      "2020-12-23 02:53:31,602 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:31,603 : INFO : entropies processed\n",
      "2020-12-23 02:53:31,603 : INFO : extropies processed\n",
      "2020-12-23 02:53:31,605 : INFO : token count processed\n",
      "2020-12-23 02:53:31,606 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:31,607 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:31,607 : INFO : vocab #2480\n",
      "2020-12-23 02:53:31,608 : INFO : diff #set()\n",
      "2020-12-23 02:53:31,866 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:31,994 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.1390653228939929, 0.46749390460272433], [0.8195614367723465, 0.18043856], [2.0, 1.2451124978365313], [4.349648912578752, 5.977819040873918, 6.159282803656522, 4.168185149796148, 1.8096338910777696, 0.18146376278260412]]\n",
      "2020-12-23 02:53:31,997 : INFO : Removed 2 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:31,998 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:31,999 : INFO : built Dictionary(85 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 304 corpus positions)\n",
      "2020-12-23 02:53:32,025 : INFO : token count processed\n",
      "2020-12-23 02:53:32,027 : INFO : frequencies processed\n",
      "2020-12-23 02:53:32,154 : INFO : scalar_distribution processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:53:32,155 : INFO : entropies processed\n",
      "2020-12-23 02:53:32,156 : INFO : extropies processed\n",
      "2020-12-23 02:53:32,157 : INFO : token count processed\n",
      "2020-12-23 02:53:32,158 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:32,159 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:32,160 : INFO : vocab #2480\n",
      "2020-12-23 02:53:32,161 : INFO : diff #set()\n",
      "2020-12-23 02:53:32,418 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:32,546 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.1978095580694685, 0.4549984762457713], [0.8918930441141129, 0.108106956], [2.0, 1.2451124978365313], [4.349648912578752, 5.901812829596593, 6.114462150689866, 4.136999591485478, 1.7648132381111141, 0.2126493210932736]]\n",
      "2020-12-23 02:53:32,549 : INFO : Removed 2 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:32,549 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:32,550 : INFO : built Dictionary(88 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 304 corpus positions)\n",
      "2020-12-23 02:53:32,588 : INFO : token count processed\n",
      "2020-12-23 02:53:32,591 : INFO : frequencies processed\n",
      "2020-12-23 02:53:32,722 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:32,723 : INFO : entropies processed\n",
      "2020-12-23 02:53:32,723 : INFO : extropies processed\n",
      "2020-12-23 02:53:32,725 : INFO : token count processed\n",
      "2020-12-23 02:53:32,726 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:32,727 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:32,728 : INFO : vocab #2480\n",
      "2020-12-23 02:53:32,729 : INFO : diff #set()\n",
      "2020-12-23 02:53:33,002 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:33,133 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.2085695658751905, 0.45278175315420943], [0.8988931849598885, 0.101106815], [1.584962500721156, 1.1699250014423124], [4.349648912578752, 5.643202320803383, 5.891508066961736, 4.101343166420399, 1.5418591543829834, 0.2483057461583531]]\n",
      "2020-12-23 02:53:33,135 : INFO : Removed 2 and 30 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:33,136 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:33,138 : INFO : built Dictionary(102 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 338 corpus positions)\n",
      "2020-12-23 02:53:33,178 : INFO : token count processed\n",
      "2020-12-23 02:53:33,182 : INFO : frequencies processed\n",
      "2020-12-23 02:53:33,311 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:33,312 : INFO : entropies processed\n",
      "2020-12-23 02:53:33,312 : INFO : extropies processed\n",
      "2020-12-23 02:53:33,313 : INFO : token count processed\n",
      "2020-12-23 02:53:33,314 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:33,314 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:33,315 : INFO : vocab #2480\n",
      "2020-12-23 02:53:33,316 : INFO : diff #set()\n",
      "2020-12-23 02:53:33,572 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:33,700 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.1889002041360797, 0.4568504302345215], [0.8812899366021156, 0.11871006], [1.0, 1.0], [4.349648912578752, 5.925214310725336, 6.140026061898075, 4.134837161406013, 1.7903771493193226, 0.2148117511727392]]\n",
      "2020-12-23 02:53:33,703 : INFO : Removed 2 and 293 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:33,704 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:33,705 : INFO : built Dictionary(171 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 1733 corpus positions)\n",
      "2020-12-23 02:53:33,775 : INFO : token count processed\n",
      "2020-12-23 02:53:33,777 : INFO : frequencies processed\n",
      "2020-12-23 02:53:33,905 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:33,906 : INFO : entropies processed\n",
      "2020-12-23 02:53:33,907 : INFO : extropies processed\n",
      "2020-12-23 02:53:33,909 : INFO : token count processed\n",
      "2020-12-23 02:53:33,910 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:33,911 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:33,912 : INFO : vocab #2480\n",
      "2020-12-23 02:53:33,913 : INFO : diff #set()\n",
      "2020-12-23 02:53:34,174 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:34,302 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.2180325341321294, 0.4508500144211272], [0.9266864955425262, 0.073313504], [2.5216406363433186, 1.2998438251349493], [4.349648912578752, 6.551685682764175, 6.6041459470224355, 4.297188648320492, 2.254497034443683, 0.05246026425826056]]\n",
      "2020-12-23 02:53:34,304 : INFO : Removed 2 and 88 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:34,305 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:34,307 : INFO : built Dictionary(150 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 574 corpus positions)\n",
      "2020-12-23 02:53:34,367 : INFO : token count processed\n",
      "2020-12-23 02:53:34,369 : INFO : frequencies processed\n",
      "2020-12-23 02:53:34,502 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:34,503 : INFO : entropies processed\n",
      "2020-12-23 02:53:34,504 : INFO : extropies processed\n",
      "2020-12-23 02:53:34,505 : INFO : token count processed\n",
      "2020-12-23 02:53:34,506 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:34,507 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:34,508 : INFO : vocab #2480\n",
      "2020-12-23 02:53:34,509 : INFO : diff #set()\n",
      "2020-12-23 02:53:34,778 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:34,906 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.1818801441834452, 0.45832031730333367], [0.8366842865943909, 0.16331571], [1.584962500721156, 1.1699250014423124], [4.349648912578752, 6.642985062562557, 6.7628337147355735, 4.229800260405736, 2.413184802156821, 0.11984865217301621]]\n",
      "2020-12-23 02:53:34,909 : INFO : Removed 2 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:34,909 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:34,910 : INFO : built Dictionary(64 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 169 corpus positions)\n",
      "2020-12-23 02:53:34,928 : INFO : token count processed\n",
      "2020-12-23 02:53:34,931 : INFO : frequencies processed\n",
      "2020-12-23 02:53:35,059 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:35,060 : INFO : entropies processed\n",
      "2020-12-23 02:53:35,060 : INFO : extropies processed\n",
      "2020-12-23 02:53:35,062 : INFO : token count processed\n",
      "2020-12-23 02:53:35,062 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:35,063 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:35,064 : INFO : vocab #2480\n",
      "2020-12-23 02:53:35,065 : INFO : diff #set()\n",
      "2020-12-23 02:53:35,323 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:35,451 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.2092931545534433, 0.4526334578726952], [0.9034527912735939, 0.09654721], [1.0, 1.0], [4.349648912578752, 5.2461980344571995, 5.648047777364531, 3.9477991696714207, 1.2983988647857787, 0.40184974290733155]]\n",
      "2020-12-23 02:53:35,453 : INFO : Removed 2 and 23 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:35,454 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:35,455 : INFO : built Dictionary(85 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 229 corpus positions)\n",
      "2020-12-23 02:53:35,480 : INFO : token count processed\n",
      "2020-12-23 02:53:35,485 : INFO : frequencies processed\n",
      "2020-12-23 02:53:35,615 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:35,616 : INFO : entropies processed\n",
      "2020-12-23 02:53:35,616 : INFO : extropies processed\n",
      "2020-12-23 02:53:35,618 : INFO : token count processed\n",
      "2020-12-23 02:53:35,619 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:35,619 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:35,620 : INFO : vocab #2480\n",
      "2020-12-23 02:53:35,621 : INFO : diff #set()\n",
      "2020-12-23 02:53:35,898 : INFO : alphabet #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:53:36,032 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/test_auth_utility.py')[[1.2165592750705867, 0.4511496765490987], [0.9272116422653198, 0.07278836], [1.584962500721156, 1.1699250014423124], [4.349648912578752, 5.903090303960449, 6.1741372227784295, 4.078601993760771, 1.8244883101996772, 0.27104691881798004]]\n",
      "2020-12-23 02:53:36,035 : INFO : Removed 2 and 442 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:36,036 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:36,038 : INFO : built Dictionary(120 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 1228 corpus positions)\n",
      "2020-12-23 02:53:36,086 : INFO : token count processed\n",
      "2020-12-23 02:53:36,089 : INFO : frequencies processed\n",
      "2020-12-23 02:53:36,222 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:36,223 : INFO : entropies processed\n",
      "2020-12-23 02:53:36,224 : INFO : extropies processed\n",
      "2020-12-23 02:53:36,225 : INFO : token count processed\n",
      "2020-12-23 02:53:36,227 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:36,228 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:36,229 : INFO : vocab #2480\n",
      "2020-12-23 02:53:36,230 : INFO : diff #set()\n",
      "2020-12-23 02:53:36,488 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:36,615 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.22220449175388, 0.4500035904484865], [0.9350824356079102, 0.064917564], [1.0, 1.0], [4.349648912578752, 6.16659449033757, 6.240571795997013, 4.275671606919309, 1.890922883418261, 0.07397730565944372]]\n",
      "2020-12-23 02:53:36,618 : INFO : Removed 2 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:36,619 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:36,620 : INFO : built Dictionary(80 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 263 corpus positions)\n",
      "2020-12-23 02:53:36,645 : INFO : token count processed\n",
      "2020-12-23 02:53:36,647 : INFO : frequencies processed\n",
      "2020-12-23 02:53:36,777 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:36,777 : INFO : entropies processed\n",
      "2020-12-23 02:53:36,778 : INFO : extropies processed\n",
      "2020-12-23 02:53:36,780 : INFO : token count processed\n",
      "2020-12-23 02:53:36,781 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:36,781 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:36,782 : INFO : vocab #2480\n",
      "2020-12-23 02:53:36,783 : INFO : diff #set()\n",
      "2020-12-23 02:53:37,049 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:37,176 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.2276350891758938, 0.44890655783750777], [0.9255189597606659, 0.07448104], [1.0, 1.0], [4.349648912578752, 5.906856253399655, 6.152523122365948, 4.103982043612459, 1.8028742097871957, 0.24566686896629264]]\n",
      "2020-12-23 02:53:37,179 : INFO : Removed 2 and 68 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:37,180 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:37,181 : INFO : built Dictionary(93 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 262 corpus positions)\n",
      "2020-12-23 02:53:37,212 : INFO : token count processed\n",
      "2020-12-23 02:53:37,215 : INFO : frequencies processed\n",
      "2020-12-23 02:53:37,344 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:37,345 : INFO : entropies processed\n",
      "2020-12-23 02:53:37,346 : INFO : extropies processed\n",
      "2020-12-23 02:53:37,347 : INFO : token count processed\n",
      "2020-12-23 02:53:37,348 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:37,349 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:37,350 : INFO : vocab #2480\n",
      "2020-12-23 02:53:37,351 : INFO : diff #set()\n",
      "2020-12-23 02:53:37,621 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:37,753 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.2080447767733424, 0.45288936642911687], [0.9182635098695755, 0.08173649], [1.0, 1.0], [4.349648912578752, 5.965115449163356, 6.202098800825423, 4.112665560916685, 1.852449888246671, 0.23698335166206697]]\n",
      "2020-12-23 02:53:37,755 : INFO : Removed 2 and 13 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:37,756 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:37,757 : INFO : built Dictionary(99 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 376 corpus positions)\n",
      "2020-12-23 02:53:37,791 : INFO : token count processed\n",
      "2020-12-23 02:53:37,797 : INFO : frequencies processed\n",
      "2020-12-23 02:53:37,928 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:37,928 : INFO : entropies processed\n",
      "2020-12-23 02:53:37,929 : INFO : extropies processed\n",
      "2020-12-23 02:53:37,930 : INFO : token count processed\n",
      "2020-12-23 02:53:37,932 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:37,933 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:37,933 : INFO : vocab #2480\n",
      "2020-12-23 02:53:37,935 : INFO : diff #set()\n",
      "2020-12-23 02:53:38,193 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:38,320 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.2071311630978065, 0.4530768341816422], [0.9128299728035927, 0.08717003], [1.584962500721156, 1.1699250014423124], [4.349648912578752, 5.791362404253194, 6.007920032498132, 4.133091284333815, 1.6582711199193794, 0.21655762824493774]]\n",
      "2020-12-23 02:53:38,322 : INFO : Removed 2 and 41 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:38,323 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:38,325 : INFO : built Dictionary(90 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 388 corpus positions)\n",
      "2020-12-23 02:53:38,362 : INFO : token count processed\n",
      "2020-12-23 02:53:38,365 : INFO : frequencies processed\n",
      "2020-12-23 02:53:38,493 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:38,494 : INFO : entropies processed\n",
      "2020-12-23 02:53:38,494 : INFO : extropies processed\n",
      "2020-12-23 02:53:38,495 : INFO : token count processed\n",
      "2020-12-23 02:53:38,496 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:38,497 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:38,499 : INFO : vocab #2480\n",
      "2020-12-23 02:53:38,500 : INFO : diff #set()\n",
      "2020-12-23 02:53:38,759 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:38,887 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.2133001154116476, 0.45181400978421393], [0.9289690628647804, 0.07103094], [1.0, 1.0], [4.349648912578752, 5.651670454631116, 5.869693398974684, 4.131625968235184, 1.520044486395932, 0.21802294434356817]]\n",
      "2020-12-23 02:53:38,889 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:38,890 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:38,892 : INFO : built Dictionary(54 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 98 corpus positions)\n",
      "2020-12-23 02:53:38,911 : INFO : token count processed\n",
      "2020-12-23 02:53:38,913 : INFO : frequencies processed\n",
      "2020-12-23 02:53:39,041 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:39,042 : INFO : entropies processed\n",
      "2020-12-23 02:53:39,043 : INFO : extropies processed\n",
      "2020-12-23 02:53:39,044 : INFO : token count processed\n",
      "2020-12-23 02:53:39,045 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:39,046 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:39,047 : INFO : vocab #2480\n",
      "2020-12-23 02:53:39,048 : INFO : diff #set()\n",
      "2020-12-23 02:53:39,305 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:39,433 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.2476452621294987, 0.4449100651463855], [0.9595488831400871, 0.040451117], [0.0, 0.0], [4.349648912578752, 4.8226207261920235, 5.469132706234195, 3.70313693253658, 1.1194837936554425, 0.6465119800421713]]\n",
      "2020-12-23 02:53:39,435 : INFO : Removed 2 and 50 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:39,436 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:53:39,437 : INFO : built Dictionary(99 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 268 corpus positions)\n",
      "2020-12-23 02:53:39,476 : INFO : token count processed\n",
      "2020-12-23 02:53:39,481 : INFO : frequencies processed\n",
      "2020-12-23 02:53:39,613 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:39,614 : INFO : entropies processed\n",
      "2020-12-23 02:53:39,614 : INFO : extropies processed\n",
      "2020-12-23 02:53:39,616 : INFO : token count processed\n",
      "2020-12-23 02:53:39,617 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:39,618 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:39,618 : INFO : vocab #2480\n",
      "2020-12-23 02:53:39,619 : INFO : diff #set()\n",
      "2020-12-23 02:53:39,882 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:40,011 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.2482455336635405, 0.44479127614255237], [0.9584728442132473, 0.041527156], [0.0, 0.0], [4.349648912578752, 6.24862851613934, 6.4764124789917155, 4.121864949726376, 2.126763566412963, 0.22778396285237523]]\n",
      "2020-12-23 02:53:40,014 : INFO : Removed 2 and 32 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:40,015 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:40,016 : INFO : built Dictionary(98 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 384 corpus positions)\n",
      "2020-12-23 02:53:40,050 : INFO : token count processed\n",
      "2020-12-23 02:53:40,059 : INFO : frequencies processed\n",
      "2020-12-23 02:53:40,189 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:40,189 : INFO : entropies processed\n",
      "2020-12-23 02:53:40,190 : INFO : extropies processed\n",
      "2020-12-23 02:53:40,191 : INFO : token count processed\n",
      "2020-12-23 02:53:40,191 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:40,192 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:40,193 : INFO : vocab #2480\n",
      "2020-12-23 02:53:40,194 : INFO : diff #set()\n",
      "2020-12-23 02:53:40,460 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:40,588 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.2148915554266233, 0.4514893731704097], [0.9216907918453217, 0.07830921], [1.584962500721156, 1.1699250014423124], [4.349648912578752, 5.850156917433494, 6.05494875265305, 4.1448570773591955, 1.7052998400742974, 0.2047918352195559]]\n",
      "2020-12-23 02:53:40,591 : INFO : Removed 2 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:40,592 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:40,594 : INFO : built Dictionary(94 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 382 corpus positions)\n",
      "2020-12-23 02:53:40,634 : INFO : token count processed\n",
      "2020-12-23 02:53:40,636 : INFO : frequencies processed\n",
      "2020-12-23 02:53:40,767 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:40,767 : INFO : entropies processed\n",
      "2020-12-23 02:53:40,768 : INFO : extropies processed\n",
      "2020-12-23 02:53:40,770 : INFO : token count processed\n",
      "2020-12-23 02:53:40,771 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:40,772 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:40,772 : INFO : vocab #2480\n",
      "2020-12-23 02:53:40,773 : INFO : diff #set()\n",
      "2020-12-23 02:53:41,035 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:41,163 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.2147932982624299, 0.4515094030601091], [0.9289234727621078, 0.07107653], [1.0, 1.0], [4.349648912578752, 5.6831976040360095, 5.90082001728884, 4.132026499325921, 1.5511711047100878, 0.2176224132528306]]\n",
      "2020-12-23 02:53:41,165 : INFO : Removed 2 and 52 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:41,166 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:41,168 : INFO : built Dictionary(81 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 360 corpus positions)\n",
      "2020-12-23 02:53:41,201 : INFO : token count processed\n",
      "2020-12-23 02:53:41,206 : INFO : frequencies processed\n",
      "2020-12-23 02:53:41,334 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:41,335 : INFO : entropies processed\n",
      "2020-12-23 02:53:41,336 : INFO : extropies processed\n",
      "2020-12-23 02:53:41,337 : INFO : token count processed\n",
      "2020-12-23 02:53:41,337 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:41,338 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:41,339 : INFO : vocab #2480\n",
      "2020-12-23 02:53:41,339 : INFO : diff #set()\n",
      "2020-12-23 02:53:41,606 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:41,734 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.2169861589520474, 0.45106280702839047], [0.9033218398690224, 0.09667816], [1.0, 1.0], [4.349648912578752, 5.749308601266266, 5.964535784493677, 4.134421729351342, 1.6148868719149245, 0.21522718322741063]]\n",
      "2020-12-23 02:53:41,737 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:41,738 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:41,740 : INFO : built Dictionary(74 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 241 corpus positions)\n",
      "2020-12-23 02:53:41,769 : INFO : token count processed\n",
      "2020-12-23 02:53:41,772 : INFO : frequencies processed\n",
      "2020-12-23 02:53:41,901 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:41,902 : INFO : entropies processed\n",
      "2020-12-23 02:53:41,903 : INFO : extropies processed\n",
      "2020-12-23 02:53:41,904 : INFO : token count processed\n",
      "2020-12-23 02:53:41,905 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:41,906 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:41,907 : INFO : vocab #2480\n",
      "2020-12-23 02:53:41,908 : INFO : diff #set()\n",
      "2020-12-23 02:53:42,171 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:42,298 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.2146399286602947, 0.4515406712661103], [0.9241600409150124, 0.07583996], [1.0, 1.0], [4.349648912578752, 5.015422548793484, 5.3837169693485, 3.981354492023735, 1.0340680567697476, 0.36829442055501627]]\n",
      "2020-12-23 02:53:42,300 : INFO : Removed 2 and 81 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:42,301 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:42,302 : INFO : built Dictionary(102 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 367 corpus positions)\n",
      "2020-12-23 02:53:42,339 : INFO : token count processed\n",
      "2020-12-23 02:53:42,344 : INFO : frequencies processed\n",
      "2020-12-23 02:53:42,473 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:42,474 : INFO : entropies processed\n",
      "2020-12-23 02:53:42,474 : INFO : extropies processed\n",
      "2020-12-23 02:53:42,475 : INFO : token count processed\n",
      "2020-12-23 02:53:42,476 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:42,477 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:42,477 : INFO : vocab #2480\n",
      "2020-12-23 02:53:42,478 : INFO : diff #set()\n",
      "2020-12-23 02:53:42,737 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:42,866 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.2051422584676355, 0.4534854820182464], [0.9194890558719635, 0.080510944], [1.584962500721156, 1.1699250014423124], [4.349648912578752, 6.030001281822029, 6.217437085617165, 4.162213108783616, 1.867788173038413, 0.18743580379513602]]\n",
      "2020-12-23 02:53:42,868 : INFO : Removed 2 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:42,869 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:42,871 : INFO : built Dictionary(91 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 330 corpus positions)\n",
      "2020-12-23 02:53:42,910 : INFO : token count processed\n",
      "2020-12-23 02:53:42,913 : INFO : frequencies processed\n",
      "2020-12-23 02:53:43,041 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:43,041 : INFO : entropies processed\n",
      "2020-12-23 02:53:43,042 : INFO : extropies processed\n",
      "2020-12-23 02:53:43,043 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:53:43,043 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:43,044 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:43,045 : INFO : vocab #2480\n",
      "2020-12-23 02:53:43,046 : INFO : diff #set()\n",
      "2020-12-23 02:53:43,305 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:43,432 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.2161587389008468, 0.4512312148253298], [0.906713992357254, 0.09328601], [1.0, 1.0], [4.349648912578752, 5.9537092545441395, 6.168329106774016, 4.135029060348875, 1.818680194195264, 0.21461985222987678]]\n",
      "2020-12-23 02:53:43,435 : INFO : Removed 2 and 90 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:43,436 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:43,437 : INFO : built Dictionary(102 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 331 corpus positions)\n",
      "2020-12-23 02:53:43,472 : INFO : token count processed\n",
      "2020-12-23 02:53:43,475 : INFO : frequencies processed\n",
      "2020-12-23 02:53:43,603 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:43,604 : INFO : entropies processed\n",
      "2020-12-23 02:53:43,604 : INFO : extropies processed\n",
      "2020-12-23 02:53:43,605 : INFO : token count processed\n",
      "2020-12-23 02:53:43,606 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:43,606 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:43,607 : INFO : vocab #2480\n",
      "2020-12-23 02:53:43,608 : INFO : diff #set()\n",
      "2020-12-23 02:53:43,862 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:43,990 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.1924866782741148, 0.4561031133777203], [0.9011630788445473, 0.09883692], [1.0, 1.0], [4.349648912578752, 6.184756445474906, 6.3746594482369145, 4.159745909816743, 2.0250105356581622, 0.18990300276200855]]\n",
      "2020-12-23 02:53:43,993 : INFO : Removed 2 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:43,994 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:43,995 : INFO : built Dictionary(111 unique tokens: ['add', 'call', 'certain', 'csb', 'default']...) from 2 documents (total 444 corpus positions)\n",
      "2020-12-23 02:53:44,034 : INFO : token count processed\n",
      "2020-12-23 02:53:44,037 : INFO : frequencies processed\n",
      "2020-12-23 02:53:44,163 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:44,163 : INFO : entropies processed\n",
      "2020-12-23 02:53:44,164 : INFO : extropies processed\n",
      "2020-12-23 02:53:44,165 : INFO : token count processed\n",
      "2020-12-23 02:53:44,166 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:44,167 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:44,167 : INFO : vocab #2480\n",
      "2020-12-23 02:53:44,168 : INFO : diff #set()\n",
      "2020-12-23 02:53:44,434 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:44,560 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.22396918718706, 0.44964651747933104], [0.9292848780751228, 0.07071512], [1.0, 1.0], [4.349648912578752, 6.212221456585881, 6.384063540250712, 4.177806828913922, 2.0344146276719597, 0.17184208366483134]]\n",
      "2020-12-23 02:53:44,563 : INFO : Removed 2 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:44,564 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:44,565 : INFO : built Dictionary(133 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 437 corpus positions)\n",
      "2020-12-23 02:53:44,626 : INFO : token count processed\n",
      "2020-12-23 02:53:44,628 : INFO : frequencies processed\n",
      "2020-12-23 02:53:44,754 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:44,755 : INFO : entropies processed\n",
      "2020-12-23 02:53:44,756 : INFO : extropies processed\n",
      "2020-12-23 02:53:44,758 : INFO : token count processed\n",
      "2020-12-23 02:53:44,759 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:44,760 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:44,761 : INFO : vocab #2480\n",
      "2020-12-23 02:53:44,762 : INFO : diff #set()\n",
      "2020-12-23 02:53:45,020 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:45,146 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.1256834548149526, 0.4704369306421747], [0.7067797183990479, 0.29322028], [1.9219280948873623, 1.2148067842293933], [4.515319531114783, 6.301552355933639, 6.501525410982559, 4.315346476065862, 1.986205879867776, 0.19997305504892005]]\n",
      "2020-12-23 02:53:45,149 : INFO : Removed 2 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:45,150 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:45,152 : INFO : built Dictionary(170 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 673 corpus positions)\n",
      "2020-12-23 02:53:45,234 : INFO : token count processed\n",
      "2020-12-23 02:53:45,237 : INFO : frequencies processed\n",
      "2020-12-23 02:53:45,364 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:45,364 : INFO : entropies processed\n",
      "2020-12-23 02:53:45,365 : INFO : extropies processed\n",
      "2020-12-23 02:53:45,367 : INFO : token count processed\n",
      "2020-12-23 02:53:45,368 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:45,369 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:45,369 : INFO : vocab #2480\n",
      "2020-12-23 02:53:45,371 : INFO : diff #set()\n",
      "2020-12-23 02:53:45,629 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:45,758 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.129624951813367, 0.46956624881226344], [0.7263979315757751, 0.27360207], [2.584962500721156, 1.315172029168969], [4.515319531114783, 6.739005504021667, 6.870958483191146, 4.383366551945304, 2.3556389520763625, 0.1319529791694789]]\n",
      "2020-12-23 02:53:45,760 : INFO : Removed 2 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:45,761 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:45,763 : INFO : built Dictionary(115 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 524 corpus positions)\n",
      "2020-12-23 02:53:45,814 : INFO : token count processed\n",
      "2020-12-23 02:53:45,817 : INFO : frequencies processed\n",
      "2020-12-23 02:53:45,946 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:45,946 : INFO : entropies processed\n",
      "2020-12-23 02:53:45,947 : INFO : extropies processed\n",
      "2020-12-23 02:53:45,948 : INFO : token count processed\n",
      "2020-12-23 02:53:45,949 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:45,949 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:45,950 : INFO : vocab #2480\n",
      "2020-12-23 02:53:45,951 : INFO : diff #set()\n",
      "2020-12-23 02:53:46,219 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:46,350 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.1656193079289618, 0.461761675442544], [0.8546675592660904, 0.14533244], [2.321928094887362, 1.2877123795494492], [4.515319531114783, 5.870833373337847, 6.057148521740885, 4.329004382711744, 1.5418289906261018, 0.18631514840303787]]\n",
      "2020-12-23 02:53:46,353 : INFO : Removed 2 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:46,354 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:46,355 : INFO : built Dictionary(78 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 207 corpus positions)\n",
      "2020-12-23 02:53:46,387 : INFO : token count processed\n",
      "2020-12-23 02:53:46,389 : INFO : frequencies processed\n",
      "2020-12-23 02:53:46,516 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:46,517 : INFO : entropies processed\n",
      "2020-12-23 02:53:46,518 : INFO : extropies processed\n",
      "2020-12-23 02:53:46,520 : INFO : token count processed\n",
      "2020-12-23 02:53:46,522 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:46,523 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:46,524 : INFO : vocab #2480\n",
      "2020-12-23 02:53:46,525 : INFO : diff #set()\n",
      "2020-12-23 02:53:46,791 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:46,919 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.1852189557678796, 0.45762004643081766], [0.8262791037559509, 0.1737209], [0.0, 0.0], [4.515319531114783, 5.371881234145534, 5.833324270156367, 4.0538764951039505, 1.3180047390415837, 0.4614430360108326]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:53:46,921 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:46,922 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:46,923 : INFO : built Dictionary(68 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 159 corpus positions)\n",
      "2020-12-23 02:53:46,947 : INFO : token count processed\n",
      "2020-12-23 02:53:46,951 : INFO : frequencies processed\n",
      "2020-12-23 02:53:47,085 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:47,086 : INFO : entropies processed\n",
      "2020-12-23 02:53:47,086 : INFO : extropies processed\n",
      "2020-12-23 02:53:47,088 : INFO : token count processed\n",
      "2020-12-23 02:53:47,089 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:47,089 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:47,090 : INFO : vocab #2480\n",
      "2020-12-23 02:53:47,091 : INFO : diff #set()\n",
      "2020-12-23 02:53:47,350 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:47,478 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.1819790440202653, 0.45829954359117686], [0.8419140577316284, 0.15808594], [1.0, 1.0], [4.515319531114783, 4.85108279267097, 5.4723702797181835, 3.8940320440675693, 0.9570507486034003, 0.6212874870472138]]\n",
      "2020-12-23 02:53:47,480 : INFO : Removed 2 and 127 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:47,481 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:47,482 : INFO : built Dictionary(107 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 432 corpus positions)\n",
      "2020-12-23 02:53:47,529 : INFO : token count processed\n",
      "2020-12-23 02:53:47,532 : INFO : frequencies processed\n",
      "2020-12-23 02:53:47,659 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:47,659 : INFO : entropies processed\n",
      "2020-12-23 02:53:47,660 : INFO : extropies processed\n",
      "2020-12-23 02:53:47,661 : INFO : token count processed\n",
      "2020-12-23 02:53:47,661 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:47,662 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:47,663 : INFO : vocab #2480\n",
      "2020-12-23 02:53:47,663 : INFO : diff #set()\n",
      "2020-12-23 02:53:47,920 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:48,047 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.1640916526619645, 0.4620876379103164], [0.8155902177095413, 0.18440978], [1.7924812503605778, 1.1575860145844845], [4.515319531114783, 6.139571208108155, 6.331578913804687, 4.323311825418251, 1.8162593826899043, 0.19200770569653258]]\n",
      "2020-12-23 02:53:48,050 : INFO : Removed 2 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:48,051 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:48,052 : INFO : built Dictionary(93 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 442 corpus positions)\n",
      "2020-12-23 02:53:48,091 : INFO : token count processed\n",
      "2020-12-23 02:53:48,093 : INFO : frequencies processed\n",
      "2020-12-23 02:53:48,219 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:48,220 : INFO : entropies processed\n",
      "2020-12-23 02:53:48,220 : INFO : extropies processed\n",
      "2020-12-23 02:53:48,222 : INFO : token count processed\n",
      "2020-12-23 02:53:48,222 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:48,223 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:48,224 : INFO : vocab #2480\n",
      "2020-12-23 02:53:48,224 : INFO : diff #set()\n",
      "2020-12-23 02:53:48,481 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:48,609 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.1407065221250308, 0.4671354945970467], [0.7438258826732635, 0.25617412], [1.8423709931771084, 1.1893232685884285], [4.515319531114783, 5.609710627339259, 5.844065122563562, 4.28096503589048, 1.3287455914487785, 0.23435449522430218]]\n",
      "2020-12-23 02:53:48,612 : INFO : Removed 2 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:48,612 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:48,614 : INFO : built Dictionary(177 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 1102 corpus positions)\n",
      "2020-12-23 02:53:48,699 : INFO : token count processed\n",
      "2020-12-23 02:53:48,701 : INFO : frequencies processed\n",
      "2020-12-23 02:53:48,836 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:48,836 : INFO : entropies processed\n",
      "2020-12-23 02:53:48,837 : INFO : extropies processed\n",
      "2020-12-23 02:53:48,838 : INFO : token count processed\n",
      "2020-12-23 02:53:48,839 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:48,840 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:48,841 : INFO : vocab #2480\n",
      "2020-12-23 02:53:48,842 : INFO : diff #set()\n",
      "2020-12-23 02:53:49,111 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:49,239 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.1479470538858263, 0.46556082385313524], [0.7741219699382782, 0.22587803], [3.4548223999466066, 1.3672062119190405], [4.515319531114783, 7.2441902753576075, 7.30220136297671, 4.4573084434956805, 2.786881831861927, 0.058011087619102675]]\n",
      "2020-12-23 02:53:49,242 : INFO : Removed 2 and 136 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:49,243 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:49,244 : INFO : built Dictionary(142 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 709 corpus positions)\n",
      "2020-12-23 02:53:49,305 : INFO : token count processed\n",
      "2020-12-23 02:53:49,308 : INFO : frequencies processed\n",
      "2020-12-23 02:53:49,437 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:49,438 : INFO : entropies processed\n",
      "2020-12-23 02:53:49,438 : INFO : extropies processed\n",
      "2020-12-23 02:53:49,439 : INFO : token count processed\n",
      "2020-12-23 02:53:49,440 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:49,441 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:49,441 : INFO : vocab #2480\n",
      "2020-12-23 02:53:49,443 : INFO : diff #set()\n",
      "2020-12-23 02:53:49,702 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:49,830 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.1314471047025143, 0.4691648213055561], [0.7629244774580002, 0.23707552], [3.0220552088742, 1.3359632893587228], [4.515319531114783, 6.2567074920449475, 6.380416477572781, 4.391610545586949, 1.8650969464579976, 0.12370898552783327]]\n",
      "2020-12-23 02:53:49,833 : INFO : Removed 2 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:49,834 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:49,835 : INFO : built Dictionary(92 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 219 corpus positions)\n",
      "2020-12-23 02:53:49,872 : INFO : token count processed\n",
      "2020-12-23 02:53:49,874 : INFO : frequencies processed\n",
      "2020-12-23 02:53:50,010 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:50,011 : INFO : entropies processed\n",
      "2020-12-23 02:53:50,012 : INFO : extropies processed\n",
      "2020-12-23 02:53:50,013 : INFO : token count processed\n",
      "2020-12-23 02:53:50,014 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:50,015 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:50,015 : INFO : vocab #2480\n",
      "2020-12-23 02:53:50,016 : INFO : diff #set()\n",
      "2020-12-23 02:53:50,286 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:50,413 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.141771287447013, 0.4669032617352891], [0.7730484902858734, 0.22695151], [2.0, 1.2451124978365313], [4.515319531114783, 5.7680018917339435, 6.127084595631988, 4.156236827216738, 1.611765064517205, 0.35908270389804464]]\n",
      "2020-12-23 02:53:50,415 : INFO : Removed 2 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:50,415 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:50,417 : INFO : built Dictionary(186 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 760 corpus positions)\n",
      "2020-12-23 02:53:50,503 : INFO : token count processed\n",
      "2020-12-23 02:53:50,509 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:53:50,637 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:50,638 : INFO : entropies processed\n",
      "2020-12-23 02:53:50,638 : INFO : extropies processed\n",
      "2020-12-23 02:53:50,639 : INFO : token count processed\n",
      "2020-12-23 02:53:50,640 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:50,641 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:50,643 : INFO : vocab #2480\n",
      "2020-12-23 02:53:50,645 : INFO : diff #set()\n",
      "2020-12-23 02:53:50,902 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:51,029 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.1448072731856538, 0.46624235776425416], [0.7067294120788574, 0.2932706], [2.8464393446710154, 1.3178207096846455], [4.515319531114783, 6.846479111193757, 6.958254469832184, 4.403544172476355, 2.442934938717401, 0.11177535863842714]]\n",
      "2020-12-23 02:53:51,032 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:51,033 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:51,034 : INFO : built Dictionary(42 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 63 corpus positions)\n",
      "2020-12-23 02:53:51,053 : INFO : token count processed\n",
      "2020-12-23 02:53:51,056 : INFO : frequencies processed\n",
      "2020-12-23 02:53:51,183 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:51,184 : INFO : entropies processed\n",
      "2020-12-23 02:53:51,185 : INFO : extropies processed\n",
      "2020-12-23 02:53:51,186 : INFO : token count processed\n",
      "2020-12-23 02:53:51,187 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:51,188 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:51,189 : INFO : vocab #2480\n",
      "2020-12-23 02:53:51,190 : INFO : diff #set()\n",
      "2020-12-23 02:53:51,450 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:51,577 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/fireException.py')[[1.1757520057361255, 0.4596112044771704], [0.870570108294487, 0.12942989], [0.0, 0.0], [4.515319531114783, 4.165013816065912, 5.306532043597981, 3.373801303582715, 0.7912125124831979, 1.1415182275320692]]\n",
      "2020-12-23 02:53:51,580 : INFO : Removed 2 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:51,581 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:51,582 : INFO : built Dictionary(69 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 170 corpus positions)\n",
      "2020-12-23 02:53:51,603 : INFO : token count processed\n",
      "2020-12-23 02:53:51,606 : INFO : frequencies processed\n",
      "2020-12-23 02:53:51,734 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:51,734 : INFO : entropies processed\n",
      "2020-12-23 02:53:51,735 : INFO : extropies processed\n",
      "2020-12-23 02:53:51,736 : INFO : token count processed\n",
      "2020-12-23 02:53:51,737 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:51,738 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:51,739 : INFO : vocab #2480\n",
      "2020-12-23 02:53:51,740 : INFO : diff #set()\n",
      "2020-12-23 02:53:51,997 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:52,126 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.1425112740105492, 0.4667420013749138], [0.7785135805606842, 0.22148642], [1.584962500721156, 1.1699250014423124], [4.515319531114783, 5.449968864419248, 5.898184525732166, 4.067103869801866, 1.3828649946173828, 0.44821566131291757]]\n",
      "2020-12-23 02:53:52,129 : INFO : Removed 2 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:52,129 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:52,131 : INFO : built Dictionary(158 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 607 corpus positions)\n",
      "2020-12-23 02:53:52,204 : INFO : token count processed\n",
      "2020-12-23 02:53:52,208 : INFO : frequencies processed\n",
      "2020-12-23 02:53:52,335 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:52,336 : INFO : entropies processed\n",
      "2020-12-23 02:53:52,336 : INFO : extropies processed\n",
      "2020-12-23 02:53:52,337 : INFO : token count processed\n",
      "2020-12-23 02:53:52,338 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:52,339 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:52,339 : INFO : vocab #2480\n",
      "2020-12-23 02:53:52,340 : INFO : diff #set()\n",
      "2020-12-23 02:53:52,597 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:52,725 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.15208800079292, 0.4646650135271224], [0.7952529042959213, 0.2047471], [2.6416041678685933, 1.2962416748397703], [4.515319531114783, 6.530294129310484, 6.664499873483257, 4.381113786942011, 2.1491803423684734, 0.13420574417277287]]\n",
      "2020-12-23 02:53:52,728 : INFO : Removed 2 and 142 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:52,729 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:52,731 : INFO : built Dictionary(132 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 623 corpus positions)\n",
      "2020-12-23 02:53:52,799 : INFO : token count processed\n",
      "2020-12-23 02:53:52,802 : INFO : frequencies processed\n",
      "2020-12-23 02:53:52,930 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:52,930 : INFO : entropies processed\n",
      "2020-12-23 02:53:52,931 : INFO : extropies processed\n",
      "2020-12-23 02:53:52,933 : INFO : token count processed\n",
      "2020-12-23 02:53:52,933 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:52,935 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:52,936 : INFO : vocab #2480\n",
      "2020-12-23 02:53:52,937 : INFO : diff #set()\n",
      "2020-12-23 02:53:53,206 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:53,335 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.1576388332611598, 0.46346959675755905], [0.793531984090805, 0.20646802], [2.4056390622295662, 1.2666171566933806], [4.515319531114783, 6.470272233491701, 6.609330562769408, 4.376261201837076, 2.0940110316546248, 0.13905832927770678]]\n",
      "2020-12-23 02:53:53,338 : INFO : Removed 2 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:53,339 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:53,340 : INFO : built Dictionary(134 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 412 corpus positions)\n",
      "2020-12-23 02:53:53,399 : INFO : token count processed\n",
      "2020-12-23 02:53:53,402 : INFO : frequencies processed\n",
      "2020-12-23 02:53:53,534 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:53,534 : INFO : entropies processed\n",
      "2020-12-23 02:53:53,535 : INFO : extropies processed\n",
      "2020-12-23 02:53:53,537 : INFO : token count processed\n",
      "2020-12-23 02:53:53,538 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:53,539 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:53,539 : INFO : vocab #2480\n",
      "2020-12-23 02:53:53,541 : INFO : diff #set()\n",
      "2020-12-23 02:53:53,803 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:53,931 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.145562043292445, 0.4660783420951382], [0.7576103210449219, 0.24238968], [2.321928094887362, 1.2877123795494492], [4.515319531114783, 6.550038223589686, 6.736704216917337, 4.328653537787131, 2.221384685802554, 0.18666599332765088]]\n",
      "2020-12-23 02:53:53,934 : INFO : Removed 2 and 70 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:53,935 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:53,936 : INFO : built Dictionary(91 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 328 corpus positions)\n",
      "2020-12-23 02:53:53,970 : INFO : token count processed\n",
      "2020-12-23 02:53:53,979 : INFO : frequencies processed\n",
      "2020-12-23 02:53:54,108 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:54,109 : INFO : entropies processed\n",
      "2020-12-23 02:53:54,109 : INFO : extropies processed\n",
      "2020-12-23 02:53:54,110 : INFO : token count processed\n",
      "2020-12-23 02:53:54,111 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:54,111 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:54,112 : INFO : vocab #2480\n",
      "2020-12-23 02:53:54,113 : INFO : diff #set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:53:54,372 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:54,500 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.1358856422030372, 0.46818986009408264], [0.7638222277164459, 0.23617777], [1.584962500721156, 1.1699250014423124], [4.515319531114783, 5.860525481261383, 6.127492016748201, 4.248352995627965, 1.612172485633418, 0.2669665354868185]]\n",
      "2020-12-23 02:53:54,503 : INFO : Removed 2 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:54,504 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:54,505 : INFO : built Dictionary(64 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 366 corpus positions)\n",
      "2020-12-23 02:53:54,528 : INFO : token count processed\n",
      "2020-12-23 02:53:54,532 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:53:54,534 : INFO : frequencies processed\n",
      "2020-12-23 02:53:54,536 : INFO : token count processed\n",
      "2020-12-23 02:53:54,537 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:54,538 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:54,539 : INFO : vocab #2480\n",
      "2020-12-23 02:53:54,540 : INFO : diff #set()\n",
      "2020-12-23 02:53:54,791 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:54,919 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.2666853306566996, 0.4411728379211251], [0.9103271290659904, 0.08967287], [nan, nan], [4.515319531114783, 5.945464049777852, 6.1863411020991, 4.274442478793534, 1.6710215709843173, 0.240877052321248]]\n",
      "2020-12-23 02:53:54,922 : INFO : Removed 2 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:54,923 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:54,924 : INFO : built Dictionary(202 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 913 corpus positions)\n",
      "2020-12-23 02:53:55,018 : INFO : token count processed\n",
      "2020-12-23 02:53:55,021 : INFO : frequencies processed\n",
      "2020-12-23 02:53:55,148 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:55,149 : INFO : entropies processed\n",
      "2020-12-23 02:53:55,149 : INFO : extropies processed\n",
      "2020-12-23 02:53:55,151 : INFO : token count processed\n",
      "2020-12-23 02:53:55,152 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:55,153 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:55,154 : INFO : vocab #2480\n",
      "2020-12-23 02:53:55,155 : INFO : diff #set()\n",
      "2020-12-23 02:53:55,423 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:55,551 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.1419513608345473, 0.4668640092790809], [0.7496573030948639, 0.2503427], [3.3248629576173565, 1.3574960179923319], [4.515319531114783, 6.811563897304216, 6.898641650128209, 4.42824177829079, 2.3833221190134255, 0.08707775282399233]]\n",
      "2020-12-23 02:53:55,554 : INFO : Removed 2 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:55,555 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:55,556 : INFO : built Dictionary(221 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 1008 corpus positions)\n",
      "2020-12-23 02:53:55,671 : INFO : token count processed\n",
      "2020-12-23 02:53:55,677 : INFO : frequencies processed\n",
      "2020-12-23 02:53:55,805 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:55,805 : INFO : entropies processed\n",
      "2020-12-23 02:53:55,806 : INFO : extropies processed\n",
      "2020-12-23 02:53:55,807 : INFO : token count processed\n",
      "2020-12-23 02:53:55,808 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:55,809 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:55,809 : INFO : vocab #2480\n",
      "2020-12-23 02:53:55,810 : INFO : diff #set()\n",
      "2020-12-23 02:53:56,070 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:56,199 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.1210992198287033, 0.47145366452058685], [0.6989287734031677, 0.30107123], [3.572431251322119, 1.3732570370060984], [4.515319531114783, 7.502034948968415, 7.55389425855119, 4.463460221532009, 3.0385747274364068, 0.05185930958277485]]\n",
      "2020-12-23 02:53:56,202 : INFO : Removed 2 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:56,202 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:56,204 : INFO : built Dictionary(268 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 1581 corpus positions)\n",
      "2020-12-23 02:53:56,353 : INFO : token count processed\n",
      "2020-12-23 02:53:56,355 : INFO : frequencies processed\n",
      "2020-12-23 02:53:56,482 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:56,483 : INFO : entropies processed\n",
      "2020-12-23 02:53:56,484 : INFO : extropies processed\n",
      "2020-12-23 02:53:56,486 : INFO : token count processed\n",
      "2020-12-23 02:53:56,487 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:56,487 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:56,488 : INFO : vocab #2480\n",
      "2020-12-23 02:53:56,489 : INFO : diff #set()\n",
      "2020-12-23 02:53:56,748 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:56,877 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.1172867569961016, 0.4723025809780953], [0.735758364200592, 0.26424164], [3.188721875540867, 1.3469079016509693], [4.515319531114783, 7.39180093901977, 7.445468140454998, 4.4616523296795565, 2.9301486093402147, 0.05366720143522752]]\n",
      "2020-12-23 02:53:56,879 : INFO : Removed 2 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:56,880 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:56,881 : INFO : built Dictionary(59 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 136 corpus positions)\n",
      "2020-12-23 02:53:56,898 : INFO : token count processed\n",
      "2020-12-23 02:53:56,901 : INFO : frequencies processed\n",
      "2020-12-23 02:53:57,038 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:57,039 : INFO : entropies processed\n",
      "2020-12-23 02:53:57,040 : INFO : extropies processed\n",
      "2020-12-23 02:53:57,042 : INFO : token count processed\n",
      "2020-12-23 02:53:57,044 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:57,045 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:57,046 : INFO : vocab #2480\n",
      "2020-12-23 02:53:57,048 : INFO : diff #set()\n",
      "2020-12-23 02:53:57,310 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:57,439 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.142997976830071, 0.4666359981726174], [0.8027447760105133, 0.19725522], [1.584962500721156, 1.1699250014423124], [4.515319531114783, 4.927561309677364, 5.547025689624396, 3.8958551511677495, 1.0317061585096132, 0.6194643799470327]]\n",
      "2020-12-23 02:53:57,441 : INFO : Removed 2 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:57,442 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:57,443 : INFO : built Dictionary(29 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 37 corpus positions)\n",
      "2020-12-23 02:53:57,448 : INFO : token count processed\n",
      "2020-12-23 02:53:57,450 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:53:57,451 : INFO : frequencies processed\n",
      "2020-12-23 02:53:57,452 : INFO : token count processed\n",
      "2020-12-23 02:53:57,453 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:57,454 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:57,454 : INFO : vocab #2480\n",
      "2020-12-23 02:53:57,455 : INFO : diff #set()\n",
      "2020-12-23 02:53:57,713 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:57,842 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.235208862088495, 0.44738548462340905], [0.9268316030502319, 0.0731684], [nan, nan], [4.515319531114783, 2.5216406363433186, 4.836433629007711, 2.20052653845039, 0.3211140978929281, 2.3147929926643926]]\n",
      "2020-12-23 02:53:57,846 : INFO : Removed 2 and 530 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:53:57,847 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:57,848 : INFO : built Dictionary(338 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 2909 corpus positions)\n",
      "2020-12-23 02:53:58,064 : INFO : token count processed\n",
      "2020-12-23 02:53:58,069 : INFO : frequencies processed\n",
      "2020-12-23 02:53:58,196 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:58,197 : INFO : entropies processed\n",
      "2020-12-23 02:53:58,197 : INFO : extropies processed\n",
      "2020-12-23 02:53:58,199 : INFO : token count processed\n",
      "2020-12-23 02:53:58,200 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:58,201 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:58,201 : INFO : vocab #2480\n",
      "2020-12-23 02:53:58,203 : INFO : diff #set()\n",
      "2020-12-23 02:53:58,464 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:58,592 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.0767408641911003, 0.4815237265480903], [0.6375624239444733, 0.36243758], [3.6464393446710153, 1.3752020230999096], [4.515319531114783, 7.480007711014331, 7.502437428387376, 4.492889813741739, 2.9871178972725927, 0.02242971737304522]]\n",
      "2020-12-23 02:53:58,595 : INFO : Removed 2 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:58,596 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:58,597 : INFO : built Dictionary(218 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 1058 corpus positions)\n",
      "2020-12-23 02:53:58,707 : INFO : token count processed\n",
      "2020-12-23 02:53:58,714 : INFO : frequencies processed\n",
      "2020-12-23 02:53:58,843 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:58,844 : INFO : entropies processed\n",
      "2020-12-23 02:53:58,844 : INFO : extropies processed\n",
      "2020-12-23 02:53:58,846 : INFO : token count processed\n",
      "2020-12-23 02:53:58,846 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:58,847 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:58,848 : INFO : vocab #2480\n",
      "2020-12-23 02:53:58,849 : INFO : diff #set()\n",
      "2020-12-23 02:53:59,108 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:59,236 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.0872547238597328, 0.47909820903450107], [0.6568086743354797, 0.34319133], [3.3248629576173565, 1.3574960179923319], [4.515319531114783, 7.131331012509435, 7.194457084574504, 4.452193459049715, 2.6791375534597206, 0.06312607206506904]]\n",
      "2020-12-23 02:53:59,239 : INFO : Removed 2 and 166 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:59,240 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:59,243 : INFO : built Dictionary(209 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 877 corpus positions)\n",
      "2020-12-23 02:53:59,345 : INFO : token count processed\n",
      "2020-12-23 02:53:59,352 : INFO : frequencies processed\n",
      "2020-12-23 02:53:59,481 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:53:59,482 : INFO : entropies processed\n",
      "2020-12-23 02:53:59,483 : INFO : extropies processed\n",
      "2020-12-23 02:53:59,484 : INFO : token count processed\n",
      "2020-12-23 02:53:59,485 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:53:59,485 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:53:59,486 : INFO : vocab #2480\n",
      "2020-12-23 02:53:59,487 : INFO : diff #set()\n",
      "2020-12-23 02:53:59,753 : INFO : alphabet #2480\n",
      "2020-12-23 02:53:59,882 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.1638417553392548, 0.46214100339477754], [0.7396320998668671, 0.2603679], [3.0271691184406184, 1.334157713553925], [4.515319531114783, 7.203742744794778, 7.289744878331132, 4.429317397578429, 2.774425347216349, 0.08600213353635411]]\n",
      "2020-12-23 02:53:59,885 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:53:59,886 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:53:59,887 : INFO : built Dictionary(72 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 209 corpus positions)\n",
      "2020-12-23 02:53:59,913 : INFO : token count processed\n",
      "2020-12-23 02:53:59,919 : INFO : frequencies processed\n",
      "2020-12-23 02:54:00,054 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:00,055 : INFO : entropies processed\n",
      "2020-12-23 02:54:00,056 : INFO : extropies processed\n",
      "2020-12-23 02:54:00,057 : INFO : token count processed\n",
      "2020-12-23 02:54:00,058 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:00,059 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:00,060 : INFO : vocab #2480\n",
      "2020-12-23 02:54:00,061 : INFO : diff #set()\n",
      "2020-12-23 02:54:00,319 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:00,446 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.072561947037563, 0.4824946252773578], [0.7190416753292084, 0.28095832], [1.9219280948873623, 1.2148067842293933], [4.515319531114783, 5.195502554608948, 5.621494141079216, 4.0893279446445145, 1.106174609964433, 0.4259915864702677]]\n",
      "2020-12-23 02:54:00,449 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:00,449 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:00,450 : INFO : built Dictionary(78 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 270 corpus positions)\n",
      "2020-12-23 02:54:00,476 : INFO : token count processed\n",
      "2020-12-23 02:54:00,479 : INFO : frequencies processed\n",
      "2020-12-23 02:54:00,607 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:00,608 : INFO : entropies processed\n",
      "2020-12-23 02:54:00,609 : INFO : extropies processed\n",
      "2020-12-23 02:54:00,610 : INFO : token count processed\n",
      "2020-12-23 02:54:00,612 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:00,613 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:00,613 : INFO : vocab #2480\n",
      "2020-12-23 02:54:00,615 : INFO : diff #set()\n",
      "2020-12-23 02:54:00,871 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:00,997 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.1254907066727655, 0.4704795917764307], [0.7367126941680908, 0.2632873], [1.7924812503605778, 1.1575860145844845], [4.515319531114783, 5.32027245610305, 5.68689493581198, 4.148697051405852, 1.1715754046971965, 0.3666224797089299]]\n",
      "2020-12-23 02:54:01,000 : INFO : Removed 2 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:01,001 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:01,003 : INFO : built Dictionary(170 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 510 corpus positions)\n",
      "2020-12-23 02:54:01,090 : INFO : token count processed\n",
      "2020-12-23 02:54:01,094 : INFO : frequencies processed\n",
      "2020-12-23 02:54:01,224 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:01,224 : INFO : entropies processed\n",
      "2020-12-23 02:54:01,225 : INFO : extropies processed\n",
      "2020-12-23 02:54:01,226 : INFO : token count processed\n",
      "2020-12-23 02:54:01,227 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:01,228 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:01,228 : INFO : vocab #2480\n",
      "2020-12-23 02:54:01,229 : INFO : diff #set()\n",
      "2020-12-23 02:54:01,488 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:01,617 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.0986560563975503, 0.47649542046282267], [0.6828498244285583, 0.31715018], [3.5465935642949384, 1.3764678303056375], [4.515319531114783, 6.898202761357263, 7.007362772302969, 4.406159520169076, 2.4920432411881857, 0.10916001094570582]]\n",
      "2020-12-23 02:54:01,619 : INFO : Removed 2 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:01,620 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:01,622 : INFO : built Dictionary(138 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 532 corpus positions)\n",
      "2020-12-23 02:54:01,692 : INFO : token count processed\n",
      "2020-12-23 02:54:01,695 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:54:01,825 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:01,826 : INFO : entropies processed\n",
      "2020-12-23 02:54:01,826 : INFO : extropies processed\n",
      "2020-12-23 02:54:01,828 : INFO : token count processed\n",
      "2020-12-23 02:54:01,829 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:01,830 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:01,831 : INFO : vocab #2480\n",
      "2020-12-23 02:54:01,832 : INFO : diff #set()\n",
      "2020-12-23 02:54:02,092 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:02,221 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.0187891814501224, 0.49534642308796556], [0.6168783605098724, 0.38312164], [2.8453509366224363, 1.3210203571681218], [4.515319531114783, 6.388500481644799, 6.519851056715172, 4.383968956044409, 2.004531525600389, 0.13135057507037295]]\n",
      "2020-12-23 02:54:02,223 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:02,224 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:02,225 : INFO : built Dictionary(63 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 196 corpus positions)\n",
      "2020-12-23 02:54:02,245 : INFO : token count processed\n",
      "2020-12-23 02:54:02,248 : INFO : frequencies processed\n",
      "2020-12-23 02:54:02,375 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:02,376 : INFO : entropies processed\n",
      "2020-12-23 02:54:02,377 : INFO : extropies processed\n",
      "2020-12-23 02:54:02,379 : INFO : token count processed\n",
      "2020-12-23 02:54:02,380 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:02,382 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:02,383 : INFO : vocab #2480\n",
      "2020-12-23 02:54:02,385 : INFO : diff #set()\n",
      "2020-12-23 02:54:02,654 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:02,783 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.0826070429479004, 0.4801673956621765], [0.7363299131393433, 0.2636701], [1.9219280948873623, 1.2148067842293933], [4.515319531114783, 4.8191513650620195, 5.3173588357484345, 4.017112060428369, 0.8020393046336514, 0.498207470686415]]\n",
      "2020-12-23 02:54:02,785 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:02,786 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:02,787 : INFO : built Dictionary(68 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 251 corpus positions)\n",
      "2020-12-23 02:54:02,815 : INFO : token count processed\n",
      "2020-12-23 02:54:02,817 : INFO : frequencies processed\n",
      "2020-12-23 02:54:02,945 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:02,946 : INFO : entropies processed\n",
      "2020-12-23 02:54:02,947 : INFO : extropies processed\n",
      "2020-12-23 02:54:02,948 : INFO : token count processed\n",
      "2020-12-23 02:54:02,949 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:02,950 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:02,951 : INFO : vocab #2480\n",
      "2020-12-23 02:54:02,953 : INFO : diff #set()\n",
      "2020-12-23 02:54:03,224 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:03,352 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.1261913535439627, 0.47032455396509226], [0.7339673340320587, 0.26603267], [1.7924812503605778, 1.1575860145844845], [4.515319531114783, 5.062480936779194, 5.4701241326523125, 4.107676335241663, 0.9548046015375293, 0.4076431958731188]]\n",
      "2020-12-23 02:54:03,355 : INFO : Removed 2 and 223 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:03,356 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:03,359 : INFO : built Dictionary(249 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 1805 corpus positions)\n",
      "2020-12-23 02:54:03,508 : INFO : token count processed\n",
      "2020-12-23 02:54:03,516 : INFO : frequencies processed\n",
      "2020-12-23 02:54:03,648 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:03,648 : INFO : entropies processed\n",
      "2020-12-23 02:54:03,649 : INFO : extropies processed\n",
      "2020-12-23 02:54:03,651 : INFO : token count processed\n",
      "2020-12-23 02:54:03,652 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:03,653 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:03,654 : INFO : vocab #2480\n",
      "2020-12-23 02:54:03,655 : INFO : diff #set()\n",
      "2020-12-23 02:54:03,919 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:04,047 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.1020670840592788, 0.4757222105723244], [0.6774744689464569, 0.32252553], [3.6835423624332306, 1.377974449419992], [4.515319531114783, 7.185085743102134, 7.2280101377367725, 4.472395136480145, 2.7126906066219894, 0.04292439463463804]]\n",
      "2020-12-23 02:54:04,050 : INFO : Removed 2 and 148 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:04,051 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:04,052 : INFO : built Dictionary(170 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 780 corpus positions)\n",
      "2020-12-23 02:54:04,135 : INFO : token count processed\n",
      "2020-12-23 02:54:04,139 : INFO : frequencies processed\n",
      "2020-12-23 02:54:04,269 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:04,270 : INFO : entropies processed\n",
      "2020-12-23 02:54:04,271 : INFO : extropies processed\n",
      "2020-12-23 02:54:04,272 : INFO : token count processed\n",
      "2020-12-23 02:54:04,273 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:04,274 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:04,274 : INFO : vocab #2480\n",
      "2020-12-23 02:54:04,275 : INFO : diff #set()\n",
      "2020-12-23 02:54:04,541 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:04,667 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.133118883970952, 0.46879712495837506], [0.7595701068639755, 0.2404299], [3.180832987205441, 1.3478475537994532], [4.515319531114783, 6.591225336124281, 6.692356122868768, 4.414188744370296, 2.1770365917539847, 0.10113078674448683]]\n",
      "2020-12-23 02:54:04,670 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:04,670 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:04,671 : INFO : built Dictionary(56 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 118 corpus positions)\n",
      "2020-12-23 02:54:04,687 : INFO : token count processed\n",
      "2020-12-23 02:54:04,690 : INFO : frequencies processed\n",
      "2020-12-23 02:54:04,818 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:04,819 : INFO : entropies processed\n",
      "2020-12-23 02:54:04,820 : INFO : extropies processed\n",
      "2020-12-23 02:54:04,822 : INFO : token count processed\n",
      "2020-12-23 02:54:04,823 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:04,824 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:04,825 : INFO : vocab #2480\n",
      "2020-12-23 02:54:04,826 : INFO : diff #set()\n",
      "2020-12-23 02:54:05,082 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:05,208 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.168675807843847, 0.4611108753014705], [0.8345791548490524, 0.16542085], [1.0, 1.0], [4.515319531114783, 4.7032114441396695, 5.443579399838709, 3.774951575415743, 0.9282598687239263, 0.7403679556990399]]\n",
      "2020-12-23 02:54:05,211 : INFO : Removed 2 and 9 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:05,212 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:05,214 : INFO : built Dictionary(127 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 358 corpus positions)\n",
      "2020-12-23 02:54:05,268 : INFO : token count processed\n",
      "2020-12-23 02:54:05,271 : INFO : frequencies processed\n",
      "2020-12-23 02:54:05,398 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:05,399 : INFO : entropies processed\n",
      "2020-12-23 02:54:05,400 : INFO : extropies processed\n",
      "2020-12-23 02:54:05,401 : INFO : token count processed\n",
      "2020-12-23 02:54:05,402 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:05,403 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:05,404 : INFO : vocab #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:54:05,405 : INFO : diff #set()\n",
      "2020-12-23 02:54:05,664 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:05,793 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.075069690487999, 0.4819115254701771], [0.6414878070354462, 0.3585122], [2.9219280948873623, 1.3359016564230495], [4.515319531114783, 6.14228447828618, 6.344595912430805, 4.313008096970158, 1.8292763813160215, 0.20231143414462505]]\n",
      "2020-12-23 02:54:05,796 : INFO : Removed 2 and 184 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:05,797 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:05,798 : INFO : built Dictionary(260 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 1148 corpus positions)\n",
      "2020-12-23 02:54:05,951 : INFO : token count processed\n",
      "2020-12-23 02:54:05,957 : INFO : frequencies processed\n",
      "2020-12-23 02:54:06,085 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:06,086 : INFO : entropies processed\n",
      "2020-12-23 02:54:06,086 : INFO : extropies processed\n",
      "2020-12-23 02:54:06,088 : INFO : token count processed\n",
      "2020-12-23 02:54:06,089 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:06,089 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:06,090 : INFO : vocab #2480\n",
      "2020-12-23 02:54:06,091 : INFO : diff #set()\n",
      "2020-12-23 02:54:06,351 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:06,478 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.1041962714746794, 0.47524083829840275], [0.6453023254871368, 0.35469767], [3.6835423624332306, 1.377974449419992], [4.515319531114783, 7.450178124335845, 7.4976849804265475, 4.4678126750240805, 2.9823654493117644, 0.0475068560907026]]\n",
      "2020-12-23 02:54:06,480 : INFO : Removed 2 and 16 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:06,481 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:06,482 : INFO : built Dictionary(72 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 196 corpus positions)\n",
      "2020-12-23 02:54:06,507 : INFO : token count processed\n",
      "2020-12-23 02:54:06,509 : INFO : frequencies processed\n",
      "2020-12-23 02:54:06,636 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:06,637 : INFO : entropies processed\n",
      "2020-12-23 02:54:06,638 : INFO : extropies processed\n",
      "2020-12-23 02:54:06,640 : INFO : token count processed\n",
      "2020-12-23 02:54:06,641 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:06,643 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:06,644 : INFO : vocab #2480\n",
      "2020-12-23 02:54:06,645 : INFO : diff #set()\n",
      "2020-12-23 02:54:06,915 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:07,047 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.0628271969299596, 0.4847715802313777], [0.7201854288578033, 0.27981457], [1.9219280948873623, 1.2148067842293933], [4.515319531114783, 5.20665021947654, 5.629188642848314, 4.09278110774301, 1.113869111733531, 0.4225384233717744]]\n",
      "2020-12-23 02:54:07,050 : INFO : Removed 2 and 89 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:07,051 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:07,053 : INFO : built Dictionary(138 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 534 corpus positions)\n",
      "2020-12-23 02:54:07,123 : INFO : token count processed\n",
      "2020-12-23 02:54:07,129 : INFO : frequencies processed\n",
      "2020-12-23 02:54:07,256 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:07,257 : INFO : entropies processed\n",
      "2020-12-23 02:54:07,258 : INFO : extropies processed\n",
      "2020-12-23 02:54:07,259 : INFO : token count processed\n",
      "2020-12-23 02:54:07,259 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:07,260 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:07,261 : INFO : vocab #2480\n",
      "2020-12-23 02:54:07,262 : INFO : diff #set()\n",
      "2020-12-23 02:54:07,518 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:07,646 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.1521583240854478, 0.4646498302697812], [0.7564172148704529, 0.24358279], [2.6416041678685933, 1.2962416748397703], [4.515319531114783, 6.524718477352, 6.675203966603603, 4.364834041863181, 2.15988443548882, 0.15048548925160343]]\n",
      "2020-12-23 02:54:07,648 : INFO : Removed 2 and 8 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:07,649 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:07,650 : INFO : built Dictionary(77 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 241 corpus positions)\n",
      "2020-12-23 02:54:07,676 : INFO : token count processed\n",
      "2020-12-23 02:54:07,678 : INFO : frequencies processed\n",
      "2020-12-23 02:54:07,805 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:07,806 : INFO : entropies processed\n",
      "2020-12-23 02:54:07,807 : INFO : extropies processed\n",
      "2020-12-23 02:54:07,808 : INFO : token count processed\n",
      "2020-12-23 02:54:07,809 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:07,809 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:07,810 : INFO : vocab #2480\n",
      "2020-12-23 02:54:07,811 : INFO : diff #set()\n",
      "2020-12-23 02:54:08,069 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:08,205 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.081092788429302, 0.4805167773200284], [0.731090784072876, 0.26890922], [1.9219280948873623, 1.2148067842293933], [4.515319531114783, 5.321859380715434, 5.695586599380558, 4.14159231244966, 1.180267068265775, 0.37372721866512393]]\n",
      "2020-12-23 02:54:08,208 : INFO : Removed 2 and 119 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:08,209 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:08,210 : INFO : built Dictionary(151 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 859 corpus positions)\n",
      "2020-12-23 02:54:08,285 : INFO : token count processed\n",
      "2020-12-23 02:54:08,287 : INFO : frequencies processed\n",
      "2020-12-23 02:54:08,415 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:08,416 : INFO : entropies processed\n",
      "2020-12-23 02:54:08,416 : INFO : extropies processed\n",
      "2020-12-23 02:54:08,418 : INFO : token count processed\n",
      "2020-12-23 02:54:08,419 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:08,419 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:08,420 : INFO : vocab #2480\n",
      "2020-12-23 02:54:08,421 : INFO : diff #set()\n",
      "2020-12-23 02:54:08,685 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:08,818 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.088959182595669, 0.47870729515999166], [0.6726331114768982, 0.3273669], [3.180832987205441, 1.3478475537994532], [4.515319531114783, 6.500767808767801, 6.594757348995236, 4.421329990887349, 2.0794378178804527, 0.09398954022743489]]\n",
      "2020-12-23 02:54:08,820 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:08,821 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:08,822 : INFO : built Dictionary(51 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 81 corpus positions)\n",
      "2020-12-23 02:54:08,837 : INFO : token count processed\n",
      "2020-12-23 02:54:08,840 : INFO : frequencies processed\n",
      "2020-12-23 02:54:08,971 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:08,972 : INFO : entropies processed\n",
      "2020-12-23 02:54:08,973 : INFO : extropies processed\n",
      "2020-12-23 02:54:08,974 : INFO : token count processed\n",
      "2020-12-23 02:54:08,975 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:08,976 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:08,977 : INFO : vocab #2480\n",
      "2020-12-23 02:54:08,978 : INFO : diff #set()\n",
      "2020-12-23 02:54:09,245 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:09,372 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.1228238475329586, 0.471070645433982], [0.7421835064888, 0.2578165], [1.0, 1.0], [4.515319531114783, 4.736228843383063, 5.539356226448467, 3.712192148049379, 1.0240366953336837, 0.8031273830654042]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:54:09,375 : INFO : Removed 2 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:09,376 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:09,378 : INFO : built Dictionary(114 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 422 corpus positions)\n",
      "2020-12-23 02:54:09,428 : INFO : token count processed\n",
      "2020-12-23 02:54:09,430 : INFO : frequencies processed\n",
      "2020-12-23 02:54:09,561 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:09,562 : INFO : entropies processed\n",
      "2020-12-23 02:54:09,562 : INFO : extropies processed\n",
      "2020-12-23 02:54:09,564 : INFO : token count processed\n",
      "2020-12-23 02:54:09,565 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:09,566 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:09,567 : INFO : vocab #2480\n",
      "2020-12-23 02:54:09,568 : INFO : diff #set()\n",
      "2020-12-23 02:54:09,830 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:09,959 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.188391063695814, 0.4569567188376162], [0.7991310507059097, 0.20086895], [1.0, 1.0], [4.515319531114783, 5.788442787590127, 6.061811550190031, 4.241950768514879, 1.5464920190752478, 0.2733687625999037]]\n",
      "2020-12-23 02:54:09,961 : INFO : Removed 2 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:09,962 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:09,963 : INFO : built Dictionary(70 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 108 corpus positions)\n",
      "2020-12-23 02:54:09,986 : INFO : token count processed\n",
      "2020-12-23 02:54:09,988 : INFO : frequencies processed\n",
      "2020-12-23 02:54:10,117 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:10,118 : INFO : entropies processed\n",
      "2020-12-23 02:54:10,118 : INFO : extropies processed\n",
      "2020-12-23 02:54:10,119 : INFO : token count processed\n",
      "2020-12-23 02:54:10,120 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:10,121 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:10,122 : INFO : vocab #2480\n",
      "2020-12-23 02:54:10,123 : INFO : diff #set()\n",
      "2020-12-23 02:54:10,381 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:10,509 : INFO : Computed distances or similarities ('279', 'sacp-python-common/setup.py')[[1.1623287662522193, 0.4624643650896875], [0.7912195771932602, 0.20878042], [1.584962500721156, 1.1699250014423124], [4.515319531114783, 5.370004292053436, 5.901138594612995, 3.9841852285552246, 1.3858190634982117, 0.5311343025595585]]\n",
      "2020-12-23 02:54:10,511 : INFO : Removed 2 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:10,512 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:10,513 : INFO : built Dictionary(95 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 353 corpus positions)\n",
      "2020-12-23 02:54:10,553 : INFO : token count processed\n",
      "2020-12-23 02:54:10,558 : INFO : frequencies processed\n",
      "2020-12-23 02:54:10,688 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:10,688 : INFO : entropies processed\n",
      "2020-12-23 02:54:10,689 : INFO : extropies processed\n",
      "2020-12-23 02:54:10,691 : INFO : token count processed\n",
      "2020-12-23 02:54:10,692 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:10,693 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:10,694 : INFO : vocab #2480\n",
      "2020-12-23 02:54:10,695 : INFO : diff #set()\n",
      "2020-12-23 02:54:10,963 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:11,091 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.1833790225225462, 0.4580056827900909], [0.8251847922801971, 0.17481521], [1.9219280948873623, 1.2148067842293933], [4.515319531114783, 5.695663584743922, 5.977642511379834, 4.233340604478872, 1.4623229802650508, 0.2819789266359116]]\n",
      "2020-12-23 02:54:11,094 : INFO : Removed 2 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:11,095 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:11,096 : INFO : built Dictionary(57 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 128 corpus positions)\n",
      "2020-12-23 02:54:11,119 : INFO : token count processed\n",
      "2020-12-23 02:54:11,122 : INFO : frequencies processed\n",
      "2020-12-23 02:54:11,253 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:11,254 : INFO : entropies processed\n",
      "2020-12-23 02:54:11,255 : INFO : extropies processed\n",
      "2020-12-23 02:54:11,256 : INFO : token count processed\n",
      "2020-12-23 02:54:11,257 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:11,258 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:11,259 : INFO : vocab #2480\n",
      "2020-12-23 02:54:11,260 : INFO : diff #set()\n",
      "2020-12-23 02:54:11,531 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:11,659 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.1626256750340183, 0.4624008729500864], [0.8105154186487198, 0.18948458], [1.584962500721156, 1.1699250014423124], [4.515319531114783, 4.9004417692112465, 5.545478187247331, 3.870283113078698, 1.0301586561325475, 0.6450364180360841]]\n",
      "2020-12-23 02:54:11,661 : INFO : Removed 2 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:11,662 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:11,663 : INFO : built Dictionary(55 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 135 corpus positions)\n",
      "2020-12-23 02:54:11,679 : INFO : token count processed\n",
      "2020-12-23 02:54:11,681 : INFO : frequencies processed\n",
      "2020-12-23 02:54:11,819 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:11,819 : INFO : entropies processed\n",
      "2020-12-23 02:54:11,820 : INFO : extropies processed\n",
      "2020-12-23 02:54:11,822 : INFO : token count processed\n",
      "2020-12-23 02:54:11,822 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:11,823 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:11,824 : INFO : vocab #2480\n",
      "2020-12-23 02:54:11,825 : INFO : diff #set()\n",
      "2020-12-23 02:54:12,095 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:12,225 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.1608457439470439, 0.4627817616325449], [0.8236473351716995, 0.17635266], [1.584962500721156, 1.1699250014423124], [4.515319531114783, 4.778624108914332, 5.436810006407583, 3.857133633621533, 0.9214904752928001, 0.6581858974932508]]\n",
      "2020-12-23 02:54:12,227 : INFO : Removed 2 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:12,228 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:12,230 : INFO : built Dictionary(56 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 166 corpus positions)\n",
      "2020-12-23 02:54:12,253 : INFO : token count processed\n",
      "2020-12-23 02:54:12,255 : INFO : frequencies processed\n",
      "2020-12-23 02:54:12,390 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:12,391 : INFO : entropies processed\n",
      "2020-12-23 02:54:12,392 : INFO : extropies processed\n",
      "2020-12-23 02:54:12,394 : INFO : token count processed\n",
      "2020-12-23 02:54:12,395 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:12,396 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:12,397 : INFO : vocab #2480\n",
      "2020-12-23 02:54:12,399 : INFO : diff #set()\n",
      "2020-12-23 02:54:12,660 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:12,788 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.1737083078028967, 0.460043326149295], [0.8690974563360214, 0.13090254], [1.5, 1.1225562489182657], [4.515319531114783, 4.773880192225086, 5.3628602765316815, 3.9263394468081882, 0.8475407454168984, 0.5889800843065958]]\n",
      "2020-12-23 02:54:12,791 : INFO : Removed 2 and 346 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:12,792 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:12,793 : INFO : built Dictionary(162 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 1991 corpus positions)\n",
      "2020-12-23 02:54:12,868 : INFO : token count processed\n",
      "2020-12-23 02:54:12,871 : INFO : frequencies processed\n",
      "2020-12-23 02:54:12,998 : INFO : scalar_distribution processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:54:12,999 : INFO : entropies processed\n",
      "2020-12-23 02:54:13,000 : INFO : extropies processed\n",
      "2020-12-23 02:54:13,002 : INFO : token count processed\n",
      "2020-12-23 02:54:13,003 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:13,004 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:13,005 : INFO : vocab #2480\n",
      "2020-12-23 02:54:13,006 : INFO : diff #set()\n",
      "2020-12-23 02:54:13,274 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:13,401 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.1456475137004742, 0.46605977618167005], [0.7986532896757126, 0.20134671], [2.94770277922009, 1.3393100707180505], [4.515319531114783, 6.620773041953877, 6.678475378610516, 4.457617194458143, 2.163155847495733, 0.057702336656639375]]\n",
      "2020-12-23 02:54:13,404 : INFO : Removed 2 and 132 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:13,405 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:13,407 : INFO : built Dictionary(94 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 554 corpus positions)\n",
      "2020-12-23 02:54:13,454 : INFO : token count processed\n",
      "2020-12-23 02:54:13,457 : INFO : frequencies processed\n",
      "2020-12-23 02:54:13,584 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:13,585 : INFO : entropies processed\n",
      "2020-12-23 02:54:13,585 : INFO : extropies processed\n",
      "2020-12-23 02:54:13,586 : INFO : token count processed\n",
      "2020-12-23 02:54:13,587 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:13,588 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:13,588 : INFO : vocab #2480\n",
      "2020-12-23 02:54:13,589 : INFO : diff #set()\n",
      "2020-12-23 02:54:13,850 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:13,976 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.1554385681010875, 0.46394270511777413], [0.7737584412097931, 0.22624156], [2.5, 1.2968140217166515], [4.515319531114783, 5.828370634755606, 6.002599529327938, 4.34109063654245, 1.4872799982131548, 0.17422889457233204]]\n",
      "2020-12-23 02:54:13,979 : INFO : Removed 2 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:13,980 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:13,982 : INFO : built Dictionary(99 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 317 corpus positions)\n",
      "2020-12-23 02:54:14,025 : INFO : token count processed\n",
      "2020-12-23 02:54:14,027 : INFO : frequencies processed\n",
      "2020-12-23 02:54:14,158 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:14,159 : INFO : entropies processed\n",
      "2020-12-23 02:54:14,160 : INFO : extropies processed\n",
      "2020-12-23 02:54:14,161 : INFO : token count processed\n",
      "2020-12-23 02:54:14,163 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:14,164 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:14,165 : INFO : vocab #2480\n",
      "2020-12-23 02:54:14,166 : INFO : diff #set()\n",
      "2020-12-23 02:54:14,427 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:14,554 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.173261280793012, 0.46013795434440585], [0.7608592212200165, 0.23914078], [1.0, 1.0], [4.515319531114783, 5.774409284925443, 6.086517432947325, 4.203211383092901, 1.5711979018325417, 0.31210814802188214]]\n",
      "2020-12-23 02:54:14,557 : INFO : Removed 2 and 51 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:14,558 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:14,560 : INFO : built Dictionary(106 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 366 corpus positions)\n",
      "2020-12-23 02:54:14,611 : INFO : token count processed\n",
      "2020-12-23 02:54:14,616 : INFO : frequencies processed\n",
      "2020-12-23 02:54:14,744 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:14,744 : INFO : entropies processed\n",
      "2020-12-23 02:54:14,745 : INFO : extropies processed\n",
      "2020-12-23 02:54:14,746 : INFO : token count processed\n",
      "2020-12-23 02:54:14,747 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:14,747 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:14,748 : INFO : vocab #2480\n",
      "2020-12-23 02:54:14,749 : INFO : diff #set()\n",
      "2020-12-23 02:54:15,009 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:15,137 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.1227210951719406, 0.47109344806270925], [0.736693948507309, 0.26330605], [2.521640636343318, 1.2998438251349491], [4.515319531114783, 5.977819040873918, 6.199081207309279, 4.294057364679423, 1.6837616761944956, 0.2212621664353609]]\n",
      "2020-12-23 02:54:15,139 : INFO : Removed 2 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:15,140 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:15,142 : INFO : built Dictionary(89 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 313 corpus positions)\n",
      "2020-12-23 02:54:15,183 : INFO : token count processed\n",
      "2020-12-23 02:54:15,185 : INFO : frequencies processed\n",
      "2020-12-23 02:54:15,311 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:15,312 : INFO : entropies processed\n",
      "2020-12-23 02:54:15,313 : INFO : extropies processed\n",
      "2020-12-23 02:54:15,314 : INFO : token count processed\n",
      "2020-12-23 02:54:15,315 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:15,316 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:15,317 : INFO : vocab #2480\n",
      "2020-12-23 02:54:15,318 : INFO : diff #set()\n",
      "2020-12-23 02:54:15,578 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:15,705 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.1175515646157763, 0.472243517801394], [0.7128010392189026, 0.28719896], [1.9219280948873623, 1.2148067842293933], [4.515319531114783, 5.901812829596593, 6.160222853112867, 4.256909507598509, 1.6449033219980835, 0.25841002351627385]]\n",
      "2020-12-23 02:54:15,708 : INFO : Removed 2 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:15,709 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:15,711 : INFO : built Dictionary(92 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 313 corpus positions)\n",
      "2020-12-23 02:54:15,755 : INFO : token count processed\n",
      "2020-12-23 02:54:15,760 : INFO : frequencies processed\n",
      "2020-12-23 02:54:15,889 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:15,890 : INFO : entropies processed\n",
      "2020-12-23 02:54:15,891 : INFO : extropies processed\n",
      "2020-12-23 02:54:15,893 : INFO : token count processed\n",
      "2020-12-23 02:54:15,895 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:15,901 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:15,902 : INFO : vocab #2480\n",
      "2020-12-23 02:54:15,903 : INFO : diff #set()\n",
      "2020-12-23 02:54:16,167 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:16,295 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.1553113576129106, 0.46397008787980315], [0.7785132080316544, 0.22148679], [1.5, 1.1225562489182657], [4.515319531114783, 5.643202320803383, 5.950792080197635, 4.2077297717205315, 1.435472549082852, 0.3075897593942525]]\n",
      "2020-12-23 02:54:16,297 : INFO : Removed 2 and 30 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:16,298 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:16,300 : INFO : built Dictionary(104 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 347 corpus positions)\n",
      "2020-12-23 02:54:16,346 : INFO : token count processed\n",
      "2020-12-23 02:54:16,350 : INFO : frequencies processed\n",
      "2020-12-23 02:54:16,480 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:16,480 : INFO : entropies processed\n",
      "2020-12-23 02:54:16,481 : INFO : extropies processed\n",
      "2020-12-23 02:54:16,482 : INFO : token count processed\n",
      "2020-12-23 02:54:16,483 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:16,485 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:16,485 : INFO : vocab #2480\n",
      "2020-12-23 02:54:16,487 : INFO : diff #set()\n",
      "2020-12-23 02:54:16,752 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:16,879 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.161662356411871, 0.4626069362931837], [0.7568545490503311, 0.24314545], [1.9219280948873623, 1.2148067842293933], [4.515319531114783, 5.925214310725336, 6.179798492400516, 4.260735349439602, 1.6644789612857327, 0.25458418167518015]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:54:16,882 : INFO : Removed 2 and 293 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:16,883 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:16,884 : INFO : built Dictionary(173 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 1742 corpus positions)\n",
      "2020-12-23 02:54:16,968 : INFO : token count processed\n",
      "2020-12-23 02:54:16,972 : INFO : frequencies processed\n",
      "2020-12-23 02:54:17,101 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:17,102 : INFO : entropies processed\n",
      "2020-12-23 02:54:17,102 : INFO : extropies processed\n",
      "2020-12-23 02:54:17,104 : INFO : token count processed\n",
      "2020-12-23 02:54:17,105 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:17,106 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:17,107 : INFO : vocab #2480\n",
      "2020-12-23 02:54:17,108 : INFO : diff #set()\n",
      "2020-12-23 02:54:17,371 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:17,498 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.1512076971748235, 0.4648551608072516], [0.8094578087329865, 0.19054219], [2.94770277922009, 1.3393100707180505], [4.515319531114783, 6.551685682764175, 6.616533610364359, 4.450471603514599, 2.1012140792495755, 0.06484792760018365]]\n",
      "2020-12-23 02:54:17,501 : INFO : Removed 2 and 88 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:17,502 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:17,504 : INFO : built Dictionary(149 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 583 corpus positions)\n",
      "2020-12-23 02:54:17,577 : INFO : token count processed\n",
      "2020-12-23 02:54:17,579 : INFO : frequencies processed\n",
      "2020-12-23 02:54:17,708 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:17,708 : INFO : entropies processed\n",
      "2020-12-23 02:54:17,709 : INFO : extropies processed\n",
      "2020-12-23 02:54:17,711 : INFO : token count processed\n",
      "2020-12-23 02:54:17,712 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:17,712 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:17,713 : INFO : vocab #2480\n",
      "2020-12-23 02:54:17,714 : INFO : diff #set()\n",
      "2020-12-23 02:54:17,973 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:18,100 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.1476500029527366, 0.4656252176216476], [0.7513334900140762, 0.24866651], [2.8453509366224363, 1.3210203571681218], [4.515319531114783, 6.642985062562557, 6.769718611916649, 4.3885859817606905, 2.254399080801866, 0.12673354935409176]]\n",
      "2020-12-23 02:54:18,102 : INFO : Removed 2 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:18,103 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:18,104 : INFO : built Dictionary(66 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 178 corpus positions)\n",
      "2020-12-23 02:54:18,125 : INFO : token count processed\n",
      "2020-12-23 02:54:18,128 : INFO : frequencies processed\n",
      "2020-12-23 02:54:18,266 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:18,267 : INFO : entropies processed\n",
      "2020-12-23 02:54:18,268 : INFO : extropies processed\n",
      "2020-12-23 02:54:18,269 : INFO : token count processed\n",
      "2020-12-23 02:54:18,270 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:18,271 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:18,271 : INFO : vocab #2480\n",
      "2020-12-23 02:54:18,272 : INFO : diff #set()\n",
      "2020-12-23 02:54:18,531 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:18,659 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.1403915973138967, 0.4672042262055966], [0.7596291601657867, 0.24037084], [2.0, 1.2451124978365313], [4.515319531114783, 5.2461980344571995, 5.70318291226592, 4.058334653306062, 1.1878633811511365, 0.4569848778087202]]\n",
      "2020-12-23 02:54:18,662 : INFO : Removed 2 and 23 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:18,663 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:18,664 : INFO : built Dictionary(86 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 238 corpus positions)\n",
      "2020-12-23 02:54:18,700 : INFO : token count processed\n",
      "2020-12-23 02:54:18,703 : INFO : frequencies processed\n",
      "2020-12-23 02:54:18,830 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:18,831 : INFO : entropies processed\n",
      "2020-12-23 02:54:18,832 : INFO : extropies processed\n",
      "2020-12-23 02:54:18,834 : INFO : token count processed\n",
      "2020-12-23 02:54:18,835 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:18,836 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:18,837 : INFO : vocab #2480\n",
      "2020-12-23 02:54:18,838 : INFO : diff #set()\n",
      "2020-12-23 02:54:19,100 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:19,229 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/test_auth_utility.py')[[1.107051948584504, 0.4745967467350721], [0.7063369154930115, 0.29366308], [2.584962500721156, 1.315172029168969], [4.515319531114783, 5.903090303960449, 6.201968896589137, 4.2164409384860955, 1.686649365474354, 0.29887859262868766]]\n",
      "2020-12-23 02:54:19,232 : INFO : Removed 2 and 442 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:19,233 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:19,235 : INFO : built Dictionary(120 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 1237 corpus positions)\n",
      "2020-12-23 02:54:19,288 : INFO : token count processed\n",
      "2020-12-23 02:54:19,291 : INFO : frequencies processed\n",
      "2020-12-23 02:54:19,417 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:19,418 : INFO : entropies processed\n",
      "2020-12-23 02:54:19,419 : INFO : extropies processed\n",
      "2020-12-23 02:54:19,421 : INFO : token count processed\n",
      "2020-12-23 02:54:19,422 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:19,423 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:19,424 : INFO : vocab #2480\n",
      "2020-12-23 02:54:19,425 : INFO : diff #set()\n",
      "2020-12-23 02:54:19,697 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:19,825 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.10590392790434, 0.4748554702564878], [0.7670239210128784, 0.23297608], [2.4464393446710155, 1.2856945251022456], [4.515319531114783, 6.16659449033757, 6.246041048486405, 4.4358729729659485, 1.730721517371622, 0.0794465581488355]]\n",
      "2020-12-23 02:54:19,828 : INFO : Removed 2 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:19,829 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:19,831 : INFO : built Dictionary(81 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 272 corpus positions)\n",
      "2020-12-23 02:54:19,871 : INFO : token count processed\n",
      "2020-12-23 02:54:19,876 : INFO : frequencies processed\n",
      "2020-12-23 02:54:20,005 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:20,005 : INFO : entropies processed\n",
      "2020-12-23 02:54:20,006 : INFO : extropies processed\n",
      "2020-12-23 02:54:20,007 : INFO : token count processed\n",
      "2020-12-23 02:54:20,008 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:20,009 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:20,009 : INFO : vocab #2480\n",
      "2020-12-23 02:54:20,010 : INFO : diff #set()\n",
      "2020-12-23 02:54:20,270 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:20,397 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.1063261632814878, 0.47476028045062113], [0.7249027788639069, 0.27509722], [2.2359263506290326, 1.2653331222512112], [4.515319531114783, 5.906856253399655, 6.167000862283683, 4.255174922230755, 1.6516813311689003, 0.260144608884028]]\n",
      "2020-12-23 02:54:20,400 : INFO : Removed 2 and 68 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:20,401 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:20,402 : INFO : built Dictionary(94 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 271 corpus positions)\n",
      "2020-12-23 02:54:20,440 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:54:20,446 : INFO : frequencies processed\n",
      "2020-12-23 02:54:20,573 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:20,573 : INFO : entropies processed\n",
      "2020-12-23 02:54:20,574 : INFO : extropies processed\n",
      "2020-12-23 02:54:20,575 : INFO : token count processed\n",
      "2020-12-23 02:54:20,576 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:20,577 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:20,577 : INFO : vocab #2480\n",
      "2020-12-23 02:54:20,578 : INFO : diff #set()\n",
      "2020-12-23 02:54:20,837 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:20,964 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.1540516476169411, 0.46424142202268665], [0.785632461309433, 0.21436754], [2.2359263506290326, 1.2653331222512112], [4.515319531114783, 5.965115449163356, 6.228801155394921, 4.2516338248832195, 1.7134816242801376, 0.2636857062315645]]\n",
      "2020-12-23 02:54:20,967 : INFO : Removed 2 and 13 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:20,968 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:20,969 : INFO : built Dictionary(100 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 385 corpus positions)\n",
      "2020-12-23 02:54:21,009 : INFO : token count processed\n",
      "2020-12-23 02:54:21,015 : INFO : frequencies processed\n",
      "2020-12-23 02:54:21,142 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:21,142 : INFO : entropies processed\n",
      "2020-12-23 02:54:21,143 : INFO : extropies processed\n",
      "2020-12-23 02:54:21,144 : INFO : token count processed\n",
      "2020-12-23 02:54:21,145 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:21,145 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:21,146 : INFO : vocab #2480\n",
      "2020-12-23 02:54:21,147 : INFO : diff #set()\n",
      "2020-12-23 02:54:21,400 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:21,527 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.1324697923546354, 0.46893981972697374], [0.7620041966438293, 0.2379958], [2.5, 1.2968140217166515], [4.515319531114783, 5.791362404253194, 6.0315804624425615, 4.275101472925415, 1.5162609313277784, 0.2402180581893676]]\n",
      "2020-12-23 02:54:21,530 : INFO : Removed 2 and 41 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:21,531 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:21,532 : INFO : built Dictionary(90 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 397 corpus positions)\n",
      "2020-12-23 02:54:21,571 : INFO : token count processed\n",
      "2020-12-23 02:54:21,573 : INFO : frequencies processed\n",
      "2020-12-23 02:54:21,699 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:21,699 : INFO : entropies processed\n",
      "2020-12-23 02:54:21,700 : INFO : extropies processed\n",
      "2020-12-23 02:54:21,701 : INFO : token count processed\n",
      "2020-12-23 02:54:21,702 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:21,703 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:21,704 : INFO : vocab #2480\n",
      "2020-12-23 02:54:21,705 : INFO : diff #set()\n",
      "2020-12-23 02:54:21,965 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:22,093 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.129611973688575, 0.4695691104083901], [0.765455573797226, 0.23454443], [2.5, 1.2968140217166515], [4.515319531114783, 5.651670454631116, 5.890031508852958, 4.2769584768929425, 1.3747119777381744, 0.2383610542218415]]\n",
      "2020-12-23 02:54:22,095 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:22,096 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:22,097 : INFO : built Dictionary(56 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 107 corpus positions)\n",
      "2020-12-23 02:54:22,113 : INFO : token count processed\n",
      "2020-12-23 02:54:22,116 : INFO : frequencies processed\n",
      "2020-12-23 02:54:22,243 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:22,244 : INFO : entropies processed\n",
      "2020-12-23 02:54:22,245 : INFO : extropies processed\n",
      "2020-12-23 02:54:22,246 : INFO : token count processed\n",
      "2020-12-23 02:54:22,247 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:22,248 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:22,248 : INFO : vocab #2480\n",
      "2020-12-23 02:54:22,249 : INFO : diff #set()\n",
      "2020-12-23 02:54:22,506 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:22,634 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.1092357343780357, 0.474105375563854], [0.6966980695724487, 0.30330193], [1.5, 1.1225562489182657], [4.515319531114783, 4.8226207261920235, 5.492908036351762, 3.8450322209550443, 0.9775885052369793, 0.6702873101597389]]\n",
      "2020-12-23 02:54:22,637 : INFO : Removed 2 and 50 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:22,637 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:22,638 : INFO : built Dictionary(97 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 277 corpus positions)\n",
      "2020-12-23 02:54:22,685 : INFO : token count processed\n",
      "2020-12-23 02:54:22,687 : INFO : frequencies processed\n",
      "2020-12-23 02:54:22,816 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:22,816 : INFO : entropies processed\n",
      "2020-12-23 02:54:22,817 : INFO : extropies processed\n",
      "2020-12-23 02:54:22,818 : INFO : token count processed\n",
      "2020-12-23 02:54:22,819 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:22,820 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:22,820 : INFO : vocab #2480\n",
      "2020-12-23 02:54:22,821 : INFO : diff #set()\n",
      "2020-12-23 02:54:23,082 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:23,210 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.0785455116983078, 0.4811056550707588], [0.6660527586936951, 0.33394724], [2.6464393446710153, 1.3017576173934455], [4.515319531114783, 6.24862851613934, 6.454286278537896, 4.309661768716228, 1.9389667474231125, 0.20565776239855538]]\n",
      "2020-12-23 02:54:23,212 : INFO : Removed 2 and 32 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:23,213 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:23,215 : INFO : built Dictionary(99 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 393 corpus positions)\n",
      "2020-12-23 02:54:23,262 : INFO : token count processed\n",
      "2020-12-23 02:54:23,264 : INFO : frequencies processed\n",
      "2020-12-23 02:54:23,392 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:23,393 : INFO : entropies processed\n",
      "2020-12-23 02:54:23,393 : INFO : extropies processed\n",
      "2020-12-23 02:54:23,395 : INFO : token count processed\n",
      "2020-12-23 02:54:23,396 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:23,397 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:23,398 : INFO : vocab #2480\n",
      "2020-12-23 02:54:23,399 : INFO : diff #set()\n",
      "2020-12-23 02:54:23,658 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:23,786 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.1506472784708193, 0.46497629342131497], [0.7914217412471771, 0.20857826], [2.5, 1.2968140217166515], [4.515319531114783, 5.850156917433494, 6.079979741851659, 4.285496706696618, 1.564660210736876, 0.22982282441816526]]\n",
      "2020-12-23 02:54:23,789 : INFO : Removed 2 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:23,789 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:23,791 : INFO : built Dictionary(94 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 391 corpus positions)\n",
      "2020-12-23 02:54:23,832 : INFO : token count processed\n",
      "2020-12-23 02:54:23,834 : INFO : frequencies processed\n",
      "2020-12-23 02:54:23,961 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:23,962 : INFO : entropies processed\n",
      "2020-12-23 02:54:23,963 : INFO : extropies processed\n",
      "2020-12-23 02:54:23,964 : INFO : token count processed\n",
      "2020-12-23 02:54:23,965 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:23,966 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:23,967 : INFO : vocab #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:54:23,968 : INFO : diff #set()\n",
      "2020-12-23 02:54:24,225 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:24,352 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.1354687189451453, 0.4682812682425845], [0.7715263068675995, 0.2284737], [2.5, 1.2968140217166515], [4.515319531114783, 5.6831976040360095, 5.921759932779345, 4.2767572023714475, 1.406440401664562, 0.2385623287433356]]\n",
      "2020-12-23 02:54:24,355 : INFO : Removed 2 and 52 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:24,355 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:24,356 : INFO : built Dictionary(83 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 369 corpus positions)\n",
      "2020-12-23 02:54:24,385 : INFO : token count processed\n",
      "2020-12-23 02:54:24,388 : INFO : frequencies processed\n",
      "2020-12-23 02:54:24,515 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:24,516 : INFO : entropies processed\n",
      "2020-12-23 02:54:24,518 : INFO : extropies processed\n",
      "2020-12-23 02:54:24,519 : INFO : token count processed\n",
      "2020-12-23 02:54:24,520 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:24,520 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:24,521 : INFO : vocab #2480\n",
      "2020-12-23 02:54:24,522 : INFO : diff #set()\n",
      "2020-12-23 02:54:24,778 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:24,906 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.1795877856182215, 0.45880235088414134], [0.8040295392274857, 0.19597046], [1.9219280948873623, 1.2148067842293933], [4.515319531114783, 5.749308601266266, 6.005718467467872, 4.258909664913176, 1.4903989363530892, 0.2564098662016061]]\n",
      "2020-12-23 02:54:24,908 : INFO : Removed 2 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:24,909 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:24,910 : INFO : built Dictionary(75 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 250 corpus positions)\n",
      "2020-12-23 02:54:24,937 : INFO : token count processed\n",
      "2020-12-23 02:54:24,940 : INFO : frequencies processed\n",
      "2020-12-23 02:54:25,066 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:25,067 : INFO : entropies processed\n",
      "2020-12-23 02:54:25,068 : INFO : extropies processed\n",
      "2020-12-23 02:54:25,069 : INFO : token count processed\n",
      "2020-12-23 02:54:25,070 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:25,071 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:25,071 : INFO : vocab #2480\n",
      "2020-12-23 02:54:25,072 : INFO : diff #set()\n",
      "2020-12-23 02:54:25,328 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:25,456 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.1502497090758192, 0.4650622649914468], [0.8139901608228683, 0.18600984], [2.2359263506290326, 1.2653331222512114], [4.515319531114783, 5.015422548793484, 5.430670917181265, 4.1000711627270015, 0.9153513860664821, 0.4152483683877817]]\n",
      "2020-12-23 02:54:25,459 : INFO : Removed 2 and 81 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:25,460 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:25,461 : INFO : built Dictionary(103 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 376 corpus positions)\n",
      "2020-12-23 02:54:25,501 : INFO : token count processed\n",
      "2020-12-23 02:54:25,503 : INFO : frequencies processed\n",
      "2020-12-23 02:54:25,630 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:25,631 : INFO : entropies processed\n",
      "2020-12-23 02:54:25,634 : INFO : extropies processed\n",
      "2020-12-23 02:54:25,635 : INFO : token count processed\n",
      "2020-12-23 02:54:25,636 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:25,636 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:25,637 : INFO : vocab #2480\n",
      "2020-12-23 02:54:25,639 : INFO : diff #set()\n",
      "2020-12-23 02:54:25,896 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:26,024 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.1098730873928413, 0.47396215723842167], [0.7533400803804398, 0.24665992], [2.5, 1.2968140217166515], [4.515319531114783, 6.030001281822029, 6.237651232851202, 4.3076695800856095, 1.7223317017364188, 0.20764995102917272]]\n",
      "2020-12-23 02:54:26,027 : INFO : Removed 2 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:26,028 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:26,029 : INFO : built Dictionary(91 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 339 corpus positions)\n",
      "2020-12-23 02:54:26,070 : INFO : token count processed\n",
      "2020-12-23 02:54:26,072 : INFO : frequencies processed\n",
      "2020-12-23 02:54:26,203 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:26,204 : INFO : entropies processed\n",
      "2020-12-23 02:54:26,204 : INFO : extropies processed\n",
      "2020-12-23 02:54:26,206 : INFO : token count processed\n",
      "2020-12-23 02:54:26,207 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:26,208 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:26,209 : INFO : vocab #2480\n",
      "2020-12-23 02:54:26,210 : INFO : diff #set()\n",
      "2020-12-23 02:54:26,472 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:26,599 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.1083882375038903, 0.47429594901548816], [0.6896959543228149, 0.31030405], [2.4464393446710155, 1.2856945251022456], [4.515319531114783, 5.9537092545441395, 6.166918215899187, 4.302110569759735, 1.6515986847844042, 0.21320896135504785]]\n",
      "2020-12-23 02:54:26,601 : INFO : Removed 2 and 90 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:26,602 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:26,603 : INFO : built Dictionary(102 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 340 corpus positions)\n",
      "2020-12-23 02:54:26,645 : INFO : token count processed\n",
      "2020-12-23 02:54:26,650 : INFO : frequencies processed\n",
      "2020-12-23 02:54:26,778 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:26,779 : INFO : entropies processed\n",
      "2020-12-23 02:54:26,780 : INFO : extropies processed\n",
      "2020-12-23 02:54:26,781 : INFO : token count processed\n",
      "2020-12-23 02:54:26,781 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:26,782 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:26,783 : INFO : vocab #2480\n",
      "2020-12-23 02:54:26,784 : INFO : diff #set()\n",
      "2020-12-23 02:54:27,047 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:27,176 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.1266831533882078, 0.47021579044664513], [0.7311518788337708, 0.26884812], [2.446439344671016, 1.2856945251022454], [4.515319531114783, 6.184756445474906, 6.382415053542941, 4.317660923046748, 1.8670955224281576, 0.1976586080680347]]\n",
      "2020-12-23 02:54:27,178 : INFO : Removed 2 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:27,179 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:27,180 : INFO : built Dictionary(110 unique tokens: ['api', 'assist', 'associ', 'bug', 'call']...) from 2 documents (total 453 corpus positions)\n",
      "2020-12-23 02:54:27,225 : INFO : token count processed\n",
      "2020-12-23 02:54:27,228 : INFO : frequencies processed\n",
      "2020-12-23 02:54:27,360 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:27,361 : INFO : entropies processed\n",
      "2020-12-23 02:54:27,362 : INFO : extropies processed\n",
      "2020-12-23 02:54:27,363 : INFO : token count processed\n",
      "2020-12-23 02:54:27,364 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:27,365 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:27,366 : INFO : vocab #2480\n",
      "2020-12-23 02:54:27,367 : INFO : diff #set()\n",
      "2020-12-23 02:54:27,634 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:27,763 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.1255151126819025, 0.47047418954280407], [0.7482430338859558, 0.25175697], [2.725480556997868, 1.3192201298976014], [4.515319531114783, 6.212221456585881, 6.387717270496261, 4.3398237172044025, 1.8723977393814781, 0.1754958139103806]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:54:27,765 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:27,766 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:27,767 : INFO : built Dictionary(122 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 417 corpus positions)\n",
      "2020-12-23 02:54:27,794 : INFO : token count processed\n",
      "2020-12-23 02:54:27,799 : INFO : frequencies processed\n",
      "2020-12-23 02:54:27,930 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:27,931 : INFO : entropies processed\n",
      "2020-12-23 02:54:27,932 : INFO : extropies processed\n",
      "2020-12-23 02:54:27,934 : INFO : token count processed\n",
      "2020-12-23 02:54:27,934 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:27,936 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:27,936 : INFO : vocab #2480\n",
      "2020-12-23 02:54:27,938 : INFO : diff #set()\n",
      "2020-12-23 02:54:28,208 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:28,337 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.2160448784253077, 0.4512543991034093], [0.8726317882537842, 0.12736821], [0.0, 0.0], [3.121928094887362, 6.301552355933639, 6.37617746296301, 3.047302987857991, 3.2542493680756484, 0.07462510702937131]]\n",
      "2020-12-23 02:54:28,340 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:28,341 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:28,342 : INFO : built Dictionary(157 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 653 corpus positions)\n",
      "2020-12-23 02:54:28,378 : INFO : token count processed\n",
      "2020-12-23 02:54:28,382 : INFO : frequencies processed\n",
      "2020-12-23 02:54:28,517 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:28,517 : INFO : entropies processed\n",
      "2020-12-23 02:54:28,518 : INFO : extropies processed\n",
      "2020-12-23 02:54:28,520 : INFO : token count processed\n",
      "2020-12-23 02:54:28,521 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:28,522 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:28,522 : INFO : vocab #2480\n",
      "2020-12-23 02:54:28,523 : INFO : diff #set()\n",
      "2020-12-23 02:54:28,792 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:28,920 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.1865179119624196, 0.45734818568327706], [0.8290255963802338, 0.1709744], [2.0, 1.2451124978365313], [3.121928094887362, 6.739005504021667, 6.774394523742378, 3.08653907516665, 3.6524664288550164, 0.03538901972071162]]\n",
      "2020-12-23 02:54:28,923 : INFO : Removed 0 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:28,924 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:28,925 : INFO : built Dictionary(103 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 504 corpus positions)\n",
      "2020-12-23 02:54:28,945 : INFO : token count processed\n",
      "2020-12-23 02:54:28,947 : INFO : frequencies processed\n",
      "2020-12-23 02:54:29,075 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:29,076 : INFO : entropies processed\n",
      "2020-12-23 02:54:29,077 : INFO : extropies processed\n",
      "2020-12-23 02:54:29,078 : INFO : token count processed\n",
      "2020-12-23 02:54:29,079 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:29,080 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:29,080 : INFO : vocab #2480\n",
      "2020-12-23 02:54:29,081 : INFO : diff #set()\n",
      "2020-12-23 02:54:29,339 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:29,467 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2455556470543427, 0.4453240788362435], [0.9493929781019688, 0.050607022], [1.0, 1.0], [3.121928094887362, 5.870833373337847, 5.936250269659986, 3.0565111985652234, 2.814322174772624, 0.0654168963221391]]\n",
      "2020-12-23 02:54:29,469 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:29,470 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:29,471 : INFO : built Dictionary(63 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 187 corpus positions)\n",
      "2020-12-23 02:54:29,482 : INFO : token count processed\n",
      "2020-12-23 02:54:29,485 : INFO : frequencies processed\n",
      "2020-12-23 02:54:29,625 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:29,626 : INFO : entropies processed\n",
      "2020-12-23 02:54:29,627 : INFO : extropies processed\n",
      "2020-12-23 02:54:29,629 : INFO : token count processed\n",
      "2020-12-23 02:54:29,630 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:29,632 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:29,633 : INFO : vocab #2480\n",
      "2020-12-23 02:54:29,635 : INFO : diff #set()\n",
      "2020-12-23 02:54:29,907 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:30,036 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.2131657392821085, 0.45184144244179975], [0.9113698825240135, 0.08863012], [1.0, 1.0], [3.121928094887362, 5.371881234145534, 5.519187497493021, 2.974621831539875, 2.3972594026056586, 0.14730626334748642]]\n",
      "2020-12-23 02:54:30,038 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:30,039 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:30,040 : INFO : built Dictionary(54 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 139 corpus positions)\n",
      "2020-12-23 02:54:30,049 : INFO : token count processed\n",
      "2020-12-23 02:54:30,052 : INFO : frequencies processed\n",
      "2020-12-23 02:54:30,180 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:30,181 : INFO : entropies processed\n",
      "2020-12-23 02:54:30,182 : INFO : extropies processed\n",
      "2020-12-23 02:54:30,184 : INFO : token count processed\n",
      "2020-12-23 02:54:30,185 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:30,187 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:30,188 : INFO : vocab #2480\n",
      "2020-12-23 02:54:30,190 : INFO : diff #set()\n",
      "2020-12-23 02:54:30,449 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:30,578 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.2067229532699144, 0.4531606464319426], [0.9041825234889984, 0.09581748], [1.0, 1.0], [3.121928094887362, 4.85108279267097, 5.061338980199898, 2.911671907358433, 1.9394108853125362, 0.2102561875289286]]\n",
      "2020-12-23 02:54:30,580 : INFO : Removed 0 and 127 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:30,581 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:30,582 : INFO : built Dictionary(92 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 412 corpus positions)\n",
      "2020-12-23 02:54:30,608 : INFO : token count processed\n",
      "2020-12-23 02:54:30,612 : INFO : frequencies processed\n",
      "2020-12-23 02:54:30,740 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:30,740 : INFO : entropies processed\n",
      "2020-12-23 02:54:30,741 : INFO : extropies processed\n",
      "2020-12-23 02:54:30,742 : INFO : token count processed\n",
      "2020-12-23 02:54:30,742 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:30,743 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:30,744 : INFO : vocab #2480\n",
      "2020-12-23 02:54:30,744 : INFO : diff #set()\n",
      "2020-12-23 02:54:31,001 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:31,129 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.1640404844726853, 0.46209856385550546], [0.8524337112903595, 0.14756629], [2.0, 1.2451124978365313], [3.121928094887362, 6.139571208108155, 6.185980878416145, 3.0755184245793723, 3.064052783528783, 0.04640967030799015]]\n",
      "2020-12-23 02:54:31,132 : INFO : Removed 0 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:31,133 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:31,134 : INFO : built Dictionary(80 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 422 corpus positions)\n",
      "2020-12-23 02:54:31,159 : INFO : token count processed\n",
      "2020-12-23 02:54:31,163 : INFO : frequencies processed\n",
      "2020-12-23 02:54:31,291 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:31,292 : INFO : entropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:54:31,292 : INFO : extropies processed\n",
      "2020-12-23 02:54:31,293 : INFO : token count processed\n",
      "2020-12-23 02:54:31,294 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:31,295 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:31,295 : INFO : vocab #2480\n",
      "2020-12-23 02:54:31,296 : INFO : diff #set()\n",
      "2020-12-23 02:54:31,554 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:31,682 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.2032633768143097, 0.4538722018090712], [0.8796351850032806, 0.120364815], [1.0, 1.0], [3.121928094887362, 5.609710627339259, 5.689874608184563, 3.041764114042059, 2.567946513297201, 0.0801639808453034]]\n",
      "2020-12-23 02:54:31,685 : INFO : Removed 0 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:31,685 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:31,687 : INFO : built Dictionary(169 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 1082 corpus positions)\n",
      "2020-12-23 02:54:31,727 : INFO : token count processed\n",
      "2020-12-23 02:54:31,729 : INFO : frequencies processed\n",
      "2020-12-23 02:54:31,856 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:31,856 : INFO : entropies processed\n",
      "2020-12-23 02:54:31,857 : INFO : extropies processed\n",
      "2020-12-23 02:54:31,858 : INFO : token count processed\n",
      "2020-12-23 02:54:31,859 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:31,860 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:31,860 : INFO : vocab #2480\n",
      "2020-12-23 02:54:31,861 : INFO : diff #set()\n",
      "2020-12-23 02:54:32,118 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:32,246 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.174192752502289, 0.459940821184826], [0.8242702782154083, 0.17572972], [2.2516291673878226, 1.2667563532600834], [3.121928094887362, 7.2441902753576075, 7.258672167411229, 3.1074462028337404, 4.136744072523866, 0.01448189205362116]]\n",
      "2020-12-23 02:54:32,249 : INFO : Removed 0 and 136 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:32,250 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:32,251 : INFO : built Dictionary(133 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 689 corpus positions)\n",
      "2020-12-23 02:54:32,281 : INFO : token count processed\n",
      "2020-12-23 02:54:32,284 : INFO : frequencies processed\n",
      "2020-12-23 02:54:32,412 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:32,412 : INFO : entropies processed\n",
      "2020-12-23 02:54:32,413 : INFO : extropies processed\n",
      "2020-12-23 02:54:32,414 : INFO : token count processed\n",
      "2020-12-23 02:54:32,415 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:32,416 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:32,416 : INFO : vocab #2480\n",
      "2020-12-23 02:54:32,417 : INFO : diff #set()\n",
      "2020-12-23 02:54:32,677 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:32,803 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.213409739228817, 0.45179163273602196], [0.888291247189045, 0.11170875], [1.584962500721156, 1.1699250014423124], [3.121928094887362, 6.2567074920449475, 6.300464377524888, 3.0781712094074223, 3.1785362826375256, 0.043756885479940166]]\n",
      "2020-12-23 02:54:32,805 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:32,806 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:32,807 : INFO : built Dictionary(81 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 199 corpus positions)\n",
      "2020-12-23 02:54:32,823 : INFO : token count processed\n",
      "2020-12-23 02:54:32,827 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:54:32,828 : INFO : frequencies processed\n",
      "2020-12-23 02:54:32,830 : INFO : token count processed\n",
      "2020-12-23 02:54:32,831 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:32,832 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:32,833 : INFO : vocab #2480\n",
      "2020-12-23 02:54:32,835 : INFO : diff #set()\n",
      "2020-12-23 02:54:33,092 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:33,220 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.2249748927739281, 0.44944327383095845], [0.8781958818435669, 0.12180412], [nan, nan], [3.121928094887362, 5.7680018917339435, 5.9159489887257335, 2.9739809978955725, 2.7940208938383715, 0.14794709699178998]]\n",
      "2020-12-23 02:54:33,223 : INFO : Removed 0 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:33,223 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:33,225 : INFO : built Dictionary(175 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 740 corpus positions)\n",
      "2020-12-23 02:54:33,268 : INFO : token count processed\n",
      "2020-12-23 02:54:33,271 : INFO : frequencies processed\n",
      "2020-12-23 02:54:33,398 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:33,399 : INFO : entropies processed\n",
      "2020-12-23 02:54:33,399 : INFO : extropies processed\n",
      "2020-12-23 02:54:33,400 : INFO : token count processed\n",
      "2020-12-23 02:54:33,401 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:33,402 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:33,402 : INFO : vocab #2480\n",
      "2020-12-23 02:54:33,403 : INFO : diff #set()\n",
      "2020-12-23 02:54:33,660 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:33,787 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.151213964601973, 0.4648538064808558], [0.7757551074028015, 0.2242449], [1.9219280948873623, 1.2148067842293933], [3.121928094887362, 6.846479111193757, 6.8729930793397145, 3.095414126741405, 3.7510649844523525, 0.026513968145957634]]\n",
      "2020-12-23 02:54:33,790 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:33,791 : INFO : built Dictionary(29 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 43 corpus positions)\n",
      "2020-12-23 02:54:33,796 : INFO : token count processed\n",
      "2020-12-23 02:54:33,798 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:54:33,799 : INFO : frequencies processed\n",
      "2020-12-23 02:54:33,800 : INFO : token count processed\n",
      "2020-12-23 02:54:33,801 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:33,802 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:33,802 : INFO : vocab #2480\n",
      "2020-12-23 02:54:33,803 : INFO : diff #set()\n",
      "2020-12-23 02:54:34,061 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:34,190 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2466216731265414, 0.4451127717504552], [0.9453618414700031, 0.05463816], [nan, nan], [3.121928094887362, 4.165013816065912, 4.704879870779915, 2.5820620401733594, 1.5829517758925529, 0.5398660547140031]]\n",
      "2020-12-23 02:54:34,192 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:34,193 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:34,195 : INFO : built Dictionary(56 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 150 corpus positions)\n",
      "2020-12-23 02:54:34,211 : INFO : token count processed\n",
      "2020-12-23 02:54:34,214 : INFO : frequencies processed\n",
      "2020-12-23 02:54:34,343 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:34,344 : INFO : entropies processed\n",
      "2020-12-23 02:54:34,344 : INFO : extropies processed\n",
      "2020-12-23 02:54:34,346 : INFO : token count processed\n",
      "2020-12-23 02:54:34,347 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:34,347 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:34,348 : INFO : vocab #2480\n",
      "2020-12-23 02:54:34,349 : INFO : diff #set()\n",
      "2020-12-23 02:54:34,608 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:34,735 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.2156180793791895, 0.4513413251620502], [0.8838426172733307, 0.11615738], [0.0, 0.0], [3.121928094887362, 5.449968864419248, 5.621038224424189, 2.950858734882421, 2.499110129536827, 0.17106936000494066]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:54:34,738 : INFO : Removed 0 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:34,739 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:34,740 : INFO : built Dictionary(146 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 587 corpus positions)\n",
      "2020-12-23 02:54:34,774 : INFO : token count processed\n",
      "2020-12-23 02:54:34,776 : INFO : frequencies processed\n",
      "2020-12-23 02:54:34,908 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:34,908 : INFO : entropies processed\n",
      "2020-12-23 02:54:34,909 : INFO : extropies processed\n",
      "2020-12-23 02:54:34,911 : INFO : token count processed\n",
      "2020-12-23 02:54:34,912 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:34,914 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:34,914 : INFO : vocab #2480\n",
      "2020-12-23 02:54:34,916 : INFO : diff #set()\n",
      "2020-12-23 02:54:35,186 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:35,318 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.2080933090893742, 0.4528794122438619], [0.8891666606068611, 0.11083334], [2.0, 1.2451124978365313], [3.121928094887362, 6.530294129310484, 6.569302701091509, 3.0829195231063373, 3.447374606204147, 0.039008571781025125]]\n",
      "2020-12-23 02:54:35,321 : INFO : Removed 0 and 142 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:35,322 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:35,324 : INFO : built Dictionary(120 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 603 corpus positions)\n",
      "2020-12-23 02:54:35,355 : INFO : token count processed\n",
      "2020-12-23 02:54:35,357 : INFO : frequencies processed\n",
      "2020-12-23 02:54:35,484 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:35,485 : INFO : entropies processed\n",
      "2020-12-23 02:54:35,486 : INFO : extropies processed\n",
      "2020-12-23 02:54:35,487 : INFO : token count processed\n",
      "2020-12-23 02:54:35,488 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:35,489 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:35,490 : INFO : vocab #2480\n",
      "2020-12-23 02:54:35,491 : INFO : diff #set()\n",
      "2020-12-23 02:54:35,751 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:35,878 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.1426632548883386, 0.46670889497851276], [0.8109141737222672, 0.18908583], [1.584962500721156, 1.1699250014423124], [3.121928094887362, 6.470272233491701, 6.51005550321368, 3.0821448251653827, 3.388127408326318, 0.039783269721978876]]\n",
      "2020-12-23 02:54:35,881 : INFO : Removed 0 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:35,882 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:35,883 : INFO : built Dictionary(122 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 392 corpus positions)\n",
      "2020-12-23 02:54:35,915 : INFO : token count processed\n",
      "2020-12-23 02:54:35,919 : INFO : frequencies processed\n",
      "2020-12-23 02:54:36,047 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:36,048 : INFO : entropies processed\n",
      "2020-12-23 02:54:36,048 : INFO : extropies processed\n",
      "2020-12-23 02:54:36,049 : INFO : token count processed\n",
      "2020-12-23 02:54:36,050 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:36,050 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:36,051 : INFO : vocab #2480\n",
      "2020-12-23 02:54:36,052 : INFO : diff #set()\n",
      "2020-12-23 02:54:36,308 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:36,437 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.2154988278092116, 0.45136561908671674], [0.8670479506254196, 0.13295205], [1.0, 1.0], [3.121928094887362, 6.550038223589686, 6.6146221669578384, 3.05734415151921, 3.4926940720704764, 0.06458394336815232]]\n",
      "2020-12-23 02:54:36,439 : INFO : Removed 0 and 70 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:36,440 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:36,441 : INFO : built Dictionary(76 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 308 corpus positions)\n",
      "2020-12-23 02:54:36,455 : INFO : token count processed\n",
      "2020-12-23 02:54:36,458 : INFO : frequencies processed\n",
      "2020-12-23 02:54:36,585 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:36,586 : INFO : entropies processed\n",
      "2020-12-23 02:54:36,587 : INFO : extropies processed\n",
      "2020-12-23 02:54:36,589 : INFO : token count processed\n",
      "2020-12-23 02:54:36,591 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:36,592 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:36,593 : INFO : vocab #2480\n",
      "2020-12-23 02:54:36,595 : INFO : diff #set()\n",
      "2020-12-23 02:54:36,866 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:36,995 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.164759770906235, 0.4619450220018498], [0.8260998427867889, 0.17390016], [1.584962500721156, 1.1699250014423124], [3.121928094887362, 5.860525481261383, 5.933970171276674, 3.04848340487207, 2.812042076389312, 0.07344469001529141]]\n",
      "2020-12-23 02:54:36,997 : INFO : Removed 0 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:36,998 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:37,000 : INFO : built Dictionary(50 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 346 corpus positions)\n",
      "2020-12-23 02:54:37,019 : INFO : token count processed\n",
      "2020-12-23 02:54:37,023 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:54:37,023 : INFO : frequencies processed\n",
      "2020-12-23 02:54:37,025 : INFO : token count processed\n",
      "2020-12-23 02:54:37,026 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:37,026 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:37,027 : INFO : vocab #2480\n",
      "2020-12-23 02:54:37,028 : INFO : diff #set()\n",
      "2020-12-23 02:54:37,281 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:37,409 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.2905377522114831, 0.43657870254900344], [0.9743040706962347, 0.02569593], [nan, nan], [3.121928094887362, 5.945464049777852, 6.0289023695131, 3.038489775152115, 2.9069742746257377, 0.08343831973524729]]\n",
      "2020-12-23 02:54:37,412 : INFO : Removed 0 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:37,413 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:37,414 : INFO : built Dictionary(194 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 893 corpus positions)\n",
      "2020-12-23 02:54:37,457 : INFO : token count processed\n",
      "2020-12-23 02:54:37,459 : INFO : frequencies processed\n",
      "2020-12-23 02:54:37,584 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:37,584 : INFO : entropies processed\n",
      "2020-12-23 02:54:37,585 : INFO : extropies processed\n",
      "2020-12-23 02:54:37,586 : INFO : token count processed\n",
      "2020-12-23 02:54:37,587 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:37,587 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:37,588 : INFO : vocab #2480\n",
      "2020-12-23 02:54:37,589 : INFO : diff #set()\n",
      "2020-12-23 02:54:37,847 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:37,975 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.2217247891134335, 0.4501007527575206], [0.8873274400830269, 0.11267256], [2.0, 1.2451124978365313], [3.121928094887362, 6.811563897304216, 6.842314917448977, 3.0911770747426015, 3.7203868225616152, 0.030751020144760943]]\n",
      "2020-12-23 02:54:37,978 : INFO : Removed 0 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:37,979 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:37,980 : INFO : built Dictionary(213 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 988 corpus positions)\n",
      "2020-12-23 02:54:38,035 : INFO : token count processed\n",
      "2020-12-23 02:54:38,038 : INFO : frequencies processed\n",
      "2020-12-23 02:54:38,165 : INFO : scalar_distribution processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:54:38,166 : INFO : entropies processed\n",
      "2020-12-23 02:54:38,167 : INFO : extropies processed\n",
      "2020-12-23 02:54:38,168 : INFO : token count processed\n",
      "2020-12-23 02:54:38,169 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:38,170 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:38,170 : INFO : vocab #2480\n",
      "2020-12-23 02:54:38,171 : INFO : diff #set()\n",
      "2020-12-23 02:54:38,429 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:38,558 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.183557071660873, 0.4579683366092981], [0.8195240050554276, 0.180476], [2.5216406363433186, 1.2998438251349493], [3.121928094887362, 7.502034948968415, 7.5147856249377325, 3.109177418918045, 4.392857530050371, 0.012750675969317449]]\n",
      "2020-12-23 02:54:38,561 : INFO : Removed 0 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:38,561 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:38,563 : INFO : built Dictionary(259 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 1561 corpus positions)\n",
      "2020-12-23 02:54:38,636 : INFO : token count processed\n",
      "2020-12-23 02:54:38,638 : INFO : frequencies processed\n",
      "2020-12-23 02:54:38,765 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:38,766 : INFO : entropies processed\n",
      "2020-12-23 02:54:38,767 : INFO : extropies processed\n",
      "2020-12-23 02:54:38,768 : INFO : token count processed\n",
      "2020-12-23 02:54:38,769 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:38,770 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:38,770 : INFO : vocab #2480\n",
      "2020-12-23 02:54:38,771 : INFO : diff #set()\n",
      "2020-12-23 02:54:39,026 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:39,153 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.2158480742727842, 0.4512944779971833], [0.8602211475372314, 0.13977885], [1.9219280948873623, 1.2148067842293933], [3.121928094887362, 7.39180093901977, 7.407690202181909, 3.1060388317252237, 4.285762107294547, 0.01588926316213879]]\n",
      "2020-12-23 02:54:39,156 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:39,156 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:39,157 : INFO : built Dictionary(47 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 116 corpus positions)\n",
      "2020-12-23 02:54:39,166 : INFO : token count processed\n",
      "2020-12-23 02:54:39,168 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:54:39,168 : INFO : frequencies processed\n",
      "2020-12-23 02:54:39,169 : INFO : token count processed\n",
      "2020-12-23 02:54:39,170 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:39,171 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:39,172 : INFO : vocab #2480\n",
      "2020-12-23 02:54:39,173 : INFO : diff #set()\n",
      "2020-12-23 02:54:39,431 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:39,560 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.2370551818486717, 0.44701624176012217], [0.9226441830396652, 0.07735582], [nan, nan], [3.121928094887362, 4.927561309677364, 5.193225856238965, 2.856263548325761, 2.071297761351603, 0.26566454656160143]]\n",
      "2020-12-23 02:54:39,562 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:39,563 : INFO : built Dictionary(15 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 17 corpus positions)\n",
      "2020-12-23 02:54:39,566 : INFO : token count processed\n",
      "2020-12-23 02:54:39,568 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:54:39,569 : INFO : frequencies processed\n",
      "2020-12-23 02:54:39,570 : INFO : token count processed\n",
      "2020-12-23 02:54:39,570 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:39,571 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:39,572 : INFO : vocab #2480\n",
      "2020-12-23 02:54:39,573 : INFO : diff #set()\n",
      "2020-12-23 02:54:39,832 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:39,959 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2538894552275843, 0.44367748279785346], [0.9510350860655308, 0.048964914], [nan, nan], [3.121928094887362, 2.5216406363433186, 3.852168723603281, 1.7914000076273995, 0.7302406287159191, 1.3305280872599625]]\n",
      "2020-12-23 02:54:39,962 : INFO : Removed 0 and 530 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:39,963 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:39,965 : INFO : built Dictionary(330 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 2889 corpus positions)\n",
      "2020-12-23 02:54:40,069 : INFO : token count processed\n",
      "2020-12-23 02:54:40,072 : INFO : frequencies processed\n",
      "2020-12-23 02:54:40,199 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:40,200 : INFO : entropies processed\n",
      "2020-12-23 02:54:40,201 : INFO : extropies processed\n",
      "2020-12-23 02:54:40,203 : INFO : token count processed\n",
      "2020-12-23 02:54:40,203 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:40,204 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:40,205 : INFO : vocab #2480\n",
      "2020-12-23 02:54:40,206 : INFO : diff #set()\n",
      "2020-12-23 02:54:40,463 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:40,591 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.1892015017026194, 0.45678755437645396], [0.8230284452438354, 0.17697155], [2.75, 1.3226647836567116], [3.121928094887362, 7.480007711014331, 7.484848378835874, 3.117087427065819, 4.362920283948512, 0.004840667821543576]]\n",
      "2020-12-23 02:54:40,594 : INFO : Removed 0 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:40,595 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:40,596 : INFO : built Dictionary(208 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 1038 corpus positions)\n",
      "2020-12-23 02:54:40,647 : INFO : token count processed\n",
      "2020-12-23 02:54:40,650 : INFO : frequencies processed\n",
      "2020-12-23 02:54:40,779 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:40,779 : INFO : entropies processed\n",
      "2020-12-23 02:54:40,780 : INFO : extropies processed\n",
      "2020-12-23 02:54:40,781 : INFO : token count processed\n",
      "2020-12-23 02:54:40,782 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:40,783 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:40,784 : INFO : vocab #2480\n",
      "2020-12-23 02:54:40,785 : INFO : diff #set()\n",
      "2020-12-23 02:54:41,055 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:41,183 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.1604239694371647, 0.4628721094316134], [0.8003685921430588, 0.19963141], [2.5216406363433186, 1.2998438251349493], [3.121928094887362, 7.131331012509435, 7.143942632919131, 3.109316474477666, 4.022014538031769, 0.012611620409696656]]\n",
      "2020-12-23 02:54:41,186 : INFO : Removed 0 and 166 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:41,187 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:41,189 : INFO : built Dictionary(199 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 857 corpus positions)\n",
      "2020-12-23 02:54:41,240 : INFO : token count processed\n",
      "2020-12-23 02:54:41,247 : INFO : frequencies processed\n",
      "2020-12-23 02:54:41,377 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:41,377 : INFO : entropies processed\n",
      "2020-12-23 02:54:41,378 : INFO : extropies processed\n",
      "2020-12-23 02:54:41,379 : INFO : token count processed\n",
      "2020-12-23 02:54:41,379 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:41,380 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:41,380 : INFO : vocab #2480\n",
      "2020-12-23 02:54:41,381 : INFO : diff #set()\n",
      "2020-12-23 02:54:41,638 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:41,766 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.2047976660056514, 0.45355635821751483], [0.8523010462522507, 0.14769895], [2.0, 1.2451124978365313], [3.121928094887362, 7.203742744794778, 7.228229305694265, 3.0974415339878743, 4.106301210806903, 0.02448656089948731]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:54:41,769 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:41,770 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:41,771 : INFO : built Dictionary(60 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 189 corpus positions)\n",
      "2020-12-23 02:54:41,781 : INFO : token count processed\n",
      "2020-12-23 02:54:41,784 : INFO : frequencies processed\n",
      "2020-12-23 02:54:41,912 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:41,912 : INFO : entropies processed\n",
      "2020-12-23 02:54:41,913 : INFO : extropies processed\n",
      "2020-12-23 02:54:41,914 : INFO : token count processed\n",
      "2020-12-23 02:54:41,915 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:41,916 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:41,917 : INFO : vocab #2480\n",
      "2020-12-23 02:54:41,918 : INFO : diff #set()\n",
      "2020-12-23 02:54:42,175 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:42,303 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.1928530370155923, 0.45602691248336924], [0.881156787276268, 0.11884321], [0.0, 0.0], [3.121928094887362, 5.195502554608948, 5.362040834858279, 2.955389814638032, 2.240112739970917, 0.16653828024933048]]\n",
      "2020-12-23 02:54:42,306 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:42,306 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:42,308 : INFO : built Dictionary(64 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 250 corpus positions)\n",
      "2020-12-23 02:54:42,325 : INFO : token count processed\n",
      "2020-12-23 02:54:42,327 : INFO : frequencies processed\n",
      "2020-12-23 02:54:42,458 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:42,459 : INFO : entropies processed\n",
      "2020-12-23 02:54:42,460 : INFO : extropies processed\n",
      "2020-12-23 02:54:42,461 : INFO : token count processed\n",
      "2020-12-23 02:54:42,462 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:42,463 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:42,464 : INFO : vocab #2480\n",
      "2020-12-23 02:54:42,465 : INFO : diff #set()\n",
      "2020-12-23 02:54:42,733 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:42,861 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.159902713292181, 0.46298381582000675], [0.8514816761016846, 0.14851832], [1.584962500721156, 1.1699250014423124], [3.121928094887362, 5.32027245610305, 5.4326984238573175, 3.0095021271330946, 2.3107703289699555, 0.11242596775426783]]\n",
      "2020-12-23 02:54:42,864 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:42,865 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:42,867 : INFO : built Dictionary(164 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 490 corpus positions)\n",
      "2020-12-23 02:54:42,910 : INFO : token count processed\n",
      "2020-12-23 02:54:42,912 : INFO : frequencies processed\n",
      "2020-12-23 02:54:43,042 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:43,043 : INFO : entropies processed\n",
      "2020-12-23 02:54:43,043 : INFO : extropies processed\n",
      "2020-12-23 02:54:43,045 : INFO : token count processed\n",
      "2020-12-23 02:54:43,046 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:43,047 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:43,048 : INFO : vocab #2480\n",
      "2020-12-23 02:54:43,049 : INFO : diff #set()\n",
      "2020-12-23 02:54:43,308 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:43,436 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.2137226041415656, 0.4517277811272016], [0.854106530547142, 0.14589347], [1.584962500721156, 1.1699250014423124], [3.121928094887362, 6.898202761357263, 6.944731421250545, 3.0753994349940807, 3.8228033263631827, 0.0465286598932817]]\n",
      "2020-12-23 02:54:43,439 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:43,439 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:43,441 : INFO : built Dictionary(128 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 512 corpus positions)\n",
      "2020-12-23 02:54:43,469 : INFO : token count processed\n",
      "2020-12-23 02:54:43,472 : INFO : frequencies processed\n",
      "2020-12-23 02:54:43,602 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:43,603 : INFO : entropies processed\n",
      "2020-12-23 02:54:43,604 : INFO : extropies processed\n",
      "2020-12-23 02:54:43,605 : INFO : token count processed\n",
      "2020-12-23 02:54:43,607 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:43,608 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:43,609 : INFO : vocab #2480\n",
      "2020-12-23 02:54:43,610 : INFO : diff #set()\n",
      "2020-12-23 02:54:43,882 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:44,011 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.107901859273705, 0.4744053882776868], [0.7626554816961288, 0.23734452], [1.584962500721156, 1.1699250014423124], [3.121928094887362, 6.388500481644799, 6.430080530593559, 3.080348045938603, 3.3081524357061967, 0.04158004894875944]]\n",
      "2020-12-23 02:54:44,013 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:44,014 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:44,015 : INFO : built Dictionary(51 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 176 corpus positions)\n",
      "2020-12-23 02:54:44,025 : INFO : token count processed\n",
      "2020-12-23 02:54:44,027 : INFO : frequencies processed\n",
      "2020-12-23 02:54:44,154 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:44,155 : INFO : entropies processed\n",
      "2020-12-23 02:54:44,156 : INFO : extropies processed\n",
      "2020-12-23 02:54:44,157 : INFO : token count processed\n",
      "2020-12-23 02:54:44,158 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:44,159 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:44,160 : INFO : vocab #2480\n",
      "2020-12-23 02:54:44,161 : INFO : diff #set()\n",
      "2020-12-23 02:54:44,419 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:44,547 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.199657870276672, 0.4546161534994623], [0.8973724469542503, 0.10262755], [0.0, 0.0], [3.121928094887362, 4.8191513650620195, 5.01460852518074, 2.926470934768642, 1.8926804302933777, 0.19545716011872027]]\n",
      "2020-12-23 02:54:44,550 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:44,551 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:44,552 : INFO : built Dictionary(54 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 231 corpus positions)\n",
      "2020-12-23 02:54:44,562 : INFO : token count processed\n",
      "2020-12-23 02:54:44,564 : INFO : frequencies processed\n",
      "2020-12-23 02:54:44,692 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:44,693 : INFO : entropies processed\n",
      "2020-12-23 02:54:44,694 : INFO : extropies processed\n",
      "2020-12-23 02:54:44,695 : INFO : token count processed\n",
      "2020-12-23 02:54:44,696 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:44,697 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:44,698 : INFO : vocab #2480\n",
      "2020-12-23 02:54:44,699 : INFO : diff #set()\n",
      "2020-12-23 02:54:44,958 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:45,086 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.155790411688082, 0.46386698566719875], [0.8518048375844955, 0.14819516], [1.584962500721156, 1.1699250014423124], [3.121928094887362, 5.062480936779194, 5.190226624836133, 2.994182406830423, 2.068298529948771, 0.12774568805693942]]\n",
      "2020-12-23 02:54:45,089 : INFO : Removed 0 and 223 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:45,090 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:45,092 : INFO : built Dictionary(243 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 1785 corpus positions)\n",
      "2020-12-23 02:54:45,165 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:54:45,168 : INFO : frequencies processed\n",
      "2020-12-23 02:54:45,298 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:45,299 : INFO : entropies processed\n",
      "2020-12-23 02:54:45,299 : INFO : extropies processed\n",
      "2020-12-23 02:54:45,301 : INFO : token count processed\n",
      "2020-12-23 02:54:45,302 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:45,303 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:45,304 : INFO : vocab #2480\n",
      "2020-12-23 02:54:45,305 : INFO : diff #set()\n",
      "2020-12-23 02:54:45,563 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:45,690 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.2047301596130573, 0.45357024561024084], [0.8509658724069595, 0.14903413], [2.321928094887362, 1.2877123795494492], [3.121928094887362, 7.185085743102134, 7.199692137220115, 3.107321700769381, 4.077764042332753, 0.014606394117980592]]\n",
      "2020-12-23 02:54:45,693 : INFO : Removed 0 and 148 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:45,694 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:45,695 : INFO : built Dictionary(160 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 760 corpus positions)\n",
      "2020-12-23 02:54:45,731 : INFO : token count processed\n",
      "2020-12-23 02:54:45,736 : INFO : frequencies processed\n",
      "2020-12-23 02:54:45,865 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:45,866 : INFO : entropies processed\n",
      "2020-12-23 02:54:45,867 : INFO : extropies processed\n",
      "2020-12-23 02:54:45,868 : INFO : token count processed\n",
      "2020-12-23 02:54:45,869 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:45,871 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:45,871 : INFO : vocab #2480\n",
      "2020-12-23 02:54:45,873 : INFO : diff #set()\n",
      "2020-12-23 02:54:46,139 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:46,268 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.2052939362838309, 0.45345429175990604], [0.8745826780796051, 0.12541732], [2.321928094887362, 1.2877123795494492], [3.121928094887362, 6.591225336124281, 6.620365060993413, 3.09278837001823, 3.4984369661060506, 0.029139724869131634]]\n",
      "2020-12-23 02:54:46,271 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:46,271 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:46,272 : INFO : built Dictionary(43 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 98 corpus positions)\n",
      "2020-12-23 02:54:46,280 : INFO : token count processed\n",
      "2020-12-23 02:54:46,282 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:54:46,283 : INFO : frequencies processed\n",
      "2020-12-23 02:54:46,284 : INFO : token count processed\n",
      "2020-12-23 02:54:46,285 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:46,286 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:46,286 : INFO : vocab #2480\n",
      "2020-12-23 02:54:46,288 : INFO : diff #set()\n",
      "2020-12-23 02:54:46,545 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:46,674 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.2401913148027208, 0.44639044593745486], [0.9275458827614784, 0.07245412], [nan, nan], [3.121928094887362, 4.7032114441396695, 5.0156752379828555, 2.8094643010441755, 1.8937471430954935, 0.312463793843186]]\n",
      "2020-12-23 02:54:46,676 : INFO : Removed 0 and 9 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:46,677 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:46,678 : INFO : built Dictionary(118 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 338 corpus positions)\n",
      "2020-12-23 02:54:46,706 : INFO : token count processed\n",
      "2020-12-23 02:54:46,715 : INFO : frequencies processed\n",
      "2020-12-23 02:54:46,846 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:46,847 : INFO : entropies processed\n",
      "2020-12-23 02:54:46,848 : INFO : extropies processed\n",
      "2020-12-23 02:54:46,849 : INFO : token count processed\n",
      "2020-12-23 02:54:46,849 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:46,850 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:46,851 : INFO : vocab #2480\n",
      "2020-12-23 02:54:46,852 : INFO : diff #set()\n",
      "2020-12-23 02:54:47,115 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:47,244 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.195999560696643, 0.4553734972892104], [0.856556162238121, 0.14344384], [1.5, 1.1225562489182657], [3.121928094887362, 6.14228447828618, 6.215980375488117, 3.048232197685424, 3.094052280600755, 0.07369589720193748]]\n",
      "2020-12-23 02:54:47,247 : INFO : Removed 0 and 184 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:47,248 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:47,249 : INFO : built Dictionary(257 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 1128 corpus positions)\n",
      "2020-12-23 02:54:47,319 : INFO : token count processed\n",
      "2020-12-23 02:54:47,321 : INFO : frequencies processed\n",
      "2020-12-23 02:54:47,456 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:47,457 : INFO : entropies processed\n",
      "2020-12-23 02:54:47,457 : INFO : extropies processed\n",
      "2020-12-23 02:54:47,459 : INFO : token count processed\n",
      "2020-12-23 02:54:47,460 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:47,461 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:47,462 : INFO : vocab #2480\n",
      "2020-12-23 02:54:47,463 : INFO : diff #set()\n",
      "2020-12-23 02:54:47,728 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:47,854 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.2224914693460278, 0.4499454840626461], [0.8597864359617233, 0.14021356], [0.9182958340544896, 0.9182958340544896], [3.121928094887362, 7.450178124335845, 7.4743826581815, 3.0977235610417067, 4.352454563294138, 0.024204533845654908]]\n",
      "2020-12-23 02:54:47,857 : INFO : Removed 0 and 16 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:47,858 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:47,859 : INFO : built Dictionary(60 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 176 corpus positions)\n",
      "2020-12-23 02:54:47,872 : INFO : token count processed\n",
      "2020-12-23 02:54:47,877 : INFO : frequencies processed\n",
      "2020-12-23 02:54:48,010 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:48,011 : INFO : entropies processed\n",
      "2020-12-23 02:54:48,012 : INFO : extropies processed\n",
      "2020-12-23 02:54:48,013 : INFO : token count processed\n",
      "2020-12-23 02:54:48,014 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:48,015 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:48,015 : INFO : vocab #2480\n",
      "2020-12-23 02:54:48,017 : INFO : diff #set()\n",
      "2020-12-23 02:54:48,285 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:48,412 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.1890455448703743, 0.45682009784735456], [0.8818160519003868, 0.11818395], [0.0, 0.0], [3.121928094887362, 5.20665021947654, 5.371681281859131, 2.956897032504771, 2.2497531869717693, 0.16503106238259146]]\n",
      "2020-12-23 02:54:48,415 : INFO : Removed 0 and 89 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:48,416 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:48,417 : INFO : built Dictionary(127 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 514 corpus positions)\n",
      "2020-12-23 02:54:48,447 : INFO : token count processed\n",
      "2020-12-23 02:54:48,452 : INFO : frequencies processed\n",
      "2020-12-23 02:54:48,583 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:48,584 : INFO : entropies processed\n",
      "2020-12-23 02:54:48,585 : INFO : extropies processed\n",
      "2020-12-23 02:54:48,586 : INFO : token count processed\n",
      "2020-12-23 02:54:48,587 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:48,588 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:48,589 : INFO : vocab #2480\n",
      "2020-12-23 02:54:48,590 : INFO : diff #set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:54:48,856 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:48,984 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.211339366665893, 0.4522146238945368], [0.8756278157234192, 0.124372184], [1.584962500721156, 1.1699250014423124], [3.121928094887362, 6.524718477352, 6.5727370856931975, 3.0739094865461647, 3.4508089908058355, 0.048018608341197755]]\n",
      "2020-12-23 02:54:48,986 : INFO : Removed 0 and 8 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:48,987 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:48,988 : INFO : built Dictionary(66 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 221 corpus positions)\n",
      "2020-12-23 02:54:49,003 : INFO : token count processed\n",
      "2020-12-23 02:54:49,007 : INFO : frequencies processed\n",
      "2020-12-23 02:54:49,141 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:49,142 : INFO : entropies processed\n",
      "2020-12-23 02:54:49,143 : INFO : extropies processed\n",
      "2020-12-23 02:54:49,145 : INFO : token count processed\n",
      "2020-12-23 02:54:49,146 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:49,148 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:49,148 : INFO : vocab #2480\n",
      "2020-12-23 02:54:49,150 : INFO : diff #set()\n",
      "2020-12-23 02:54:49,407 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:49,534 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.2177136298582878, 0.4509148460542672], [0.8988085612654686, 0.10119144], [0.0, 0.0], [3.121928094887362, 5.321859380715434, 5.46762607540757, 2.9761614001952266, 2.345697980520208, 0.14576669469213588]]\n",
      "2020-12-23 02:54:49,537 : INFO : Removed 0 and 119 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:49,538 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:49,539 : INFO : built Dictionary(141 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 839 corpus positions)\n",
      "2020-12-23 02:54:49,571 : INFO : token count processed\n",
      "2020-12-23 02:54:49,582 : INFO : frequencies processed\n",
      "2020-12-23 02:54:49,714 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:49,715 : INFO : entropies processed\n",
      "2020-12-23 02:54:49,715 : INFO : extropies processed\n",
      "2020-12-23 02:54:49,717 : INFO : token count processed\n",
      "2020-12-23 02:54:49,717 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:49,718 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:49,719 : INFO : vocab #2480\n",
      "2020-12-23 02:54:49,720 : INFO : diff #set()\n",
      "2020-12-23 02:54:49,978 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:50,106 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.1731693158532515, 0.4601574266234152], [0.8316324651241302, 0.16836753], [2.321928094887362, 1.2877123795494492], [3.121928094887362, 6.500767808767801, 6.528930906342984, 3.0937649973121797, 3.4070028114556217, 0.02816309757518276]]\n",
      "2020-12-23 02:54:50,109 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:50,110 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:50,110 : INFO : built Dictionary(38 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 61 corpus positions)\n",
      "2020-12-23 02:54:50,117 : INFO : token count processed\n",
      "2020-12-23 02:54:50,120 : INFO : frequencies processed\n",
      "2020-12-23 02:54:50,247 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:50,248 : INFO : entropies processed\n",
      "2020-12-23 02:54:50,249 : INFO : extropies processed\n",
      "2020-12-23 02:54:50,250 : INFO : token count processed\n",
      "2020-12-23 02:54:50,251 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:50,252 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:50,252 : INFO : vocab #2480\n",
      "2020-12-23 02:54:50,254 : INFO : diff #set()\n",
      "2020-12-23 02:54:50,509 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:50,638 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.2130551529962963, 0.45186402094230754], [0.9094259589910507, 0.09057404], [0.0, 0.0], [3.121928094887362, 4.736228843383063, 5.080987189622072, 2.777169748648352, 1.9590590947347102, 0.3447583462390096]]\n",
      "2020-12-23 02:54:50,640 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:50,641 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:50,642 : INFO : built Dictionary(101 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 402 corpus positions)\n",
      "2020-12-23 02:54:50,667 : INFO : token count processed\n",
      "2020-12-23 02:54:50,671 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:54:50,672 : INFO : frequencies processed\n",
      "2020-12-23 02:54:50,673 : INFO : token count processed\n",
      "2020-12-23 02:54:50,674 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:50,676 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:50,677 : INFO : vocab #2480\n",
      "2020-12-23 02:54:50,678 : INFO : diff #set()\n",
      "2020-12-23 02:54:50,936 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:51,064 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.2375247272892627, 0.44692243522666647], [0.8972203731536865, 0.10277963], [nan, nan], [3.121928094887362, 5.788442787590127, 5.887894981084385, 3.022475901393104, 2.7659668861970226, 0.09945219349425738]]\n",
      "2020-12-23 02:54:51,066 : INFO : Removed 0 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:51,067 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:51,069 : INFO : built Dictionary(56 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 88 corpus positions)\n",
      "2020-12-23 02:54:51,086 : INFO : token count processed\n",
      "2020-12-23 02:54:51,088 : INFO : frequencies processed\n",
      "2020-12-23 02:54:51,220 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:51,221 : INFO : entropies processed\n",
      "2020-12-23 02:54:51,221 : INFO : extropies processed\n",
      "2020-12-23 02:54:51,223 : INFO : token count processed\n",
      "2020-12-23 02:54:51,224 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:51,225 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:51,225 : INFO : vocab #2480\n",
      "2020-12-23 02:54:51,226 : INFO : diff #set()\n",
      "2020-12-23 02:54:51,487 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:51,615 : INFO : Computed distances or similarities ('276', 'sacp-python-common/setup.py')[[1.2172561902072256, 0.45100787379312246], [0.9261028468608856, 0.07389715], [1.0, 1.0], [3.121928094887362, 5.370004292053436, 5.577202649752879, 2.914729737187919, 2.455274554865517, 0.2071983576994425]]\n",
      "2020-12-23 02:54:51,618 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:51,618 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:51,619 : INFO : built Dictionary(81 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 333 corpus positions)\n",
      "2020-12-23 02:54:51,638 : INFO : token count processed\n",
      "2020-12-23 02:54:51,642 : INFO : frequencies processed\n",
      "2020-12-23 02:54:51,772 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:51,773 : INFO : entropies processed\n",
      "2020-12-23 02:54:51,773 : INFO : extropies processed\n",
      "2020-12-23 02:54:51,775 : INFO : token count processed\n",
      "2020-12-23 02:54:51,776 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:51,777 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:51,777 : INFO : vocab #2480\n",
      "2020-12-23 02:54:51,779 : INFO : diff #set()\n",
      "2020-12-23 02:54:52,038 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:52,166 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.2062227289827963, 0.45326339306687374], [0.8901159465312958, 0.10988405], [1.584962500721156, 1.1699250014423124], [3.121928094887362, 5.695663584743922, 5.779551736640227, 3.038039942991058, 2.6576236417528647, 0.08388815189630439]]\n",
      "2020-12-23 02:54:52,168 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:52,169 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:54:52,171 : INFO : built Dictionary(44 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 108 corpus positions)\n",
      "2020-12-23 02:54:52,185 : INFO : token count processed\n",
      "2020-12-23 02:54:52,188 : INFO : frequencies processed\n",
      "2020-12-23 02:54:52,321 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:52,322 : INFO : entropies processed\n",
      "2020-12-23 02:54:52,323 : INFO : extropies processed\n",
      "2020-12-23 02:54:52,324 : INFO : token count processed\n",
      "2020-12-23 02:54:52,325 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:52,327 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:52,328 : INFO : vocab #2480\n",
      "2020-12-23 02:54:52,333 : INFO : diff #set()\n",
      "2020-12-23 02:54:52,590 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:52,717 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.195089849209931, 0.4555622178107769], [0.8877115920186043, 0.11228841], [0.0, 0.0], [3.121928094887362, 4.9004417692112465, 5.14349809262919, 2.878871771469419, 2.0215699977418278, 0.24305632341794325]]\n",
      "2020-12-23 02:54:52,719 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:52,720 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:52,721 : INFO : built Dictionary(42 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 115 corpus positions)\n",
      "2020-12-23 02:54:52,729 : INFO : token count processed\n",
      "2020-12-23 02:54:52,731 : INFO : frequencies processed\n",
      "2020-12-23 02:54:52,859 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:52,860 : INFO : entropies processed\n",
      "2020-12-23 02:54:52,861 : INFO : extropies processed\n",
      "2020-12-23 02:54:52,862 : INFO : token count processed\n",
      "2020-12-23 02:54:52,863 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:52,864 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:52,865 : INFO : vocab #2480\n",
      "2020-12-23 02:54:52,866 : INFO : diff #set()\n",
      "2020-12-23 02:54:53,123 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:53,251 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.228290270031214, 0.4487745665137209], [0.9199069961905479, 0.080093004], [0.0, 0.0], [3.121928094887362, 4.778624108914332, 5.033563947895145, 2.8669882559065503, 1.9116358530077826, 0.2549398389808122]]\n",
      "2020-12-23 02:54:53,254 : INFO : Removed 0 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:53,255 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:53,255 : INFO : built Dictionary(43 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 146 corpus positions)\n",
      "2020-12-23 02:54:53,263 : INFO : token count processed\n",
      "2020-12-23 02:54:53,266 : INFO : frequencies processed\n",
      "2020-12-23 02:54:53,394 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:53,395 : INFO : entropies processed\n",
      "2020-12-23 02:54:53,396 : INFO : extropies processed\n",
      "2020-12-23 02:54:53,397 : INFO : token count processed\n",
      "2020-12-23 02:54:53,398 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:53,399 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:53,400 : INFO : vocab #2480\n",
      "2020-12-23 02:54:53,401 : INFO : diff #set()\n",
      "2020-12-23 02:54:53,658 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:53,786 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.2282776007121652, 0.44877711811149407], [0.9337617456912994, 0.066238254], [0.0, 0.0], [3.121928094887362, 4.773880192225086, 4.9962661356777085, 2.8995421514347397, 1.8743380407903465, 0.22238594345262275]]\n",
      "2020-12-23 02:54:53,789 : INFO : Removed 0 and 346 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:53,790 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:53,791 : INFO : built Dictionary(153 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 1971 corpus positions)\n",
      "2020-12-23 02:54:53,835 : INFO : token count processed\n",
      "2020-12-23 02:54:53,838 : INFO : frequencies processed\n",
      "2020-12-23 02:54:53,967 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:53,967 : INFO : entropies processed\n",
      "2020-12-23 02:54:53,968 : INFO : extropies processed\n",
      "2020-12-23 02:54:53,970 : INFO : token count processed\n",
      "2020-12-23 02:54:53,971 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:53,972 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:53,972 : INFO : vocab #2480\n",
      "2020-12-23 02:54:53,974 : INFO : diff #set()\n",
      "2020-12-23 02:54:54,236 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:54,364 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.2186260054439984, 0.45072941430697633], [0.8895852044224739, 0.110414796], [1.0, 1.0], [3.121928094887362, 6.620773041953877, 6.640432677759922, 3.1022684590813174, 3.5185045828725596, 0.019659635806045017]]\n",
      "2020-12-23 02:54:54,366 : INFO : Removed 0 and 132 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:54,367 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:54,369 : INFO : built Dictionary(83 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 534 corpus positions)\n",
      "2020-12-23 02:54:54,394 : INFO : token count processed\n",
      "2020-12-23 02:54:54,397 : INFO : frequencies processed\n",
      "2020-12-23 02:54:54,525 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:54,525 : INFO : entropies processed\n",
      "2020-12-23 02:54:54,526 : INFO : extropies processed\n",
      "2020-12-23 02:54:54,527 : INFO : token count processed\n",
      "2020-12-23 02:54:54,528 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:54,528 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:54,529 : INFO : vocab #2480\n",
      "2020-12-23 02:54:54,530 : INFO : diff #set()\n",
      "2020-12-23 02:54:54,787 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:54,914 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.219490748063503, 0.4505538042330188], [0.8887217417359352, 0.11127826], [0.9182958340544896, 0.9182958340544896], [3.121928094887362, 5.828370634755606, 5.888758818866724, 3.0615399107762444, 2.766830723979362, 0.060388184111118015]]\n",
      "2020-12-23 02:54:54,917 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:54,918 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:54,919 : INFO : built Dictionary(83 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 297 corpus positions)\n",
      "2020-12-23 02:54:54,935 : INFO : token count processed\n",
      "2020-12-23 02:54:54,938 : INFO : frequencies processed\n",
      "2020-12-23 02:54:55,066 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:55,067 : INFO : entropies processed\n",
      "2020-12-23 02:54:55,067 : INFO : extropies processed\n",
      "2020-12-23 02:54:55,071 : INFO : token count processed\n",
      "2020-12-23 02:54:55,073 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:55,074 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:55,075 : INFO : vocab #2480\n",
      "2020-12-23 02:54:55,076 : INFO : diff #set()\n",
      "2020-12-23 02:54:55,335 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:55,463 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.173548451622525, 0.4600771605774481], [0.828058198094368, 0.1719418], [1.584962500721156, 1.1699250014423124], [3.121928094887362, 5.774409284925443, 5.861394605931003, 3.0349427738818013, 2.739466511043641, 0.08698532100556022]]\n",
      "2020-12-23 02:54:55,465 : INFO : Removed 0 and 51 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:55,466 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:55,467 : INFO : built Dictionary(95 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 346 corpus positions)\n",
      "2020-12-23 02:54:55,488 : INFO : token count processed\n",
      "2020-12-23 02:54:55,491 : INFO : frequencies processed\n",
      "2020-12-23 02:54:55,619 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:55,620 : INFO : entropies processed\n",
      "2020-12-23 02:54:55,621 : INFO : extropies processed\n",
      "2020-12-23 02:54:55,622 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:54:55,623 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:55,624 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:55,624 : INFO : vocab #2480\n",
      "2020-12-23 02:54:55,625 : INFO : diff #set()\n",
      "2020-12-23 02:54:55,883 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:56,012 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.229258991728713, 0.4485795520889812], [0.892270602285862, 0.1077294], [1.0, 1.0], [3.121928094887362, 5.977819040873918, 6.06356357419121, 3.03618356157007, 2.9416354793038484, 0.08574453331729259]]\n",
      "2020-12-23 02:54:56,015 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:56,015 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:56,016 : INFO : built Dictionary(77 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 293 corpus positions)\n",
      "2020-12-23 02:54:56,031 : INFO : token count processed\n",
      "2020-12-23 02:54:56,034 : INFO : frequencies processed\n",
      "2020-12-23 02:54:56,161 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:56,162 : INFO : entropies processed\n",
      "2020-12-23 02:54:56,163 : INFO : extropies processed\n",
      "2020-12-23 02:54:56,164 : INFO : token count processed\n",
      "2020-12-23 02:54:56,165 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:56,166 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:56,167 : INFO : vocab #2480\n",
      "2020-12-23 02:54:56,168 : INFO : diff #set()\n",
      "2020-12-23 02:54:56,425 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:56,552 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.2327130441894167, 0.4478855904042288], [0.898749090731144, 0.10125091], [0.0, 0.0], [3.121928094887362, 5.901812829596593, 6.00632219709173, 3.017418727392224, 2.8843941022043684, 0.10450936749513762]]\n",
      "2020-12-23 02:54:56,555 : INFO : Removed 0 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:56,555 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:56,556 : INFO : built Dictionary(78 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 293 corpus positions)\n",
      "2020-12-23 02:54:56,571 : INFO : token count processed\n",
      "2020-12-23 02:54:56,573 : INFO : frequencies processed\n",
      "2020-12-23 02:54:56,701 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:56,702 : INFO : entropies processed\n",
      "2020-12-23 02:54:56,703 : INFO : extropies processed\n",
      "2020-12-23 02:54:56,704 : INFO : token count processed\n",
      "2020-12-23 02:54:56,705 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:56,706 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:56,706 : INFO : vocab #2480\n",
      "2020-12-23 02:54:56,707 : INFO : diff #set()\n",
      "2020-12-23 02:54:56,965 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:57,092 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.2096128394466608, 0.4525679712516622], [0.8843820691108704, 0.11561793], [1.0, 1.0], [3.121928094887362, 5.643202320803383, 5.746919576876666, 3.0182108388140794, 2.6249914819893037, 0.10371725607328308]]\n",
      "2020-12-23 02:54:57,095 : INFO : Removed 0 and 30 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:57,095 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:57,096 : INFO : built Dictionary(92 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 327 corpus positions)\n",
      "2020-12-23 02:54:57,116 : INFO : token count processed\n",
      "2020-12-23 02:54:57,119 : INFO : frequencies processed\n",
      "2020-12-23 02:54:57,250 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:57,250 : INFO : entropies processed\n",
      "2020-12-23 02:54:57,251 : INFO : extropies processed\n",
      "2020-12-23 02:54:57,252 : INFO : token count processed\n",
      "2020-12-23 02:54:57,253 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:57,255 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:57,256 : INFO : vocab #2480\n",
      "2020-12-23 02:54:57,257 : INFO : diff #set()\n",
      "2020-12-23 02:54:57,526 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:57,653 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.2404597836986986, 0.4463369560461979], [0.8914964944124222, 0.108503506], [0.0, 0.0], [3.121928094887362, 5.925214310725336, 6.025406574889142, 3.0217358307235562, 2.90347848000178, 0.10019226416380622]]\n",
      "2020-12-23 02:54:57,656 : INFO : Removed 0 and 293 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:57,657 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:57,658 : INFO : built Dictionary(163 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 1722 corpus positions)\n",
      "2020-12-23 02:54:57,698 : INFO : token count processed\n",
      "2020-12-23 02:54:57,703 : INFO : frequencies processed\n",
      "2020-12-23 02:54:57,834 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:57,835 : INFO : entropies processed\n",
      "2020-12-23 02:54:57,836 : INFO : extropies processed\n",
      "2020-12-23 02:54:57,837 : INFO : token count processed\n",
      "2020-12-23 02:54:57,838 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:57,839 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:57,840 : INFO : vocab #2480\n",
      "2020-12-23 02:54:57,840 : INFO : diff #set()\n",
      "2020-12-23 02:54:58,098 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:58,226 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.2162251738214696, 0.45121768844258964], [0.8921928405761719, 0.10780716], [1.584962500721156, 1.1699250014423124], [3.121928094887362, 6.551685682764175, 6.572312255976709, 3.101301521674828, 3.4503841610893473, 0.02062657321253436]]\n",
      "2020-12-23 02:54:58,228 : INFO : Removed 0 and 88 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:58,229 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:58,230 : INFO : built Dictionary(139 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 563 corpus positions)\n",
      "2020-12-23 02:54:58,262 : INFO : token count processed\n",
      "2020-12-23 02:54:58,271 : INFO : frequencies processed\n",
      "2020-12-23 02:54:58,398 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:58,398 : INFO : entropies processed\n",
      "2020-12-23 02:54:58,399 : INFO : extropies processed\n",
      "2020-12-23 02:54:58,400 : INFO : token count processed\n",
      "2020-12-23 02:54:58,400 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:58,401 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:58,401 : INFO : vocab #2480\n",
      "2020-12-23 02:54:58,402 : INFO : diff #set()\n",
      "2020-12-23 02:54:58,662 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:58,789 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.205947877412909, 0.45331986772633076], [0.8530802428722382, 0.14691976], [1.584962500721156, 1.1699250014423124], [3.121928094887362, 6.642985062562557, 6.686027349970155, 3.0788858074797645, 3.5640992550827932, 0.04304228740759797]]\n",
      "2020-12-23 02:54:58,791 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:58,792 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:58,793 : INFO : built Dictionary(55 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 158 corpus positions)\n",
      "2020-12-23 02:54:58,803 : INFO : token count processed\n",
      "2020-12-23 02:54:58,805 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:54:58,806 : INFO : frequencies processed\n",
      "2020-12-23 02:54:58,807 : INFO : token count processed\n",
      "2020-12-23 02:54:58,808 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:58,809 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:58,810 : INFO : vocab #2480\n",
      "2020-12-23 02:54:58,811 : INFO : diff #set()\n",
      "2020-12-23 02:54:59,069 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:59,196 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.2348969702952435, 0.44744791965416375], [0.9102082923054695, 0.08979171], [nan, nan], [3.121928094887362, 5.2461980344571995, 5.442708830950686, 2.925417298393876, 2.3207807360633237, 0.19651079649348624]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:54:59,199 : INFO : Removed 0 and 23 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:59,200 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:59,201 : INFO : built Dictionary(76 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 218 corpus positions)\n",
      "2020-12-23 02:54:59,214 : INFO : token count processed\n",
      "2020-12-23 02:54:59,217 : INFO : frequencies processed\n",
      "2020-12-23 02:54:59,345 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:59,346 : INFO : entropies processed\n",
      "2020-12-23 02:54:59,346 : INFO : extropies processed\n",
      "2020-12-23 02:54:59,348 : INFO : token count processed\n",
      "2020-12-23 02:54:59,348 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:59,349 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:59,350 : INFO : vocab #2480\n",
      "2020-12-23 02:54:59,351 : INFO : diff #set()\n",
      "2020-12-23 02:54:59,610 : INFO : alphabet #2480\n",
      "2020-12-23 02:54:59,738 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/test_auth_utility.py')[[1.202426333465181, 0.4540446982517926], [0.8581925481557846, 0.14180745], [0.0, 0.0], [3.121928094887362, 5.903090303960449, 6.020605921781268, 3.004412477066543, 2.898677826893906, 0.11751561782081854]]\n",
      "2020-12-23 02:54:59,740 : INFO : Removed 0 and 442 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:54:59,741 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:54:59,742 : INFO : built Dictionary(107 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 1217 corpus positions)\n",
      "2020-12-23 02:54:59,772 : INFO : token count processed\n",
      "2020-12-23 02:54:59,777 : INFO : frequencies processed\n",
      "2020-12-23 02:54:59,905 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:54:59,906 : INFO : entropies processed\n",
      "2020-12-23 02:54:59,906 : INFO : extropies processed\n",
      "2020-12-23 02:54:59,908 : INFO : token count processed\n",
      "2020-12-23 02:54:59,909 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:54:59,909 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:54:59,910 : INFO : vocab #2480\n",
      "2020-12-23 02:54:59,911 : INFO : diff #set()\n",
      "2020-12-23 02:55:00,176 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:00,304 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.2179532821407293, 0.4508661242110644], [0.871836706995964, 0.1281633], [1.9219280948873623, 1.2148067842293933], [3.121928094887362, 6.16659449033757, 6.189200426193548, 3.0993221590313844, 3.0672723313061856, 0.022605935855978032]]\n",
      "2020-12-23 02:55:00,306 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:00,307 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:00,308 : INFO : built Dictionary(68 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 252 corpus positions)\n",
      "2020-12-23 02:55:00,321 : INFO : token count processed\n",
      "2020-12-23 02:55:00,324 : INFO : frequencies processed\n",
      "2020-12-23 02:55:00,451 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:00,451 : INFO : entropies processed\n",
      "2020-12-23 02:55:00,452 : INFO : extropies processed\n",
      "2020-12-23 02:55:00,453 : INFO : token count processed\n",
      "2020-12-23 02:55:00,454 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:00,455 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:00,456 : INFO : vocab #2480\n",
      "2020-12-23 02:55:00,457 : INFO : diff #set()\n",
      "2020-12-23 02:55:00,714 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:00,841 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.1571680043226527, 0.46357075480266], [0.8280502110719681, 0.17194979], [1.584962500721156, 1.1699250014423124], [3.121928094887362, 5.906856253399655, 5.988203238842129, 3.0405811094448882, 2.8662751439547667, 0.08134698544247332]]\n",
      "2020-12-23 02:55:00,843 : INFO : Removed 0 and 68 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:00,844 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:00,845 : INFO : built Dictionary(83 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 251 corpus positions)\n",
      "2020-12-23 02:55:00,871 : INFO : token count processed\n",
      "2020-12-23 02:55:00,873 : INFO : frequencies processed\n",
      "2020-12-23 02:55:00,999 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:01,000 : INFO : entropies processed\n",
      "2020-12-23 02:55:01,000 : INFO : extropies processed\n",
      "2020-12-23 02:55:01,001 : INFO : token count processed\n",
      "2020-12-23 02:55:01,002 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:01,002 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:01,003 : INFO : vocab #2480\n",
      "2020-12-23 02:55:01,004 : INFO : diff #set()\n",
      "2020-12-23 02:55:01,275 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:01,407 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.232271132899752, 0.44797425602193125], [0.9245950058102608, 0.075404994], [0.0, 0.0], [3.121928094887362, 5.965115449163356, 6.0708251472100905, 3.016218396840628, 2.9488970523227285, 0.10570969804673425]]\n",
      "2020-12-23 02:55:01,409 : INFO : Removed 0 and 13 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:01,410 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:01,411 : INFO : built Dictionary(90 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 365 corpus positions)\n",
      "2020-12-23 02:55:01,432 : INFO : token count processed\n",
      "2020-12-23 02:55:01,437 : INFO : frequencies processed\n",
      "2020-12-23 02:55:01,565 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:01,566 : INFO : entropies processed\n",
      "2020-12-23 02:55:01,566 : INFO : extropies processed\n",
      "2020-12-23 02:55:01,567 : INFO : token count processed\n",
      "2020-12-23 02:55:01,568 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:01,568 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:01,569 : INFO : vocab #2480\n",
      "2020-12-23 02:55:01,570 : INFO : diff #set()\n",
      "2020-12-23 02:55:01,828 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:01,955 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.2182125051012342, 0.4508134354577369], [0.8899675905704498, 0.11003241], [0.0, 0.0], [3.121928094887362, 5.791362404253194, 5.886075375242549, 3.0272151238980074, 2.764147280355187, 0.09471297098935505]]\n",
      "2020-12-23 02:55:01,958 : INFO : Removed 0 and 41 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:01,958 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:01,960 : INFO : built Dictionary(79 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 377 corpus positions)\n",
      "2020-12-23 02:55:01,984 : INFO : token count processed\n",
      "2020-12-23 02:55:01,987 : INFO : frequencies processed\n",
      "2020-12-23 02:55:02,116 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:02,117 : INFO : entropies processed\n",
      "2020-12-23 02:55:02,118 : INFO : extropies processed\n",
      "2020-12-23 02:55:02,120 : INFO : token count processed\n",
      "2020-12-23 02:55:02,121 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:02,123 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:02,124 : INFO : vocab #2480\n",
      "2020-12-23 02:55:02,125 : INFO : diff #set()\n",
      "2020-12-23 02:55:02,385 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:02,513 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.222984878234389, 0.44984561514167937], [0.9080855622887611, 0.09191444], [1.0, 1.0], [3.121928094887362, 5.651670454631116, 5.741538303055257, 3.032060246463222, 2.6196102081678947, 0.08986784842414064]]\n",
      "2020-12-23 02:55:02,516 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:02,516 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:02,517 : INFO : built Dictionary(42 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 87 corpus positions)\n",
      "2020-12-23 02:55:02,525 : INFO : token count processed\n",
      "2020-12-23 02:55:02,527 : INFO : frequencies processed\n",
      "2020-12-23 02:55:02,655 : INFO : scalar_distribution processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:55:02,656 : INFO : entropies processed\n",
      "2020-12-23 02:55:02,656 : INFO : extropies processed\n",
      "2020-12-23 02:55:02,658 : INFO : token count processed\n",
      "2020-12-23 02:55:02,658 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:02,659 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:02,660 : INFO : vocab #2480\n",
      "2020-12-23 02:55:02,661 : INFO : diff #set()\n",
      "2020-12-23 02:55:02,921 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:03,049 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.2128336805435191, 0.45190924595579124], [0.8875993937253952, 0.112400606], [1.0, 1.0], [3.121928094887362, 4.8226207261920235, 5.094693477406869, 2.849855343672516, 1.972765382519507, 0.27207275121484553]]\n",
      "2020-12-23 02:55:03,052 : INFO : Removed 0 and 50 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:03,052 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:03,053 : INFO : built Dictionary(85 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 257 corpus positions)\n",
      "2020-12-23 02:55:03,080 : INFO : token count processed\n",
      "2020-12-23 02:55:03,086 : INFO : frequencies processed\n",
      "2020-12-23 02:55:03,216 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:03,216 : INFO : entropies processed\n",
      "2020-12-23 02:55:03,217 : INFO : extropies processed\n",
      "2020-12-23 02:55:03,218 : INFO : token count processed\n",
      "2020-12-23 02:55:03,219 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:03,220 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:03,220 : INFO : vocab #2480\n",
      "2020-12-23 02:55:03,221 : INFO : diff #set()\n",
      "2020-12-23 02:55:03,490 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:03,618 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.1719539689780596, 0.46041491407413043], [0.8261589258909225, 0.17384107], [2.0, 1.2451124978365313], [3.121928094887362, 6.24862851613934, 6.312130574986616, 3.0584260360400854, 3.1902024800992543, 0.06350205884727611]]\n",
      "2020-12-23 02:55:03,621 : INFO : Removed 0 and 32 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:03,622 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:03,624 : INFO : built Dictionary(89 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 373 corpus positions)\n",
      "2020-12-23 02:55:03,652 : INFO : token count processed\n",
      "2020-12-23 02:55:03,655 : INFO : frequencies processed\n",
      "2020-12-23 02:55:03,782 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:03,783 : INFO : entropies processed\n",
      "2020-12-23 02:55:03,784 : INFO : extropies processed\n",
      "2020-12-23 02:55:03,785 : INFO : token count processed\n",
      "2020-12-23 02:55:03,786 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:03,787 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:03,788 : INFO : vocab #2480\n",
      "2020-12-23 02:55:03,789 : INFO : diff #set()\n",
      "2020-12-23 02:55:04,047 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:04,174 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.2195580826604153, 0.4505401358099969], [0.8997344449162483, 0.100265555], [0.0, 0.0], [3.121928094887362, 5.850156917433494, 5.939593603761343, 3.0324914085595127, 2.8176655088739806, 0.08943668632784885]]\n",
      "2020-12-23 02:55:04,177 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:04,178 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:04,179 : INFO : built Dictionary(83 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 371 corpus positions)\n",
      "2020-12-23 02:55:04,195 : INFO : token count processed\n",
      "2020-12-23 02:55:04,197 : INFO : frequencies processed\n",
      "2020-12-23 02:55:04,325 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:04,325 : INFO : entropies processed\n",
      "2020-12-23 02:55:04,326 : INFO : extropies processed\n",
      "2020-12-23 02:55:04,328 : INFO : token count processed\n",
      "2020-12-23 02:55:04,329 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:04,330 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:04,331 : INFO : vocab #2480\n",
      "2020-12-23 02:55:04,332 : INFO : diff #set()\n",
      "2020-12-23 02:55:04,599 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:04,727 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.224234305279565, 0.44959292176473714], [0.9123246893286705, 0.08767531], [1.0, 1.0], [3.121928094887362, 5.6831976040360095, 5.774017164012993, 3.031108534910378, 2.652089069125631, 0.09081955997698365]]\n",
      "2020-12-23 02:55:04,729 : INFO : Removed 0 and 52 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:04,730 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:04,732 : INFO : built Dictionary(69 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 349 corpus positions)\n",
      "2020-12-23 02:55:04,750 : INFO : token count processed\n",
      "2020-12-23 02:55:04,754 : INFO : frequencies processed\n",
      "2020-12-23 02:55:04,891 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:04,891 : INFO : entropies processed\n",
      "2020-12-23 02:55:04,892 : INFO : extropies processed\n",
      "2020-12-23 02:55:04,894 : INFO : token count processed\n",
      "2020-12-23 02:55:04,894 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:04,895 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:04,896 : INFO : vocab #2480\n",
      "2020-12-23 02:55:04,897 : INFO : diff #set()\n",
      "2020-12-23 02:55:05,155 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:05,282 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.1321846786442205, 0.4690025259143425], [0.7968250662088394, 0.20317493], [1.584962500721156, 1.1699250014423124], [3.121928094887362, 5.749308601266266, 5.822450091766761, 3.0487866043868674, 2.700521996879399, 0.07314149050049501]]\n",
      "2020-12-23 02:55:05,285 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:05,286 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:05,287 : INFO : built Dictionary(65 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 230 corpus positions)\n",
      "2020-12-23 02:55:05,305 : INFO : token count processed\n",
      "2020-12-23 02:55:05,307 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:55:05,308 : INFO : frequencies processed\n",
      "2020-12-23 02:55:05,309 : INFO : token count processed\n",
      "2020-12-23 02:55:05,310 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:05,312 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:05,313 : INFO : vocab #2480\n",
      "2020-12-23 02:55:05,315 : INFO : diff #set()\n",
      "2020-12-23 02:55:05,576 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:05,703 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.2442838629768902, 0.4455764337553842], [0.9254576116800308, 0.07454239], [nan, nan], [3.121928094887362, 5.015422548793484, 5.190631801839666, 2.9467188418411787, 2.0687037069523044, 0.17520925304618284]]\n",
      "2020-12-23 02:55:05,705 : INFO : Removed 0 and 81 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:05,706 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:05,707 : INFO : built Dictionary(92 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 356 corpus positions)\n",
      "2020-12-23 02:55:05,730 : INFO : token count processed\n",
      "2020-12-23 02:55:05,735 : INFO : frequencies processed\n",
      "2020-12-23 02:55:05,866 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:05,866 : INFO : entropies processed\n",
      "2020-12-23 02:55:05,867 : INFO : extropies processed\n",
      "2020-12-23 02:55:05,867 : INFO : token count processed\n",
      "2020-12-23 02:55:05,868 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:05,869 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:05,869 : INFO : vocab #2480\n",
      "2020-12-23 02:55:05,870 : INFO : diff #set()\n",
      "2020-12-23 02:55:06,139 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:06,267 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.1840301583765167, 0.4578691352610913], [0.8565445691347122, 0.14345543], [1.0, 1.0], [3.121928094887362, 6.030001281822029, 6.100890204944221, 3.0510391717651704, 2.9789621100568593, 0.07088892312219208]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:55:06,270 : INFO : Removed 0 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:06,271 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:06,272 : INFO : built Dictionary(78 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 319 corpus positions)\n",
      "2020-12-23 02:55:06,289 : INFO : token count processed\n",
      "2020-12-23 02:55:06,294 : INFO : frequencies processed\n",
      "2020-12-23 02:55:06,429 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:06,429 : INFO : entropies processed\n",
      "2020-12-23 02:55:06,430 : INFO : extropies processed\n",
      "2020-12-23 02:55:06,432 : INFO : token count processed\n",
      "2020-12-23 02:55:06,433 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:06,434 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:06,435 : INFO : vocab #2480\n",
      "2020-12-23 02:55:06,436 : INFO : diff #set()\n",
      "2020-12-23 02:55:06,696 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:06,824 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.1556294671834313, 0.46390161909718686], [0.7874984741210938, 0.21250153], [1.9219280948873623, 1.2148067842293933], [3.121928094887362, 5.9537092545441395, 6.0147456061474704, 3.0608917432840306, 2.8928175112601084, 0.061036351603330985]]\n",
      "2020-12-23 02:55:06,826 : INFO : Removed 0 and 90 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:06,827 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:06,829 : INFO : built Dictionary(90 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 320 corpus positions)\n",
      "2020-12-23 02:55:06,859 : INFO : token count processed\n",
      "2020-12-23 02:55:06,861 : INFO : frequencies processed\n",
      "2020-12-23 02:55:06,988 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:06,989 : INFO : entropies processed\n",
      "2020-12-23 02:55:06,989 : INFO : extropies processed\n",
      "2020-12-23 02:55:06,990 : INFO : token count processed\n",
      "2020-12-23 02:55:06,991 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:06,992 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:06,993 : INFO : vocab #2480\n",
      "2020-12-23 02:55:06,994 : INFO : diff #set()\n",
      "2020-12-23 02:55:07,249 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:07,377 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.2031221253065563, 0.4539013014818022], [0.8719586133956909, 0.12804139], [1.5, 1.1225562489182657], [3.121928094887362, 6.184756445474906, 6.25133367938394, 3.0553508609783275, 3.129405584496578, 0.06657723390903403]]\n",
      "2020-12-23 02:55:07,379 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:07,380 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:07,382 : INFO : built Dictionary(101 unique tokens: ['fix', 'function', 'imag', 'list', 'merg']...) from 2 documents (total 433 corpus positions)\n",
      "2020-12-23 02:55:07,412 : INFO : token count processed\n",
      "2020-12-23 02:55:07,415 : INFO : frequencies processed\n",
      "2020-12-23 02:55:07,547 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:07,548 : INFO : entropies processed\n",
      "2020-12-23 02:55:07,548 : INFO : extropies processed\n",
      "2020-12-23 02:55:07,550 : INFO : token count processed\n",
      "2020-12-23 02:55:07,551 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:07,552 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:07,552 : INFO : vocab #2480\n",
      "2020-12-23 02:55:07,553 : INFO : diff #set()\n",
      "2020-12-23 02:55:07,811 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:07,939 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.232500736377558, 0.44792818372037535], [0.8927407488226891, 0.10725925], [0.0, 0.0], [3.121928094887362, 6.212221456585881, 6.285008730754108, 3.049140820719134, 3.1630806358667463, 0.07278727416822761]]\n",
      "2020-12-23 02:55:07,942 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:07,943 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:07,945 : INFO : built Dictionary(126 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 431 corpus positions)\n",
      "2020-12-23 02:55:07,995 : INFO : token count processed\n",
      "2020-12-23 02:55:08,000 : INFO : frequencies processed\n",
      "2020-12-23 02:55:08,127 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:08,127 : INFO : entropies processed\n",
      "2020-12-23 02:55:08,128 : INFO : extropies processed\n",
      "2020-12-23 02:55:08,129 : INFO : token count processed\n",
      "2020-12-23 02:55:08,130 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:08,131 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:08,131 : INFO : vocab #2480\n",
      "2020-12-23 02:55:08,132 : INFO : diff #set()\n",
      "2020-12-23 02:55:08,392 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:08,520 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.0995827596092314, 0.47628510732585616], [0.6898349523544312, 0.31016505], [1.9502120649147465, 1.2308224193674733], [3.8868421881310113, 6.301552355933639, 6.419017485291111, 3.7693770587735393, 2.5321752971600993, 0.11746512935747155]]\n",
      "2020-12-23 02:55:08,522 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:08,523 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:08,525 : INFO : built Dictionary(164 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 667 corpus positions)\n",
      "2020-12-23 02:55:08,594 : INFO : token count processed\n",
      "2020-12-23 02:55:08,597 : INFO : frequencies processed\n",
      "2020-12-23 02:55:08,723 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:08,724 : INFO : entropies processed\n",
      "2020-12-23 02:55:08,724 : INFO : extropies processed\n",
      "2020-12-23 02:55:08,725 : INFO : token count processed\n",
      "2020-12-23 02:55:08,726 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:08,727 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:08,727 : INFO : vocab #2480\n",
      "2020-12-23 02:55:08,728 : INFO : diff #set()\n",
      "2020-12-23 02:55:08,998 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:09,128 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.1601197187803278, 0.46293730449561926], [0.700630396604538, 0.2993696], [1.9219280948873623, 1.2148067842293933], [3.8868421881310113, 6.739005504021667, 6.83396168813515, 3.7918860040175284, 2.947119500004139, 0.09495618411348339]]\n",
      "2020-12-23 02:55:09,131 : INFO : Removed 0 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:09,132 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:09,133 : INFO : built Dictionary(108 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 518 corpus positions)\n",
      "2020-12-23 02:55:09,170 : INFO : token count processed\n",
      "2020-12-23 02:55:09,175 : INFO : frequencies processed\n",
      "2020-12-23 02:55:09,302 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:09,303 : INFO : entropies processed\n",
      "2020-12-23 02:55:09,304 : INFO : extropies processed\n",
      "2020-12-23 02:55:09,306 : INFO : token count processed\n",
      "2020-12-23 02:55:09,307 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:09,308 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:09,309 : INFO : vocab #2480\n",
      "2020-12-23 02:55:09,310 : INFO : diff #set()\n",
      "2020-12-23 02:55:09,569 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:09,697 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.1968952341266723, 0.4551878416712612], [0.8546259105205536, 0.14537409], [1.9219280948873623, 1.2148067842293933], [3.8868421881310113, 5.870833373337847, 6.004188291954171, 3.753487269514687, 2.11734610382316, 0.1333549186163241]]\n",
      "2020-12-23 02:55:09,700 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:09,701 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:09,702 : INFO : built Dictionary(68 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 201 corpus positions)\n",
      "2020-12-23 02:55:09,726 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:55:09,729 : INFO : frequencies processed\n",
      "2020-12-23 02:55:09,857 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:09,858 : INFO : entropies processed\n",
      "2020-12-23 02:55:09,858 : INFO : extropies processed\n",
      "2020-12-23 02:55:09,859 : INFO : token count processed\n",
      "2020-12-23 02:55:09,860 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:09,861 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:09,862 : INFO : vocab #2480\n",
      "2020-12-23 02:55:09,863 : INFO : diff #set()\n",
      "2020-12-23 02:55:10,121 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:10,249 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.1442409543843974, 0.46636549775586944], [0.8051024824380875, 0.19489752], [2.0, 1.2451124978365313], [3.8868421881310113, 5.371881234145534, 5.656723257079704, 3.6020001651968423, 1.7698810689486923, 0.2848420229341695]]\n",
      "2020-12-23 02:55:10,252 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:10,253 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:10,253 : INFO : built Dictionary(58 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 153 corpus positions)\n",
      "2020-12-23 02:55:10,269 : INFO : token count processed\n",
      "2020-12-23 02:55:10,271 : INFO : frequencies processed\n",
      "2020-12-23 02:55:10,399 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:10,399 : INFO : entropies processed\n",
      "2020-12-23 02:55:10,400 : INFO : extropies processed\n",
      "2020-12-23 02:55:10,401 : INFO : token count processed\n",
      "2020-12-23 02:55:10,402 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:10,403 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:10,404 : INFO : vocab #2480\n",
      "2020-12-23 02:55:10,405 : INFO : diff #set()\n",
      "2020-12-23 02:55:10,662 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:10,790 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.1455939878045651, 0.4660714029233599], [0.825312539935112, 0.17468746], [2.321928094887362, 1.2877123795494492], [3.8868421881310113, 4.85108279267097, 5.237359818922411, 3.50056516187957, 1.3505176307914, 0.38627702625144167]]\n",
      "2020-12-23 02:55:10,793 : INFO : Removed 0 and 127 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:10,793 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:10,794 : INFO : built Dictionary(98 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 426 corpus positions)\n",
      "2020-12-23 02:55:10,828 : INFO : token count processed\n",
      "2020-12-23 02:55:10,830 : INFO : frequencies processed\n",
      "2020-12-23 02:55:10,960 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:10,961 : INFO : entropies processed\n",
      "2020-12-23 02:55:10,962 : INFO : extropies processed\n",
      "2020-12-23 02:55:10,963 : INFO : token count processed\n",
      "2020-12-23 02:55:10,964 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:10,965 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:10,966 : INFO : vocab #2480\n",
      "2020-12-23 02:55:10,967 : INFO : diff #set()\n",
      "2020-12-23 02:55:11,233 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:11,362 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.1644392710861495, 0.46201342461236355], [0.7993793338537216, 0.20062067], [2.2359263506290326, 1.2653331222512112], [3.8868421881310113, 6.139571208108155, 6.261309863780765, 3.765103532458401, 2.3744676756497536, 0.12173865567261011]]\n",
      "2020-12-23 02:55:11,364 : INFO : Removed 0 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:11,365 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:11,367 : INFO : built Dictionary(86 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 436 corpus positions)\n",
      "2020-12-23 02:55:11,399 : INFO : token count processed\n",
      "2020-12-23 02:55:11,401 : INFO : frequencies processed\n",
      "2020-12-23 02:55:11,529 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:11,530 : INFO : entropies processed\n",
      "2020-12-23 02:55:11,530 : INFO : extropies processed\n",
      "2020-12-23 02:55:11,532 : INFO : token count processed\n",
      "2020-12-23 02:55:11,533 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:11,534 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:11,535 : INFO : vocab #2480\n",
      "2020-12-23 02:55:11,536 : INFO : diff #set()\n",
      "2020-12-23 02:55:11,795 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:11,923 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.1210219269161983, 0.47147084493083136], [0.6589361131191254, 0.3410639], [1.4591479170272446, 1.1091703386755989], [3.8868421881310113, 5.609710627339259, 5.769960079389797, 3.726592736080473, 1.8831178912587858, 0.16024945205053776]]\n",
      "2020-12-23 02:55:11,926 : INFO : Removed 0 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:11,927 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:11,930 : INFO : built Dictionary(175 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 1096 corpus positions)\n",
      "2020-12-23 02:55:11,999 : INFO : token count processed\n",
      "2020-12-23 02:55:12,005 : INFO : frequencies processed\n",
      "2020-12-23 02:55:12,132 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:12,133 : INFO : entropies processed\n",
      "2020-12-23 02:55:12,134 : INFO : extropies processed\n",
      "2020-12-23 02:55:12,135 : INFO : token count processed\n",
      "2020-12-23 02:55:12,136 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:12,137 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:12,138 : INFO : vocab #2480\n",
      "2020-12-23 02:55:12,139 : INFO : diff #set()\n",
      "2020-12-23 02:55:12,400 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:12,528 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.175836725827161, 0.459593308693621], [0.7588419914245605, 0.24115801], [2.481714572986073, 1.294745644396516], [3.8868421881310113, 7.2441902753576075, 7.288096748567692, 3.8429357149209276, 3.4012545604366804, 0.04390647321008423]]\n",
      "2020-12-23 02:55:12,531 : INFO : Removed 0 and 136 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:12,532 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:12,533 : INFO : built Dictionary(137 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 703 corpus positions)\n",
      "2020-12-23 02:55:12,578 : INFO : token count processed\n",
      "2020-12-23 02:55:12,580 : INFO : frequencies processed\n",
      "2020-12-23 02:55:12,708 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:12,708 : INFO : entropies processed\n",
      "2020-12-23 02:55:12,709 : INFO : extropies processed\n",
      "2020-12-23 02:55:12,711 : INFO : token count processed\n",
      "2020-12-23 02:55:12,712 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:12,712 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:12,713 : INFO : vocab #2480\n",
      "2020-12-23 02:55:12,714 : INFO : diff #set()\n",
      "2020-12-23 02:55:12,973 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:13,101 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.139980234093982, 0.4672940357429875], [0.7064307332038879, 0.29356927], [2.4193819456463714, 1.2761517340193214], [3.8868421881310113, 6.2567074920449475, 6.348756511788434, 3.7947931683875247, 2.4619143236574224, 0.09204901974348623]]\n",
      "2020-12-23 02:55:13,104 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:13,105 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:13,106 : INFO : built Dictionary(87 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 213 corpus positions)\n",
      "2020-12-23 02:55:13,130 : INFO : token count processed\n",
      "2020-12-23 02:55:13,132 : INFO : frequencies processed\n",
      "2020-12-23 02:55:13,260 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:13,260 : INFO : entropies processed\n",
      "2020-12-23 02:55:13,261 : INFO : extropies processed\n",
      "2020-12-23 02:55:13,262 : INFO : token count processed\n",
      "2020-12-23 02:55:13,263 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:13,264 : INFO : alphabet_target #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:55:13,265 : INFO : vocab #2480\n",
      "2020-12-23 02:55:13,266 : INFO : diff #set()\n",
      "2020-12-23 02:55:13,523 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:13,651 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.157721508126008, 0.4634518385407879], [0.7421576976776123, 0.2578423], [0.0, 0.0], [3.8868421881310113, 5.7680018917339435, 6.03694393614127, 3.617900143723684, 2.150101748010259, 0.2689420444073267]]\n",
      "2020-12-23 02:55:13,653 : INFO : Removed 0 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:13,654 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:13,656 : INFO : built Dictionary(179 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 754 corpus positions)\n",
      "2020-12-23 02:55:13,715 : INFO : token count processed\n",
      "2020-12-23 02:55:13,718 : INFO : frequencies processed\n",
      "2020-12-23 02:55:13,845 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:13,846 : INFO : entropies processed\n",
      "2020-12-23 02:55:13,847 : INFO : extropies processed\n",
      "2020-12-23 02:55:13,848 : INFO : token count processed\n",
      "2020-12-23 02:55:13,849 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:13,850 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:13,851 : INFO : vocab #2480\n",
      "2020-12-23 02:55:13,852 : INFO : diff #set()\n",
      "2020-12-23 02:55:14,109 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:14,236 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.1174948318747342, 0.4722561703324892], [0.6367194950580597, 0.3632805], [2.73215889136457, 1.3224858119561373], [3.8868421881310113, 6.846479111193757, 6.90867375413803, 3.824647545186738, 3.0218315660070183, 0.06219464294427279]]\n",
      "2020-12-23 02:55:14,239 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:14,240 : INFO : built Dictionary(36 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 57 corpus positions)\n",
      "2020-12-23 02:55:14,247 : INFO : token count processed\n",
      "2020-12-23 02:55:14,250 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:55:14,250 : INFO : frequencies processed\n",
      "2020-12-23 02:55:14,251 : INFO : token count processed\n",
      "2020-12-23 02:55:14,252 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:14,253 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:14,254 : INFO : vocab #2480\n",
      "2020-12-23 02:55:14,255 : INFO : diff #set()\n",
      "2020-12-23 02:55:14,512 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:14,640 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2115205963253302, 0.45217756581675217], [0.8940287679433823, 0.10597123], [nan, nan], [3.8868421881310113, 4.165013816065912, 5.029829706957422, 3.022026297239501, 1.1429875188264105, 0.86481589089151]]\n",
      "2020-12-23 02:55:14,642 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:14,643 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:14,645 : INFO : built Dictionary(62 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 164 corpus positions)\n",
      "2020-12-23 02:55:14,669 : INFO : token count processed\n",
      "2020-12-23 02:55:14,672 : INFO : frequencies processed\n",
      "2020-12-23 02:55:14,804 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:14,805 : INFO : entropies processed\n",
      "2020-12-23 02:55:14,806 : INFO : extropies processed\n",
      "2020-12-23 02:55:14,807 : INFO : token count processed\n",
      "2020-12-23 02:55:14,808 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:14,809 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:14,810 : INFO : vocab #2480\n",
      "2020-12-23 02:55:14,811 : INFO : diff #set()\n",
      "2020-12-23 02:55:15,078 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:15,205 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.1449683880205423, 0.4662073369402137], [0.7269269227981567, 0.27307308], [0.9182958340544896, 0.9182958340544896], [3.8868421881310113, 5.449968864419248, 5.762469650428857, 3.5743414021214033, 1.8756274622978455, 0.3125007860096085]]\n",
      "2020-12-23 02:55:15,208 : INFO : Removed 0 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:15,208 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:15,210 : INFO : built Dictionary(152 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 601 corpus positions)\n",
      "2020-12-23 02:55:15,263 : INFO : token count processed\n",
      "2020-12-23 02:55:15,266 : INFO : frequencies processed\n",
      "2020-12-23 02:55:15,394 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:15,395 : INFO : entropies processed\n",
      "2020-12-23 02:55:15,398 : INFO : extropies processed\n",
      "2020-12-23 02:55:15,399 : INFO : token count processed\n",
      "2020-12-23 02:55:15,399 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:15,400 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:15,401 : INFO : vocab #2480\n",
      "2020-12-23 02:55:15,401 : INFO : diff #set()\n",
      "2020-12-23 02:55:15,660 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:15,788 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.1736541227787765, 0.46005479414618666], [0.7957705408334732, 0.20422946], [2.2359263506290326, 1.2653331222512112], [3.8868421881310113, 6.530294129310484, 6.624196209681788, 3.7929401077597067, 2.7373540215507766, 0.09390208037130421]]\n",
      "2020-12-23 02:55:15,791 : INFO : Removed 0 and 142 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:15,792 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:15,793 : INFO : built Dictionary(125 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 617 corpus positions)\n",
      "2020-12-23 02:55:15,834 : INFO : token count processed\n",
      "2020-12-23 02:55:15,836 : INFO : frequencies processed\n",
      "2020-12-23 02:55:15,964 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:15,964 : INFO : entropies processed\n",
      "2020-12-23 02:55:15,966 : INFO : extropies processed\n",
      "2020-12-23 02:55:15,975 : INFO : token count processed\n",
      "2020-12-23 02:55:15,976 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:15,977 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:15,978 : INFO : vocab #2480\n",
      "2020-12-23 02:55:15,979 : INFO : diff #set()\n",
      "2020-12-23 02:55:16,238 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:16,365 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.144945350030989, 0.46621234428446046], [0.7680141627788544, 0.23198584], [2.25, 1.2709632597765914], [3.8868421881310113, 6.470272233491701, 6.556047609762102, 3.8010668118606112, 2.6692054216310903, 0.08577537627040055]]\n",
      "2020-12-23 02:55:16,368 : INFO : Removed 0 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:16,369 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:16,371 : INFO : built Dictionary(126 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 406 corpus positions)\n",
      "2020-12-23 02:55:16,419 : INFO : token count processed\n",
      "2020-12-23 02:55:16,422 : INFO : frequencies processed\n",
      "2020-12-23 02:55:16,551 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:16,551 : INFO : entropies processed\n",
      "2020-12-23 02:55:16,552 : INFO : extropies processed\n",
      "2020-12-23 02:55:16,553 : INFO : token count processed\n",
      "2020-12-23 02:55:16,554 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:16,554 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:16,555 : INFO : vocab #2480\n",
      "2020-12-23 02:55:16,556 : INFO : diff #set()\n",
      "2020-12-23 02:55:16,826 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:16,956 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.1335044721176397, 0.46871239927959285], [0.7005912959575653, 0.2994087], [2.2516291673878226, 1.2667563532600834], [3.8868421881310113, 6.550038223589686, 6.668365840875133, 3.7685145708455643, 2.7815236527441214, 0.1183276172854466]]\n",
      "2020-12-23 02:55:16,959 : INFO : Removed 0 and 70 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:55:16,962 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:16,963 : INFO : built Dictionary(82 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 322 corpus positions)\n",
      "2020-12-23 02:55:16,997 : INFO : token count processed\n",
      "2020-12-23 02:55:17,002 : INFO : frequencies processed\n",
      "2020-12-23 02:55:17,132 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:17,132 : INFO : entropies processed\n",
      "2020-12-23 02:55:17,133 : INFO : extropies processed\n",
      "2020-12-23 02:55:17,134 : INFO : token count processed\n",
      "2020-12-23 02:55:17,135 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:17,136 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:17,137 : INFO : vocab #2480\n",
      "2020-12-23 02:55:17,138 : INFO : diff #set()\n",
      "2020-12-23 02:55:17,400 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:17,529 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.1239416733311216, 0.47082272199670727], [0.7299365401268005, 0.27006346], [1.9219280948873623, 1.2148067842293933], [3.8868421881310113, 5.860525481261383, 6.027620221220135, 3.719747448172259, 2.140778033089124, 0.16709473995875257]]\n",
      "2020-12-23 02:55:17,532 : INFO : Removed 0 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:17,533 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:17,534 : INFO : built Dictionary(57 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 360 corpus positions)\n",
      "2020-12-23 02:55:17,561 : INFO : token count processed\n",
      "2020-12-23 02:55:17,564 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:55:17,565 : INFO : frequencies processed\n",
      "2020-12-23 02:55:17,566 : INFO : token count processed\n",
      "2020-12-23 02:55:17,567 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:17,568 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:17,569 : INFO : vocab #2480\n",
      "2020-12-23 02:55:17,569 : INFO : diff #set()\n",
      "2020-12-23 02:55:17,824 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:17,953 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.2652574168866808, 0.4414509329250437], [0.9008089229464531, 0.09919108], [nan, nan], [3.8868421881310113, 5.945464049777852, 6.118655316596932, 3.713650921311931, 2.231813128465921, 0.17319126681907981]]\n",
      "2020-12-23 02:55:17,956 : INFO : Removed 0 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:17,957 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:17,958 : INFO : built Dictionary(198 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 907 corpus positions)\n",
      "2020-12-23 02:55:18,027 : INFO : token count processed\n",
      "2020-12-23 02:55:18,029 : INFO : frequencies processed\n",
      "2020-12-23 02:55:18,155 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:18,155 : INFO : entropies processed\n",
      "2020-12-23 02:55:18,156 : INFO : extropies processed\n",
      "2020-12-23 02:55:18,157 : INFO : token count processed\n",
      "2020-12-23 02:55:18,158 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:18,159 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:18,159 : INFO : vocab #2480\n",
      "2020-12-23 02:55:18,160 : INFO : diff #set()\n",
      "2020-12-23 02:55:18,417 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:18,545 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.1341549429677693, 0.4685695400397657], [0.6842335164546967, 0.31576648], [2.6464393446710153, 1.3017576173934455], [3.8868421881310113, 6.811563897304216, 6.873882650324806, 3.8245234351104216, 2.9870404621937943, 0.06231875302058931]]\n",
      "2020-12-23 02:55:18,548 : INFO : Removed 0 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:18,549 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:18,550 : INFO : built Dictionary(217 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 1002 corpus positions)\n",
      "2020-12-23 02:55:18,632 : INFO : token count processed\n",
      "2020-12-23 02:55:18,634 : INFO : frequencies processed\n",
      "2020-12-23 02:55:18,762 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:18,763 : INFO : entropies processed\n",
      "2020-12-23 02:55:18,764 : INFO : extropies processed\n",
      "2020-12-23 02:55:18,766 : INFO : token count processed\n",
      "2020-12-23 02:55:18,767 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:18,768 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:18,768 : INFO : vocab #2480\n",
      "2020-12-23 02:55:18,770 : INFO : diff #set()\n",
      "2020-12-23 02:55:19,030 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:19,158 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.1441357185554903, 0.4663883873329169], [0.6790348291397095, 0.32096517], [3.077819531114783, 1.3480851448593865], [3.8868421881310113, 7.502034948968415, 7.535947594587528, 3.852929542511898, 3.6491054064565165, 0.03391264561911278]]\n",
      "2020-12-23 02:55:19,161 : INFO : Removed 0 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:19,162 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:19,165 : INFO : built Dictionary(262 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 1575 corpus positions)\n",
      "2020-12-23 02:55:19,284 : INFO : token count processed\n",
      "2020-12-23 02:55:19,304 : INFO : frequencies processed\n",
      "2020-12-23 02:55:19,438 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:19,439 : INFO : entropies processed\n",
      "2020-12-23 02:55:19,440 : INFO : extropies processed\n",
      "2020-12-23 02:55:19,441 : INFO : token count processed\n",
      "2020-12-23 02:55:19,443 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:19,444 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:19,445 : INFO : vocab #2480\n",
      "2020-12-23 02:55:19,446 : INFO : diff #set()\n",
      "2020-12-23 02:55:19,716 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:19,847 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.1719400099271147, 0.46041787315919364], [0.7343453168869019, 0.26565468], [2.9139770731827523, 1.3356231683419404], [3.8868421881310113, 7.39180093901977, 7.427278102536182, 3.8513650246145996, 3.5404359144051702, 0.03547716351641128]]\n",
      "2020-12-23 02:55:19,850 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:19,851 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:19,852 : INFO : built Dictionary(52 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 130 corpus positions)\n",
      "2020-12-23 02:55:19,870 : INFO : token count processed\n",
      "2020-12-23 02:55:19,872 : INFO : frequencies processed\n",
      "2020-12-23 02:55:20,000 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:20,001 : INFO : entropies processed\n",
      "2020-12-23 02:55:20,001 : INFO : extropies processed\n",
      "2020-12-23 02:55:20,002 : INFO : token count processed\n",
      "2020-12-23 02:55:20,003 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:20,004 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:20,005 : INFO : vocab #2480\n",
      "2020-12-23 02:55:20,006 : INFO : diff #set()\n",
      "2020-12-23 02:55:20,264 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:20,391 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.1306854960864554, 0.4693325231887831], [0.7510022521018982, 0.24899775], [0.9182958340544896, 0.9182958340544896], [3.8868421881310113, 4.927561309677364, 5.361015469655628, 3.4533880281527463, 1.474173281524617, 0.43345415997826464]]\n",
      "2020-12-23 02:55:20,393 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:20,394 : INFO : built Dictionary(22 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 31 corpus positions)\n",
      "2020-12-23 02:55:20,398 : INFO : token count processed\n",
      "2020-12-23 02:55:20,403 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:55:20,404 : INFO : frequencies processed\n",
      "2020-12-23 02:55:20,406 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:55:20,410 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:20,411 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:20,412 : INFO : vocab #2480\n",
      "2020-12-23 02:55:20,413 : INFO : diff #set()\n",
      "2020-12-23 02:55:20,674 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:20,801 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2009722808360932, 0.4543446588160236], [0.9171154424548149, 0.08288456], [nan, nan], [3.8868421881310113, 2.5216406363433186, 4.349199939349344, 2.0592828851249863, 0.46235775121833234, 1.827559303006025]]\n",
      "2020-12-23 02:55:20,805 : INFO : Removed 0 and 530 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:20,806 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:20,810 : INFO : built Dictionary(332 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 2903 corpus positions)\n",
      "2020-12-23 02:55:20,975 : INFO : token count processed\n",
      "2020-12-23 02:55:20,982 : INFO : frequencies processed\n",
      "2020-12-23 02:55:21,112 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:21,113 : INFO : entropies processed\n",
      "2020-12-23 02:55:21,114 : INFO : extropies processed\n",
      "2020-12-23 02:55:21,116 : INFO : token count processed\n",
      "2020-12-23 02:55:21,117 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:21,118 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:21,118 : INFO : vocab #2480\n",
      "2020-12-23 02:55:21,120 : INFO : diff #set()\n",
      "2020-12-23 02:55:21,380 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:21,509 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.0874707233826575, 0.479048634693924], [0.6172387301921844, 0.38276127], [3.461320140211008, 1.3685396245922048], [3.8868421881310113, 7.480007711014331, 7.491287658504984, 3.875562240640358, 3.6044454703739723, 0.011279947490653086]]\n",
      "2020-12-23 02:55:21,512 : INFO : Removed 0 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:21,512 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:21,514 : INFO : built Dictionary(213 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 1052 corpus positions)\n",
      "2020-12-23 02:55:21,598 : INFO : token count processed\n",
      "2020-12-23 02:55:21,601 : INFO : frequencies processed\n",
      "2020-12-23 02:55:21,728 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:21,728 : INFO : entropies processed\n",
      "2020-12-23 02:55:21,729 : INFO : extropies processed\n",
      "2020-12-23 02:55:21,730 : INFO : token count processed\n",
      "2020-12-23 02:55:21,731 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:21,732 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:21,732 : INFO : vocab #2480\n",
      "2020-12-23 02:55:21,733 : INFO : diff #set()\n",
      "2020-12-23 02:55:21,990 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:22,118 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.1040794624868266, 0.4752672215231324], [0.6168246567249298, 0.38317534], [2.8553885422075336, 1.3250186770664762], [3.8868421881310113, 7.131331012509435, 7.172644554751763, 3.845528645888683, 3.2858023666207514, 0.04131354224232808]]\n",
      "2020-12-23 02:55:22,121 : INFO : Removed 0 and 166 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:22,122 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:22,124 : INFO : built Dictionary(203 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 871 corpus positions)\n",
      "2020-12-23 02:55:22,202 : INFO : token count processed\n",
      "2020-12-23 02:55:22,210 : INFO : frequencies processed\n",
      "2020-12-23 02:55:22,337 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:22,338 : INFO : entropies processed\n",
      "2020-12-23 02:55:22,339 : INFO : extropies processed\n",
      "2020-12-23 02:55:22,340 : INFO : token count processed\n",
      "2020-12-23 02:55:22,341 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:22,342 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:22,343 : INFO : vocab #2480\n",
      "2020-12-23 02:55:22,344 : INFO : diff #set()\n",
      "2020-12-23 02:55:22,616 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:22,746 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.1471804514926642, 0.46572704185380687], [0.6756488978862762, 0.3243511], [2.6464393446710153, 1.3017576173934455], [3.8868421881310113, 7.203742744794778, 7.25826072579024, 3.8323242071355494, 3.371418537659229, 0.054517980995462345]]\n",
      "2020-12-23 02:55:22,749 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:22,750 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:22,751 : INFO : built Dictionary(65 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 203 corpus positions)\n",
      "2020-12-23 02:55:22,771 : INFO : token count processed\n",
      "2020-12-23 02:55:22,776 : INFO : frequencies processed\n",
      "2020-12-23 02:55:22,906 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:22,907 : INFO : entropies processed\n",
      "2020-12-23 02:55:22,908 : INFO : extropies processed\n",
      "2020-12-23 02:55:22,910 : INFO : token count processed\n",
      "2020-12-23 02:55:22,911 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:22,912 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:22,913 : INFO : vocab #2480\n",
      "2020-12-23 02:55:22,914 : INFO : diff #set()\n",
      "2020-12-23 02:55:23,170 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:23,296 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.128806173560166, 0.4697468526820472], [0.728550374507904, 0.27144963], [1.5, 1.1225562489182657], [3.8868421881310113, 5.195502554608948, 5.505278356619073, 3.5770663861208867, 1.6184361684880613, 0.3097758020101242]]\n",
      "2020-12-23 02:55:23,299 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:23,300 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:23,301 : INFO : built Dictionary(70 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 264 corpus positions)\n",
      "2020-12-23 02:55:23,327 : INFO : token count processed\n",
      "2020-12-23 02:55:23,330 : INFO : frequencies processed\n",
      "2020-12-23 02:55:23,460 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:23,461 : INFO : entropies processed\n",
      "2020-12-23 02:55:23,462 : INFO : extropies processed\n",
      "2020-12-23 02:55:23,463 : INFO : token count processed\n",
      "2020-12-23 02:55:23,464 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:23,465 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:23,465 : INFO : vocab #2480\n",
      "2020-12-23 02:55:23,466 : INFO : diff #set()\n",
      "2020-12-23 02:55:23,733 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:23,861 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.0801547397707338, 0.4807334670257348], [0.6757628619670868, 0.32423714], [1.8423709931771084, 1.1893232685884285], [3.8868421881310113, 5.32027245610305, 5.5487150324903745, 3.658399611743686, 1.661872844359363, 0.2284425763873248]]\n",
      "2020-12-23 02:55:23,863 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:23,864 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:23,865 : INFO : built Dictionary(169 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 504 corpus positions)\n",
      "2020-12-23 02:55:23,924 : INFO : token count processed\n",
      "2020-12-23 02:55:23,927 : INFO : frequencies processed\n",
      "2020-12-23 02:55:24,055 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:24,056 : INFO : entropies processed\n",
      "2020-12-23 02:55:24,057 : INFO : extropies processed\n",
      "2020-12-23 02:55:24,058 : INFO : token count processed\n",
      "2020-12-23 02:55:24,059 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:24,060 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:24,061 : INFO : vocab #2480\n",
      "2020-12-23 02:55:24,062 : INFO : diff #set()\n",
      "2020-12-23 02:55:24,319 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:24,446 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.1567670532616394, 0.46365693433962574], [0.6946543157100677, 0.30534568], [2.2516291673878226, 1.2667563532600834], [3.8868421881310113, 6.898202761357263, 6.995876604032135, 3.7891683454561385, 3.109034415901124, 0.09767384267487245]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:55:24,448 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:24,449 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:24,450 : INFO : built Dictionary(134 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 526 corpus positions)\n",
      "2020-12-23 02:55:24,493 : INFO : token count processed\n",
      "2020-12-23 02:55:24,496 : INFO : frequencies processed\n",
      "2020-12-23 02:55:24,623 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:24,624 : INFO : entropies processed\n",
      "2020-12-23 02:55:24,626 : INFO : extropies processed\n",
      "2020-12-23 02:55:24,627 : INFO : token count processed\n",
      "2020-12-23 02:55:24,627 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:24,628 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:24,629 : INFO : vocab #2480\n",
      "2020-12-23 02:55:24,630 : INFO : diff #set()\n",
      "2020-12-23 02:55:24,889 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:25,026 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.118466254248148, 0.47203961733858435], [0.7138615548610687, 0.28613845], [1.8423709931771084, 1.1893232685884285], [3.8868421881310113, 6.388500481644799, 6.490174940705272, 3.7851677290705386, 2.6033327525742602, 0.10167445906047234]]\n",
      "2020-12-23 02:55:25,028 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:25,029 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:25,030 : INFO : built Dictionary(58 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 190 corpus positions)\n",
      "2020-12-23 02:55:25,046 : INFO : token count processed\n",
      "2020-12-23 02:55:25,048 : INFO : frequencies processed\n",
      "2020-12-23 02:55:25,182 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:25,183 : INFO : entropies processed\n",
      "2020-12-23 02:55:25,184 : INFO : extropies processed\n",
      "2020-12-23 02:55:25,185 : INFO : token count processed\n",
      "2020-12-23 02:55:25,186 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:25,187 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:25,188 : INFO : vocab #2480\n",
      "2020-12-23 02:55:25,189 : INFO : diff #set()\n",
      "2020-12-23 02:55:25,454 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:25,582 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.1689243548031973, 0.46105803449781335], [0.7647223025560379, 0.2352777], [0.0, 0.0], [3.8868421881310113, 4.8191513650620195, 5.218554475238367, 3.4874390779546633, 1.3317122871073557, 0.39940311017634755]]\n",
      "2020-12-23 02:55:25,585 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:25,586 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:25,587 : INFO : built Dictionary(60 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 245 corpus positions)\n",
      "2020-12-23 02:55:25,609 : INFO : token count processed\n",
      "2020-12-23 02:55:25,611 : INFO : frequencies processed\n",
      "2020-12-23 02:55:25,743 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:25,744 : INFO : entropies processed\n",
      "2020-12-23 02:55:25,745 : INFO : extropies processed\n",
      "2020-12-23 02:55:25,747 : INFO : token count processed\n",
      "2020-12-23 02:55:25,749 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:25,750 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:25,751 : INFO : vocab #2480\n",
      "2020-12-23 02:55:25,752 : INFO : diff #set()\n",
      "2020-12-23 02:55:26,018 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:26,145 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.0903977962176088, 0.47837784837384173], [0.6811129748821259, 0.31888703], [1.8423709931771084, 1.1893232685884285], [3.8868421881310113, 5.062480936779194, 5.325953930502748, 3.6233691944074566, 1.4391117423717366, 0.26347299372355426]]\n",
      "2020-12-23 02:55:26,149 : INFO : Removed 0 and 223 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:26,149 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:26,151 : INFO : built Dictionary(246 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 1799 corpus positions)\n",
      "2020-12-23 02:55:26,253 : INFO : token count processed\n",
      "2020-12-23 02:55:26,256 : INFO : frequencies processed\n",
      "2020-12-23 02:55:26,383 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:26,384 : INFO : entropies processed\n",
      "2020-12-23 02:55:26,384 : INFO : extropies processed\n",
      "2020-12-23 02:55:26,386 : INFO : token count processed\n",
      "2020-12-23 02:55:26,387 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:26,388 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:26,389 : INFO : vocab #2480\n",
      "2020-12-23 02:55:26,390 : INFO : diff #set()\n",
      "2020-12-23 02:55:26,644 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:26,771 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.130897935868504, 0.4692857331021926], [0.654110461473465, 0.34588954], [3.085055102756477, 1.348691494104856], [3.8868421881310113, 7.185085743102134, 7.2155743999001505, 3.8563535313329957, 3.328732211769139, 0.030488656798016045]]\n",
      "2020-12-23 02:55:26,774 : INFO : Removed 0 and 148 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:26,775 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:26,777 : INFO : built Dictionary(164 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 774 corpus positions)\n",
      "2020-12-23 02:55:26,839 : INFO : token count processed\n",
      "2020-12-23 02:55:26,842 : INFO : frequencies processed\n",
      "2020-12-23 02:55:26,969 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:26,970 : INFO : entropies processed\n",
      "2020-12-23 02:55:26,971 : INFO : extropies processed\n",
      "2020-12-23 02:55:26,973 : INFO : token count processed\n",
      "2020-12-23 02:55:26,974 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:26,975 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:26,976 : INFO : vocab #2480\n",
      "2020-12-23 02:55:26,978 : INFO : diff #set()\n",
      "2020-12-23 02:55:27,240 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:27,368 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.1306588731408855, 0.4693383875786094], [0.7116177976131439, 0.2883822], [2.8453509366224368, 1.321020357168122], [3.8868421881310113, 6.591225336124281, 6.661093052504871, 3.816974471750422, 2.7742508643738595, 0.06986771638058986]]\n",
      "2020-12-23 02:55:27,370 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:27,371 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:27,372 : INFO : built Dictionary(48 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 112 corpus positions)\n",
      "2020-12-23 02:55:27,384 : INFO : token count processed\n",
      "2020-12-23 02:55:27,387 : INFO : frequencies processed\n",
      "2020-12-23 02:55:27,514 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:27,515 : INFO : entropies processed\n",
      "2020-12-23 02:55:27,516 : INFO : extropies processed\n",
      "2020-12-23 02:55:27,517 : INFO : token count processed\n",
      "2020-12-23 02:55:27,518 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:27,519 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:27,520 : INFO : vocab #2480\n",
      "2020-12-23 02:55:27,521 : INFO : diff #set()\n",
      "2020-12-23 02:55:27,777 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:27,905 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.1480717244815024, 0.4655338034587175], [0.7812933772802353, 0.21870662], [0.9182958340544896, 0.9182958340544896], [3.8868421881310113, 4.7032114441396695, 5.216070716115137, 3.373982916155544, 1.3292285279841258, 0.5128592719754677]]\n",
      "2020-12-23 02:55:27,907 : INFO : Removed 0 and 9 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:27,908 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:27,909 : INFO : built Dictionary(122 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 352 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:55:27,952 : INFO : token count processed\n",
      "2020-12-23 02:55:27,955 : INFO : frequencies processed\n",
      "2020-12-23 02:55:28,082 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:28,083 : INFO : entropies processed\n",
      "2020-12-23 02:55:28,084 : INFO : extropies processed\n",
      "2020-12-23 02:55:28,085 : INFO : token count processed\n",
      "2020-12-23 02:55:28,086 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:28,087 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:28,088 : INFO : vocab #2480\n",
      "2020-12-23 02:55:28,089 : INFO : diff #set()\n",
      "2020-12-23 02:55:28,347 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:28,475 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.090156760572726, 0.4784330146251753], [0.6644088625907898, 0.33559114], [2.503258334775646, 1.2991301890771525], [3.8868421881310113, 6.14228447828618, 6.27524280940913, 3.753883857008061, 2.388400621278119, 0.13295833112295075]]\n",
      "2020-12-23 02:55:28,478 : INFO : Removed 0 and 184 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:28,479 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:28,482 : INFO : built Dictionary(256 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 1142 corpus positions)\n",
      "2020-12-23 02:55:28,598 : INFO : token count processed\n",
      "2020-12-23 02:55:28,602 : INFO : frequencies processed\n",
      "2020-12-23 02:55:28,729 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:28,729 : INFO : entropies processed\n",
      "2020-12-23 02:55:28,730 : INFO : extropies processed\n",
      "2020-12-23 02:55:28,731 : INFO : token count processed\n",
      "2020-12-23 02:55:28,732 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:28,732 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:28,733 : INFO : vocab #2480\n",
      "2020-12-23 02:55:28,734 : INFO : diff #set()\n",
      "2020-12-23 02:55:28,992 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:29,120 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.1280287886168294, 0.4699184547451435], [0.6302647292613983, 0.36973527], [3.240223928941852, 1.3591202789297814], [3.8868421881310113, 7.450178124335845, 7.47842094229293, 3.858599370173927, 3.5915787541619184, 0.02824281795708483]]\n",
      "2020-12-23 02:55:29,123 : INFO : Removed 0 and 16 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:29,124 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:29,125 : INFO : built Dictionary(65 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 190 corpus positions)\n",
      "2020-12-23 02:55:29,143 : INFO : token count processed\n",
      "2020-12-23 02:55:29,145 : INFO : frequencies processed\n",
      "2020-12-23 02:55:29,273 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:29,274 : INFO : entropies processed\n",
      "2020-12-23 02:55:29,275 : INFO : extropies processed\n",
      "2020-12-23 02:55:29,276 : INFO : token count processed\n",
      "2020-12-23 02:55:29,277 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:29,278 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:29,279 : INFO : vocab #2480\n",
      "2020-12-23 02:55:29,280 : INFO : diff #set()\n",
      "2020-12-23 02:55:29,538 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:29,666 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.118426477076321, 0.47204848071013455], [0.7273522317409515, 0.27264777], [1.5, 1.1225562489182657], [3.8868421881310113, 5.20665021947654, 5.513863769544402, 3.5796286380631486, 1.6270215814133908, 0.30721355006786233]]\n",
      "2020-12-23 02:55:29,668 : INFO : Removed 0 and 89 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:29,669 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:29,670 : INFO : built Dictionary(132 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 528 corpus positions)\n",
      "2020-12-23 02:55:29,720 : INFO : token count processed\n",
      "2020-12-23 02:55:29,722 : INFO : frequencies processed\n",
      "2020-12-23 02:55:29,848 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:29,849 : INFO : entropies processed\n",
      "2020-12-23 02:55:29,850 : INFO : extropies processed\n",
      "2020-12-23 02:55:29,851 : INFO : token count processed\n",
      "2020-12-23 02:55:29,852 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:29,853 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:29,853 : INFO : vocab #2480\n",
      "2020-12-23 02:55:29,854 : INFO : diff #set()\n",
      "2020-12-23 02:55:30,113 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:30,241 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.1540175816069256, 0.4642487640486141], [0.7070578634738922, 0.29294214], [2.1556390622295662, 1.2407663947533205], [3.8868421881310113, 6.524718477352, 6.628700121531189, 3.7828605439518217, 2.7418579334001776, 0.1039816441791892]]\n",
      "2020-12-23 02:55:30,243 : INFO : Removed 0 and 8 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:30,244 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:30,246 : INFO : built Dictionary(72 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 235 corpus positions)\n",
      "2020-12-23 02:55:30,272 : INFO : token count processed\n",
      "2020-12-23 02:55:30,274 : INFO : frequencies processed\n",
      "2020-12-23 02:55:30,407 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:30,408 : INFO : entropies processed\n",
      "2020-12-23 02:55:30,409 : INFO : extropies processed\n",
      "2020-12-23 02:55:30,410 : INFO : token count processed\n",
      "2020-12-23 02:55:30,411 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:30,412 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:30,413 : INFO : vocab #2480\n",
      "2020-12-23 02:55:30,414 : INFO : diff #set()\n",
      "2020-12-23 02:55:30,679 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:30,808 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.161102180829445, 0.46272684784214757], [0.7517415285110474, 0.24825847], [0.9182958340544896, 0.9182958340544896], [3.8868421881310113, 5.321859380715434, 5.60917078887202, 3.599530779974425, 1.7223286007410086, 0.28731140815658573]]\n",
      "2020-12-23 02:55:30,810 : INFO : Removed 0 and 119 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:30,811 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:30,812 : INFO : built Dictionary(143 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 853 corpus positions)\n",
      "2020-12-23 02:55:30,861 : INFO : token count processed\n",
      "2020-12-23 02:55:30,864 : INFO : frequencies processed\n",
      "2020-12-23 02:55:30,995 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:30,995 : INFO : entropies processed\n",
      "2020-12-23 02:55:30,996 : INFO : extropies processed\n",
      "2020-12-23 02:55:30,998 : INFO : token count processed\n",
      "2020-12-23 02:55:30,999 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:31,000 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:31,001 : INFO : vocab #2480\n",
      "2020-12-23 02:55:31,002 : INFO : diff #set()\n",
      "2020-12-23 02:55:31,274 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:31,403 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.0891640731541485, 0.47866034690623127], [0.6185562014579773, 0.3814438], [3.180832987205441, 1.3478475537994532], [3.8868421881310113, 6.500767808767801, 6.558204159238496, 3.8294058376603157, 2.6713619711074847, 0.057436350470695174]]\n",
      "2020-12-23 02:55:31,405 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:31,406 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:31,407 : INFO : built Dictionary(44 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 75 corpus positions)\n",
      "2020-12-23 02:55:31,417 : INFO : token count processed\n",
      "2020-12-23 02:55:31,420 : INFO : frequencies processed\n",
      "2020-12-23 02:55:31,546 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:31,547 : INFO : entropies processed\n",
      "2020-12-23 02:55:31,548 : INFO : extropies processed\n",
      "2020-12-23 02:55:31,550 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:55:31,552 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:31,553 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:31,553 : INFO : vocab #2480\n",
      "2020-12-23 02:55:31,555 : INFO : diff #set()\n",
      "2020-12-23 02:55:31,822 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:31,951 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.1348730059055585, 0.46841193702565254], [0.8136318027973175, 0.1863682], [1.0, 1.0], [3.8868421881310113, 4.736228843383063, 5.288797868843835, 3.3342731626702395, 1.4019556807128235, 0.5525690254607722]]\n",
      "2020-12-23 02:55:31,953 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:31,954 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:31,956 : INFO : built Dictionary(106 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 416 corpus positions)\n",
      "2020-12-23 02:55:31,998 : INFO : token count processed\n",
      "2020-12-23 02:55:32,000 : INFO : frequencies processed\n",
      "2020-12-23 02:55:32,127 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:32,128 : INFO : entropies processed\n",
      "2020-12-23 02:55:32,128 : INFO : extropies processed\n",
      "2020-12-23 02:55:32,129 : INFO : token count processed\n",
      "2020-12-23 02:55:32,130 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:32,131 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:32,133 : INFO : vocab #2480\n",
      "2020-12-23 02:55:32,134 : INFO : diff #set()\n",
      "2020-12-23 02:55:32,391 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:32,519 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.1644489726953384, 0.4620113537510302], [0.7415291666984558, 0.25847083], [0.9182958340544896, 0.9182958340544896], [3.8868421881310113, 5.788442787590127, 5.972448129589732, 3.702836846131407, 2.0856059414587205, 0.1840053419996046]]\n",
      "2020-12-23 02:55:32,521 : INFO : Removed 0 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:32,522 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:32,523 : INFO : built Dictionary(63 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 102 corpus positions)\n",
      "2020-12-23 02:55:32,545 : INFO : token count processed\n",
      "2020-12-23 02:55:32,547 : INFO : frequencies processed\n",
      "2020-12-23 02:55:32,675 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:32,676 : INFO : entropies processed\n",
      "2020-12-23 02:55:32,677 : INFO : extropies processed\n",
      "2020-12-23 02:55:32,678 : INFO : token count processed\n",
      "2020-12-23 02:55:32,679 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:32,680 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:32,681 : INFO : vocab #2480\n",
      "2020-12-23 02:55:32,682 : INFO : diff #set()\n",
      "2020-12-23 02:55:32,940 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:33,069 : INFO : Computed distances or similarities ('275', 'sacp-python-common/setup.py')[[1.1853385012070534, 0.45759501305983413], [0.8117706477642059, 0.18822935], [1.0, 1.0], [3.8868421881310113, 5.370004292053436, 5.753600784687379, 3.5032456954970694, 1.8667585965563673, 0.38359649263394235]]\n",
      "2020-12-23 02:55:33,071 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:33,072 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:33,073 : INFO : built Dictionary(86 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 347 corpus positions)\n",
      "2020-12-23 02:55:33,105 : INFO : token count processed\n",
      "2020-12-23 02:55:33,108 : INFO : frequencies processed\n",
      "2020-12-23 02:55:33,235 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:33,236 : INFO : entropies processed\n",
      "2020-12-23 02:55:33,236 : INFO : extropies processed\n",
      "2020-12-23 02:55:33,237 : INFO : token count processed\n",
      "2020-12-23 02:55:33,238 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:33,239 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:33,239 : INFO : vocab #2480\n",
      "2020-12-23 02:55:33,240 : INFO : diff #set()\n",
      "2020-12-23 02:55:33,498 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:33,626 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.1505204473555515, 0.46500371630024645], [0.7909849435091019, 0.20901506], [2.2516291673878226, 1.2667563532600834], [3.8868421881310113, 5.695663584743922, 5.878005811911407, 3.704499960963526, 1.991163623780396, 0.18234222716748505]]\n",
      "2020-12-23 02:55:33,629 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:33,629 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:33,630 : INFO : built Dictionary(50 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 122 corpus positions)\n",
      "2020-12-23 02:55:33,643 : INFO : token count processed\n",
      "2020-12-23 02:55:33,645 : INFO : frequencies processed\n",
      "2020-12-23 02:55:33,773 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:33,774 : INFO : entropies processed\n",
      "2020-12-23 02:55:33,774 : INFO : extropies processed\n",
      "2020-12-23 02:55:33,775 : INFO : token count processed\n",
      "2020-12-23 02:55:33,776 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:33,777 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:33,778 : INFO : vocab #2480\n",
      "2020-12-23 02:55:33,779 : INFO : diff #set()\n",
      "2020-12-23 02:55:34,036 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:34,164 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.1477214649679586, 0.46560972468323253], [0.807260200381279, 0.1927398], [1.0, 1.0], [3.8868421881310113, 4.9004417692112465, 5.362222686010163, 3.425061271332094, 1.475380497879152, 0.46178091679891686]]\n",
      "2020-12-23 02:55:34,166 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:34,167 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:34,168 : INFO : built Dictionary(48 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 129 corpus positions)\n",
      "2020-12-23 02:55:34,187 : INFO : token count processed\n",
      "2020-12-23 02:55:34,189 : INFO : frequencies processed\n",
      "2020-12-23 02:55:34,322 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:34,323 : INFO : entropies processed\n",
      "2020-12-23 02:55:34,323 : INFO : extropies processed\n",
      "2020-12-23 02:55:34,325 : INFO : token count processed\n",
      "2020-12-23 02:55:34,326 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:34,327 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:34,327 : INFO : vocab #2480\n",
      "2020-12-23 02:55:34,328 : INFO : diff #set()\n",
      "2020-12-23 02:55:34,595 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:34,722 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.1478418165582085, 0.46558363483324017], [0.8216674476861954, 0.17833255], [1.0, 1.0], [3.8868421881310113, 4.778624108914332, 5.251352005958884, 3.4141142910864595, 1.3645098178278725, 0.4727278970445514]]\n",
      "2020-12-23 02:55:34,724 : INFO : Removed 0 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:34,725 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:34,726 : INFO : built Dictionary(49 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 160 corpus positions)\n",
      "2020-12-23 02:55:34,738 : INFO : token count processed\n",
      "2020-12-23 02:55:34,741 : INFO : frequencies processed\n",
      "2020-12-23 02:55:34,868 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:34,869 : INFO : entropies processed\n",
      "2020-12-23 02:55:34,870 : INFO : extropies processed\n",
      "2020-12-23 02:55:34,871 : INFO : token count processed\n",
      "2020-12-23 02:55:34,872 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:34,873 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:34,873 : INFO : vocab #2480\n",
      "2020-12-23 02:55:34,874 : INFO : diff #set()\n",
      "2020-12-23 02:55:35,148 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:35,276 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.1689554849563648, 0.4610514171156989], [0.8598238527774811, 0.14017615], [1.0, 1.0], [3.8868421881310113, 4.773880192225086, 5.200430979867227, 3.4602914004888694, 1.313588791736216, 0.4265507876421415]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:55:35,279 : INFO : Removed 0 and 346 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:35,280 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:35,281 : INFO : built Dictionary(159 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 1985 corpus positions)\n",
      "2020-12-23 02:55:35,341 : INFO : token count processed\n",
      "2020-12-23 02:55:35,346 : INFO : frequencies processed\n",
      "2020-12-23 02:55:35,475 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:35,476 : INFO : entropies processed\n",
      "2020-12-23 02:55:35,477 : INFO : extropies processed\n",
      "2020-12-23 02:55:35,479 : INFO : token count processed\n",
      "2020-12-23 02:55:35,481 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:35,482 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:35,483 : INFO : vocab #2480\n",
      "2020-12-23 02:55:35,484 : INFO : diff #set()\n",
      "2020-12-23 02:55:35,747 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:35,875 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.2027872897337537, 0.45397029693269564], [0.8474771976470947, 0.1525228], [1.584962500721156, 1.1699250014423124], [3.8868421881310113, 6.620773041953877, 6.669313240688846, 3.838301989396042, 2.7824710525578342, 0.04854019873496895]]\n",
      "2020-12-23 02:55:35,878 : INFO : Removed 0 and 132 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:35,878 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:35,879 : INFO : built Dictionary(87 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 548 corpus positions)\n",
      "2020-12-23 02:55:35,908 : INFO : token count processed\n",
      "2020-12-23 02:55:35,913 : INFO : frequencies processed\n",
      "2020-12-23 02:55:36,047 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:36,048 : INFO : entropies processed\n",
      "2020-12-23 02:55:36,048 : INFO : extropies processed\n",
      "2020-12-23 02:55:36,050 : INFO : token count processed\n",
      "2020-12-23 02:55:36,051 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:36,052 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:36,052 : INFO : vocab #2480\n",
      "2020-12-23 02:55:36,054 : INFO : diff #set()\n",
      "2020-12-23 02:55:36,315 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:36,444 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.1326163712684458, 0.4689075885716924], [0.7264268100261688, 0.2735732], [2.25, 1.2709632597765914], [3.8868421881310113, 5.828370634755606, 5.94590688815819, 3.7693059347284272, 2.0590647000271782, 0.11753625340258367]]\n",
      "2020-12-23 02:55:36,446 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:36,447 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:36,449 : INFO : built Dictionary(90 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 311 corpus positions)\n",
      "2020-12-23 02:55:36,484 : INFO : token count processed\n",
      "2020-12-23 02:55:36,487 : INFO : frequencies processed\n",
      "2020-12-23 02:55:36,614 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:36,615 : INFO : entropies processed\n",
      "2020-12-23 02:55:36,616 : INFO : extropies processed\n",
      "2020-12-23 02:55:36,617 : INFO : token count processed\n",
      "2020-12-23 02:55:36,618 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:36,620 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:36,620 : INFO : vocab #2480\n",
      "2020-12-23 02:55:36,622 : INFO : diff #set()\n",
      "2020-12-23 02:55:36,883 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:37,012 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.1154836866833435, 0.47270513419453525], [0.6798425614833832, 0.32015744], [1.5, 1.1225562489182657], [3.8868421881310113, 5.774409284925443, 5.969192500173129, 3.6920589728833253, 2.082350312042118, 0.1947832152476865]]\n",
      "2020-12-23 02:55:37,014 : INFO : Removed 0 and 51 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:37,015 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:37,017 : INFO : built Dictionary(99 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 360 corpus positions)\n",
      "2020-12-23 02:55:37,054 : INFO : token count processed\n",
      "2020-12-23 02:55:37,056 : INFO : frequencies processed\n",
      "2020-12-23 02:55:37,183 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:37,184 : INFO : entropies processed\n",
      "2020-12-23 02:55:37,185 : INFO : extropies processed\n",
      "2020-12-23 02:55:37,186 : INFO : token count processed\n",
      "2020-12-23 02:55:37,187 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:37,188 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:37,189 : INFO : vocab #2480\n",
      "2020-12-23 02:55:37,190 : INFO : diff #set()\n",
      "2020-12-23 02:55:37,448 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:37,576 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.1627556729330901, 0.46237307917626125], [0.7708425670862198, 0.22915743], [2.2516291673878226, 1.2667563532600834], [3.8868421881310113, 5.977819040873918, 6.135345181442029, 3.7293160475629, 2.248502993311017, 0.15752614056811076]]\n",
      "2020-12-23 02:55:37,578 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:37,579 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:37,581 : INFO : built Dictionary(82 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 307 corpus positions)\n",
      "2020-12-23 02:55:37,609 : INFO : token count processed\n",
      "2020-12-23 02:55:37,612 : INFO : frequencies processed\n",
      "2020-12-23 02:55:37,747 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:37,747 : INFO : entropies processed\n",
      "2020-12-23 02:55:37,748 : INFO : extropies processed\n",
      "2020-12-23 02:55:37,750 : INFO : token count processed\n",
      "2020-12-23 02:55:37,752 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:37,753 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:37,754 : INFO : vocab #2480\n",
      "2020-12-23 02:55:37,755 : INFO : diff #set()\n",
      "2020-12-23 02:55:38,015 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:38,143 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.1307417458655533, 0.46932013320731103], [0.6704065799713135, 0.32959342], [1.5219280948873621, 1.1419011889093373], [3.8868421881310113, 5.901812829596593, 6.080192686390718, 3.7084623313368867, 2.1933504982597065, 0.17837985679412505]]\n",
      "2020-12-23 02:55:38,145 : INFO : Removed 0 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:38,146 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:38,147 : INFO : built Dictionary(84 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 307 corpus positions)\n",
      "2020-12-23 02:55:38,175 : INFO : token count processed\n",
      "2020-12-23 02:55:38,180 : INFO : frequencies processed\n",
      "2020-12-23 02:55:38,320 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:38,321 : INFO : entropies processed\n",
      "2020-12-23 02:55:38,322 : INFO : extropies processed\n",
      "2020-12-23 02:55:38,324 : INFO : token count processed\n",
      "2020-12-23 02:55:38,325 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:38,326 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:38,327 : INFO : vocab #2480\n",
      "2020-12-23 02:55:38,328 : INFO : diff #set()\n",
      "2020-12-23 02:55:38,588 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:38,715 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.1083070528721877, 0.4743142127412991], [0.7536081075668335, 0.24639189], [1.5, 1.1225562489182657], [3.8868421881310113, 5.643202320803383, 5.844784571475167, 3.685259937459226, 1.957942383344156, 0.2015822506717848]]\n",
      "2020-12-23 02:55:38,717 : INFO : Removed 0 and 30 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:38,718 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:38,719 : INFO : built Dictionary(97 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 341 corpus positions)\n",
      "2020-12-23 02:55:38,750 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:55:38,758 : INFO : frequencies processed\n",
      "2020-12-23 02:55:38,885 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:38,885 : INFO : entropies processed\n",
      "2020-12-23 02:55:38,886 : INFO : extropies processed\n",
      "2020-12-23 02:55:38,887 : INFO : token count processed\n",
      "2020-12-23 02:55:38,887 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:38,888 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:38,889 : INFO : vocab #2480\n",
      "2020-12-23 02:55:38,890 : INFO : diff #set()\n",
      "2020-12-23 02:55:39,150 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:39,278 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.160130493041925, 0.46293499546492045], [0.7540548145771027, 0.24594519], [1.5, 1.1225562489182657], [3.8868421881310113, 5.925214310725336, 6.1071336043672835, 3.704922894489063, 2.220291416236272, 0.1819192936419478]]\n",
      "2020-12-23 02:55:39,281 : INFO : Removed 0 and 293 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:39,282 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:39,284 : INFO : built Dictionary(168 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 1736 corpus positions)\n",
      "2020-12-23 02:55:39,350 : INFO : token count processed\n",
      "2020-12-23 02:55:39,353 : INFO : frequencies processed\n",
      "2020-12-23 02:55:39,480 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:39,481 : INFO : entropies processed\n",
      "2020-12-23 02:55:39,482 : INFO : extropies processed\n",
      "2020-12-23 02:55:39,484 : INFO : token count processed\n",
      "2020-12-23 02:55:39,485 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:39,486 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:39,486 : INFO : vocab #2480\n",
      "2020-12-23 02:55:39,487 : INFO : diff #set()\n",
      "2020-12-23 02:55:39,755 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:39,883 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.2076138175735023, 0.4529777771997955], [0.8582397550344467, 0.14176024], [2.321928094887362, 1.2877123795494492], [3.8868421881310113, 6.551685682764175, 6.604079880778963, 3.8344479901162227, 2.717237692647952, 0.052394198014788174]]\n",
      "2020-12-23 02:55:39,886 : INFO : Removed 0 and 88 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:39,887 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:39,888 : INFO : built Dictionary(141 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 577 corpus positions)\n",
      "2020-12-23 02:55:39,941 : INFO : token count processed\n",
      "2020-12-23 02:55:39,943 : INFO : frequencies processed\n",
      "2020-12-23 02:55:40,076 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:40,077 : INFO : entropies processed\n",
      "2020-12-23 02:55:40,078 : INFO : extropies processed\n",
      "2020-12-23 02:55:40,079 : INFO : token count processed\n",
      "2020-12-23 02:55:40,079 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:40,080 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:40,080 : INFO : vocab #2480\n",
      "2020-12-23 02:55:40,081 : INFO : diff #set()\n",
      "2020-12-23 02:55:40,335 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:40,461 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.1555326538811412, 0.4639224547118094], [0.7205734550952911, 0.27942654], [2.9139770731827523, 1.3356231683419404], [3.8868421881310113, 6.642985062562557, 6.722776368164043, 3.8070508825295253, 2.8359341800330315, 0.07979130560148562]]\n",
      "2020-12-23 02:55:40,464 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:40,464 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:40,465 : INFO : built Dictionary(59 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 172 corpus positions)\n",
      "2020-12-23 02:55:40,481 : INFO : token count processed\n",
      "2020-12-23 02:55:40,483 : INFO : frequencies processed\n",
      "2020-12-23 02:55:40,611 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:40,612 : INFO : entropies processed\n",
      "2020-12-23 02:55:40,613 : INFO : extropies processed\n",
      "2020-12-23 02:55:40,614 : INFO : token count processed\n",
      "2020-12-23 02:55:40,615 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:40,616 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:40,617 : INFO : vocab #2480\n",
      "2020-12-23 02:55:40,618 : INFO : diff #set()\n",
      "2020-12-23 02:55:40,875 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:41,003 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.122081536869178, 0.4712354273980227], [0.6983634233474731, 0.30163658], [1.5219280948873621, 1.1419011889093373], [3.8868421881310113, 5.2461980344571995, 5.555040951612495, 3.577999270975716, 1.6681987634814832, 0.3088429171552951]]\n",
      "2020-12-23 02:55:41,005 : INFO : Removed 0 and 23 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:41,006 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:41,007 : INFO : built Dictionary(81 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 232 corpus positions)\n",
      "2020-12-23 02:55:41,031 : INFO : token count processed\n",
      "2020-12-23 02:55:41,033 : INFO : frequencies processed\n",
      "2020-12-23 02:55:41,170 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:41,171 : INFO : entropies processed\n",
      "2020-12-23 02:55:41,172 : INFO : extropies processed\n",
      "2020-12-23 02:55:41,174 : INFO : token count processed\n",
      "2020-12-23 02:55:41,176 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:41,177 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:41,178 : INFO : vocab #2480\n",
      "2020-12-23 02:55:41,180 : INFO : diff #set()\n",
      "2020-12-23 02:55:41,452 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:41,582 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/test_auth_utility.py')[[1.1178497722089382, 0.4721770227153506], [0.7005617618560791, 0.29943824], [1.5, 1.1225562489182657], [3.8868421881310113, 5.903090303960449, 6.114983641898575, 3.6749488501928864, 2.2281414537675635, 0.21189333793812537]]\n",
      "2020-12-23 02:55:41,585 : INFO : Removed 0 and 442 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:41,586 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:41,588 : INFO : built Dictionary(112 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 1231 corpus positions)\n",
      "2020-12-23 02:55:41,633 : INFO : token count processed\n",
      "2020-12-23 02:55:41,636 : INFO : frequencies processed\n",
      "2020-12-23 02:55:41,762 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:41,762 : INFO : entropies processed\n",
      "2020-12-23 02:55:41,763 : INFO : extropies processed\n",
      "2020-12-23 02:55:41,764 : INFO : token count processed\n",
      "2020-12-23 02:55:41,765 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:41,766 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:41,767 : INFO : vocab #2480\n",
      "2020-12-23 02:55:41,768 : INFO : diff #set()\n",
      "2020-12-23 02:55:42,038 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:42,167 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.1599300980289347, 0.46297794586619256], [0.7968275547027588, 0.20317245], [2.503258334775646, 1.2991301890771525], [3.8868421881310113, 6.16659449033757, 6.2225038198227125, 3.830932858645868, 2.335661631691701, 0.05590932948514293]]\n",
      "2020-12-23 02:55:42,170 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:42,171 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:42,172 : INFO : built Dictionary(75 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 266 corpus positions)\n",
      "2020-12-23 02:55:42,205 : INFO : token count processed\n",
      "2020-12-23 02:55:42,210 : INFO : frequencies processed\n",
      "2020-12-23 02:55:42,339 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:42,339 : INFO : entropies processed\n",
      "2020-12-23 02:55:42,340 : INFO : extropies processed\n",
      "2020-12-23 02:55:42,341 : INFO : token count processed\n",
      "2020-12-23 02:55:42,342 : INFO : alphabet_source #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:55:42,343 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:42,344 : INFO : vocab #2480\n",
      "2020-12-23 02:55:42,345 : INFO : diff #set()\n",
      "2020-12-23 02:55:42,605 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:42,732 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.167105231250332, 0.46144505840311245], [0.7167991101741791, 0.2832009], [1.5, 1.1225562489182657], [3.8868421881310113, 5.906856253399655, 6.104694849867896, 3.689003591662771, 2.217852661736885, 0.19783859646824098]]\n",
      "2020-12-23 02:55:42,735 : INFO : Removed 0 and 68 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:42,736 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:42,737 : INFO : built Dictionary(87 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 265 corpus positions)\n",
      "2020-12-23 02:55:42,770 : INFO : token count processed\n",
      "2020-12-23 02:55:42,772 : INFO : frequencies processed\n",
      "2020-12-23 02:55:42,900 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:42,901 : INFO : entropies processed\n",
      "2020-12-23 02:55:42,902 : INFO : extropies processed\n",
      "2020-12-23 02:55:42,904 : INFO : token count processed\n",
      "2020-12-23 02:55:42,905 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:42,906 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:42,906 : INFO : vocab #2480\n",
      "2020-12-23 02:55:42,907 : INFO : diff #set()\n",
      "2020-12-23 02:55:43,166 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:43,293 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.1249694707843831, 0.4705949961864032], [0.7519851326942444, 0.24801487], [1.9219280948873623, 1.2148067842293933], [3.8868421881310113, 5.965115449163356, 6.141763366237374, 3.7101942710569933, 2.2549211781063625, 0.1766479170740176]]\n",
      "2020-12-23 02:55:43,296 : INFO : Removed 0 and 13 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:43,297 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:43,298 : INFO : built Dictionary(94 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 379 corpus positions)\n",
      "2020-12-23 02:55:43,326 : INFO : token count processed\n",
      "2020-12-23 02:55:43,329 : INFO : frequencies processed\n",
      "2020-12-23 02:55:43,459 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:43,460 : INFO : entropies processed\n",
      "2020-12-23 02:55:43,461 : INFO : extropies processed\n",
      "2020-12-23 02:55:43,462 : INFO : token count processed\n",
      "2020-12-23 02:55:43,463 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:43,464 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:43,465 : INFO : vocab #2480\n",
      "2020-12-23 02:55:43,466 : INFO : diff #set()\n",
      "2020-12-23 02:55:43,731 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:43,859 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.1677368441856621, 0.4613106072733024], [0.7810318470001221, 0.21896815], [1.9219280948873623, 1.2148067842293933], [3.8868421881310113, 5.791362404253194, 5.970911069619419, 3.707293522764786, 2.0840688814884074, 0.17954866536622482]]\n",
      "2020-12-23 02:55:43,862 : INFO : Removed 0 and 41 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:43,862 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:43,863 : INFO : built Dictionary(84 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 391 corpus positions)\n",
      "2020-12-23 02:55:43,887 : INFO : token count processed\n",
      "2020-12-23 02:55:43,895 : INFO : frequencies processed\n",
      "2020-12-23 02:55:44,026 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:44,027 : INFO : entropies processed\n",
      "2020-12-23 02:55:44,027 : INFO : extropies processed\n",
      "2020-12-23 02:55:44,028 : INFO : token count processed\n",
      "2020-12-23 02:55:44,029 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:44,029 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:44,030 : INFO : vocab #2480\n",
      "2020-12-23 02:55:44,031 : INFO : diff #set()\n",
      "2020-12-23 02:55:44,291 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:44,418 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.1381376747544303, 0.4676967305741208], [0.7866308838129044, 0.21336912], [1.9219280948873623, 1.2148067842293933], [3.8868421881310113, 5.651670454631116, 5.823817721076534, 3.7146949216855933, 1.9369755329455223, 0.17214726644541756]]\n",
      "2020-12-23 02:55:44,420 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:44,421 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:44,422 : INFO : built Dictionary(49 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 101 corpus positions)\n",
      "2020-12-23 02:55:44,434 : INFO : token count processed\n",
      "2020-12-23 02:55:44,437 : INFO : frequencies processed\n",
      "2020-12-23 02:55:44,565 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:44,566 : INFO : entropies processed\n",
      "2020-12-23 02:55:44,566 : INFO : extropies processed\n",
      "2020-12-23 02:55:44,567 : INFO : token count processed\n",
      "2020-12-23 02:55:44,568 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:44,569 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:44,570 : INFO : vocab #2480\n",
      "2020-12-23 02:55:44,571 : INFO : diff #set()\n",
      "2020-12-23 02:55:44,829 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:44,956 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.1319601347988955, 0.4690519225371578], [0.7291461229324341, 0.27085388], [0.9182958340544896, 0.9182958340544896], [3.8868421881310113, 4.8226207261920235, 5.308538023962363, 3.400924890360672, 1.4216958358313518, 0.48591729777033965]]\n",
      "2020-12-23 02:55:44,958 : INFO : Removed 0 and 50 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:44,959 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:44,960 : INFO : built Dictionary(92 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 271 corpus positions)\n",
      "2020-12-23 02:55:44,993 : INFO : token count processed\n",
      "2020-12-23 02:55:44,999 : INFO : frequencies processed\n",
      "2020-12-23 02:55:45,130 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:45,130 : INFO : entropies processed\n",
      "2020-12-23 02:55:45,131 : INFO : extropies processed\n",
      "2020-12-23 02:55:45,131 : INFO : token count processed\n",
      "2020-12-23 02:55:45,132 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:45,133 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:45,133 : INFO : vocab #2480\n",
      "2020-12-23 02:55:45,134 : INFO : diff #set()\n",
      "2020-12-23 02:55:45,509 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:45,637 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.1280284786681283, 0.4699185231890652], [0.6996867060661316, 0.3003133], [1.8423709931771084, 1.1893232685884285], [3.8868421881310113, 6.24862851613934, 6.399124619344371, 3.736346084925981, 2.5122824312133596, 0.1504961032050307]]\n",
      "2020-12-23 02:55:45,640 : INFO : Removed 0 and 32 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:45,640 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:45,641 : INFO : built Dictionary(93 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 387 corpus positions)\n",
      "2020-12-23 02:55:45,674 : INFO : token count processed\n",
      "2020-12-23 02:55:45,680 : INFO : frequencies processed\n",
      "2020-12-23 02:55:45,811 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:45,812 : INFO : entropies processed\n",
      "2020-12-23 02:55:45,812 : INFO : extropies processed\n",
      "2020-12-23 02:55:45,814 : INFO : token count processed\n",
      "2020-12-23 02:55:45,814 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:45,815 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:45,816 : INFO : vocab #2480\n",
      "2020-12-23 02:55:45,817 : INFO : diff #set()\n",
      "2020-12-23 02:55:46,086 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:46,215 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.188117727101489, 0.4570138012293604], [0.8140287697315216, 0.18597123], [1.9219280948873623, 1.2148067842293933], [3.8868421881310113, 5.850156917433494, 6.024976933382196, 3.7120221721823095, 2.1381347452511847, 0.17482001594870233]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:55:46,217 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:46,218 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:46,219 : INFO : built Dictionary(89 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 385 corpus positions)\n",
      "2020-12-23 02:55:46,253 : INFO : token count processed\n",
      "2020-12-23 02:55:46,255 : INFO : frequencies processed\n",
      "2020-12-23 02:55:46,389 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:46,390 : INFO : entropies processed\n",
      "2020-12-23 02:55:46,390 : INFO : extropies processed\n",
      "2020-12-23 02:55:46,392 : INFO : token count processed\n",
      "2020-12-23 02:55:46,393 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:46,394 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:46,394 : INFO : vocab #2480\n",
      "2020-12-23 02:55:46,395 : INFO : diff #set()\n",
      "2020-12-23 02:55:46,653 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:46,781 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.1520987357041532, 0.46466269572562446], [0.7927204370498657, 0.20727956], [1.5, 1.1225562489182657], [3.8868421881310113, 5.6831976040360095, 5.863523939985333, 3.7065158521816883, 1.9766817518543216, 0.18032633594932346]]\n",
      "2020-12-23 02:55:46,783 : INFO : Removed 0 and 52 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:46,784 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:46,785 : INFO : built Dictionary(75 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 363 corpus positions)\n",
      "2020-12-23 02:55:46,807 : INFO : token count processed\n",
      "2020-12-23 02:55:46,810 : INFO : frequencies processed\n",
      "2020-12-23 02:55:46,949 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:46,950 : INFO : entropies processed\n",
      "2020-12-23 02:55:46,951 : INFO : extropies processed\n",
      "2020-12-23 02:55:46,953 : INFO : token count processed\n",
      "2020-12-23 02:55:46,955 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:46,956 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:46,957 : INFO : vocab #2480\n",
      "2020-12-23 02:55:46,959 : INFO : diff #set()\n",
      "2020-12-23 02:55:47,222 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:47,351 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.165657556122618, 0.46175352015966686], [0.749151885509491, 0.2508481], [1.9182958340544896, 1.2183406773511978], [3.8868421881310113, 5.749308601266266, 5.920942350015423, 3.715208439381854, 2.0341001618844117, 0.17163374874915682]]\n",
      "2020-12-23 02:55:47,353 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:47,354 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:47,355 : INFO : built Dictionary(69 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 244 corpus positions)\n",
      "2020-12-23 02:55:47,380 : INFO : token count processed\n",
      "2020-12-23 02:55:47,382 : INFO : frequencies processed\n",
      "2020-12-23 02:55:47,510 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:47,511 : INFO : entropies processed\n",
      "2020-12-23 02:55:47,512 : INFO : extropies processed\n",
      "2020-12-23 02:55:47,513 : INFO : token count processed\n",
      "2020-12-23 02:55:47,514 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:47,515 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:47,515 : INFO : vocab #2480\n",
      "2020-12-23 02:55:47,516 : INFO : diff #set()\n",
      "2020-12-23 02:55:47,774 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:47,903 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.1700319574223847, 0.4608227065871526], [0.816268265247345, 0.18373173], [1.584962500721156, 1.1699250014423124], [3.8868421881310113, 5.015422548793484, 5.331626276363904, 3.570638460560591, 1.4447840882328928, 0.3162037275704206]]\n",
      "2020-12-23 02:55:47,905 : INFO : Removed 0 and 81 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:47,906 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:47,907 : INFO : built Dictionary(97 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 370 corpus positions)\n",
      "2020-12-23 02:55:47,937 : INFO : token count processed\n",
      "2020-12-23 02:55:47,939 : INFO : frequencies processed\n",
      "2020-12-23 02:55:48,066 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:48,067 : INFO : entropies processed\n",
      "2020-12-23 02:55:48,070 : INFO : extropies processed\n",
      "2020-12-23 02:55:48,071 : INFO : token count processed\n",
      "2020-12-23 02:55:48,072 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:48,073 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:48,073 : INFO : vocab #2480\n",
      "2020-12-23 02:55:48,074 : INFO : diff #set()\n",
      "2020-12-23 02:55:48,348 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:48,479 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.1693560836128698, 0.4609662782214106], [0.7795193642377853, 0.22048064], [1.9219280948873623, 1.2148067842293933], [3.8868421881310113, 6.030001281822029, 6.190111393773025, 3.7267320761800153, 2.3032692056420134, 0.16011011195099556]]\n",
      "2020-12-23 02:55:48,481 : INFO : Removed 0 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:48,482 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:48,484 : INFO : built Dictionary(83 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 333 corpus positions)\n",
      "2020-12-23 02:55:48,514 : INFO : token count processed\n",
      "2020-12-23 02:55:48,516 : INFO : frequencies processed\n",
      "2020-12-23 02:55:48,643 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:48,644 : INFO : entropies processed\n",
      "2020-12-23 02:55:48,645 : INFO : extropies processed\n",
      "2020-12-23 02:55:48,646 : INFO : token count processed\n",
      "2020-12-23 02:55:48,647 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:48,648 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:48,648 : INFO : vocab #2480\n",
      "2020-12-23 02:55:48,649 : INFO : diff #set()\n",
      "2020-12-23 02:55:48,915 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:49,044 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.0916314110411476, 0.47809570783899824], [0.6415593028068542, 0.3584407], [2.446439344671015, 1.2856945251022454], [3.8868421881310113, 5.9537092545441395, 6.084405088176117, 3.756146354499034, 2.197562900045106, 0.1306958336319779]]\n",
      "2020-12-23 02:55:49,046 : INFO : Removed 0 and 90 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:49,047 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:49,049 : INFO : built Dictionary(94 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 334 corpus positions)\n",
      "2020-12-23 02:55:49,087 : INFO : token count processed\n",
      "2020-12-23 02:55:49,091 : INFO : frequencies processed\n",
      "2020-12-23 02:55:49,219 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:49,220 : INFO : entropies processed\n",
      "2020-12-23 02:55:49,220 : INFO : extropies processed\n",
      "2020-12-23 02:55:49,221 : INFO : token count processed\n",
      "2020-12-23 02:55:49,223 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:49,223 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:49,224 : INFO : vocab #2480\n",
      "2020-12-23 02:55:49,225 : INFO : diff #set()\n",
      "2020-12-23 02:55:49,481 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:49,609 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.1091638636467838, 0.47412153092315035], [0.6942522823810577, 0.30574772], [2.4464393446710155, 1.2856945251022454], [3.8868421881310113, 6.184756445474906, 6.304384765253943, 3.767213868351975, 2.4175425771229313, 0.11962831977903665]]\n",
      "2020-12-23 02:55:49,612 : INFO : Removed 0 and 56 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:49,613 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:49,614 : INFO : built Dictionary(105 unique tokens: ['chang', 'check', 'dict', 'function', 'imag']...) from 2 documents (total 447 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:55:49,648 : INFO : token count processed\n",
      "2020-12-23 02:55:49,651 : INFO : frequencies processed\n",
      "2020-12-23 02:55:49,779 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:49,780 : INFO : entropies processed\n",
      "2020-12-23 02:55:49,781 : INFO : extropies processed\n",
      "2020-12-23 02:55:49,782 : INFO : token count processed\n",
      "2020-12-23 02:55:49,783 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:49,784 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:49,785 : INFO : vocab #2480\n",
      "2020-12-23 02:55:49,786 : INFO : diff #set()\n",
      "2020-12-23 02:55:50,043 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:50,170 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.1888340816829757, 0.45686423122172354], [0.7973712980747223, 0.2026287], [1.9219280948873623, 1.2148067842293933], [3.8868421881310113, 6.212221456585881, 6.3524325910236445, 3.746631053693248, 2.465590402892633, 0.1402111344377639]]\n",
      "2020-12-23 02:55:50,173 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:50,173 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:50,175 : INFO : built Dictionary(120 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 416 corpus positions)\n",
      "2020-12-23 02:55:50,196 : INFO : token count processed\n",
      "2020-12-23 02:55:50,199 : INFO : frequencies processed\n",
      "2020-12-23 02:55:50,326 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:50,327 : INFO : entropies processed\n",
      "2020-12-23 02:55:50,328 : INFO : extropies processed\n",
      "2020-12-23 02:55:50,329 : INFO : token count processed\n",
      "2020-12-23 02:55:50,330 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:50,331 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:50,332 : INFO : vocab #2480\n",
      "2020-12-23 02:55:50,333 : INFO : diff #set()\n",
      "2020-12-23 02:55:50,593 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:50,720 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.2283031843088, 0.4487719656112197], [0.8985778987407684, 0.1014221], [0.9182958340544896, 0.9182958340544896], [2.94770277922009, 6.301552355933639, 6.361288958610674, 2.8879661765430544, 3.4135861793905837, 0.05973660267703451]]\n",
      "2020-12-23 02:55:50,722 : INFO : Removed 0 and 55 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:50,723 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:50,724 : INFO : built Dictionary(155 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 652 corpus positions)\n",
      "2020-12-23 02:55:50,755 : INFO : token count processed\n",
      "2020-12-23 02:55:50,758 : INFO : frequencies processed\n",
      "2020-12-23 02:55:50,884 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:50,885 : INFO : entropies processed\n",
      "2020-12-23 02:55:50,886 : INFO : extropies processed\n",
      "2020-12-23 02:55:50,887 : INFO : token count processed\n",
      "2020-12-23 02:55:50,888 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:50,889 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:50,890 : INFO : vocab #2480\n",
      "2020-12-23 02:55:50,891 : INFO : diff #set()\n",
      "2020-12-23 02:55:51,163 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:51,293 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.1096988555733913, 0.47400129992875767], [0.6765555441379547, 0.32344446], [2.251629167387823, 1.2667563532600834], [2.94770277922009, 6.739005504021667, 6.753559136430226, 2.9331491468115303, 3.8058563572101365, 0.014553632408559558]]\n",
      "2020-12-23 02:55:51,296 : INFO : Removed 0 and 123 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:51,297 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:51,298 : INFO : built Dictionary(103 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 503 corpus positions)\n",
      "2020-12-23 02:55:51,322 : INFO : token count processed\n",
      "2020-12-23 02:55:51,327 : INFO : frequencies processed\n",
      "2020-12-23 02:55:51,455 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:51,456 : INFO : entropies processed\n",
      "2020-12-23 02:55:51,456 : INFO : extropies processed\n",
      "2020-12-23 02:55:51,458 : INFO : token count processed\n",
      "2020-12-23 02:55:51,459 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:51,460 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:51,461 : INFO : vocab #2480\n",
      "2020-12-23 02:55:51,462 : INFO : diff #set()\n",
      "2020-12-23 02:55:51,723 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:51,852 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.1953038098414088, 0.45551781740507297], [0.8601993918418884, 0.13980061], [0.0, 0.0], [2.94770277922009, 5.870833373337847, 5.92862293960515, 2.889913212952786, 2.98092016038506, 0.057789566267302916]]\n",
      "2020-12-23 02:55:51,854 : INFO : Removed 0 and 14 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:51,855 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:51,856 : INFO : built Dictionary(62 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 186 corpus positions)\n",
      "2020-12-23 02:55:51,866 : INFO : token count processed\n",
      "2020-12-23 02:55:51,868 : INFO : frequencies processed\n",
      "2020-12-23 02:55:51,996 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:51,996 : INFO : entropies processed\n",
      "2020-12-23 02:55:51,997 : INFO : extropies processed\n",
      "2020-12-23 02:55:51,998 : INFO : token count processed\n",
      "2020-12-23 02:55:51,999 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:52,000 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:52,001 : INFO : vocab #2480\n",
      "2020-12-23 02:55:52,002 : INFO : diff #set()\n",
      "2020-12-23 02:55:52,260 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:52,388 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.1768271437981448, 0.45938420184121387], [0.8665878176689148, 0.13341218], [1.0, 1.0], [2.94770277922009, 5.371881234145534, 5.494283127449302, 2.825300885916322, 2.546580348229212, 0.12240189330376783]]\n",
      "2020-12-23 02:55:52,391 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:52,392 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:52,393 : INFO : built Dictionary(54 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 138 corpus positions)\n",
      "2020-12-23 02:55:52,402 : INFO : token count processed\n",
      "2020-12-23 02:55:52,404 : INFO : frequencies processed\n",
      "2020-12-23 02:55:52,533 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:52,533 : INFO : entropies processed\n",
      "2020-12-23 02:55:52,534 : INFO : extropies processed\n",
      "2020-12-23 02:55:52,536 : INFO : token count processed\n",
      "2020-12-23 02:55:52,537 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:52,539 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:52,539 : INFO : vocab #2480\n",
      "2020-12-23 02:55:52,540 : INFO : diff #set()\n",
      "2020-12-23 02:55:52,806 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:52,937 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.1875034990543256, 0.45714212591308206], [0.9002650231122971, 0.09973498], [0.0, 0.0], [2.94770277922009, 4.85108279267097, 5.0478857265299135, 2.750899845361146, 2.1001829473098237, 0.19680293385894387]]\n",
      "2020-12-23 02:55:52,939 : INFO : Removed 0 and 127 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:52,940 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:52,941 : INFO : built Dictionary(94 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 411 corpus positions)\n",
      "2020-12-23 02:55:52,960 : INFO : token count processed\n",
      "2020-12-23 02:55:52,966 : INFO : frequencies processed\n",
      "2020-12-23 02:55:53,095 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:53,096 : INFO : entropies processed\n",
      "2020-12-23 02:55:53,096 : INFO : extropies processed\n",
      "2020-12-23 02:55:53,097 : INFO : token count processed\n",
      "2020-12-23 02:55:53,098 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:53,099 : INFO : alphabet_target #2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:55:53,099 : INFO : vocab #2480\n",
      "2020-12-23 02:55:53,100 : INFO : diff #set()\n",
      "2020-12-23 02:55:53,356 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:53,485 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.2128414418263636, 0.4519076609369049], [0.8764931485056877, 0.12350685], [0.0, 0.0], [2.94770277922009, 6.139571208108155, 6.200412724207021, 2.886861263121225, 3.252709944986931, 0.06084151609886579]]\n",
      "2020-12-23 02:55:53,487 : INFO : Removed 0 and 38 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:53,488 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:53,490 : INFO : built Dictionary(80 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 421 corpus positions)\n",
      "2020-12-23 02:55:53,511 : INFO : token count processed\n",
      "2020-12-23 02:55:53,514 : INFO : frequencies processed\n",
      "2020-12-23 02:55:53,642 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:53,643 : INFO : entropies processed\n",
      "2020-12-23 02:55:53,643 : INFO : extropies processed\n",
      "2020-12-23 02:55:53,644 : INFO : token count processed\n",
      "2020-12-23 02:55:53,645 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:53,646 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:53,646 : INFO : vocab #2480\n",
      "2020-12-23 02:55:53,647 : INFO : diff #set()\n",
      "2020-12-23 02:55:53,910 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:54,038 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.107124350983828, 0.47458043922898735], [0.7970699667930603, 0.20293003], [0.0, 0.0], [2.94770277922009, 5.609710627339259, 5.683424893147359, 2.873988513411989, 2.7357221139272694, 0.07371426580809981]]\n",
      "2020-12-23 02:55:54,041 : INFO : Removed 0 and 408 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:54,042 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:54,043 : INFO : built Dictionary(170 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 1081 corpus positions)\n",
      "2020-12-23 02:55:54,079 : INFO : token count processed\n",
      "2020-12-23 02:55:54,081 : INFO : frequencies processed\n",
      "2020-12-23 02:55:54,209 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:54,210 : INFO : entropies processed\n",
      "2020-12-23 02:55:54,211 : INFO : extropies processed\n",
      "2020-12-23 02:55:54,213 : INFO : token count processed\n",
      "2020-12-23 02:55:54,213 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:54,214 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:54,215 : INFO : vocab #2480\n",
      "2020-12-23 02:55:54,216 : INFO : diff #set()\n",
      "2020-12-23 02:55:54,474 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:54,601 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.1916462084055042, 0.4562780234167145], [0.8148604482412338, 0.18513955], [1.584962500721156, 1.1699250014423124], [2.94770277922009, 7.2441902753576075, 7.262585133941439, 2.9293079206362584, 4.314882354721349, 0.018394858583831386]]\n",
      "2020-12-23 02:55:54,604 : INFO : Removed 0 and 136 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:54,605 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:54,606 : INFO : built Dictionary(130 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 688 corpus positions)\n",
      "2020-12-23 02:55:54,631 : INFO : token count processed\n",
      "2020-12-23 02:55:54,633 : INFO : frequencies processed\n",
      "2020-12-23 02:55:54,761 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:54,761 : INFO : entropies processed\n",
      "2020-12-23 02:55:54,762 : INFO : extropies processed\n",
      "2020-12-23 02:55:54,764 : INFO : token count processed\n",
      "2020-12-23 02:55:54,765 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:54,766 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:54,766 : INFO : vocab #2480\n",
      "2020-12-23 02:55:54,767 : INFO : diff #set()\n",
      "2020-12-23 02:55:55,026 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:55,154 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.0643781629178277, 0.4844073716545145], [0.6978031396865845, 0.30219686], [2.251629167387823, 1.2667563532600834], [2.94770277922009, 6.2567074920449475, 6.2736919965333655, 2.930718274731671, 3.3259892173132757, 0.01698450448841804]]\n",
      "2020-12-23 02:55:55,157 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:55,157 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:55,158 : INFO : built Dictionary(78 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 198 corpus positions)\n",
      "2020-12-23 02:55:55,171 : INFO : token count processed\n",
      "2020-12-23 02:55:55,174 : INFO : frequencies processed\n",
      "2020-12-23 02:55:55,302 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:55,303 : INFO : entropies processed\n",
      "2020-12-23 02:55:55,303 : INFO : extropies processed\n",
      "2020-12-23 02:55:55,305 : INFO : token count processed\n",
      "2020-12-23 02:55:55,306 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:55,307 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:55,308 : INFO : vocab #2480\n",
      "2020-12-23 02:55:55,310 : INFO : diff #set()\n",
      "2020-12-23 02:55:55,581 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:55,712 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.1047629844744733, 0.47511287844587613], [0.7241228520870209, 0.27587715], [0.9182958340544896, 0.9182958340544896], [2.94770277922009, 5.7680018917339435, 5.852600569700717, 2.863104101253316, 2.904897790480627, 0.08459867796677312]]\n",
      "2020-12-23 02:55:55,715 : INFO : Removed 0 and 113 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:55,716 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:55,718 : INFO : built Dictionary(173 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 739 corpus positions)\n",
      "2020-12-23 02:55:55,757 : INFO : token count processed\n",
      "2020-12-23 02:55:55,760 : INFO : frequencies processed\n",
      "2020-12-23 02:55:55,889 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:55,889 : INFO : entropies processed\n",
      "2020-12-23 02:55:55,890 : INFO : extropies processed\n",
      "2020-12-23 02:55:55,892 : INFO : token count processed\n",
      "2020-12-23 02:55:55,893 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:55,894 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:55,895 : INFO : vocab #2480\n",
      "2020-12-23 02:55:55,896 : INFO : diff #set()\n",
      "2020-12-23 02:55:56,158 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:56,286 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.1208964359647777, 0.4714987413070495], [0.6871868073940277, 0.3128132], [2.251629167387823, 1.2667563532600834], [2.94770277922009, 6.846479111193757, 6.86089818283915, 2.933283707574696, 3.91319540361906, 0.014419071645392911]]\n",
      "2020-12-23 02:55:56,288 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:56,289 : INFO : built Dictionary(28 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 42 corpus positions)\n",
      "2020-12-23 02:55:56,294 : INFO : token count processed\n",
      "2020-12-23 02:55:56,296 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:55:56,297 : INFO : frequencies processed\n",
      "2020-12-23 02:55:56,298 : INFO : token count processed\n",
      "2020-12-23 02:55:56,298 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:56,299 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:56,300 : INFO : vocab #2480\n",
      "2020-12-23 02:55:56,301 : INFO : diff #set()\n",
      "2020-12-23 02:55:56,559 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:56,687 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2312119486098179, 0.4481869150185671], [0.9257733672857285, 0.07422663], [nan, nan], [2.94770277922009, 4.165013816065912, 4.653756708287002, 2.4589598869989997, 1.7060539290669121, 0.48874289222109013]]\n",
      "2020-12-23 02:55:56,689 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:55:56,690 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:56,692 : INFO : built Dictionary(55 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 149 corpus positions)\n",
      "2020-12-23 02:55:56,707 : INFO : token count processed\n",
      "2020-12-23 02:55:56,710 : INFO : frequencies processed\n",
      "2020-12-23 02:55:56,855 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:56,856 : INFO : entropies processed\n",
      "2020-12-23 02:55:56,856 : INFO : extropies processed\n",
      "2020-12-23 02:55:56,858 : INFO : token count processed\n",
      "2020-12-23 02:55:56,858 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:56,859 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:56,860 : INFO : vocab #2480\n",
      "2020-12-23 02:55:56,861 : INFO : diff #set()\n",
      "2020-12-23 02:55:57,132 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:57,261 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.1249131369466134, 0.4706074721891675], [0.7695065587759018, 0.23049344], [0.0, 0.0], [2.94770277922009, 5.449968864419248, 5.589227306537887, 2.8084443371014514, 2.641524527317797, 0.13925844211863847]]\n",
      "2020-12-23 02:55:57,263 : INFO : Removed 0 and 147 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:57,264 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:57,265 : INFO : built Dictionary(144 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 586 corpus positions)\n",
      "2020-12-23 02:55:57,295 : INFO : token count processed\n",
      "2020-12-23 02:55:57,298 : INFO : frequencies processed\n",
      "2020-12-23 02:55:57,428 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:57,429 : INFO : entropies processed\n",
      "2020-12-23 02:55:57,430 : INFO : extropies processed\n",
      "2020-12-23 02:55:57,431 : INFO : token count processed\n",
      "2020-12-23 02:55:57,432 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:57,433 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:57,434 : INFO : vocab #2480\n",
      "2020-12-23 02:55:57,436 : INFO : diff #set()\n",
      "2020-12-23 02:55:57,702 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:57,831 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.152334258386032, 0.4646118492533165], [0.7802032083272934, 0.21979679], [2.251629167387823, 1.2667563532600834], [2.94770277922009, 6.530294129310484, 6.553777659522339, 2.9242192490082353, 3.6060748803022493, 0.023483530211855452]]\n",
      "2020-12-23 02:55:57,833 : INFO : Removed 0 and 142 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:57,834 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:57,835 : INFO : built Dictionary(121 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 602 corpus positions)\n",
      "2020-12-23 02:55:57,863 : INFO : token count processed\n",
      "2020-12-23 02:55:57,868 : INFO : frequencies processed\n",
      "2020-12-23 02:55:57,997 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:57,997 : INFO : entropies processed\n",
      "2020-12-23 02:55:57,998 : INFO : extropies processed\n",
      "2020-12-23 02:55:57,999 : INFO : token count processed\n",
      "2020-12-23 02:55:58,000 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:58,000 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:58,001 : INFO : vocab #2480\n",
      "2020-12-23 02:55:58,001 : INFO : diff #set()\n",
      "2020-12-23 02:55:58,258 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:58,386 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.2037455759195534, 0.45377289054011216], [0.8702853173017502, 0.12971468], [0.0, 0.0], [2.94770277922009, 6.470272233491701, 6.514456923594583, 2.903518089117208, 3.5667541443744932, 0.04418469010288195]]\n",
      "2020-12-23 02:55:58,389 : INFO : Removed 0 and 53 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:58,390 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:58,391 : INFO : built Dictionary(118 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 391 corpus positions)\n",
      "2020-12-23 02:55:58,415 : INFO : token count processed\n",
      "2020-12-23 02:55:58,417 : INFO : frequencies processed\n",
      "2020-12-23 02:55:58,546 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:58,547 : INFO : entropies processed\n",
      "2020-12-23 02:55:58,548 : INFO : extropies processed\n",
      "2020-12-23 02:55:58,549 : INFO : token count processed\n",
      "2020-12-23 02:55:58,551 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:58,552 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:58,553 : INFO : vocab #2480\n",
      "2020-12-23 02:55:58,554 : INFO : diff #set()\n",
      "2020-12-23 02:55:58,815 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:58,944 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.0833613219848677, 0.47999355150131917], [0.6552702188491821, 0.34472978], [2.251629167387823, 1.2667563532600834], [2.94770277922009, 6.550038223589686, 6.567570923579305, 2.9301700792304697, 3.6198681443592156, 0.01753269998961926]]\n",
      "2020-12-23 02:55:58,946 : INFO : Removed 0 and 70 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:58,947 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:58,949 : INFO : built Dictionary(77 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 307 corpus positions)\n",
      "2020-12-23 02:55:58,969 : INFO : token count processed\n",
      "2020-12-23 02:55:58,975 : INFO : frequencies processed\n",
      "2020-12-23 02:55:59,103 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:55:59,104 : INFO : entropies processed\n",
      "2020-12-23 02:55:59,105 : INFO : extropies processed\n",
      "2020-12-23 02:55:59,107 : INFO : token count processed\n",
      "2020-12-23 02:55:59,108 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:59,109 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:59,111 : INFO : vocab #2480\n",
      "2020-12-23 02:55:59,112 : INFO : diff #set()\n",
      "2020-12-23 02:55:59,369 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:59,496 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.1785757954416518, 0.45901547336216275], [0.8610653579235077, 0.13893464], [0.0, 0.0], [2.94770277922009, 5.860525481261383, 5.939593623745815, 2.868634636735657, 2.9918908445257255, 0.07906814248443261]]\n",
      "2020-12-23 02:55:59,498 : INFO : Removed 0 and 168 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:59,499 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:59,500 : INFO : built Dictionary(49 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 345 corpus positions)\n",
      "2020-12-23 02:55:59,509 : INFO : token count processed\n",
      "2020-12-23 02:55:59,512 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:55:59,512 : INFO : frequencies processed\n",
      "2020-12-23 02:55:59,514 : INFO : token count processed\n",
      "2020-12-23 02:55:59,514 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:55:59,515 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:55:59,516 : INFO : vocab #2480\n",
      "2020-12-23 02:55:59,517 : INFO : diff #set()\n",
      "2020-12-23 02:55:59,776 : INFO : alphabet #2480\n",
      "2020-12-23 02:55:59,904 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.291699780424665, 0.4363573311573536], [0.9656683504581451, 0.03433165], [nan, nan], [2.94770277922009, 5.945464049777852, 6.020290258577968, 2.8728765704199732, 3.0725874793578782, 0.0748262088001157]]\n",
      "2020-12-23 02:55:59,907 : INFO : Removed 0 and 160 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:55:59,908 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:55:59,909 : INFO : built Dictionary(192 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 892 corpus positions)\n",
      "2020-12-23 02:55:59,954 : INFO : token count processed\n",
      "2020-12-23 02:55:59,957 : INFO : frequencies processed\n",
      "2020-12-23 02:56:00,083 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:56:00,084 : INFO : entropies processed\n",
      "2020-12-23 02:56:00,084 : INFO : extropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:56:00,086 : INFO : token count processed\n",
      "2020-12-23 02:56:00,086 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:56:00,087 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:56:00,088 : INFO : vocab #2480\n",
      "2020-12-23 02:56:00,089 : INFO : diff #set()\n",
      "2020-12-23 02:56:00,347 : INFO : alphabet #2480\n",
      "2020-12-23 02:56:00,475 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.1116901531211245, 0.4735543225988803], [0.717448502779007, 0.2825515], [2.251629167387823, 1.2667563532600834], [2.94770277922009, 6.811563897304216, 6.8234833103895385, 2.9357833661347668, 3.8757805311694487, 0.011919413085322184]]\n",
      "2020-12-23 02:56:00,477 : INFO : Removed 0 and 383 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:56:00,478 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:56:00,481 : INFO : built Dictionary(213 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 987 corpus positions)\n",
      "2020-12-23 02:56:00,530 : INFO : token count processed\n",
      "2020-12-23 02:56:00,533 : INFO : frequencies processed\n",
      "2020-12-23 02:56:00,660 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:56:00,661 : INFO : entropies processed\n",
      "2020-12-23 02:56:00,661 : INFO : extropies processed\n",
      "2020-12-23 02:56:00,663 : INFO : token count processed\n",
      "2020-12-23 02:56:00,664 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:56:00,665 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:56:00,665 : INFO : vocab #2480\n",
      "2020-12-23 02:56:00,666 : INFO : diff #set()\n",
      "2020-12-23 02:56:00,926 : INFO : alphabet #2480\n",
      "2020-12-23 02:56:01,053 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.17729323631525, 0.45928586160142293], [0.7807941287755966, 0.21920587], [2.251629167387823, 1.2667563532600834], [2.94770277922009, 7.502034948968415, 7.514535959936327, 2.9352017682521785, 4.5668331807162375, 0.01250101096791223]]\n",
      "2020-12-23 02:56:01,056 : INFO : Removed 0 and 310 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:56:01,057 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:56:01,060 : INFO : built Dictionary(257 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 1560 corpus positions)\n",
      "2020-12-23 02:56:01,130 : INFO : token count processed\n",
      "2020-12-23 02:56:01,133 : INFO : frequencies processed\n",
      "2020-12-23 02:56:01,259 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:56:01,260 : INFO : entropies processed\n",
      "2020-12-23 02:56:01,261 : INFO : extropies processed\n",
      "2020-12-23 02:56:01,263 : INFO : token count processed\n",
      "2020-12-23 02:56:01,263 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:56:01,264 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:56:01,265 : INFO : vocab #2480\n",
      "2020-12-23 02:56:01,266 : INFO : diff #set()\n",
      "2020-12-23 02:56:01,535 : INFO : alphabet #2480\n",
      "2020-12-23 02:56:01,663 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.1366458599587534, 0.4680232783262006], [0.6858486533164978, 0.31415135], [2.251629167387823, 1.2667563532600834], [2.94770277922009, 7.39180093901977, 7.397772975495876, 2.941730742743985, 4.450070196275786, 0.005972036476105558]]\n",
      "2020-12-23 02:56:01,666 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:56:01,667 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:56:01,667 : INFO : built Dictionary(44 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 115 corpus positions)\n",
      "2020-12-23 02:56:01,675 : INFO : token count processed\n",
      "2020-12-23 02:56:01,678 : INFO : frequencies processed\n",
      "2020-12-23 02:56:01,805 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:56:01,806 : INFO : entropies processed\n",
      "2020-12-23 02:56:01,807 : INFO : extropies processed\n",
      "2020-12-23 02:56:01,808 : INFO : token count processed\n",
      "2020-12-23 02:56:01,809 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:56:01,810 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:56:01,811 : INFO : vocab #2480\n",
      "2020-12-23 02:56:01,813 : INFO : diff #set()\n",
      "2020-12-23 02:56:02,074 : INFO : alphabet #2480\n",
      "2020-12-23 02:56:02,202 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.0546074699497143, 0.48671097259491347], [0.7491681277751923, 0.25083187], [1.0, 1.0], [2.94770277922009, 4.927561309677364, 5.090730826232695, 2.784533262664758, 2.1430280470126055, 0.16316951655533174]]\n",
      "2020-12-23 02:56:02,204 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:56:02,205 : INFO : built Dictionary(14 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 16 corpus positions)\n",
      "2020-12-23 02:56:02,208 : INFO : token count processed\n",
      "2020-12-23 02:56:02,210 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2020-12-23 02:56:02,211 : INFO : frequencies processed\n",
      "2020-12-23 02:56:02,212 : INFO : token count processed\n",
      "2020-12-23 02:56:02,213 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:56:02,214 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:56:02,214 : INFO : vocab #2480\n",
      "2020-12-23 02:56:02,215 : INFO : diff #set()\n",
      "2020-12-23 02:56:02,473 : INFO : alphabet #2480\n",
      "2020-12-23 02:56:02,601 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2390121164579377, 0.44662554197427723], [0.8827798739075661, 0.117220126], [nan, nan], [2.94770277922009, 2.5216406363433186, 3.75, 1.719343415563408, 0.8022972207799102, 1.2283593636566814]]\n",
      "2020-12-23 02:56:02,604 : INFO : Removed 0 and 530 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:56:02,605 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:56:02,607 : INFO : built Dictionary(332 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 2888 corpus positions)\n",
      "2020-12-23 02:56:02,690 : INFO : token count processed\n",
      "2020-12-23 02:56:02,693 : INFO : frequencies processed\n",
      "2020-12-23 02:56:02,820 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:56:02,821 : INFO : entropies processed\n",
      "2020-12-23 02:56:02,821 : INFO : extropies processed\n",
      "2020-12-23 02:56:02,824 : INFO : token count processed\n",
      "2020-12-23 02:56:02,825 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:56:02,826 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:56:02,826 : INFO : vocab #2480\n",
      "2020-12-23 02:56:02,827 : INFO : diff #set()\n",
      "2020-12-23 02:56:03,087 : INFO : alphabet #2480\n",
      "2020-12-23 02:56:03,216 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.2201510735369372, 0.45041979886841377], [0.8560263514518738, 0.14397365], [1.9219280948873623, 1.2148067842293933], [2.94770277922009, 7.480007711014331, 7.489285035308743, 2.938425454925678, 4.541582256088653, 0.009277324294412637]]\n",
      "2020-12-23 02:56:03,219 : INFO : Removed 0 and 173 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:56:03,219 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:56:03,221 : INFO : built Dictionary(208 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 1037 corpus positions)\n",
      "2020-12-23 02:56:03,269 : INFO : token count processed\n",
      "2020-12-23 02:56:03,277 : INFO : frequencies processed\n",
      "2020-12-23 02:56:03,409 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:56:03,409 : INFO : entropies processed\n",
      "2020-12-23 02:56:03,410 : INFO : extropies processed\n",
      "2020-12-23 02:56:03,411 : INFO : token count processed\n",
      "2020-12-23 02:56:03,412 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:56:03,413 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:56:03,414 : INFO : vocab #2480\n",
      "2020-12-23 02:56:03,415 : INFO : diff #set()\n",
      "2020-12-23 02:56:03,684 : INFO : alphabet #2480\n",
      "2020-12-23 02:56:03,816 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.1573120414037876, 0.4635398036110198], [0.7536683976650238, 0.2463316], [2.251629167387823, 1.2667563532600834], [2.94770277922009, 7.131331012509435, 7.142241514017526, 2.9367922777119997, 4.194538734797436, 0.01091050150809103]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-23 02:56:03,818 : INFO : Removed 0 and 166 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:56:03,819 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:56:03,822 : INFO : built Dictionary(197 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 856 corpus positions)\n",
      "2020-12-23 02:56:03,871 : INFO : token count processed\n",
      "2020-12-23 02:56:03,876 : INFO : frequencies processed\n",
      "2020-12-23 02:56:04,005 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:56:04,005 : INFO : entropies processed\n",
      "2020-12-23 02:56:04,006 : INFO : extropies processed\n",
      "2020-12-23 02:56:04,008 : INFO : token count processed\n",
      "2020-12-23 02:56:04,009 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:56:04,010 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:56:04,010 : INFO : vocab #2480\n",
      "2020-12-23 02:56:04,011 : INFO : diff #set()\n",
      "2020-12-23 02:56:04,274 : INFO : alphabet #2480\n",
      "2020-12-23 02:56:04,403 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.1608087560816167, 0.46278968334679804], [0.7575841397047043, 0.24241586], [2.251629167387823, 1.2667563532600834], [2.94770277922009, 7.203742744794778, 7.215567517486509, 2.935878006528359, 4.267864738266419, 0.011824772691730878]]\n",
      "2020-12-23 02:56:04,406 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:56:04,406 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:56:04,407 : INFO : built Dictionary(58 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 188 corpus positions)\n",
      "2020-12-23 02:56:04,417 : INFO : token count processed\n",
      "2020-12-23 02:56:04,420 : INFO : frequencies processed\n",
      "2020-12-23 02:56:04,549 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:56:04,549 : INFO : entropies processed\n",
      "2020-12-23 02:56:04,550 : INFO : extropies processed\n",
      "2020-12-23 02:56:04,551 : INFO : token count processed\n",
      "2020-12-23 02:56:04,552 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:56:04,553 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:56:04,554 : INFO : vocab #2480\n",
      "2020-12-23 02:56:04,555 : INFO : diff #set()\n",
      "2020-12-23 02:56:04,815 : INFO : alphabet #2480\n",
      "2020-12-23 02:56:04,943 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.1359836713175464, 0.4681683729272923], [0.8310165852308273, 0.16898341], [1.0, 1.0], [2.94770277922009, 5.195502554608948, 5.31998208115068, 2.823223252678357, 2.3722793019305906, 0.124479526541732]]\n",
      "2020-12-23 02:56:04,945 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2020-12-23 02:56:04,946 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-12-23 02:56:04,948 : INFO : built Dictionary(65 unique tokens: ['argument', 'cic', 'csb', 'directori', 'enforc']...) from 2 documents (total 249 corpus positions)\n",
      "2020-12-23 02:56:04,964 : INFO : token count processed\n",
      "2020-12-23 02:56:04,967 : INFO : frequencies processed\n",
      "2020-12-23 02:56:05,098 : INFO : scalar_distribution processed\n",
      "2020-12-23 02:56:05,099 : INFO : entropies processed\n",
      "2020-12-23 02:56:05,100 : INFO : extropies processed\n",
      "2020-12-23 02:56:05,101 : INFO : token count processed\n",
      "2020-12-23 02:56:05,102 : INFO : alphabet_source #2480\n",
      "2020-12-23 02:56:05,103 : INFO : alphabet_target #2480\n",
      "2020-12-23 02:56:05,104 : INFO : vocab #2480\n",
      "2020-12-23 02:56:05,105 : INFO : diff #set()\n"
     ]
    }
   ],
   "source": [
    "#[step 2]NonGroundTruth Computation\n",
    "metric_list = [DistanceMetric.WMD,DistanceMetric.SCM,EntropyMetric.MSI_I,EntropyMetric.MI]\n",
    "#metric_list = [EntropyMetric.MSI_I,EntropyMetric.MI]\n",
    "word2vec.ComputeDistanceArtifacts( sampling=False, samples = 100, metric_list = metric_list )\n",
    "word2vec.df_nonground_link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_nonground_link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 3]Saving Non-GroundTruth Links\n",
    "word2vec.SaveLinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_nonglinks = ds.mining.ir.LoadLinks(timestamp=1608689944.245888, params=parameters)\n",
    "df_nonglinks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 4]GroundTruthMatching Testing\n",
    "#TODO change the path for a param\n",
    "path_to_ground_truth =  parameters['path_mappings']\n",
    "word2vec.MatchWithGroundTruth(path_to_ground_truth, semeru_format=True)\n",
    "word2vec.df_ground_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 4.1]GroundTruthMatching Testing For CISCO Mappings\n",
    "word2vec.MatchWithGroundTruth(from_mappings=True)\n",
    "word2vec.df_ground_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[optional]GroundTruth Direct Processing\n",
    "ground_links = word2vec.ground_truth_processing(path_to_ground_truth)\n",
    "ground_links[141] # A tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 5]Saving GroundTruth Links\n",
    "word2vec.SaveLinks(grtruth = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_glinks = ds.mining.ir.LoadLinks(timestamp=1608690009.09251, params=parameters,grtruth = True)\n",
    "df_glinks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_build_docs #<-------- [Activate when stable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
