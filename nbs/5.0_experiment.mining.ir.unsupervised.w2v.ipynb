{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiment.mining.ir.unsupervised.w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting Neural Unsupervised Approaches for Software Information Retrieval [w2v]\n",
    "\n",
    "> Just Paper. Full Experimentation. This module is dedicated to experiment with word2vec. Consider to Copy the entire notebook for a new and separeted empirical evaluation. \n",
    "> Implementing mutual information analysis\n",
    "> Author: @danaderp April 2020\n",
    "> Author: @danielrc Nov 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This copy is for Cisco purposes. It was adapted to process private github data from cisco. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds4se.mining.ir import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prg import prg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ds4se as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Similarity with BasicSequenceVectorization\n",
    "\n",
    "We test diferent similarities based on [blog](https://www.kdnuggets.com/2017/08/comparing-distance-measurements-python-scipy.html) and [blog2](https://www.kdnuggets.com/2019/01/comparison-text-distance-metrics.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experients Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../dvc-ds4se/' #dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiments 1.2.2 <<-- word2vec\n",
    "path_model_prefix = path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_128k'\n",
    "path_to_trained_model = path_data+'/models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model'\n",
    "def sacp_params():\n",
    "        return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.issue2src,\n",
    "        \"system\": 'sacp-python-common',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_type\": SoftwareArtifacts.PR.value,\n",
    "        \"target_type\": SoftwareArtifacts.PY.value,\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": '/tf/data/cisco/sacp_data/[sacp-python-common-all-corpus-1609224778.517111].csv',\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','bpe128k'],\n",
    "            \"prep\": Preprocessing.bpe\n",
    "        },\n",
    "        \"path_mappings\": \"/tf/data/cisco/sacp_data/sacp-pr-mappings.csv\",\n",
    "        \"saving_path\": path_data + 'metrics/traceability/experiments1.2.x/',\n",
    "        \"names\": ['Source','Target','Linked?'],\n",
    "        \"model_prefix\": path_model_prefix\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectorizationType': <VectorizationType.word2vec: 1>,\n",
       " 'linkType': <LinkType.issue2src: 3>,\n",
       " 'system': 'sacp-python-common',\n",
       " 'path_to_trained_model': '../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model',\n",
       " 'source_type': 'pr',\n",
       " 'target_type': 'py',\n",
       " 'system_path_config': {'system_path': '/tf/data/cisco/sacp_data/[sacp-python-common-all-corpus-1609224778.517111].csv',\n",
       "  'sep': '~',\n",
       "  'names': ['ids', 'bpe128k'],\n",
       "  'prep': <Preprocessing.bpe: 2>},\n",
       " 'path_mappings': '/tf/data/cisco/sacp_data/sacp-pr-mappings.csv',\n",
       " 'saving_path': '../dvc-ds4se/metrics/traceability/experiments1.2.x/',\n",
       " 'names': ['Source', 'Target', 'Linked?'],\n",
       " 'model_prefix': '../dvc-ds4se/models/bpe/sentencepiece/wiki_py_java_bpe_128k'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = sacp_params()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Similarity with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 03:15:18,924 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:15:18,993 : INFO : built Dictionary(3580 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 362 documents (total 149985 corpus positions)\n",
      "2021-01-15 03:15:19,156 : INFO : bpe preprocessing documents, dictionary, and vocab for the test corpus\n",
      "2021-01-15 03:15:19,157 : INFO : loading Word2Vec object from ../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model\n",
      "2021-01-15 03:15:19,248 : INFO : loading wv recursively from ../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.wv.* with mmap=None\n",
      "2021-01-15 03:15:19,249 : INFO : loading vectors from ../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.wv.vectors.npy with mmap=None\n",
      "2021-01-15 03:15:19,272 : INFO : setting ignored attribute vectors_norm to None\n",
      "2021-01-15 03:15:19,273 : INFO : loading vocabulary recursively from ../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.vocabulary.* with mmap=None\n",
      "2021-01-15 03:15:19,274 : INFO : loading trainables recursively from ../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.trainables.* with mmap=None\n",
      "2021-01-15 03:15:19,274 : INFO : loading syn1neg from ../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.trainables.syn1neg.npy with mmap=None\n",
      "2021-01-15 03:15:19,298 : INFO : setting ignored attribute cum_table to None\n",
      "2021-01-15 03:15:19,298 : INFO : loaded ../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model\n",
      "2021-01-15 03:15:19,376 : INFO : precomputing L2-norms of word weight vectors\n",
      "2021-01-15 03:15:19,416 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7ff47f6c47f0>\n",
      "2021-01-15 03:15:19,418 : INFO : iterating over columns in dictionary order\n",
      "2021-01-15 03:15:19,421 : INFO : PROGRESS: at 0.03% columns (1 / 3580, 0.027933% density, 0.027933% projected density)\n",
      "2021-01-15 03:15:26,728 : INFO : PROGRESS: at 27.96% columns (1001 / 3580, 0.509160% density, 1.749005% projected density)\n",
      "2021-01-15 03:15:33,503 : INFO : PROGRESS: at 55.89% columns (2001 / 3580, 0.777597% density, 1.369162% projected density)\n",
      "2021-01-15 03:15:40,088 : INFO : PROGRESS: at 83.83% columns (3001 / 3580, 1.015808% density, 1.206404% projected density)\n",
      "2021-01-15 03:15:43,610 : INFO : constructed a sparse term similarity matrix with 1.097687% density\n"
     ]
    }
   ],
   "source": [
    "#[step 1]Creating the Vectorization Class\n",
    "word2vec = ds.mining.ir.Word2VecSeqVect( params = parameters, logging = logging )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 03:15:43,719 : INFO : Removed 1 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 03:15:43,720 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:15:43,721 : INFO : built Dictionary(261 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 2 documents (total 1208 corpus positions)\n",
      "2021-01-15 03:15:43,875 : INFO : token count processed\n",
      "2021-01-15 03:15:44,024 : INFO : frequencies processed\n",
      "2021-01-15 03:21:01,581 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:21:01,582 : INFO : entropies processed\n",
      "2021-01-15 03:21:01,583 : INFO : extropies processed\n",
      "2021-01-15 03:21:01,647 : INFO : token count processed\n",
      "2021-01-15 03:21:01,665 : INFO : alphabet_source #128006\n",
      "2021-01-15 03:21:01,681 : INFO : alphabet_target #128010\n",
      "2021-01-15 03:21:01,682 : INFO : vocab #128006\n",
      "2021-01-15 03:21:01,708 : INFO : diff #set()\n",
      "2021-01-15 03:31:37,351 : INFO : alphabet #128006\n",
      "2021-01-15 03:36:53,833 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.248352530937981, 0.44477010888626706], [0.9333877861499786, 0.066612214], [0.9182958340544896, 0.9182958340544896], [4.328599539040249, 6.786873156936815, 6.988899732877526, 4.126572963099539, 2.6603001938372763, 0.20202657594071116]]\n",
      "2021-01-15 03:36:53,838 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 03:36:53,839 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:36:53,843 : INFO : built Dictionary(358 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 2 documents (total 2013 corpus positions)\n",
      "2021-01-15 03:36:54,091 : INFO : token count processed\n",
      "2021-01-15 03:36:54,242 : INFO : frequencies processed\n",
      "2021-01-15 03:42:10,041 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:42:10,042 : INFO : entropies processed\n",
      "2021-01-15 03:42:10,043 : INFO : extropies processed\n",
      "2021-01-15 03:42:10,087 : INFO : token count processed\n",
      "2021-01-15 03:42:10,107 : INFO : alphabet_source #128006\n",
      "2021-01-15 03:42:10,124 : INFO : alphabet_target #128009\n",
      "2021-01-15 03:42:10,125 : INFO : vocab #128006\n",
      "2021-01-15 03:42:10,153 : INFO : diff #set()\n",
      "2021-01-15 03:52:39,205 : INFO : alphabet #128006\n",
      "2021-01-15 03:57:56,527 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.2472553644112103, 0.44498725682739837], [0.9242691695690155, 0.07573083], [0.9182958340544896, 0.9182958340544896], [4.328599539040249, 7.0224182909882895, 7.30700798312403, 4.044009846904509, 2.9784084440837804, 0.28458969213574026]]\n",
      "2021-01-15 03:57:56,532 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 03:57:56,533 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:57:56,538 : INFO : built Dictionary(282 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 2 documents (total 1857 corpus positions)\n",
      "2021-01-15 03:57:56,706 : INFO : token count processed\n",
      "2021-01-15 03:57:56,831 : INFO : frequencies processed\n",
      "2021-01-15 04:03:14,847 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:03:14,848 : INFO : entropies processed\n",
      "2021-01-15 04:03:14,849 : INFO : extropies processed\n",
      "2021-01-15 04:03:14,886 : INFO : token count processed\n",
      "2021-01-15 04:03:14,901 : INFO : alphabet_source #128006\n",
      "2021-01-15 04:03:14,916 : INFO : alphabet_target #128009\n",
      "2021-01-15 04:03:14,917 : INFO : vocab #128006\n",
      "2021-01-15 04:03:14,941 : INFO : diff #set()\n",
      "2021-01-15 04:13:50,775 : INFO : alphabet #128006\n",
      "2021-01-15 04:19:07,506 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2498384683188444, 0.444476354227881], [0.9322489202022552, 0.06775108], [1.9182958340544896, 1.2183406773511978], [4.328599539040249, 6.44252055201325, 6.580975370451398, 4.1901447206021025, 2.2523758314111486, 0.13845481843814778]]\n",
      "2021-01-15 04:19:07,510 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 04:19:07,511 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:19:07,515 : INFO : built Dictionary(153 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 2 documents (total 561 corpus positions)\n",
      "2021-01-15 04:19:07,603 : INFO : token count processed\n",
      "2021-01-15 04:19:07,751 : INFO : frequencies processed\n",
      "2021-01-15 04:24:24,257 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:24:24,258 : INFO : entropies processed\n",
      "2021-01-15 04:24:24,259 : INFO : extropies processed\n",
      "2021-01-15 04:24:24,295 : INFO : token count processed\n",
      "2021-01-15 04:24:24,312 : INFO : alphabet_source #128006\n",
      "2021-01-15 04:24:24,328 : INFO : alphabet_target #128009\n",
      "2021-01-15 04:24:24,329 : INFO : vocab #128006\n",
      "2021-01-15 04:24:24,355 : INFO : diff #set()\n",
      "2021-01-15 04:35:01,027 : INFO : alphabet #128006\n",
      "2021-01-15 04:40:18,264 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.2471384784196713, 0.4450104030541378], [0.9349607676267624, 0.06503923], [0.0, 0.0], [4.328599539040249, 5.855292654715939, 6.11548592468311, 4.068406269073078, 1.7868863856428607, 0.2601932699671714]]\n",
      "2021-01-15 04:40:18,268 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 04:40:18,269 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:40:18,273 : INFO : built Dictionary(130 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 2 documents (total 353 corpus positions)\n",
      "2021-01-15 04:40:18,344 : INFO : token count processed\n",
      "2021-01-15 04:40:18,488 : INFO : frequencies processed\n",
      "2021-01-15 04:45:34,918 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:45:34,919 : INFO : entropies processed\n",
      "2021-01-15 04:45:34,920 : INFO : extropies processed\n",
      "2021-01-15 04:45:34,959 : INFO : token count processed\n",
      "2021-01-15 04:45:34,975 : INFO : alphabet_source #128006\n",
      "2021-01-15 04:45:34,991 : INFO : alphabet_target #128009\n",
      "2021-01-15 04:45:34,992 : INFO : vocab #128006\n",
      "2021-01-15 04:45:35,018 : INFO : diff #set()\n",
      "2021-01-15 04:56:06,431 : INFO : alphabet #128006\n",
      "2021-01-15 05:01:21,527 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.2564126497563755, 0.44318134810490883], [0.945445828139782, 0.05455417], [0.0, 0.0], [4.328599539040249, 5.781790887408139, 6.071398673158924, 4.038991753289465, 1.7427991341186742, 0.2896077857507846]]\n",
      "2021-01-15 05:01:21,533 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 05:01:21,534 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 05:01:21,538 : INFO : built Dictionary(232 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 2 documents (total 1812 corpus positions)\n",
      "2021-01-15 05:01:21,673 : INFO : token count processed\n",
      "2021-01-15 05:01:21,823 : INFO : frequencies processed\n",
      "2021-01-15 05:06:37,493 : INFO : scalar_distribution processed\n",
      "2021-01-15 05:06:37,494 : INFO : entropies processed\n",
      "2021-01-15 05:06:37,495 : INFO : extropies processed\n",
      "2021-01-15 05:06:37,534 : INFO : token count processed\n",
      "2021-01-15 05:06:37,551 : INFO : alphabet_source #128006\n",
      "2021-01-15 05:06:37,567 : INFO : alphabet_target #128009\n",
      "2021-01-15 05:06:37,568 : INFO : vocab #128006\n",
      "2021-01-15 05:06:37,594 : INFO : diff #set()\n"
     ]
    }
   ],
   "source": [
    "#[step 2]NonGroundTruth Computation\n",
    "metric_list = [DistanceMetric.WMD,DistanceMetric.SCM,EntropyMetric.MSI_I,EntropyMetric.MI]\n",
    "#metric_list = [EntropyMetric.MSI_I,EntropyMetric.MI]\n",
    "word2vec.ComputeDistanceArtifacts( sampling=False, samples = 100, metric_list = metric_list )\n",
    "word2vec.df_nonground_link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_nonground_link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_nonground_link['Target'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 3]Saving Non-GroundTruth Links\n",
    "word2vec.SaveLinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_nonglinks = ds.mining.ir.LoadLinks(timestamp=1610679697.733263, params=parameters, logging=logging)\n",
    "df_nonglinks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_nonground_link = df_nonglinks # Only to load links from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 4]GroundTruthMatching Testing\n",
    "#TODO change the path for a param\n",
    "path_to_ground_truth =  parameters['path_mappings']\n",
    "word2vec.MatchWithGroundTruth(path_to_ground_truth, semeru_format=True)\n",
    "word2vec.df_ground_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_ground_link[word2vec.df_ground_link ['Linked?']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_ground_link['Source'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Only SACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 4.1]GroundTruthMatching Testing For CISCO Mappings\n",
    "word2vec.MatchWithGroundTruth(from_mappings=True)\n",
    "word2vec.df_ground_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[optional]GroundTruth Direct Processing\n",
    "ground_links = word2vec.ground_truth_processing(path_to_ground_truth)\n",
    "ground_links # A tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 5]Saving GroundTruth Links\n",
    "word2vec.SaveLinks(grtruth = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_glinks = ds.mining.ir.LoadLinks(timestamp=1610679764.287777, params=parameters,grtruth = True, logging=logging)\n",
    "df_glinks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_build_docs #<-------- [Activate when stable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
