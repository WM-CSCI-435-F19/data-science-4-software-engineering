{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "structural-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp mgmnt.prep.token_mgmnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "social-pottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from typing import Dict, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-pioneer",
   "metadata": {},
   "source": [
    "### Pre-processing methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-fellowship",
   "metadata": {},
   "source": [
    "> @Alvaro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "super-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# dicts of special tokens we are adding to the tokenizers so they do not get split\n",
    "\n",
    "extra_tokens = {\"<n>\": \"\\n\"}\n",
    "\n",
    "# from https://docs.oracle.com/javase/tutorial/java/nutsandbolts/_keywords.html\n",
    "java_reserved_tokens = {\n",
    "    \"<abstract>\": \"abstract\",\n",
    "    \"<assert>\": \"assert\",\n",
    "    \"<boolean>\": \"boolean\",\n",
    "    \"<break>\": \"break\",\n",
    "    \"<byte>\": \"byte\",\n",
    "    \"<case>\": \"case\",\n",
    "    \"<catch>\": \"catch\",\n",
    "    \"<char>\": \"char\",\n",
    "    \"<class>\": \"class\",\n",
    "    \"<const>\": \"const\",\n",
    "    \"<continue>\": \"continue\",\n",
    "    \"<default>\": \"default\",\n",
    "    \"<do>\": \"do\",\n",
    "    \"<double>\": \"double\",\n",
    "    \"<else>\": \"else\",\n",
    "    \"<enum>\": \"enum\",\n",
    "    \"<extends>\": \"extends\",\n",
    "    \"<final>\": \"final\",\n",
    "    \"<finally>\": \"finally\",\n",
    "    \"<float>\": \"float\",\n",
    "    \"<for>\": \"for\",\n",
    "    \"<goto>\": \"goto\",\n",
    "    \"<if>\": \"if\",\n",
    "    \"<implements>\": \"implements\",\n",
    "    \"<import>\": \"import\",\n",
    "    \"<instanceof>\": \"instanceof\",\n",
    "    \"<int>\": \"int\",\n",
    "    \"<interface>\": \"interface\",\n",
    "    \"<long>\": \"long\",\n",
    "    \"<native>\": \"native\",\n",
    "    \"<new>\": \"new\",\n",
    "    \"<package>\": \"package\",\n",
    "    \"<private>\": \"private\",\n",
    "    \"<protected>\": \"protected\",\n",
    "    \"<public>\": \"public\",\n",
    "    \"<return>\": \"return\",\n",
    "    \"<short>\": \"short\",\n",
    "    \"<static>\": \"static\",\n",
    "    \"<strictfp>\": \"strictfp\",\n",
    "    \"<super>\": \"super\",\n",
    "    \"<switch>\": \"switch\",\n",
    "    \"<synchronized>\": \"synchronized\",\n",
    "    \"<this>\": \"this\",\n",
    "    \"<throw>\": \"throw\",\n",
    "    \"<throws>\": \"throws\",\n",
    "    \"<transient>\": \"transient\",\n",
    "    \"<try>\": \"try\",\n",
    "    \"<void>\": \"void\",\n",
    "    \"<volatile>\": \"volatile\",\n",
    "    \"<while>\": \"while\",\n",
    "}\n",
    "\n",
    "# from https://docs.oracle.com/javase/tutorial/java/nutsandbolts/opsummary.html\n",
    "java_operator_tokens = {\n",
    "    \"<=>\": \"=\",\n",
    "    \"<+>\": \"+\",\n",
    "    \"<->\": \"-\",\n",
    "    \"<*>\": \"*\",\n",
    "    \"</>\": \"/\",\n",
    "    \"<%>\": \"%\",\n",
    "    \"<++>\": \"++\",\n",
    "    \"<-->\": \"--\",\n",
    "    \"<!>\": \"!\",\n",
    "    \"<==>\": \"==\",\n",
    "    \"<!=>\": \"!=\",\n",
    "    \"<greater>\": \">\",\n",
    "    \"<greater_equal>\": \">=\",\n",
    "    \"<lesser>\": \"<\",\n",
    "    \"<lesser_equal>\": \"<=\",\n",
    "    \"<&&>\": \"&&\",\n",
    "    \"<||>\": \"||\",\n",
    "    \"<?>\": \"?\",\n",
    "    \"<:>\": \":\",\n",
    "    \"<~>\": \"~\",\n",
    "    \"<double_lesser>\": \"<<\",\n",
    "    \"<double_greater>\": \">>\",\n",
    "    \"<triple_greater>\": \">>>\",\n",
    "    \"<&>\": \"&\",\n",
    "    \"<^>\": \"^\",\n",
    "    \"<|>\": \"|\",\n",
    "}\n",
    "\n",
    "java_structural_tokens = {\n",
    "    \"<{>\": \"{\",\n",
    "    \"<}>\": \"}\",\n",
    "    \"<[>\": \"[\",\n",
    "    \"<]>\": \"]\",\n",
    "    \"<lesser>\": \"<\",\n",
    "    \"<greater>\": \">\",\n",
    "    \"<(>\": \"(\",\n",
    "    \"<)>\": \")\",\n",
    "    \"<;>\": \";\",\n",
    "}\n",
    "\n",
    "java_extra_tokens = {\n",
    "    \"<@>\": \"@\",\n",
    "    \"<...>\": \"...\",\n",
    "    \"<null>\": \"null\",\n",
    "    \"<true>\": \"true\",\n",
    "    \"<false>\": \"false\",\n",
    "}\n",
    "\n",
    "# combination of all dictionaries\n",
    "java_special_tokens = {\n",
    "    **java_reserved_tokens,\n",
    "    **java_operator_tokens,\n",
    "    **java_structural_tokens,\n",
    "    **java_extra_tokens,\n",
    "    **extra_tokens,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-victorian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def __replace_tokenizer_toks(code_snippet: str) -> str:\n",
    "    \"\"\"\n",
    "    Function to replace special tokens introduced by the tokenizer/model (bos, eos, pad)\n",
    "    :param code_snippet: String representing the code snippet\n",
    "    :return: String containing the clean string\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\"|\".join([\"<pad>\", \"<sos>\", \"<eos>\"]))\n",
    "    \n",
    "    clean_snippet = pattern.sub(\"\", code_snippet)\n",
    "    return clean_snippet\n",
    "\n",
    "def replace_tokenizer_toks(df: pd.DataFrame, n: Optional[int]=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to replreace\n",
    "    :param df: Pandas DataFrame containing the collection of code snippets\n",
    "    :return: Clean DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "    df = df.iloc[:n].copy()\n",
    "    df.code = df.code.apply(lambda snippet: __replace_tokenizer_toks(snippet))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "novel-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def _replace_spec_toks(mthd: str, spec_toks: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Performs the replacement of special tokens by the original ones\n",
    "    \"\"\"\n",
    "    spec_toks = spec_toks.copy()\n",
    "    # Add special tokenizer tokens -> deleted for code analysis\n",
    "    spec_toks['<bos>'] = \"\"\n",
    "    spec_toks['<eos>'] = \"\"\n",
    "    spec_toks['<pad>'] = \"\"\n",
    "    \n",
    "    spec_toks = dict(\n",
    "        (re.escape(k), v)\n",
    "        for k, v in sorted(\n",
    "            spec_toks.items(), key=lambda x: len(x[1]), reverse=True\n",
    "        )\n",
    "    )\n",
    "    # construct regex pattern for finding all special tokens in a method\n",
    "    pattern = re.compile(\"|\".join(spec_toks.keys()))\n",
    "    # replace all special tokens in a method\n",
    "    mthd = pattern.sub(lambda m: spec_toks[re.escape(m.group(0))], mthd)\n",
    "    \n",
    "    mthd = __replace_tokenizer_toks(mthd)\n",
    "    return mthd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "olive-wellington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def replace_spec_toks_to_original(df: pd.DataFrame, spec_toks: Dict[str, str],\n",
    "                        n: Optional[int] = None) -> pd.DataFrame:\n",
    "    if n is None:\n",
    "        n = len(df)\n",
    "    df = df.iloc[:n].copy()\n",
    "    df.code = df.code.apply(lambda m: _replace_spec_toks(m, spec_toks))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sorted-helping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 0.1_mgmnt.prep.ipynb.\n",
      "Converted 0.2_mgmnt.prep.files_mgmnt.ipynb.\n",
      "Converted 0.3_mgmnt.prep.bpe_tokenization.ipynb.\n",
      "Converted 0.4_mgmnt.prep.tokenization_counting.ipynb.\n",
      "Converted 0.5_mgmnt.prep.token_mgmnt.ipynb.\n",
      "Converted 1.1_exp.info.ipynb.\n",
      "Converted 1.2_exp.desc.metrics.java.ipynb.\n",
      "Converted 1.4_exp.metrics_python.ipynb.\n",
      "Converted 1.5_exp.metrics_java.ipynb.\n",
      "Converted 2.0_repr.codebert.ipynb.\n",
      "Converted 2.0_repr.i.ipynb.\n",
      "Converted 2.1_repr.codeberta.ipynb.\n",
      "Converted 2.1_repr.roberta.train.ipynb.\n",
      "Converted 2.2_repr.roberta.eval.ipynb.\n",
      "Converted 2.3_repr.word2vec.train.ipynb.\n",
      "Converted 2.6_repr.word2vec.eval.ipynb.\n",
      "Converted 2.7_repr.distmetrics.ipynb.\n",
      "Converted 2.8_repr.sentence_transformers.ipynb.\n",
      "Converted 3.1_traceability.unsupervised.eda.ipynb.\n",
      "Converted 3.2_traceability.unsupervised.approach.d2v.ipynb.\n",
      "Converted 3.2_traceability.unsupervised.approach.w2v.ipynb.\n",
      "Converted 4.0_infoxplainer.ir.ipynb.\n",
      "Converted 4.1_infoxplainer.ir.unsupervised.d2v.ipynb.\n",
      "Converted 4.2_infoxplainer.ir.unsupervised.w2v.ipynb.\n",
      "Converted 4.3_infoxplainer.ir.eval.x2v.ipynb.\n",
      "Converted 4.4_infoxplainer.causality.eval.traceability.ipynb.\n",
      "Converted 4.5_infoxplainer.description.eval.traceability.ipynb.\n",
      "Converted 4.6_infoxplainer.prediction.eval.traceability.ipynb.\n",
      "Converted 5.0_utils.clusterization.ipynb.\n",
      "Converted 5.1_utils.visualization.ipynb.\n",
      "Converted 5.2_utils.distances.ipynb.\n",
      "Converted 5.3_utils.statistics.ipynb.\n",
      "Converted 8.1_codexplainer.error_checker.ipynb.\n",
      "Converted 8.2_codexplainer.metrics.ipynb.\n",
      "Converted 8.4_codexplainer.metrics_example.ipynb.\n",
      "Converted 8.5_codexplainer.d2v_vectorization.ipynb.\n",
      "Converted 8.6_codexplainer.prototypes_criticisms.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
