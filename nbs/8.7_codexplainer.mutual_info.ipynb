{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fleet-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp codexplainer.mutual_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "greenhouse-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import dit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ds4se.mgmnt.prep.tokenization_counting import *\n",
    "from ds4se.mgmnt.prep.bpe_tokenization import HFTokenizer\n",
    "\n",
    "from typing import Optional, List, Tuple, Dict\n",
    "\n",
    "from ds4se.mgmnt.prep.files_mgmnt import jsonl_to_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "varying-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "#Logging configuration\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='mylog.log', mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formacompute_info_theory_measurester)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-quebec",
   "metadata": {},
   "source": [
    "### mutual_info explainer\n",
    "\n",
    "> This module provides methods for performing comparisons between code samples leveraging information theory measurements\n",
    "\n",
    "> @Alvaro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-calibration",
   "metadata": {},
   "source": [
    "Common use leverage HuggingFace tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "corresponding-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"hf_tokenizer_path\": \"/home/jovyan/work/int-data/tokenizer/tokenizer-java-v1.json\",\n",
    "    \"sample_path\": \"/home/jovyan/work/int-data/samples/generators/transformers/tfr_layers6_vocab10000_embd768_heads12/5000-samples_1625693099.45201.jsonl\",\n",
    "    \"hmn_sample_path\": \"/home/jovyan/work/int-data/samples/human/train/5000-samples_1621479183.15598.jsonl\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "phantom-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer = HFTokenizer(params[\"hf_tokenizer_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "finished-closer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-11 18:08:54,885 : INFO : Note: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "2021-09-11 18:08:54,886 : INFO : Note: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2021-09-11 18:08:54,887 : INFO : NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "java_samples = jsonl_to_dataframe(params[\"sample_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dynamic-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks_frequencies = get_tokens_frequency_hf_tkzr(java_samples, hf_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "checked-georgia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def get_mutual_info_hf(hf_tokenizer: HFTokenizer,\n",
    "                       sample_set1: List[str],\n",
    "                       sample_set2: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform mutual info. analysis for 2 sample sets based on token counting\n",
    "    :param hf_tokenizer: HuggingFace tokenizer for performing counting\n",
    "    :param sample_set1: List containing paths for sample set 1 paths\n",
    "    :param sample_set2: List containing paths for sample set 2 paths\n",
    "    \n",
    "    :return: Dict containing data for\n",
    "             - Mutual information\n",
    "             - Entropy for sample set 1\n",
    "             - Entropy for sample set 2\n",
    "             - Entropy for joint distributions\n",
    "             \n",
    "    \"\"\"\n",
    "    vocab = list(hf_tokenizer.tokenizer.get_vocab().keys())\n",
    "    # For numerical stability\n",
    "    epsilon = 4.9406564584124654e-324\n",
    "    \n",
    "    if(len(sample_set1) != len(sample_set2)):\n",
    "        logging.error(\"Sample sets have different sizes, cannot compare them.\")\n",
    "        return\n",
    "    \n",
    "    n_experiments = len(sample_set1)\n",
    "    mutual_info_data = []\n",
    "    entropy_s1_data = []\n",
    "    entropy_s2_data = []\n",
    "    joint_entropy_data = []\n",
    "    \n",
    "    for e in range(n_experiments):\n",
    "        s1 = jsonl_to_dataframe(sample_set1[e])\n",
    "        s2 = jsonl_to_dataframe(sample_set2[e])\n",
    "        \n",
    "        # Perform counting\n",
    "        freqs_s1 = get_tokens_frequency_hf_tkzr(s1, hf_tokenizer)\n",
    "        freqs_s2 = get_tokens_frequency_hf_tkzr(s2, hf_tokenizer)\n",
    "        \n",
    "        count_s1 = np.zeros(len(vocab))\n",
    "        count_s2 = np.zeros(len(vocab))\n",
    "        \n",
    "        total_toks_s1 = 0\n",
    "        total_toks_s2 = 0\n",
    "        \n",
    "        for i in range(len(vocab)):\n",
    "            token = vocab[i]\n",
    "            tk_c_s1 = freqs_s1[token] if token in freqs_s1 else epsilon\n",
    "            tk_c_s2 = freqs_s2[token] if token in freqs_s2 else epsilon\n",
    "            \n",
    "            count_s1[i] = tk_c_s1\n",
    "            count_s2[i] = tk_c_s2\n",
    "            \n",
    "            total_toks_s1 += tk_c_s1\n",
    "            total_toks_s2 += tk_c_s2\n",
    "            \n",
    "        mutual_info, entropy_s1, entropy_s2, entropy_joint = compute_info_theory_measures(\n",
    "            count_s1, count_s2,\n",
    "            total_toks_s1, total_toks_s2,\n",
    "            vocab\n",
    "        )\n",
    "        \n",
    "        mutual_info_data.append(mutual_info)\n",
    "        entropy_s1_data.append(entropy_s1)\n",
    "        entropy_s2_data.append(entropy_s2)\n",
    "        joint_entropy_data.append(entropy_joint)\n",
    "    \n",
    "    data = {\n",
    "        \"Mutual-info\": mutual_info_data,\n",
    "        \"Entropy-S1\": entropy_s1_data,\n",
    "        \"Entropy-S2\": entropy_s2_data,\n",
    "        \"Entropy-joint\": joint_entropy_data\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "chinese-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def compute_info_theory_measures(vector_s1: np.ndarray, vector_s2: np.ndarray,\n",
    "                                 tot_toks_s1: int, tot_toks_s2: int,\n",
    "                                 vocab: List[str]) -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute info. theory measures based on tokens count and vocabulary\n",
    "    (Distributions are computed leveraging log_2)\n",
    "    \n",
    "    :param vector_s1: Vector containing counts for the entire vocab (sample set 1)\n",
    "    :param vector_s2: Vector containing counts for the entire vocab (sample set 2)\n",
    "    :param tot_toks_s1: int indicating total number of counted tokens for sample set 1\n",
    "    :param tot_toks_s2: int indicating total number of counted tokens for sample set 2\n",
    "    :param vocab: List containing the entire vocabulary\n",
    "    \n",
    "    :return: Tuple containing\n",
    "             - Mutual info. calculation\n",
    "             - Entropy for sample set 1 distribution \n",
    "             - Entropy for sample set 2 distribution \n",
    "             - Entropy for joint distribution\n",
    "    \"\"\"\n",
    "    # Get individual distributions\n",
    "    proportions_s1 = vector_s1 / tot_toks_s1\n",
    "    proportions_s2 = vector_s2 / tot_toks_s2\n",
    "    \n",
    "    log_proportions_s1 = np.log2(proportions_s1)\n",
    "    log_proportions_s2 = np.log2(proportions_s2)\n",
    "    \n",
    "    sum_p1 = np.sum(proportions_s1)\n",
    "    sum_p2 = np.sum(proportions_s2)\n",
    "    if sum_p1 != 1 or sum_p2 != 1:\n",
    "        logging.warning(\"Vector don't correspond to distributions (don't add up to 1.0)\")\n",
    "        logging.warning(\"Vector sum for dist 1: {}\".format(sum_p1))\n",
    "        logging.warning(\"Vector sum for dist 2: {}\".format(sum_p2))\n",
    "    \n",
    "    if len(proportions_s1) != len(vocab) or len(proportions_s2) != len(vocab):\n",
    "        logging.error(\"Different lengths for vocab distributions\")\n",
    "        # return\n",
    "    \n",
    "    dist_s1 = dit.Distribution.from_ndarray(log_proportions_s1, base=2)\n",
    "    dist_s2 = dit.Distribution.from_ndarray(log_proportions_s2, base=2)\n",
    "    \n",
    "    entropy_s1 = dit.shannon.entropy(dist_s1)\n",
    "    entropy_s2 = dit.shannon.entropy(dist_s2)\n",
    "    \n",
    "    # Get joint distribution\n",
    "    joint = (vector_s1 + vector_s2) / (tot_toks_s1 + tot_toks_s2)\n",
    "    log_joint = np.log2(joint)\n",
    "    joint_dist = dit.Distribution.from_ndarray(log_joint, base=2)\n",
    "    entropy_joint = dit.shannon.entropy(joint_dist)\n",
    "    \n",
    "    # Get mutual information\n",
    "    mutual_info = entropy_s1 + entropy_s2 - entropy_joint\n",
    "    return mutual_info, entropy_s1, entropy_s2, entropy_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "promotional-rochester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 0.1_mgmnt.prep.ipynb.\n",
      "Converted 0.2_mgmnt.prep.files_mgmnt.ipynb.\n",
      "Converted 0.3_mgmnt.prep.bpe_tokenization.ipynb.\n",
      "Converted 0.4_mgmnt.prep.tokenization_counting.ipynb.\n",
      "Converted 0.5_mgmnt.prep.token_mgmnt.ipynb.\n",
      "Converted 1.1_exp.info.ipynb.\n",
      "Converted 1.2_exp.desc.metrics.java.ipynb.\n",
      "Converted 1.4_exp.metrics_python.ipynb.\n",
      "Converted 1.5_exp.metrics_java.ipynb.\n",
      "Converted 2.0_repr.codebert.ipynb.\n",
      "Converted 2.0_repr.i.ipynb.\n",
      "Converted 2.1_repr.codeberta.ipynb.\n",
      "Converted 2.1_repr.roberta.train.ipynb.\n",
      "Converted 2.2_repr.roberta.eval.ipynb.\n",
      "Converted 2.3_repr.word2vec.train.ipynb.\n",
      "Converted 2.6_repr.word2vec.eval.ipynb.\n",
      "Converted 2.7_repr.distmetrics.ipynb.\n",
      "Converted 2.8_repr.sentence_transformers.ipynb.\n",
      "Converted 3.1_traceability.unsupervised.eda.ipynb.\n",
      "Converted 3.2_traceability.unsupervised.approach.d2v.ipynb.\n",
      "Converted 3.2_traceability.unsupervised.approach.w2v.ipynb.\n",
      "Converted 4.0_infoxplainer.ir.ipynb.\n",
      "Converted 4.1_infoxplainer.ir.unsupervised.d2v.ipynb.\n",
      "Converted 4.2_infoxplainer.ir.unsupervised.w2v.ipynb.\n",
      "Converted 4.3_infoxplainer.ir.eval.x2v.ipynb.\n",
      "Converted 4.4_infoxplainer.causality.eval.traceability.ipynb.\n",
      "Converted 4.5_infoxplainer.description.eval.traceability.ipynb.\n",
      "Converted 4.6_infoxplainer.prediction.eval.traceability.ipynb.\n",
      "Converted 5.0_utils.clusterization.ipynb.\n",
      "Converted 5.1_utils.visualization.ipynb.\n",
      "Converted 5.2_utils.distances.ipynb.\n",
      "Converted 5.3_utils.plotting.ipynb.\n",
      "Converted 5.3_utils.statistics.ipynb.\n",
      "Converted 8.1_codexplainer.error_checker.ipynb.\n",
      "Converted 8.2_codexplainer.metrics_java.ipynb.\n",
      "Converted 8.4_codexplainer.metrics_example.ipynb.\n",
      "Converted 8.5_codexplainer.d2v_vectorization.ipynb.\n",
      "Converted 8.6_codexplainer.prototypes_criticisms.ipynb.\n",
      "Converted 8.7_codexplainer.mutual_info.ipynb.\n",
      "Converted 8.8_codexplainer.utils.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
