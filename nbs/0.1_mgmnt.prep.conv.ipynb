{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp mgmnt.prep.conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conventional Preprocessing\n",
    "\n",
    "> This module comprises preprocessing techniques applied to software artifacts (TODO:cite here the papers employed for this preprocessings):\n",
    ">\n",
    ">This is an adapted version of Daniel McCrystal Nov 2019\n",
    ">\n",
    ">This version also includes BPE preprocesing and NLTK. It's the main class to execute conventional pipelines. \n",
    "\n",
    ">Author: @danaderp March 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install dit\n",
    "#! pip install nltk\n",
    "#! pip install tokenizers\n",
    "#! pip install tensorflow_datasets\n",
    "! pip install -U tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import List, Set, Callable, Tuple, Dict, Optional\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import pathlib\n",
    "from string import punctuation\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "englishStemmer=SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! pip install nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tensorflow.keras.preprocessing import text\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import sentencepiece as sp\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import sentencepiece as spm\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip -qq cisco/CSB-CICDPipelineEdition-master.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "path_data = '../dvc-ds4se/' #dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def libest_params():\n",
    "    return {\n",
    "        'system': 'libest',\n",
    "        #'path_zip': Path(\"cisco/sacp-python-common.zip\"),\n",
    "        'saving_path': path_data+ 'se-benchmarking/traceability/testbeds/processed/libest_data',\n",
    "        'language': 'english',\n",
    "        'dataset' : path_data + ''\n",
    "        #'model_prefix': path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_128k' #For BPE Analysis\n",
    "        #'model_prefix': path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_32k'\n",
    "        'model_prefix':path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_8k'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prefix = {\n",
    "    'bpe8k' : path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_8k',\n",
    "    'bpe32k' : path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_32k',\n",
    "    'bpe128k' : path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_128k'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = default_params()\n",
    "params = libest_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conventional Preprocessing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConventionalPreprocessing():\n",
    "    '''NLTK libraries for Conventional Preprocessing'''\n",
    "    def __init__(self, params, bpe = False):\n",
    "        self.params = params\n",
    "        \n",
    "        #If BPE provided, then preprocessing with BPE is allowed on CONV\n",
    "        if bpe:\n",
    "            self.sp_bpe = spm.SentencePieceProcessor()\n",
    "            self.sp_bpe.load(params['model_prefix']+'.model')\n",
    "        else:\n",
    "            self.sp_bpe = None\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def bpe_pieces_pipeline(self, doc_list):\n",
    "        '''Computes BPE preprocessing according to params'''\n",
    "        encoded_str = ''\n",
    "        if self.sp_bpe is None:\n",
    "            logging.info('Provide a BPE Model!')\n",
    "        else:\n",
    "            encoded_str = [self.sp_bpe.encode_as_pieces(doc) for doc in doc_list]  \n",
    "        return encoded_str\n",
    "    \n",
    "    #ToDo Transforme it into a For-Comprenhension\n",
    "    def clean_punctuation(self, token): \n",
    "        #remove terms !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~0123456789\n",
    "        return re.sub(r'[^a-zA-Z\\s]', ' ', token, re.I|re.A)\n",
    "\n",
    "    def split_camel_case_token(self, token):\n",
    "        return re.sub('([a-z])([A-Z])', r'\\1 \\2', token)\n",
    "\n",
    "    def remove_terms(self, filtered_tokens):\n",
    "        remove_terms = punctuation + '0123456789'\n",
    "        return [token for token in filtered_tokens if token not in remove_terms and len(token)>2 and len(token)<21]\n",
    "\n",
    "    def stemmer(self, filtered_tokens):\n",
    "        return [englishStemmer.stem(token) for token in filtered_tokens ]\n",
    "\n",
    "    def stop_words(self, filtered_tokens):\n",
    "        stop_words = nltk.corpus.stopwords.words(self.params['language'])\n",
    "        return [token for token in filtered_tokens if token not in stop_words]\n",
    "    \n",
    "    def basic_pipeline(self, dict_filenames):\n",
    "        '''@dict_filenames: {filename: code}'''\n",
    "        pre_process = [( key.replace('.txt', '-pre.txt') , self.clean_punctuation(dict_filenames[key][0])  ) for key in dict_filenames]\n",
    "        pre_process = [( doc[0] , self.split_camel_case_token(doc[1])  ) for doc in pre_process]\n",
    "        pre_process = [( doc[0] , doc[1].lower()  ) for doc in pre_process]\n",
    "        pre_process = [( doc[0] , doc[1].strip()) for doc in pre_process] # Leading whitepsace are removed\n",
    "        pre_process_tokens = [(doc[0] , nltk.WordPunctTokenizer().tokenize(doc[1])) for doc in pre_process]\n",
    "        filtered_tokens = [(doc[0], self.stop_words(doc[1]) ) for doc in pre_process_tokens] #Stop Words\n",
    "        filtered_tokens = [(doc[0], self.stemmer(doc[1]) ) for doc in filtered_tokens] #Filtering Stemmings\n",
    "        filtered_tokens = [(doc[0], self.remove_terms(doc[1])) for doc in filtered_tokens] #Filtering remove-terms\n",
    "        pre_process = [(doc[0], ' '.join(doc[1])) for doc in filtered_tokens]\n",
    "        return pre_process\n",
    "    \n",
    "    def fromdocs_pipeline(self, docs):\n",
    "        #TODO\n",
    "        \"\"\"@tokenized_file: a list of tokens that represents a document/code\"\"\"\n",
    "        pre_process = [ self.clean_punctuation(doc) for doc in docs]\n",
    "        logging.info('fromtokens_pipeline: clean punctuation')\n",
    "        pre_process = [ self.split_camel_case_token(doc) for doc in pre_process]\n",
    "        logging.info('fromtokens_pipeline: camel case')\n",
    "        pre_process = [ doc.lower() for doc in pre_process] \n",
    "        logging.info('fromtokens_pipeline: lowe case')\n",
    "        pre_process = [ doc.strip() for doc in pre_process] # Leading whitepsace are removed\n",
    "        logging.info('fromtokens_pipeline: white space removed')\n",
    "        pre_process_tokens = [ nltk.WordPunctTokenizer().tokenize(doc) for doc in pre_process]\n",
    "        logging.info('fromtokens_pipeline: WordPunctTokenizer')\n",
    "        filtered_tokens = [ self.stop_words(doc) for doc in pre_process_tokens] #Stop Words\n",
    "        logging.info('fromtokens_pipeline: Stop words')\n",
    "        filtered_tokens = [ self.stemmer(doc) for doc in filtered_tokens] #Filtering Stemmings\n",
    "        logging.info('fromtokens_pipeline: Stemmings')\n",
    "        filtered_tokens = [ self.remove_terms(doc) for doc in filtered_tokens] #Filtering remove-terms\n",
    "        logging.info('fromtokens_pipeline: Removed Special Terns')\n",
    "        pre_process = [ ' '.join(doc) for doc in filtered_tokens]\n",
    "        logging.info('fromtokens_pipeline END')\n",
    "        return pre_process\n",
    "    \n",
    "    def frombatch_pipeline(self, batch):\n",
    "        #TODO\n",
    "        \"\"\"@batch: a TensorFlow Dataset Batch\"\"\"\n",
    "        pre_process = [ self.clean_punctuation( doc.decode(\"utf-8\") ) for doc in batch]\n",
    "        logging.info('frombatch_pipeline: clean punctuation')\n",
    "        pre_process = [ self.split_camel_case_token(doc) for doc in pre_process]\n",
    "        logging.info('frombatch_pipeline: camel case')\n",
    "        pre_process = [ doc.lower() for doc in pre_process] \n",
    "        logging.info('frombatch_pipeline: lowe case')\n",
    "        pre_process = [ doc.strip() for doc in pre_process] # Leading whitepsace are removed\n",
    "        logging.info('frombatch_pipeline: white space removed')\n",
    "        pre_process_tokens = [ nltk.WordPunctTokenizer().tokenize(doc) for doc in pre_process]\n",
    "        logging.info('frombatch_pipeline: WordPunctTokenizer')\n",
    "        filtered_tokens = [ self.stop_words(doc) for doc in pre_process_tokens] #Stop Words\n",
    "        logging.info('frombatch_pipeline: Stop words')\n",
    "        filtered_tokens = [ self.stemmer(doc) for doc in filtered_tokens] #Filtering Stemmings\n",
    "        logging.info('frombatch_pipeline: Stemmings')\n",
    "        filtered_tokens = [ self.remove_terms(doc) for doc in filtered_tokens] #Filtering remove-terms\n",
    "        logging.info('frombatch_pipeline: Removed Special Terns')\n",
    "        #pre_process = [ ' '.join(doc) for doc in filtered_tokens]\n",
    "        logging.info('frombatch_pipeline [END]')\n",
    "        return filtered_tokens\n",
    "    \n",
    "    def fromtensor_pipeline(self, ts_x):\n",
    "        \"\"\"@ts_x: es un elemento del tensor\"\"\"\n",
    "        #TODO\n",
    "        pre_process = self.clean_punctuation(ts_x)\n",
    "        pre_process = self.split_camel_case_token(pre_process)\n",
    "        pre_process = pre_process.lower()\n",
    "        pre_process = pre_process.strip()\n",
    "        pre_process = nltk.WordPunctTokenizer().tokenize(pre_process)\n",
    "        filtered_tokens = self.stop_words(pre_process)\n",
    "        filtered_tokens = self.stemmer(filtered_tokens)\n",
    "        filtered_tokens = self.remove_terms(filtered_tokens)\n",
    "        pre_process = ' '.join(filtered_tokens)\n",
    "        logging.info('fromtokens_pipeline END')\n",
    "        return pre_process\n",
    "    \n",
    "    def SaveCorpus(self, df, language='js', sep=',', mode='a'):\n",
    "        timestamp = datetime.timestamp(datetime.now())\n",
    "        path_to_link = self.params['saving_path'] + '['+ self.params['system']  + '-' + language + '-{}].csv'.format(timestamp)\n",
    "\n",
    "        df.to_csv(path_to_link, header=True, index=True, sep=sep, mode=mode)     \n",
    "        logging.info('Saving in...' + path_to_link)\n",
    "        pass\n",
    "    \n",
    "    def LoadCorpus(self, timestamp, language='js', sep=',', mode='a'):\n",
    "        path_to_link = self.params['saving_path'] + '['+ self.params['system']  + '-' + language + '-{}].csv'.format(timestamp)\n",
    "        return pd.read_csv(path_to_link, header=0, index_col=0, sep=sep)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def open_file(f, encoding='utf-8'):\n",
    "    try:\n",
    "        #return open(filename, 'r', encoding=\"ISO-8859-1\").read()\n",
    "        return open(f, 'r', encoding = encoding).read()\n",
    "    except:\n",
    "        print(\"Exception: \", sys.exc_info()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_files(system, ends):\n",
    "    path = Path(\"cisco/CSB-CICDPipelineEdition-master/\")\n",
    "    names = [entry for entry in path.glob('**/*' +ends)]\n",
    "    filenames = [(filename, os.path.basename(filename), open_file(filename) ) for filename in names]\n",
    "    return pd.DataFrame( filenames ,columns = ['names','filenames','content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Processing Software Corpora from GitHub\n",
    "> Cisco Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"cisco/CSB-CICDPipelineEdition-master/\")\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sacp_params(model_prefix = path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_8k'):\n",
    "    return {\n",
    "        'system': 'sacp-python-common',\n",
    "        'path_zip': Path(\"tf/data/cisco/sacp_data/sacp-python-common.zip\"),\n",
    "        'dataset': 'tf/data/cisco/sacp_data/',\n",
    "        'saving_path': '../../'+'data/cisco/sacp_data/',\n",
    "        'language': 'english',\n",
    "        'model_prefix':model_prefix #For BPE Analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': 'sacp-python-common',\n",
       " 'path_zip': PosixPath('tf/data/cisco/sacp_data/sacp-python-common.zip'),\n",
       " 'dataset': 'tf/data/cisco/sacp_data/',\n",
       " 'saving_path': '../../data/cisco/sacp_data/',\n",
       " 'language': 'english',\n",
       " 'model_prefix': '../dvc-ds4se/models/bpe/sentencepiece/wiki_py_java_bpe_8k'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = sacp_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = ConventionalPreprocessing(params, bpe = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore\n",
    "archive = ZipFile(Path(\"cisco/sacp-python-common.zip\"), 'r')\n",
    "files = archive.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore\n",
    "files = [name for name in archive.namelist() if name.endswith('.py')] #recursively finds files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_file_zip(params, ends):\n",
    "    archive = ZipFile( params['path_zip'], 'r')\n",
    "    names = [name for name in archive.namelist() if name.endswith(ends)]\n",
    "    filenames = [(filename, os.path.basename(filename), archive.read(filename) ) for filename in names]\n",
    "    return pd.DataFrame( filenames ,columns = ['names','filenames','content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "df_sampling = get_file_zip(params = params, ends='.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "df_sampling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "prep.SaveCorpus(df_sampling, language='py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tst\n",
    "df_sampling = prep.LoadCorpus(1595859280.080238, language='py')\n",
    "df_sampling.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating standard dataframe for issues and pull-request (cisco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_data = pd.read_csv('cisco/sacp-pullrequest-01.csv', sep = '~', header = 0, encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging all the system artifacts in one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_all_sys = pr_data.copy()\n",
    "pr_all_sys = pr_all_sys.replace(np.nan, ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_all_sys['text'] = pr_all_sys['title'].astype(str) + pr_all_sys['labels'].astype(str) + pr_all_sys['body'].astype(str)#merging tree columns for the text\n",
    "pr_all_sys = pr_all_sys[['id-pr','text']]\n",
    "pr_all_sys = pr_all_sys.rename(columns={'id-pr': 'ids'})\n",
    "pr_all_sys['type'] = 'pr' #<------- File Type Standard for Target or Source\n",
    "pr_all_sys.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_all_code = df_sampling.copy()\n",
    "#pr_all_code['text'] =  pr_all_code.apply(lambda row: row['content'].decode(\"utf-8\"), axis = 1)\n",
    "pr_all_code['content'] =  pr_all_code['content'].apply(lambda x: eval(x))\n",
    "pr_all_code['text'] =  pr_all_code['content'].apply(lambda x: x.decode(\"utf-8\"))\n",
    "pr_all_code = pr_all_code[['names','text']]\n",
    "pr_all_code = pr_all_code.rename(columns={'names': 'ids'})\n",
    "pr_all_code['type'] = 'py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_all_sys = pd.concat([pr_all_sys, pr_all_code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_all_sys['conv'] = prep.fromdocs_pipeline( pr_all_sys['text'].values ) #Conventional Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_all_sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.SaveCorpus(pr_all_sys, language='all-corpus', sep='~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sacp = prep.LoadCorpus(1595953540.866044, language='all-corpus', sep='~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>conv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295</td>\n",
       "      <td>Production Merge * Feed release name through t...</td>\n",
       "      <td>pr</td>\n",
       "      <td>product merg feed releas name upload bom allow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>294</td>\n",
       "      <td>Add test fields for DARE push * Added test dat...</td>\n",
       "      <td>pr</td>\n",
       "      <td>add test field dare push test data json sent d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>293</td>\n",
       "      <td>Allow passing a release to uploadBom by name, ...</td>\n",
       "      <td>pr</td>\n",
       "      <td>allow pass releas upload bom name rather chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>287</td>\n",
       "      <td>Allow append images #363 - Changed how image n...</td>\n",
       "      <td>pr</td>\n",
       "      <td>allow append imag chang imag name creat send c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>274</td>\n",
       "      <td>Move docker/blackduck test to slave 4</td>\n",
       "      <td>pr</td>\n",
       "      <td>move docker blackduck test slave</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ids                                               text type  \\\n",
       "0  295  Production Merge * Feed release name through t...   pr   \n",
       "1  294  Add test fields for DARE push * Added test dat...   pr   \n",
       "2  293  Allow passing a release to uploadBom by name, ...   pr   \n",
       "3  287  Allow append images #363 - Changed how image n...   pr   \n",
       "4  274            Move docker/blackduck test to slave 4     pr   \n",
       "\n",
       "                                                conv  \n",
       "0  product merg feed releas name upload bom allow...  \n",
       "1  add test field dare push test data json sent d...  \n",
       "2  allow pass releas upload bom name rather chang...  \n",
       "3  allow append imag chang imag name creat send c...  \n",
       "4                   move docker blackduck test slave  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sacp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sacp.dropna( inplace = True ) #empty files are not considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterating All Possible BPEs configs\n",
    "for bpe in model_prefix.keys():\n",
    "    mpr = model_prefix[bpe]\n",
    "    prep = ConventionalPreprocessing(sacp_params(model_prefix = mpr), bpe = True)\n",
    "    df_sacp[bpe] = prep.bpe_pieces_pipeline( df_sacp['text'].values ) #BPE Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>conv</th>\n",
       "      <th>bpe8k</th>\n",
       "      <th>bpe32k</th>\n",
       "      <th>bpe128k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295</td>\n",
       "      <td>Production Merge * Feed release name through t...</td>\n",
       "      <td>pr</td>\n",
       "      <td>product merg feed releas name upload bom allow...</td>\n",
       "      <td>[▁production, ▁mer, ge, ▁*, ▁feed, ▁release, ▁...</td>\n",
       "      <td>[▁production, ▁merge, ▁*, ▁feed, ▁release, ▁na...</td>\n",
       "      <td>[▁production, ▁merge, ▁*, ▁feed, ▁release, ▁na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>294</td>\n",
       "      <td>Add test fields for DARE push * Added test dat...</td>\n",
       "      <td>pr</td>\n",
       "      <td>add test field dare push test data json sent d...</td>\n",
       "      <td>[▁add, ▁test, ▁fields, ▁for, ▁d, are, ▁p, ush,...</td>\n",
       "      <td>[▁add, ▁test, ▁fields, ▁for, ▁dare, ▁push, ▁*,...</td>\n",
       "      <td>[▁add, ▁test, ▁fields, ▁for, ▁dare, ▁push, ▁*,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>293</td>\n",
       "      <td>Allow passing a release to uploadBom by name, ...</td>\n",
       "      <td>pr</td>\n",
       "      <td>allow pass releas upload bom name rather chang...</td>\n",
       "      <td>[▁allow, ▁passing, ▁a, ▁release, ▁to, ▁up, loa...</td>\n",
       "      <td>[▁allow, ▁passing, ▁a, ▁release, ▁to, ▁up, loa...</td>\n",
       "      <td>[▁allow, ▁passing, ▁a, ▁release, ▁to, ▁upload,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>287</td>\n",
       "      <td>Allow append images #363 - Changed how image n...</td>\n",
       "      <td>pr</td>\n",
       "      <td>allow append imag chang imag name creat send c...</td>\n",
       "      <td>[▁allow, ▁app, end, ▁images, ▁#, 3, 63, ▁-, ▁c...</td>\n",
       "      <td>[▁allow, ▁append, ▁images, ▁#3, 63, ▁-, ▁chang...</td>\n",
       "      <td>[▁allow, ▁append, ▁images, ▁#3, 63, ▁-, ▁chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>274</td>\n",
       "      <td>Move docker/blackduck test to slave 4</td>\n",
       "      <td>pr</td>\n",
       "      <td>move docker blackduck test slave</td>\n",
       "      <td>[▁move, ▁d, ock, er, /, black, d, uck, ▁test, ...</td>\n",
       "      <td>[▁move, ▁dock, er, /, black, d, uck, ▁test, ▁t...</td>\n",
       "      <td>[▁move, ▁docker, /, black, duck, ▁test, ▁to, ▁...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ids                                               text type  \\\n",
       "0  295  Production Merge * Feed release name through t...   pr   \n",
       "1  294  Add test fields for DARE push * Added test dat...   pr   \n",
       "2  293  Allow passing a release to uploadBom by name, ...   pr   \n",
       "3  287  Allow append images #363 - Changed how image n...   pr   \n",
       "4  274            Move docker/blackduck test to slave 4     pr   \n",
       "\n",
       "                                                conv  \\\n",
       "0  product merg feed releas name upload bom allow...   \n",
       "1  add test field dare push test data json sent d...   \n",
       "2  allow pass releas upload bom name rather chang...   \n",
       "3  allow append imag chang imag name creat send c...   \n",
       "4                   move docker blackduck test slave   \n",
       "\n",
       "                                               bpe8k  \\\n",
       "0  [▁production, ▁mer, ge, ▁*, ▁feed, ▁release, ▁...   \n",
       "1  [▁add, ▁test, ▁fields, ▁for, ▁d, are, ▁p, ush,...   \n",
       "2  [▁allow, ▁passing, ▁a, ▁release, ▁to, ▁up, loa...   \n",
       "3  [▁allow, ▁app, end, ▁images, ▁#, 3, 63, ▁-, ▁c...   \n",
       "4  [▁move, ▁d, ock, er, /, black, d, uck, ▁test, ...   \n",
       "\n",
       "                                              bpe32k  \\\n",
       "0  [▁production, ▁merge, ▁*, ▁feed, ▁release, ▁na...   \n",
       "1  [▁add, ▁test, ▁fields, ▁for, ▁dare, ▁push, ▁*,...   \n",
       "2  [▁allow, ▁passing, ▁a, ▁release, ▁to, ▁up, loa...   \n",
       "3  [▁allow, ▁append, ▁images, ▁#3, 63, ▁-, ▁chang...   \n",
       "4  [▁move, ▁dock, er, /, black, d, uck, ▁test, ▁t...   \n",
       "\n",
       "                                             bpe128k  \n",
       "0  [▁production, ▁merge, ▁*, ▁feed, ▁release, ▁na...  \n",
       "1  [▁add, ▁test, ▁fields, ▁for, ▁dare, ▁push, ▁*,...  \n",
       "2  [▁allow, ▁passing, ▁a, ▁release, ▁to, ▁upload,...  \n",
       "3  [▁allow, ▁append, ▁images, ▁#3, 63, ▁-, ▁chang...  \n",
       "4  [▁move, ▁docker, /, black, duck, ▁test, ▁to, ▁...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sacp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-29 06:52:58,801 : INFO : Saving in...../../data/cisco/sacp_data/[sacp-python-common-all-corpus-1609224778.517111].csv\n"
     ]
    }
   ],
   "source": [
    "prep.SaveCorpus(df_sacp, language='all-corpus', sep='~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old code down [becareful]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugging\n",
    "path = Path(\"cisco/CSB-CICDPipelineEdition-master/\")\n",
    "names = [entry for entry in path.glob('**/*.py')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#here looking for a file with encoding problems\n",
    "temp_list=[]\n",
    "for filename in names:\n",
    "    print(filename)\n",
    "    try:\n",
    "        temp_list.append(open(filename, 'r', encoding=\"ISO-8859-1\").read())\n",
    "    except FileNotFoundError as err:\n",
    "        print('lookattheerr' + str(err))\n",
    "    except:\n",
    "        print('bydefault')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_java = get_files(system = params['system'], ends='.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_java.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_java.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SaveCorpus(df_java, language='py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = LoadCorpus(1592266849.29903,language='py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Processing Software Corpora from CodeSearchNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CodeSearchNet Parameters\n",
    "params = {\n",
    "    'system':'codesearchnet',\n",
    "    'saving_path': 'test_data/',\n",
    "    'language': 'english'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step1] Create Preprocesser <----------\n",
    "preprocess_pipeline = ConventionalPreprocessing(params= params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_files = sorted(Path('codesearch/python/').glob('**/*.gz'))\n",
    "java_files = sorted(Path('codesearch/java/').glob('**/*.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_long_list = ['repo', 'path', 'url', 'code', \n",
    "                     'code_tokens', 'docstring', 'docstring_tokens', \n",
    "                     'language', 'partition']\n",
    "\n",
    "columns_short_list = ['code_tokens', 'docstring_tokens', \n",
    "                      'language', 'partition']\n",
    "\n",
    "def jsonl_list_to_dataframe(file_list, columns=columns_long_list):\n",
    "    \"\"\"Load a list of jsonl.gz files into a pandas DataFrame.\"\"\"\n",
    "    return pd.concat([pd.read_json(f, \n",
    "                                   orient='records', \n",
    "                                   compression='gzip',\n",
    "                                   lines=True)[columns] \n",
    "                      for f in file_list], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_searchnet_df = jsonl_list_to_dataframe(python_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_searchnet_df = jsonl_list_to_dataframe(java_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_searchnet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_searchnet_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytrain = java_searchnet_df[java_searchnet_df.partition.eq('train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "javatrain = java_searchnet_df[java_searchnet_df.partition.eq('train')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "javatrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline.SaveCorpus(javatrain, language='java') #Saving codesearchnet only training samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Preprocessing for CodeSearchNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_searchnet_df = preprocess_pipeline.LoadCorpus(1592409554.097457, language='java')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_searchnet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_searchnet_df['code'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = preprocess_pipeline.fromdocs_pipeline(java_searchnet_df['code'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed = java_searchnet_df.copy()\n",
    "df_preprocessed['preprocessed'] = preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SaveCorpus(df_preprocessed, language='preprocessed-java') #Saving codesearchnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Processing from Wikipedia \n",
    ">Inspired by [KD](https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html)\n",
    ">\n",
    ">Dump Wiki File [here](https://dumps.wikimedia.org/enwiki/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config description: Wikipedia dataset for en, parsed from 20190301 dump.\n",
    "#Download size: 15.72 GiB\n",
    "#Dataset size: Unknown size\n",
    "#Examples: train 5,824596\n",
    "dataset_name = 'wikipedia/20200301.en' #'wikipedia/20190301.en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the dataset and create a tf.data.Dataset\n",
    "ds, info = tfds.load(dataset_name, split='train', with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accessing Metadata with DatasetInfo\n",
    "print(info.splits['train'].num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wiki = []\n",
    "#dataset_wiki = ds.map(lambda ex_text, ex_title: preprocess_pipeline.fromtensor_pipeline( ex_text.decode(\"utf-8\") ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_wiki = [preprocess_pipeline.fromtensor_pipeline( ex['text'].decode(\"utf-8\") ) for ex in  tfds.as_numpy(ds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_wiki = pd.DataFrame( dataset_wiki ,columns = ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your input pipeline\n",
    "ds = ds.batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Numpy Arrays\n",
    "for ex in tfds.as_numpy(ds):\n",
    "    #print( preprocess_pipeline.fromtensor_pipeline( ex['text'].decode(\"utf-8\") ) )\n",
    "    #print(\"NEXT!!!\")\n",
    "    #print(ex['text'].decode(\"utf-8\"))\n",
    "    #print(ex)\n",
    "    #np_text, np_title = ex['text'], ex['title']\n",
    "    print(preprocess_pipeline.frombatch_pipeline( ex['text'] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_text[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex in ds.take(4):\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'system':'wiki',\n",
    "    'saving_path': 'test_data/',\n",
    "    'language': 'english'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Processing from Semeru Format and Converting into Mappings\n",
    "> @danaderp July 29'20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "semeru_format =  path_data + 'se-benchmarking/traceability/datasets/formatted/semeru_format/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting Up SemeruFormat\n",
    "def libest_params(model_prefix = path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_8k'):\n",
    "    return {\n",
    "        'system': 'libest',\n",
    "        'saving_path': path_data+ 'se-benchmarking/traceability/testbeds/processed/',\n",
    "        'language': 'english',\n",
    "        'dataset' : {\n",
    "            'req':pathlib.Path( semeru_format + 'LibEST_semeru_format/requirements'),\n",
    "            'src':pathlib.Path( semeru_format + 'LibEST_semeru_format/source_code'),\n",
    "            'tc':pathlib.Path( semeru_format + 'LibEST_semeru_format/test')\n",
    "        },\n",
    "        'ends': ['.txt','.c','.h'],\n",
    "        'model_prefix':model_prefix,\n",
    "        'encoding':'utf-8'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ebt_params(model_prefix = path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_8k'):\n",
    "    return {\n",
    "        'system': 'ebt',\n",
    "        'saving_path': path_data+ 'se-benchmarking/traceability/testbeds/processed/',\n",
    "        'language': 'english',\n",
    "        'dataset' : {\n",
    "            'req':pathlib.Path( semeru_format + 'EBT_semeru_format/requirements'),\n",
    "            'tc': pathlib.Path( semeru_format + 'EBT_semeru_format/test_cases'),\n",
    "            'src':pathlib.Path( semeru_format + 'EBT_semeru_format/source_code')\n",
    "        },\n",
    "        'ends': ['.txt','.java','.c','.h','.TXT'],\n",
    "        'model_prefix':model_prefix,\n",
    "        #'encoding':'ISO-8859-1'\n",
    "        'encoding':'utf-8' #english encoding\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters = libest_params(model_prefix=model_prefix['bpe8k'])\n",
    "parameters = ebt_params()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters['dataset'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"artifacts: \"  +  str(parameters['dataset'].keys()) )\n",
    "logging.info(\"artifacts: \"  +  str(parameters['dataset']['req'] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [entry for entry in parameters['dataset']['req'].glob('**/*' + \".txt\" )]\n",
    "lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [(filename, os.path.basename(filename), open_file(filename, encoding=parameters['encoding']) ) for filename in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_artifacts( params ):\n",
    "    #Creating the mappings\n",
    "    \n",
    "    df_sys_g = pd.DataFrame( [] ,columns = ['ids','filenames','text']) #global dataframe\n",
    "    \n",
    "    for art in parameters['dataset'].keys():\n",
    "        sys_names = [[entry for entry in parameters['dataset'][art].glob('**/*' + ex )] for ex in parameters['ends']]\n",
    "        sys_names = functools.reduce(lambda a,b : a+b,sys_names) #Flatting\n",
    "        logging.info(\"artifacts: \"  +  str( len(sys_names) ) )\n",
    "        sys_filenames = [(filename, os.path.basename(filename), open_file(filename, encoding=params['encoding']) ) for filename in sys_names]\n",
    "        df_sys_l = pd.DataFrame( sys_filenames ,columns = ['ids','filenames','text']) #local dataframe\n",
    "        df_sys_l['type'] = art\n",
    "        df_sys_g = pd.concat([df_sys_g, df_sys_l ], ignore_index=True, sort=False)\n",
    "    \n",
    "    return df_sys_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = loading_artifacts( params = parameters )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[df_test['type']=='src'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_artifacts( model_prefix, df_sys_all, funct_params ):\n",
    "    df_sys_all = df_sys_all.copy()\n",
    "    for bpe in model_prefix.keys(): #BPE Preprocessing\n",
    "        prep = ConventionalPreprocessing( funct_params( model_prefix[bpe] ) , bpe = True) #Creating the Preprocessing Object\n",
    "        df_sys_all[ bpe ] = prep.bpe_pieces_pipeline( df_sys_all['text'].values ) \n",
    "        \n",
    "    df_sys_all['conv'] = prep.fromdocs_pipeline( df_sys_all['text'].values ) #Conventional Preprocessing\n",
    "    return df_sys_all, prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_sys,r_prep = processing_artifacts( model_prefix = model_prefix, \n",
    "                                          df_sys_all = df_test, \n",
    "                                          funct_params = ebt_params\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_sys.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_prep.SaveCorpus(df_test_sys, language='all-corpus', sep='~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_prep.LoadCorpus(1609221582.171744,language='all-corpus', sep='~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing from Semeru Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Special Case EBT To Create Separate Files [Only one implementation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Canonical EBT\n",
    "def ebt_params(model_prefix = path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_8k'):\n",
    "    return {\n",
    "        'system': 'ebt',\n",
    "        'saving_path': path_data+ 'se-benchmarking/traceability/testbeds/processed/',\n",
    "        'language': 'english',\n",
    "        'dataset' : {\n",
    "            'req':pathlib.Path( semeru_format + 'EBT_semeru_format/requirements.txt'),\n",
    "            'tc': pathlib.Path( semeru_format + 'EBT_semeru_format/test_cases.txt'),\n",
    "            'src':pathlib.Path( semeru_format + 'EBT_semeru_format/source_code')\n",
    "        },\n",
    "        'ends': ['.txt','.java','.c','.h','.TXT'],\n",
    "        'model_prefix':model_prefix,\n",
    "        #'encoding':'ISO-8859-1'\n",
    "        'encoding':'utf-8' #english encoding\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ebt_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['dataset']['req']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_ebt = pd.read_fwf(params['dataset']['req'],header=None,sep=\"/t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(params['dataset']['tc']) as fp:\n",
    "    Lines = fp.readlines()\n",
    "    for line in Lines: \n",
    "        print(line.split(\"\\t\"))\n",
    "        l = line.split(\"\\t\")\n",
    "        p = semeru_format + 'EBT_semeru_format/test_cases/'+l[0]+'.txt'\n",
    "        with open(p, \"w\") as wp: \n",
    "            wp.writelines(l[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_filenames = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the function\n",
    "#base_dir = os.path.abspath(os.getcwd())\n",
    "test_dir = pathlib.Path('test_data/LibEST_semeru_format/test')\n",
    "#path = os.path.join(base_dir, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading all files in a folder\n",
    "for filename in glob.glob(os.path.join(test_dir, '*.txt')):\n",
    "    with open(filename, 'r') as f: # open in readonly mode\n",
    "        dict_filenames[filename] = [f.read()]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[os.path.join(test_dir,filename) for filename in os.listdir(test_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading all files in a folder\n",
    "for filename in [os.path.join(test_dir,filename) for filename in os.listdir(test_dir)]:\n",
    "    with open(filename, 'r') as f: # open in readonly mode\n",
    "        dict_filenames[filename] = [f.read()]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.basename('test_data/LibEST_semeru_format/requirements/RQ17.txt').replace('.txt', '-pre.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(r'[^a-zA-Z\\s]', ' ', \"Ho:;<le_C$%&\\oMe_estTa?@[\\\\is34~\", re.I|re.A).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_terms(clean_punctuation(\"their corresponding URIs:\\n\\n\\n   +------------------------+-----------------+-------------------+\\n   | Operation              |Operation path   | Details           |\\n   +========================+=================+===================+\\n   | Distribution of CA     | /cacerts        | Section 4.1       |\\n   | Certificates (MUST)    |                 |                   |\\n   +------------------------+-----------------+-------------------+\\n   | Enrollment of          | /simpleenroll   | Section 4.2       |\\n   | Clients (MUST)         |                 |                   |\\n   +------------------------+-----------------+-------------------+\\n   | Re-enrollment of       | /simplereenroll | Section 4.2.2     |\\n   | Clients (MUST)         |                 |                   |\\n   +------------------------+-----------------+-------------------+\\n   | Full CMC (OPTIONAL)    | /fullcmc        | Section 4.3       |\\n   +------------------------+-----------------+-------------------+\\n   | Server-Side Key        | /serverkeygen   | Section 4.4       |\\n   | Generation (OPTIONAL)  |                 |                   |\\n   +------------------------+-----------------+-------------------+\\n   | CSR Attributes         | /csrattrs       | Section 4.5       |\\n   | (OPTIONAL)             |                 |                   |\\n   +------------------------+-----------------+-------------------+\\n\\n  \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_terms(split_camel_case_token(dict_filenames['test_data/LibEST_semeru_format/requirements/RQ17.txt'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process = preprocess_pipeline.basic_pipeline(dict_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing Into A File\n",
    "df_pre_processed = pd.DataFrame(pre_process, columns =['filename', 'text']) \n",
    "#/.../benchmarking/traceability/testbeds/nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(os.getcwd())\n",
    "pre_path = '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-tc].csv'\n",
    "final_path = '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-vocab-tc].csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_processed.to_csv(pre_path, header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(a_dict, path):\n",
    "    a_file = open(path, \"w\")\n",
    "\n",
    "    writer = csv.writer(a_file)\n",
    "    for key, value in a_dict.items():\n",
    "        writer.writerow([key, value])\n",
    "    a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1-Building the corpus vocabulary\n",
    "tokenizer_corpora = text.Tokenizer()\n",
    "tokenizer_corpora.fit_on_texts([doc[1] for doc in pre_process])\n",
    "\n",
    "word2id = tokenizer_corpora.word_index\n",
    "id2word = {v:k for k, v in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict(id2word,final_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging All the Vocabulary\n",
    "vocab_path_tc = '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-vocab-tc].csv'\n",
    "df_read_vocab_tc = pd.read_csv(vocab_path_tc, names=['ids', 'text'], header=None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_vocab_tc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path_src = '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-vocab-src].csv'\n",
    "df_read_vocab_src = pd.read_csv(vocab_path_src, names=['ids', 'text'], header=None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_vocab_src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path_req = '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-vocab-req].csv'\n",
    "df_read_vocab_req = pd.read_csv(vocab_path_req, names=['ids', 'text'], header=None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read_vocab_req.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_vocab_set = set(df_read_vocab_tc['text']) | set(df_read_vocab_src['text']) | set(df_read_vocab_req['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(super_vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_vocab = pd.DataFrame(list(super_vocab_set))\n",
    "print(df_all_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_vocab.to_csv('/tf/main/benchmarking/traceability/testbeds/nltk/[libest-vocab-all].csv', \n",
    "                    header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging all the corpuses\n",
    "pre_doc_path_tc = '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-tc].csv'\n",
    "pre_doc_path_req = '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-req].csv'\n",
    "pre_doc_path_src = '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-src].csv'\n",
    "\n",
    "#df_read_pre_tc = pd.read_csv(pre_doc_path_tc, header=None, sep=' ') #Need to inclide sep \n",
    "pre_doc_path = [pre_doc_path_tc, pre_doc_path_req, pre_doc_path_src]\n",
    "lis= [list(df_read[1]) for df_read in [pd.read_csv(path, header=None, sep=' ')for path in pre_doc_path]]\n",
    "print(len(lis[0]), len(lis[1]), len(lis[2]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = functools.reduce(lambda a,b : a+b,lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced_pre = pd.DataFrame(lis) \n",
    "df_reduced_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced_pre.to_csv('/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-all].csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_build_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
