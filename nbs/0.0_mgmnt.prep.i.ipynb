{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp mgmnt.prep.i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "> This module comprises all preprocessing techniques applied to software artifacts:\n",
    ">\n",
    ">> Text-based Artifacts: Classical preprocessing (stemming, lemas, etc) and BPE Binary Artifacts:\n",
    ">\n",
    ">> To Do Vision-based Artifacts:\n",
    ">\n",
    ">> To Do Parsing: Techniques to control and manipulate source code (complete with deep generator project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install dit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import sentencepiece as sp\n",
    "\n",
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def jsonl_list_to_dataframe(file_list, columns=None):\n",
    "    \"\"\"Load a list of jsonl.gz files into a pandas DataFrame.\"\"\"\n",
    "    return pd.concat([pd.read_json(f,\n",
    "                                   orient='records', \n",
    "                                   compression='gzip',\n",
    "                                   lines=True)[columns] \n",
    "                      for f in file_list], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_dfs(path):\n",
    "    \"\"\"\n",
    "        Grabs the different data splits and converts them into dataframes.\n",
    "        Expects format from Code Search Net Challenge.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        files = sorted((path/split).glob(\"**/*.gz\"))\n",
    "        df = jsonl_list_to_dataframe(files, [\"code\", \"docstring\"])\n",
    "        dfs.append(df)\n",
    "        \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/tf/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>protected final void bindIndexed(Configuration...</td>\n",
       "      <td>Bind indexed elements to the supplied collecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>public void setServletRegistrationBeans(\\n\\t\\t...</td>\n",
       "      <td>Set {@link ServletRegistrationBean}s that the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>public void addServletRegistrationBeans(\\n\\t\\t...</td>\n",
       "      <td>Add {@link ServletRegistrationBean}s for the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>public void setServletNames(Collection&lt;String&gt;...</td>\n",
       "      <td>Set servlet names that the filter will be regi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>public void addServletNames(String... servletN...</td>\n",
       "      <td>Add servlet names for the filter.\\n@param serv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  \\\n",
       "0  protected final void bindIndexed(Configuration...   \n",
       "1  public void setServletRegistrationBeans(\\n\\t\\t...   \n",
       "2  public void addServletRegistrationBeans(\\n\\t\\t...   \n",
       "3  public void setServletNames(Collection<String>...   \n",
       "4  public void addServletNames(String... servletN...   \n",
       "\n",
       "                                           docstring  \n",
       "0  Bind indexed elements to the supplied collecti...  \n",
       "1  Set {@link ServletRegistrationBean}s that the ...  \n",
       "2  Add {@link ServletRegistrationBean}s for the f...  \n",
       "3  Set servlet names that the filter will be regi...  \n",
       "4  Add servlet names for the filter.\\n@param serv...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn, df_val, df_tst = get_dfs(path/\"java/final/jsonl\")\n",
    "df_trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save some test data\n",
    "df_trn.sample(frac = 0.01).to_csv('./test_data/trn.csv', index = False)\n",
    "df_val.sample(frac = 0.01).to_csv('./test_data/val.csv', index = False)\n",
    "df_tst.sample(frac = 0.01).to_csv('./test_data/tst.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def df_to_txt_file(df, output, cols):\n",
    "    \"\"\"Converts a dataframe and converts it into a text file that SentencePiece can use to train a BPE model\"\"\"\n",
    "    if cols is None: cols = list(df.columns)\n",
    "    merged_df = pd.concat([df[col] for col in cols])\n",
    "    \n",
    "    with open(output/'text.txt', 'w') as f:\n",
    "        f.write('\\n'.join(list(merged_df)))\n",
    "    return output/'text.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def sp_model_from_df(df, output, model_name, cols = None):\n",
    "    \"\"\"Trains a SentencePiece BPE model from a pandas dataframe\"\"\"\n",
    "    fname = df_to_txt_file(df, output, cols)\n",
    "    sp.SentencePieceTrainer.train(f'--input={fname} --model_prefix={output / model_name} --hard_vocab_limit=false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def sp_model_from_glob(path, glob, model_name):\n",
    "    fns = list(path.glob(glob))\n",
    "    fns = \",\".join(map(str, fns))\n",
    "    sp.SentencePieceTrainer.train(f'--input={fns} --model_prefix={path / model_name} --hard_vocab_limit=false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def gen_hugface_model(df, output, tokenizer = ByteLevelBPETokenizer(), vocab_sz = 30_000, min_freq = 3, cols = None):\n",
    "    fname = df_to_txt_file(df, output, cols)\n",
    "    tokenizer.train(files = [str(fname)], vocab_size = vocab_sz, min_frequency = min_freq, special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<mask>\",\n",
    "    ])\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"./test_data\")\n",
    "model_name = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>private static void createCode(String packageN...</td>\n",
       "      <td>Create the Java</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Override\\n    public void flushCache() {\\n   ...</td>\n",
       "      <td>LI3492-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>public void addRule(IntDependency dependency, ...</td>\n",
       "      <td>Add this dependency with the given count to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Override\\n  public boolean removeIfEquals(K k...</td>\n",
       "      <td>Remove the object from the cache.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>public void marshall(DatasetContentDeliveryDes...</td>\n",
       "      <td>Marshall the given parameter object.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  \\\n",
       "0  private static void createCode(String packageN...   \n",
       "1  @Override\\n    public void flushCache() {\\n   ...   \n",
       "2  public void addRule(IntDependency dependency, ...   \n",
       "3  @Override\\n  public boolean removeIfEquals(K k...   \n",
       "4  public void marshall(DatasetContentDeliveryDes...   \n",
       "\n",
       "                                           docstring  \n",
       "0                                    Create the Java  \n",
       "1                                           LI3492-2  \n",
       "2  Add this dependency with the given count to th...  \n",
       "3                  Remove the object from the cache.  \n",
       "4               Marshall the given parameter object.  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path / 'trn.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = gen_hugface_model(df, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'public', 'Ġstatic', 'Ġvoid', 'Ġmain', '(', 'String', '[]', 'Ġargs', ')', 'Ġ{', 'Ġget', 'Dir', 'From', 'Lib', '();', 'Ġ}', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"public static void main(String[] args) { getDirFromLib(); }\").tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_data/java_tokenizer-vocab.json', 'test_data/java_tokenizer-merges.txt']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save(str(path), \"java_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = {\n",
    "        'first': ['1', '2', '6', '7', '8'],\n",
    "        'second': ['K', 'M', 'O', 'Q', 'S'],\n",
    "        'third': ['L', 'N', 'P', 'R', 'T']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Feature1</th>\n",
       "      <th>Feature2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>K</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>O</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>Q</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>S</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id Feature1 Feature2\n",
       "0  1        K        L\n",
       "1  2        M        N\n",
       "2  6        O        P\n",
       "3  7        Q        R\n",
       "4  8        S        T"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dummy_data2); df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('test_data/text.txt')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_txt_file(df, Path('./test_data'), list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"./test_data\")\n",
    "model_name = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_model_from_dfs(df, path, model_name, list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm = sp.SentencePieceProcessor()\n",
    "spm.Load(str(path/f\"{model_name}.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', 'Hello,', '▁', 'world!']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.EncodeAsPieces(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def tokenize_fns(fns, tokenizer, exts, output, data_type):\n",
    "    docs = []\n",
    "    for fn in fns:\n",
    "        system = fn.parent.name\n",
    "        output_path = output/system/data_type\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        files = []\n",
    "        for ext in exts:\n",
    "            files.extend(fn.glob(f'**/*.{ext}'))\n",
    "        for file in files:\n",
    "            if 'README' not in file.name:\n",
    "                with open(file, encoding='ISO-8859-1') as f:\n",
    "                    docs.append(tokenizer.EncodeAsPieces(f.read()))\n",
    "                with open((output_path/file.name).with_suffix('.bpe'), 'w') as f:\n",
    "                    f.write(' '.join(docs[-1]))\n",
    "            \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def read_bpe_files(path):\n",
    "    bpe_files = []\n",
    "    for file in path.glob('**/*.bpe'):\n",
    "        with open(file) as f:\n",
    "            bpe_files.append(f.read().split(' '))\n",
    "    \n",
    "    return bpe_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def split_lines_to_files(lines, fn_pattern, output_path, tokenizer):\n",
    "    for line in lines:\n",
    "        fn, content = line.split(fn_pattern)\n",
    "        fn = fn.replace('\"', '')\n",
    "        fn = fn.replace(' Test ', '')\n",
    "        content = tokenizer.EncodeAsPieces(content)\n",
    "        with open((output_path/fn).with_suffix('.bpe'), 'w') as f:\n",
    "                    f.write(' '.join(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('../benchmarking/traceability/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm = sp.SentencePieceProcessor()\n",
    "spm.Load(str(path/'datasets/italian/italian_bpe.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../benchmarking/traceability/datasets/italian/ebt')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ebt_path = path/'datasets/italian/ebt'; ebt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ebt_path/'[ebt-raw-req].txt') as f:\n",
    "    split_lines_to_files(f.read().split('\\n')[:-1], '\\t', path/'testbeds/bpe/italian/ebt/req', spm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ebt_path/'[ebt-raw-tc].txt') as f:\n",
    "    split_lines_to_files(f.read().split('\\n')[:-1], 'case:', path/'testbeds/bpe/italian/ebt/tc', spm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00_data.preprocessing.ipynb\t 07_model.train.ipynb\r\n",
      "01_data.exploratory.ipynb\t 08_data.exploratory.information.ipynb\r\n",
      "02_data.management.ipynb\t 09_data.exploratory.stats.ipynb\r\n",
      "03_data.representation.ipynb\t 10_data.exploratory.visualize.ipynb\r\n",
      "04_templates.example.ipynb\t index.ipynb\r\n",
      "05_blog.example.ipynb\t\t test_data\r\n",
      "06_benchmark.traceability.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.preprocessing.ipynb.\n",
      "Converted 01_data.exploratory.ipynb.\n",
      "Converted 02_data.management.ipynb.\n",
      "Converted 03_data.representation.ipynb.\n",
      "Converted 04_templates.example.ipynb.\n",
      "Converted 05_blog.example.ipynb.\n",
      "Converted 06_benchmark.traceability.ipynb.\n",
      "Converted 07_model.train.ipynb.\n",
      "Converted 08_data.exploratory.information.ipynb.\n",
      "Converted 09_data.exploratory.stats.ipynb.\n",
      "Converted 10_data.exploratory.visualize.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
