{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils.clusterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyclustering\n",
      "  Downloading pyclustering-0.10.1.2.tar.gz (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 7.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from pyclustering) (1.5.4)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from pyclustering) (3.1.1)\n",
      "Requirement already satisfied: numpy>=1.15.2 in /usr/local/lib/python3.6/dist-packages (from pyclustering) (1.19.5)\n",
      "Collecting Pillow>=5.2.0\n",
      "  Downloading Pillow-8.2.0-cp36-cp36m-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 32.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->pyclustering) (2.8.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->pyclustering) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->pyclustering) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->pyclustering) (1.1.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=3.0.0->pyclustering) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.0->pyclustering) (41.2.0)\n",
      "Building wheels for collected packages: pyclustering\n",
      "  Building wheel for pyclustering (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyclustering: filename=pyclustering-0.10.1.2-py3-none-any.whl size=2395105 sha256=51deee957a1775079a5e9e71acbb383af11cb1b6a306647375519dc586752d86\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/69/89/146543430cba41ea0dd0c553a8a325367ce91dba20cf1c9086\n",
      "Successfully built pyclustering\n",
      "Installing collected packages: Pillow, pyclustering\n",
      "Successfully installed Pillow-8.2.0 pyclustering-0.10.1.2\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install pyclustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clusterization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import logging\n",
    "\n",
    "import sentencepiece as sp\n",
    "\n",
    "from pyclustering.cluster.kmedoids import kmedoids\n",
    "from pyclustering.utils.metric import euclidean_distance_square\n",
    "from pyclustering.cluster.silhouette import silhouette, silhouette_ksearch_type, silhouette_ksearch\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances_argmin_min\n",
    "\n",
    "import numpy as np\n",
    "from abc import ABC\n",
    "from typing import Tuple\n",
    "\n",
    "# Configs\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to allow flexible implementation of several clustering techniques, a base CustomDistance class is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class CustomDistance(ABC):\n",
    "    def compute_distance(self, x, y) -> float:\n",
    "        \"\"\"\n",
    "        Computes the distance between 2 vectors according to a \n",
    "        particular distance metric\n",
    "        :param x: Vector\n",
    "        :param y: Vector\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class EuclideanDistance(CustomDistance):\n",
    "    \"\"\"Euclidean implementation of distance\"\"\"\n",
    "    def compute_distance(self, x, y) -> float:\n",
    "        return euclidean_distance_square(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# Uses PCA first and then t-SNE\n",
    "def reduce_dims(feature_vectors, dims = 2):\n",
    "    # hyperparameters from https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b\n",
    "    pca = PCA(n_components=50)\n",
    "    pca_features = pca.fit_transform(feature_vectors)\n",
    "\n",
    "    tsne = TSNE(n_components=dims, verbose=1, perplexity=40, n_iter=300)\n",
    "    tsne_features = tsne.fit_transform(pca_features)\n",
    "    \n",
    "    return tsne_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def get_silhouette(samples1, samples2):\n",
    "    cluster1, medoid_id1, kmedoid_instance1 = run_kmedoids(samples1, 1)\n",
    "    cluster2, medoid_id2, kmedoid_instance12 = run_kmedoids(samples2, 1)\n",
    "    cluster2 = np.array([[len(samples1) + x for x in cluster2[0]]])\n",
    "    samples = np.concatenate((samples1, samples2), axis=0)\n",
    "    clusters = np.concatenate((cluster1, cluster2), axis=0)\n",
    "    score = sum(silhouette(samples, clusters).process().get_score()) / len(samples)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def k_means(feature_vectors, k_range=[2, 3]):\n",
    "    # finding best k\n",
    "    bst_k          = k_range[0]\n",
    "    bst_silhouette = -1\n",
    "    bst_labels     = None\n",
    "    bst_centroids  = None\n",
    "    bst_kmeans     = None\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters = k)\n",
    "        kmeans.fit(feature_vectors)\n",
    "\n",
    "        labels    = kmeans.predict(feature_vectors)\n",
    "        centroids = kmeans.cluster_centers_\n",
    "        \n",
    "        silhouette_avg = silhouette_score(feature_vectors, labels)\n",
    "        if silhouette_avg > bst_silhouette:\n",
    "            bst_k          = k\n",
    "            bst_silhouette = silhouette_avg\n",
    "            bst_labels     = labels\n",
    "            bst_centroids  = centroids\n",
    "            bst_kmeans     = kmeans\n",
    "    logger.info(f'Best k = {bst_k} with a silhouette score of {bst_silhouette}')\n",
    "    \n",
    "    centroid_mthds = pairwise_distances_argmin_min(bst_centroids, feature_vectors)\n",
    "    return bst_labels, bst_centroids, bst_kmeans, centroid_mthds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def clusterize(feature_vecs, k_range = [2], dims = 2):\n",
    "    feature_vectors = reduce_dims(np.array(list(zip(*feature_vecs))[1]), dims = dims)\n",
    "    experimental_vectors = feature_vectors#[:len(feature_vectors) * 0.1]\n",
    "    labels, centroids, kmeans, centroid_mthds = k_means(experimental_vectors, k_range = k_range)\n",
    "    return (feature_vectors, centroid_mthds, labels, centroids, kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def find_best_k(samples):\n",
    "    search_instance = silhouette_ksearch(samples, 2, 10, algorithm=silhouette_ksearch_type.KMEDOIDS).process()\n",
    "    amount = search_instance.get_amount()\n",
    "    scores = search_instance.get_scores()\n",
    "    \n",
    "    print(f\"Best Silhouette Score for k = {amount}: {scores[amount]}\")\n",
    "    \n",
    "    return amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def run_kmedoids(samples, k):\n",
    "    initial_medoids = list(range(k))\n",
    "    # Create instance of K-Medoids algorithm.\n",
    "    kmedoids_instance = kmedoids(samples, initial_medoids)\n",
    "    kmedoids_instance.process()\n",
    "    clusters = kmedoids_instance.get_clusters()\n",
    "    medoid_ids = kmedoids_instance.get_medoids()\n",
    "    \n",
    "    return clusters, medoid_ids, kmedoids_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def perform_clusterize_kmedoids(data: np.array, dims: int = 2) -> Tuple:\n",
    "    reduced_data = reduce_dims(data, dims = dims)\n",
    "    k = find_best_k(reduced_data)\n",
    "    \n",
    "    clusters, medoid_ids, kmedoids_instance = run_kmedoids(reduced_data, k)\n",
    "    \n",
    "    return reduced_data, clusters, medoid_ids, kmedoids_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def clusterize_kmedoids(samples, dims = 2):\n",
    "    samples = reduce_dims(np.array(list(zip(*samples))[1]), dims = dims)\n",
    "    k = find_best_k(samples)\n",
    "    clusters, medoid_ids, kmedoids_instance = run_kmedoids(samples, k)\n",
    "    \n",
    "    return samples, clusters, medoid_ids, kmedoids_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def new_clusterize_kmedoids(h_samples, m1_samples, m2_samples, m3_samples, dims = 2):\n",
    "    samples = np.concatenate((h_samples, m1_samples, m2_samples, m3_samples), axis=0)\n",
    "    samples = reduce_dims(samples, dims = dims) # np.array(list(zip(*samples)))[0], dims = dims)\n",
    "    h_samples, m1_samples, m2_samples, m3_samples = samples[:len(h_samples)], samples[len(h_samples):len(h_samples) + len(m1_samples)], samples[len(h_samples) + len(m1_samples):len(h_samples) + len(m1_samples) + len(m2_samples)], samples[len(h_samples) + len(m1_samples) + len(m2_samples):]\n",
    "    h_k = find_best_k(h_samples)\n",
    "    h_clusters, h_medoid_ids, h_kmedoids_instance = run_kmedoids(h_samples, h_k)\n",
    "    m1_k = find_best_k(m1_samples)\n",
    "    m1_clusters, m1_medoid_ids, m1_kmedoids_instance = run_kmedoids(m1_samples, m1_k)\n",
    "    m2_k = find_best_k(m2_samples)\n",
    "    m2_clusters, m2_medoid_ids, m2_kmedoids_instance = run_kmedoids(m2_samples, m2_k)\n",
    "    m3_k = find_best_k(m3_samples)\n",
    "    m3_clusters, m3_medoid_ids, m3_kmedoids_instance = run_kmedoids(m3_samples, m3_k)\n",
    "    \n",
    "    return (\n",
    "        (h_samples, h_clusters, h_medoid_ids, h_kmedoids_instance),\n",
    "        (m1_samples, m1_clusters, m1_medoid_ids, m1_kmedoids_instance),\n",
    "        (m2_samples, m2_clusters, m2_medoid_ids, m2_kmedoids_instance),\n",
    "        (m3_samples, m3_clusters, m3_medoid_ids, m3_kmedoids_instance)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototypes and criticisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def gen_criticisms(samples, prototypes, n = None, distance = None):\n",
    "    if n is None: n = len(prototypes)\n",
    "    if distance is None:\n",
    "        distance = EuclideanDistance()\n",
    "    crits = []\n",
    "    for x in samples:\n",
    "        mean_dist_x = 0.\n",
    "        for x_i in samples:\n",
    "            mean_dist_x += distance.compute_distance(x, x_i)\n",
    "        mean_dist_x = mean_dist_x / len(x)\n",
    "        \n",
    "        mean_dist_proto = 0.\n",
    "        for z_j in prototypes:\n",
    "            mean_dist_proto += distance.compute_distance(x, z_j)\n",
    "        mean_dist_proto = mean_dist_proto / len(prototypes)\n",
    "        \n",
    "        crits.append(mean_dist_x - mean_dist_proto)\n",
    "    \n",
    "    crits = np.array(crits)\n",
    "    crit_ids = crits.argsort()[-n:][::-1]\n",
    "    \n",
    "    return crits, crit_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 0.0_mgmnt.prep.i.ipynb.\n",
      "Converted 0.1_mgmnt.prep.conv.ipynb.\n",
      "Converted 0.3_mgmnt.prep.bpe.ipynb.\n",
      "Converted 0.6_mgmnt.prep.nltk.ipynb.\n",
      "Converted 0.7_mgmnt.prep.files_mgmnt.ipynb.\n",
      "Converted 0.8_mgmnt.prep.bpe_tokenization.ipynb.\n",
      "Converted 1.0_exp.i.ipynb.\n",
      "Converted 1.1_exp.info-[inspect].ipynb.\n",
      "Converted 1.1_exp.info.ipynb.\n",
      "Converted 1.2_exp.csnc.ipynb.\n",
      "Converted 1.2_exp.gen.code.ipynb.\n",
      "Converted 1.3_exp.csnc_python.ipynb.\n",
      "Converted 10.0_utils.clusterization.ipynb.\n",
      "Converted 10.1_utils.visualization.ipynb.\n",
      "Converted 2.0_repr.codebert.ipynb.\n",
      "Converted 2.0_repr.i.ipynb.\n",
      "Converted 2.1_repr.codeberta.ipynb.\n",
      "Converted 2.1_repr.roberta.train.ipynb.\n",
      "Converted 2.2_repr.roberta.eval.ipynb.\n",
      "Converted 2.3_repr.word2vec.train.ipynb.\n",
      "Converted 2.6_repr.word2vec.eval.ipynb.\n",
      "Converted 2.7_repr.distmetrics.ipynb.\n",
      "Converted 2.8_repr.sentence_transformers.ipynb.\n",
      "Converted 3.1_mining.unsupervised.traceability.eda.ipynb.\n",
      "Converted 3.2_mining.unsupervised.eda.traceability.d2v.ipynb.\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "h\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "Converted 3.2_mining.unsupervised.mutual_information.traceability.approach.sacp-w2v.ipynb.\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "h\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "E\n",
      "Converted 3.2_mining.unsupervised.mutual_information.traceability.approach.sacp.w2v.ipynb.\n",
      "Converted 3.2_mutual_information_theory.eval.ipynb.\n",
      "Converted 3.4_facade.ipynb.\n",
      "Converted 4.0_mining.ir.ipynb.\n",
      "Converted 5.0_experiment.mining.ir.unsupervised.d2v.ipynb.\n",
      "Converted 5.0_experiment.mining.ir.unsupervised.w2v-exp4.ipynb.\n",
      "Converted 5.0_experiment.mining.ir.unsupervised.w2v-exp5.ipynb.\n",
      "Converted 5.0_experiment.mining.ir.unsupervised.w2v-exp6.ipynb.\n",
      "Converted 5.0_experiment.mining.ir.unsupervised.w2v.ipynb.\n",
      "Converted 6.0_desc.stats.ipynb.\n",
      "Converted 6.0_eval.mining.ir.unsupervised.x2v.ipynb.\n",
      "Converted 6.1_desc.metrics.java.ipynb.\n",
      "Converted 6.1_desc.metrics.main.ipynb.\n",
      "Converted 6.1_desc.metrics.se.ipynb.\n",
      "Converted 6.2_desc.metrics.java.ipynb.\n",
      "Converted 6.2_desc.metrics.main.ipynb.\n",
      "Converted 7.0_inf.i.ipynb.\n",
      "Converted 7.1_inf.bayesian.ipynb.\n",
      "Converted 7.2_inf.causal.ipynb.\n",
      "Converted 7.3_statistical_analysis.ipynb.\n",
      "Converted 8.0_interpretability.i.ipynb.\n",
      "Converted 8.1_interpretability.error_checker.ipynb.\n",
      "Converted 8.2_interpretability.metrics_python.ipynb.\n",
      "Converted 8.3_interpretability.metrics_java.ipynb.\n",
      "Converted 8.4_interpretability.metrics_example.ipynb.\n",
      "Converted 8.5_interpretability.d2v_vectorization.ipynb.\n",
      "Converted 8.6_interpretability.prototypes_criticisms.ipynb.\n",
      "Converted 8.7_interpretability.info_theory_processing.ipynb.\n",
      "Converted 9.0_ds.causality.eval.traceability.ipynb.\n",
      "Converted 9.0_ds.description.eval.traceability.ipynb.\n",
      "Converted 9.0_ds.prediction.eval.traceability.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
