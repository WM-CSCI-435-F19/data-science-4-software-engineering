{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp mgmnt.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Representation\n",
    "\n",
    "> This module comprises corpus representation from a file or Mongo DB\n",
    ">This is an adapted version of Daniel McCrystal June 2018\n",
    "\n",
    ">Author: @danaderp March 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import unittest\n",
    "import random\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from pathlib import Path\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip -qq cisco/CSB-CICDPipelineEdition-master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Corpus:\n",
    "    \"\"\"\n",
    "    Represents a set of source artifacts, a set of target artifacts,\n",
    "    and a ground truth model that represents the links between them.\n",
    "    NOTE: This assumes that artifacts separated by line in the same file begin with\n",
    "    an ID. Thus, the first word of every line will not be considered part of the artifact\n",
    "    for semantic purposes\n",
    "    Attributes:\n",
    "        _name (str): The identifier of the corpus dataset\n",
    "        _corpus_root (str): Path to the root directory of the corpus, from\n",
    "            which the source, target, and truth files will be derived\n",
    "        _sources (list of str): one str per artifact\n",
    "        _targets (list of str): one str per artifact\n",
    "        _truth (dict of str:(dict of str:int)): Holds the truth values of links\n",
    "            between sources and targets. _truth[source][target] == 1 if link exists,\n",
    "            0 otherwise.\n",
    "        _source_index (list of str): aligned with source so that source_index[i]\n",
    "            contains the filename or identifier for source[i]\n",
    "        _target_index (list of str): aligned with target so that\n",
    "            target_index[i] contains the filename or identifier for target[i]\n",
    "        _filetype_whitelist (list of str): See __init__ documentation\n",
    "        _filetype_blacklist(list of str): See __init__ documentation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, corpus_root='', \n",
    "                 source_path='requirements', \n",
    "                 target_path='source_code', \n",
    "                 stop_words_path=None,\n",
    "                 truth_path=None, \n",
    "                 execution_traces=None,\n",
    "                 corpus_code=None, \n",
    "                 languages=['english'], \n",
    "                 filetype_whitelist=None, \n",
    "                 filetype_blacklist=None, \n",
    "                 blank=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            name (str): The identifier of the corpus dataset\n",
    "            corpus_root (str): Path to the root directory of the corpus, from\n",
    "                which the source, target, and truth files will be derived\n",
    "            source_path (str): Path from corpus root to file or directory of\n",
    "                source corpus. If directory, artifacts will be parsed from\n",
    "                files in the directory. If file, artifacts will be parsed from\n",
    "                lines in the file.\n",
    "            target_path (str): Path from corpus root to file or directory of\n",
    "                target corpus. If directory, artifacts will be parsed from\n",
    "                files in the directory. If file, artifacts will be parsed from\n",
    "                lines in the file.\n",
    "            truth_path (str): Path to file containing ground truth\n",
    "            natural_language (str or list of str, optional): The human language(s) that artifacts\n",
    "                are written in. Will be matched to the corresponding stop words\n",
    "                list and stemmer.\n",
    "            code_language (str or list of str, optional): The programming language(s) that artifacts\n",
    "                are written in. Will be matched to the corresponding stop words list.\n",
    "            filetype_whitelist (list of str, optional): List of file extensions\n",
    "                which will be converted into artifacts. If None (default), all\n",
    "                file extensions will be converted. Do not include dot.\n",
    "            filetype_blacklist(list of str, optional): List of file extensions\n",
    "                which will be ignored when converting into artifacts. If None\n",
    "                (default), all file extensions will be converted. Do not include\n",
    "                dot.\n",
    "        \"\"\"\n",
    "        self._name = name\n",
    "\n",
    "        if blank:\n",
    "            return\n",
    "\n",
    "        self._corpus_root = corpus_root\n",
    "\n",
    "        self._filetype_whitelist = filetype_whitelist\n",
    "        self._filetype_blacklist = filetype_blacklist\n",
    "\n",
    "        self._source_index = []\n",
    "        self._target_index = []\n",
    "\n",
    "        if type(source_path) is not list:\n",
    "            source_path = [source_path]\n",
    "\n",
    "        if type(target_path) is not list:\n",
    "            target_path = [target_path]\n",
    "\n",
    "        self._sources = []\n",
    "        for sp in source_path:\n",
    "            self._sources += self._parse_artifacts(sp, self._source_index)\n",
    "\n",
    "        self._targets = []\n",
    "        for tp in target_path:\n",
    "            self._targets += self._parse_artifacts(tp, self._target_index)\n",
    "\n",
    "        self._truth = None\n",
    "        if truth_path is not None:\n",
    "            self._truth = dict()\n",
    "            self._all_links = []\n",
    "\n",
    "            for source in self._source_index:\n",
    "                self._truth[source] = dict()\n",
    "                for target in self._target_index:\n",
    "                    self._truth[source][target] = 0\n",
    "                    self._all_links.append((source, target))\n",
    "\n",
    "            if type(truth_path) is not list:\n",
    "                truth_path = [truth_path]\n",
    "\n",
    "            for tp in truth_path:\n",
    "                with open(corpus_root + tp, 'r') as truth_file:\n",
    "                    for line in truth_file.readlines():\n",
    "                        tokens = line.split()\n",
    "\n",
    "                        source = tokens[0]\n",
    "\n",
    "                        if source not in self._truth:\n",
    "                            raise KeyError(\"Source artifact \\'\" + source + \"\\' in truth file not a recognized artifact\")\n",
    "\n",
    "                        for target in tokens[1:]:\n",
    "                            if target not in self._truth[source]:\n",
    "                                print(source)\n",
    "                                print(self._truth[source])\n",
    "                                raise KeyError(\"Target artifact \\'\" + target + \"\\' in truth file not a recognized artifact\")\n",
    "\n",
    "                            self._truth[source][target] = 1\n",
    "\n",
    "        self._execution_traces = None\n",
    "        if execution_traces is not None:\n",
    "            self._execution_traces = dict()\n",
    "            with open(corpus_root + execution_traces, 'r') as et_file:\n",
    "                for line in et_file.readlines():\n",
    "                    tokens = line.split()\n",
    "                    if tokens[0] not in self._execution_traces:\n",
    "                        self._execution_traces[tokens[0]] = []\n",
    "                    if tokens[1] not in self._execution_traces:\n",
    "                        self._execution_traces[tokens[1]] = []\n",
    "\n",
    "                    self._execution_traces[tokens[0]].append(tokens[1])\n",
    "                    self._execution_traces[tokens[1]].append(tokens[0])\n",
    "\n",
    "        self._stop_words = []\n",
    "        self._stemmers = []\n",
    "\n",
    "        if languages is not None:\n",
    "            for language in languages:\n",
    "                # check if there's a stop word list\n",
    "                stop_words_path =  os.path.join(os.getcwd(), 'test_data/config_corpus/' + \n",
    "                                                language + '_stop_words.txt')\n",
    "                try:\n",
    "                    with open(stop_words_path) as stop_words_file:\n",
    "                        self._stop_words +=         stop_words_file.read().split()\n",
    "                        print(\"Found stop words file: \" + language + \"_stop_words.txt\")\n",
    "                except FileNotFoundError:\n",
    "                    print(\"No stop words found for language: [\" + language + \"]\")\n",
    "\n",
    "                # check if there's a stemmer\n",
    "                try:\n",
    "                    stemmer = SnowballStemmer(language)\n",
    "                    print(\"Detected natural language: [\" + language + \"], generating stemmer\")\n",
    "                    self._stemmers.append(stemmer)\n",
    "                except ValueError:\n",
    "                    print(\"No natural language stemmer detected for language: [\" + language + \"]\")\n",
    "\n",
    "        self._corpus_code = corpus_code\n",
    "\n",
    "    def _parse_artifacts(self, path, index):\n",
    "        \"\"\"\n",
    "        Reads and indexes artifacts from file or files given in path\n",
    "        Args:\n",
    "            path (str): Path to the file or folder from which to parse artifacts\n",
    "            index (list of str): Stores the identifiers for each artifact. This\n",
    "                list should be empty when this method is called (the method will\n",
    "                populate it).\n",
    "        Returns:\n",
    "            list of str: The list of artifacts\n",
    "        \"\"\"\n",
    "        root = self._corpus_root\n",
    "        print(\"Finding artifacts in: \" + root + path)\n",
    "        store = []\n",
    "        if os.path.isfile(root + path):\n",
    "            print(\"Getting artifacts by line from file\")\n",
    "            with open(root + path, 'r', encoding='utf-8', errors='ignore') as artifacts_file:\n",
    "                for line in artifacts_file.readlines():\n",
    "                    tokens = line.split()\n",
    "                    artifact = ' '.join(tokens[1:])\n",
    "                    index.append(tokens[0])\n",
    "                    store.append(artifact)\n",
    "                print(\"Read \" + str(len(store)) + \" artifacts from file\")\n",
    "        else:\n",
    "            print(\"Getting artifacts by file from directory\")\n",
    "            for element in os.listdir(root + path):\n",
    "                self._parse_artifacts_recur(path + '/', element, index, store)\n",
    "            print(\"Read \" + str(len(store)) + \" artifacts from directory\")\n",
    "\n",
    "        print()\n",
    "        return store\n",
    "\n",
    "    def _parse_artifacts_recur(self, subroot, element, index, store):\n",
    "        \"\"\"\n",
    "        Recursively searches for files in the directory given in the directory\n",
    "        path from the corpus root.\n",
    "        Args:\n",
    "            path (str): Path to a file or directory. If path is a file, this is\n",
    "                the base case and the file is read and an artifact is created\n",
    "                and indexed\n",
    "            index (list of str): Cumulatively stores the identifiers for each artifact.\n",
    "            store (list of str): Cumulatively stores the artifacts.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        root = self._corpus_root\n",
    "        full_path = root + subroot + element\n",
    "\n",
    "        if os.path.isfile(full_path):\n",
    "            extension = element[element.index('.')+1:].lower()\n",
    "\n",
    "            if self._filetype_whitelist and extension not in self._filetype_whitelist:\n",
    "                return\n",
    "            if self._filetype_blacklist and extension in self._filetype_blacklist:\n",
    "                return\n",
    "\n",
    "            element_clean = element.replace('/', '.')\n",
    "            index.append(element_clean)\n",
    "            with open(full_path, 'r', encoding='utf-8', errors='ignore') as artifact_file:\n",
    "                store.append(artifact_file.read())\n",
    "\n",
    "        else:\n",
    "            for sub_element in os.listdir(root + subroot + element):\n",
    "                sub_element = element + '/' + sub_element\n",
    "                self._parse_artifacts_recur(subroot, sub_element, index, store)\n",
    "\n",
    "    #Getters            \n",
    "    def get_sources(self):\n",
    "        return self._sources\n",
    "\n",
    "    def get_targets(self):\n",
    "        return self._targets\n",
    "\n",
    "    def get_source_names(self):\n",
    "        return self._source_index\n",
    "\n",
    "    def get_target_names(self):\n",
    "        return self._target_index\n",
    "\n",
    "    def get_source_artifact_at_index(self, index):\n",
    "        return self._sources[index]\n",
    "\n",
    "    def get_target_artifact_at_index(self, index):\n",
    "        return self._targets[index]\n",
    "\n",
    "    def get_source_artifact_by_name(self, name):\n",
    "        try:\n",
    "            index = self._source_index.index(name)\n",
    "            return self._sources[index]\n",
    "        except ValueError:\n",
    "            print(\"Source artifact \\'\" + name + \"\\' not found\")\n",
    "            return None\n",
    "\n",
    "    def get_target_artifact_by_name(self, name):\n",
    "        try:\n",
    "            index = self._target_index.index(name)\n",
    "            return self._targets[index]\n",
    "        except ValueError:\n",
    "            print(\"Target artifact \\'\" + name + \"\\' not found\")\n",
    "            return None\n",
    "\n",
    "    def get_source_name_by_index(self, index):\n",
    "        return self._source_index[index]\n",
    "\n",
    "    def get_target_name_by_index(self, index):\n",
    "        return self._target_index[index]\n",
    "\n",
    "    def get_truth_value(self, source, target):\n",
    "        if self._truth is not None:\n",
    "            return self._truth[source][target]\n",
    "\n",
    "    def get_truth_dict(self):\n",
    "        return dict(self._truth)\n",
    "\n",
    "    def get_execution_trace(self, artifact):\n",
    "        if artifact in self._execution_traces:\n",
    "            return self._execution_traces[artifact]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def get_all_execution_traces(self):\n",
    "        return dict(self._execution_traces)\n",
    "\n",
    "    def get_stop_words(self):\n",
    "        return self._stop_words\n",
    "\n",
    "    def get_stemmers(self):\n",
    "        return self._stemmers\n",
    "\n",
    "    def get_subset(self, percent):\n",
    "        if self._truth is None:\n",
    "            print(\"No ground truth in corpus\")\n",
    "            return\n",
    "\n",
    "        total_num_links = len(self._all_links)\n",
    "        num_links = int(total_num_links * (percent / 100))\n",
    "\n",
    "        link_subset = random.sample(self._all_links, num_links)\n",
    "\n",
    "        link_subset_dict = dict()\n",
    "\n",
    "        for link in link_subset:\n",
    "            source = link[0]\n",
    "            target = link[1]\n",
    "            link_status = self.get_truth_value(source, target)\n",
    "\n",
    "            if source not in link_subset_dict:\n",
    "                link_subset_dict[source] = dict()\n",
    "\n",
    "            link_subset_dict[source][target] = link_status\n",
    "\n",
    "        return link_subset_dict\n",
    "\n",
    "    def get_subsets(self, percent, n_trials):\n",
    "        if self._truth is None:\n",
    "            print(\"No ground truth in corpus\")\n",
    "            return\n",
    "\n",
    "        subsets = []\n",
    "        for n in range(n_trials):\n",
    "            subsets.append(self.get_subset(percent))\n",
    "        return subsets\n",
    "\n",
    "    def get_positive_link_subset(self, num_sources, accuracy=0.75):\n",
    "        if self._truth is None:\n",
    "            print(\"No ground truth in corpus\")\n",
    "            return\n",
    "\n",
    "        source_samples = random.sample(self.get_source_names(), num_sources)\n",
    "        link_subset_dict = dict()\n",
    "\n",
    "        targets = self.get_target_names()\n",
    "\n",
    "        for source in source_samples:\n",
    "            link_subset_dict[source] = dict()\n",
    "\n",
    "            recall_rate = random.gauss(accuracy, 0.1)\n",
    "            if recall_rate < 0:\n",
    "                recall_rate = 0\n",
    "            elif recall_rate > 1:\n",
    "                recall_rate = 1\n",
    "\n",
    "            actual_links = [target for target in targets if self.get_truth_value(source, target) == 1]\n",
    "\n",
    "            num_links_found = int(len(actual_links) * recall_rate)\n",
    "\n",
    "            for target in targets:\n",
    "                link_subset_dict[source][target] = 0\n",
    "\n",
    "            links_found = random.sample(actual_links, num_links_found)\n",
    "            for target in links_found:\n",
    "                link_subset_dict[source][target] = 1\n",
    "\n",
    "        return link_subset_dict\n",
    "\n",
    "    def get_positive_link_subsets(self, num_sources, n_trials, accuracy=0.75):\n",
    "        if self._truth is None:\n",
    "            print(\"No ground truth in corpus\")\n",
    "            return\n",
    "\n",
    "        subsets = []\n",
    "        for n in range(n_trials):\n",
    "            subsets.append(self.get_positive_link_subset(num_sources, accuracy=accuracy))\n",
    "\n",
    "        return subsets\n",
    "\n",
    "    def get_link_sample(self, num_positive=5, num_negative=5):\n",
    "        if self._truth is None:\n",
    "            print(\"No ground truth in corpus\")\n",
    "            return\n",
    "\n",
    "        sources = self.get_source_names()\n",
    "        targets = self.get_target_names()\n",
    "\n",
    "        positives = []\n",
    "        negatives = []\n",
    "\n",
    "        for source in sources:\n",
    "            for target in targets:\n",
    "                if self.get_truth_value(source, target) == 1:\n",
    "                    positives.append((source, target))\n",
    "                else:\n",
    "                    negatives.append((source, target))\n",
    "\n",
    "        if len(positives) < num_positive:\n",
    "            print(\"Warning: \" + str(num_positive) + \" positive links were requested, but only \" + str(len(positives)) + \" positive links exist.\")\n",
    "            positive_subset = positives\n",
    "        else:\n",
    "            positive_subset = random.sample(positives, num_positive)\n",
    "\n",
    "        if len(negatives) < num_negative:\n",
    "            print(\"Warning: \" + str(num_negative) + \" negative links were requested, but only \" + str(len(negatives)) + \" negative links exist.\")\n",
    "            negative_subset = negatives\n",
    "        else:\n",
    "            negative_subset = random.sample(negatives, num_negative)\n",
    "\n",
    "        return positive_subset + negative_subset\n",
    "\n",
    "    def get_corpus_name(self):\n",
    "        return self._name\n",
    "\n",
    "    def get_corpus_code(self):\n",
    "        if self._corpus_code is not None:\n",
    "            return self._corpus_code\n",
    "        else:\n",
    "            print(\"No corpus code found for: \" + self.get_corpus_name())\n",
    "\n",
    "    def get_corpus_root(self):\n",
    "        return self._corpus_root\n",
    "\n",
    "    def get_raw_string(self):\n",
    "        output = ''\n",
    "        for doc in self._sources + self._targets:\n",
    "            output += doc + '\\n'\n",
    "        return output\n",
    "\n",
    "    def generate_raw_file(self, output_filename=None):\n",
    "        if output_filename is None:\n",
    "            output_filename = self._corpus_root + self._corpus_code + '_raw_corpus.txt'\n",
    "        with open(output_filename, 'w+') as output_file:\n",
    "            output_file.write(self.get_raw_string())\n",
    "\n",
    "    def verify_datastore(self, datastore_manager):\n",
    "        sources = self.get_source_names()\n",
    "        targets = self.get_target_names()\n",
    "\n",
    "        complete = True\n",
    "        for source in sources:\n",
    "            for target in targets:\n",
    "                if not datastore_manager.file_exists(source, target, 'NUTS'):\n",
    "                    print(\"Missing file for (\" + source + \", \" + target + \")\")\n",
    "                    complete = False\n",
    "\n",
    "        if complete:\n",
    "            print(\"All links have been generated for \" + self.get_corpus_name() + \"!\")\n",
    "        else:\n",
    "            print(\"Some links are missing for \" + self.get_corpus_name() + \"...\")\n",
    "    # STATIC\n",
    "    @classmethod\n",
    "    def get_preset_corpus(cls, corpus_code):\n",
    "        if corpus_code == 'LibEST':\n",
    "            corpus_code = '0_1'\n",
    "        elif corpus_code == 'EBT':\n",
    "            corpus_code = '1_1'\n",
    "        elif corpus_code == 'eTOUR':\n",
    "            corpus_code = '2_0'\n",
    "        elif corpus_code == 'iTrust':\n",
    "            corpus_code = '3_0'\n",
    "        elif corpus_code == 'Albergate':\n",
    "            corpus_code = '4_0'\n",
    "        elif corpus_code == 'SMOS':\n",
    "            corpus_code = '5_0'\n",
    "\n",
    "        try:\n",
    "            separator_index = corpus_code.index('_')\n",
    "        except ValueError:\n",
    "            print(\"Invalid corpus code: \" + corpus_code)\n",
    "            return\n",
    "\n",
    "        dataset = corpus_code[:separator_index]\n",
    "        subset = corpus_code[separator_index+1:]\n",
    "\n",
    "        dataset_name = None\n",
    "        modifier = None\n",
    "        source_path = None\n",
    "        target_path = None\n",
    "        truth_path = None\n",
    "        execution_traces = None\n",
    "        languages = None\n",
    "\n",
    "        if dataset == '0':\n",
    "            dataset_name = 'LibEST'\n",
    "            source_path = 'requirements'\n",
    "            languages = ['english', 'C']\n",
    "            execution_traces = 'execution_traces.txt'\n",
    "\n",
    "            if subset == '0':\n",
    "                modifier = '(RQ to Code and Tests)'\n",
    "                target_path = ['source_code', 'test']\n",
    "                truth_path = ['req_to_code_ground.txt', 'req_to_test_ground.txt']\n",
    "\n",
    "            elif subset == '1':\n",
    "                modifier = '(RQ to Code)'\n",
    "                target_path = 'source_code'\n",
    "                truth_path = 'req_to_code_ground.txt'\n",
    "\n",
    "            elif subset == '2':\n",
    "                modifier = '(RQ to Tests)'\n",
    "                target_path = 'test'\n",
    "                truth_path = 'req_to_test_ground.txt'\n",
    "\n",
    "            else:\n",
    "                print(\"Unrecognized corpus code: \" + corpus_code)\n",
    "                return\n",
    "\n",
    "        elif dataset == '1':\n",
    "            dataset_name = 'EBT'\n",
    "            source_path = 'requirements.txt'\n",
    "            languages = ['english']\n",
    "\n",
    "            if subset == '0':\n",
    "                modifier = '(RQ to Code and Tests)'\n",
    "                target_path = ['source_code', 'test_cases.txt']\n",
    "                truth_path = 'both_ground.txt'\n",
    "\n",
    "            elif subset == '1':\n",
    "                modifier = '(RQ to Code)'\n",
    "                target_path = 'source_code'\n",
    "                truth_path = 'code_ground.txt'\n",
    "\n",
    "            elif subset == '2':\n",
    "                modifier = '(RQ to Test)'\n",
    "                target_path = 'test_cases.txt'\n",
    "                truth_path = 'tests_ground.txt'\n",
    "\n",
    "            else:\n",
    "                print(\"Unrecognized corpus code: \" + corpus_code)\n",
    "                return\n",
    "\n",
    "        elif dataset == '2':\n",
    "            dataset_name = 'eTOUR'\n",
    "            source_path = 'use_cases_with_translation'\n",
    "            languages = ['english', 'italian', 'java']\n",
    "            if subset == '0':\n",
    "                modifier = '(UC to Code)'\n",
    "                target_path = 'source_code'\n",
    "                truth_path = 'ground.txt'\n",
    "\n",
    "            else:\n",
    "                print(\"Unrecognized corpus code: \" + corpus_code)\n",
    "                return\n",
    "\n",
    "        elif dataset == '3':\n",
    "            dataset_name = 'iTrust'\n",
    "            source_path = 'use_cases'\n",
    "            languages = ['english', 'java']\n",
    "\n",
    "            if subset == '0':\n",
    "                modifier = '(UC to Code)'\n",
    "                target_path = 'source_code'\n",
    "                truth_path = 'ground.txt'\n",
    "\n",
    "            else:\n",
    "                print(\"Unrecognized corpus code: \" + corpus_code)\n",
    "                return\n",
    "\n",
    "        elif dataset == '4':\n",
    "            dataset_name = 'Albergate'\n",
    "            source_path = 'requirements'\n",
    "            languages = ['italian', 'java']\n",
    "\n",
    "            if subset == '0':\n",
    "                modifier = '(RQ to Code)'\n",
    "                target_path = 'source_code'\n",
    "                truth_path = 'ground.txt'\n",
    "\n",
    "            else:\n",
    "                print(\"Unrecognized corpus code: \" + corpus_code)\n",
    "                return\n",
    "\n",
    "        elif dataset == '5':\n",
    "            dataset_name = 'SMOS'\n",
    "            source_path = 'use_cases'\n",
    "            languages = ['italian', 'java']\n",
    "\n",
    "            if subset == '0':\n",
    "                modifier = '(UC to Code)'\n",
    "                target_path = 'source_code'\n",
    "                truth_path = 'ground.txt'\n",
    "\n",
    "            else:\n",
    "                print(\"Unrecognized corpus code: \" + corpus_code)\n",
    "                return\n",
    "        else:\n",
    "            print(\"Unrecognized corpus code: \" + corpus_code)\n",
    "            return\n",
    "\n",
    "        file_path = os.path.dirname(os.path.abspath(__file__))\n",
    "        corpus_root = file_path + '/../../data/raw/' + dataset_name + '_semeru_format/'\n",
    "\n",
    "        corpus_name = dataset_name + ' (' + corpus_code + ')'\n",
    "        filetype_whitelist = ['java', 'txt', 'jsp', 'h', 'c']\n",
    "\n",
    "        corpus = Corpus(\n",
    "            corpus_name,\n",
    "            corpus_root,\n",
    "            source_path,\n",
    "            target_path,\n",
    "            truth_path,\n",
    "            execution_traces=execution_traces,\n",
    "            corpus_code=corpus_code,\n",
    "            languages=languages,\n",
    "            filetype_whitelist=filetype_whitelist\n",
    "        )\n",
    "\n",
    "        return corpus\n",
    "\n",
    "    @classmethod\n",
    "    def get_all_preset_corpus_codes(cls):\n",
    "        codes = [\n",
    "            '0_0', '0_1', '0_2',\n",
    "            '1_0', '1_1', '1_2',\n",
    "            '2_0',\n",
    "            '3_0',\n",
    "            '4_0',\n",
    "            '5_0'\n",
    "        ]\n",
    "        return codes\n",
    "\n",
    "    @classmethod\n",
    "    def get_all_preset_corpora(cls):\n",
    "        codes = Corpus.get_all_preset_corpus_codes()\n",
    "\n",
    "        return [Corpus.get_preset_corpus(code) for code in codes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory is : /tf/main/nbs\n",
      "Directory name is : nbs\n"
     ]
    }
   ],
   "source": [
    "# Exploring!\n",
    "dirpath = os.getcwd()\n",
    "print(\"Current directory is : \" + dirpath)\n",
    "foldername = os.path.basename(dirpath)\n",
    "print(\"Directory name is : \" + foldername)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(dirpath, \"test_data\")\n",
    "data_folder = os.path.join(data_folder, \"LibEST_semeru_format/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding artifacts in: /tf/main/nbs/test_data/LibEST_semeru_format/requirements\n",
      "Getting artifacts by file from directory\n",
      "Read 52 artifacts from directory\n",
      "\n",
      "Finding artifacts in: /tf/main/nbs/test_data/LibEST_semeru_format/source_code\n",
      "Getting artifacts by file from directory\n",
      "Read 14 artifacts from directory\n",
      "\n",
      "Found stop words file: english_stop_words.txt\n",
      "Detected natural language: [english], generating stemmer\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(name='libest', corpus_root=data_folder, \n",
    "                        source_path='requirements', target_path='source_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.get_sources())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(os.path.join(os.getcwd(), 'test_data/config_corpus/' + language + '_stop_words.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_corpus_creation():\n",
    "    corpus = Corpus(name, corpus_root=self._data_folder, \n",
    "                        source_path='requirements', target_path='source_code')\n",
    "    assert len(corpus.get_sources()) == 52\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCorpus(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        dirpath = os.getcwd()\n",
    "        self._data_folder = os.path.join(dirpath, \"test_data\")\n",
    "        self._data_folder = os.path.join(data_folder, \"LibEST_semeru_format/\")\n",
    "        pass\n",
    "    def test_corpus_creation(self):\n",
    "        corpus = Corpus(name, corpus_root=self._data_folder, \n",
    "                        source_path='requirements', target_path='source_code')\n",
    "        self.assertEqual(corpus.get_sources(), 52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(corpus.get_sources()) == 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 0 tests in 0.000s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "!python -m unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E\n",
      "======================================================================\n",
      "ERROR: /root/ (unittest.loader._FailedTest)\n",
      "----------------------------------------------------------------------\n",
      "AttributeError: module '__main__' has no attribute '/root/'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ERROR: /root/ (unittest.loader._FailedTest)\n",
      "----------------------------------------------------------------------\n",
      "AttributeError: module '__main__' has no attribute '/root/'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print('hello')\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
