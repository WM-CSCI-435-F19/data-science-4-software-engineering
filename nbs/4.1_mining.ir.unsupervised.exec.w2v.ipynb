{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Unsupervised Approaches for SE Traceability [approach]\n",
    "\n",
    "> This module is dedicated to evaluate word2vec/doc2vec or any neural unsupervised approaches on traceability datasets. Consider to Copy the entire notebook for a new and separeted empirical evaluation. \n",
    "> Implementing mutual information analysis\n",
    "> Author: @danaderp April 2020\n",
    "> Author: @danielrc Nov 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This copy is for Cisco purposes. It was adapted to process private github data from cisco. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from itertools import product \n",
    "from random import sample \n",
    "import functools \n",
    "import os\n",
    "from enum import Enum, unique, auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "from pandas.plotting import lag_plot\n",
    "import math as m\n",
    "import random as r\n",
    "import collections\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html\n",
    "#export\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ds4se as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Similarity with BasicSequenceVectorization\n",
    "\n",
    "We test diferent similarities based on [blog](https://www.kdnuggets.com/2017/08/comparing-distance-measurements-python-scipy.html) and [blog2](https://www.kdnuggets.com/2019/01/comparison-text-distance-metrics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#@unique\n",
    "class VectorizationType(Enum):\n",
    "    word2vec = auto()\n",
    "    doc2vec = auto()\n",
    "    vsm2vec = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VectorizationType.word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#@unique\n",
    "class LinkType(Enum):\n",
    "    req2tc = auto()\n",
    "    req2src = auto()\n",
    "    issue2src = auto()\n",
    "    pr2src = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#@unique\n",
    "class DistanceMetric(Enum):\n",
    "    WMD = auto()\n",
    "    COS = auto()\n",
    "    SCM = auto()\n",
    "    EUC = auto()\n",
    "    MAN = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#@unique\n",
    "class SimilarityMetric(Enum):\n",
    "    WMD_sim = auto()\n",
    "    COS_sim = auto()\n",
    "    SCM_sim = auto()\n",
    "    EUC_sim = auto()\n",
    "    MAN_sim = auto()\n",
    "    Pearson = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntropyMetric(Enum):\n",
    "    MSI_I = auto() #Minimum shared information Entropy\n",
    "    MSI_X = auto() #Minimum shared information Extropy\n",
    "    MI = auto() #Mutual information\n",
    "    JI = auto() #Joint information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftwareArtifacts(Enum):\n",
    "    REQ = auto()\n",
    "    TC = auto()\n",
    "    SRC = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#@unique\n",
    "class Preprocessing(Enum):\n",
    "    conv = auto()\n",
    "    bpe = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinkType.req2tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preprocessing.bpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experients Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../dvc-ds4se/' #dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_trained_model = path_data+'models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model'\n",
    "#path_to_trained_model = path_data/'models/wv/bpe128k/[word2vec-Java-Py-Wiki-SK-500-20E-128k[15]-1595189771.501188].model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing default params\n",
    "def default_params():\n",
    "    return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.req2tc,\n",
    "        \"system\": 'sacp',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_path\": '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-req].csv',\n",
    "        #\"target_path\": '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-tc].csv',\n",
    "        #\"system_path\": '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-all].csv',\n",
    "        \"saving_path\": path_data +'se-benchmarking/traceability',\n",
    "        \"names\": ['Source','Target','Linked?']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 1 with Libest Conv preprocessing\n",
    "def libest_params():\n",
    "        return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.req2src,\n",
    "        \"system\": 'libest',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_type\": SoftwareArtifacts.REQ,\n",
    "        \"target_type\": SoftwareArtifacts.TC,\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": path_data + 'se-benchmarking/traceability/cisco/libest_data/[libest-all-corpus-1596063103.098236].csv',\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','conv'],\n",
    "            \"prep\": Preprocessing.conv\n",
    "        },\n",
    "        \"saving_path\": path_data + 'se-benchmarking/traceability',\n",
    "        \"names\": ['Source','Target','Linked?']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 2 with Libest BPE preprocessing\n",
    "def libest_params_bpe():\n",
    "        return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.req2src,\n",
    "        \"system\": 'libest',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_type\": 'req', #TODO Standardize the artifacts \n",
    "        \"target_type\": 'tc',\n",
    "        #\"path_mappings\": 'cisco/libest_data/sacp-pr-mappings.csv',\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": path_data + 'se-benchmarking/traceability/cisco/libest_data/[libest-all-corpus-1596063103.098236].csv',\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','bpe128k'],\n",
    "            \"prep\": Preprocessing.bpe\n",
    "        },\n",
    "        \"saving_path\": path_data + 'se-benchmarking/traceability',\n",
    "        \"names\": ['Source','Target','Linked?'],\n",
    "        \"model_prefix\":path_data + 'models/bpe/sentencepiece/wiki_py_java_bpe_8k' #For BPE Analysis    \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CISCO GitHub Parameters\n",
    "def sacp_params():\n",
    "    return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.issue2src,\n",
    "        \"system\": 'sacp-python-common',\n",
    "        \"path_to_trained_model\": path_data + 'models/wv/conv/[word2vec-Py-Java-Wiki-SK-500-20E[0]-1592979270.711115].model',\n",
    "        \"source_type\": 'pr', #TODO Standardize the artifacts \n",
    "        \"target_type\": 'py',\n",
    "        \"path_mappings\": '/tf/data/cisco/sacp_data/sacp-pr-mappings.csv',\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": '/tf/data/cisco/sacp_data/[sacp-python-common-all-corpus-1596383717.992744].csv', #MUST have bpe8k <----\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','conv'],\n",
    "            \"prep\": Preprocessing.conv\n",
    "        },\n",
    "        \"saving_path\":  path_data/'se-benchmarking/traceability/cisco/sacp',\n",
    "        \"names\": ['Source','Target','Linked?']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_trained_model = path_data + 'models/wv/bpe8k/[word2vec-Java-Py-Wiki-SK-500-20E-8k[12]-1594546477.788739].model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sacp_params_bpe():\n",
    "    return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.issue2src,\n",
    "        \"system\": 'sacp-python-common',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_type\": 'pr', #TODO Standardize the artifacts \n",
    "        \"target_type\": 'py',\n",
    "        \"path_mappings\": '/tf/data/cisco/sacp_data/sacp-pr-mappings.csv',\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": '/tf/data/cisco/sacp_data/[sacp-python-common-all-corpus-1596383717.992744].csv',\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','bpe8k'],\n",
    "            \"prep\": Preprocessing.bpe\n",
    "        },\n",
    "        \"saving_path\": path_data + 'se-benchmarking/traceability/cisco/sacp',\n",
    "        \"names\": ['Source','Target','Linked?'],\n",
    "        \"model_prefix\":path_data + 'models/bpe/sentencepiece/wiki_py_java_bpe_8k' #For BPE Analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters = default_params()\n",
    "#parameters = libest_params()\n",
    "#parameters = _params()\n",
    "parameters = sacp_params_bpe()\n",
    "#parameters = libest_params_bpe()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Similarity with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 1]Creating the Vectorization Class\n",
    "word2vec = Word2VecSeqVect( params = parameters )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 2]NonGroundTruth Computation\n",
    "#metric_list = [DistanceMetric.WMD,DistanceMetric.SCM,EntropyMetric.MSI_I]\n",
    "metric_list = [EntropyMetric.MSI_I,EntropyMetric.MI]\n",
    "word2vec.ComputeDistanceArtifacts( sampling=False, samples = 5, metric_list = metric_list )\n",
    "word2vec.df_nonground_link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_nonground_link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 3]Saving Non-GroundTruth Links\n",
    "word2vec.SaveLinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_nonglinks = LoadLinks(timestamp=1596416340.728148, params=parameters)\n",
    "df_nonglinks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 4]GroundTruthMatching Testing\n",
    "#TODO change the path for a param\n",
    "path_to_ground_truth = '/tf/main/benchmarking/traceability/testbeds/groundtruth/english/[libest-ground-req-to-tc].txt'\n",
    "word2vec.MatchWithGroundTruth(path_to_ground_truth, semeru_format=True)\n",
    "word2vec.df_ground_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 4.1]GroundTruthMatching Testing For CISCO Mappings\n",
    "word2vec.MatchWithGroundTruth(from_mappings=True)\n",
    "word2vec.df_ground_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[optional]GroundTruth Direct Processing\n",
    "ground_links = word2vec.ground_truth_processing(path_to_ground_truth)\n",
    "ground_links[141] # A tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 5]Saving GroundTruth Links\n",
    "word2vec.SaveLinks(grtruth = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_glinks = LoadLinks(timestamp=1596426181.318831, params=parameters,grtruth = True)\n",
    "df_glinks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Traceability with Artifacts Representation \n",
    "We are employing two techniques for analyzing software artifacts without groundtruth:\n",
    "- Prototypes and Criticisms for Paragraph Vectors \n",
    "- Information Theory for Software Traceability (Shared Information and Mutual Information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach Evaluation and Interpretation (word2vec)\n",
    "Classification/evaluation metrics for highly imbalanced data [(see Forum)](https://stats.stackexchange.com/questions/222558/classification-evaluation-metrics-for-highly-imbalanced-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VectorEvaluation():\n",
    "    '''Approaches Common Evaluations and Interpretations (statistical analysis)'''\n",
    "    def __init__(self, sequenceVectorization):\n",
    "        self.seqVect = sequenceVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SupervisedVectorEvaluation(VectorEvaluation):\n",
    "    def __init__(self, sequenceVectorization, sim_list):\n",
    "        super().__init__(sequenceVectorization)\n",
    "        self.sim_list = sim_list\n",
    "        \n",
    "        self.df_filtered = sequenceVectorization.df_ground_link \n",
    "        self.df_filtered = self.df_filtered[~self.df_filtered.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "        \n",
    "        #CreateFilters Here\n",
    "        \n",
    "        self.y_test = self.df_filtered['Linked?'].values\n",
    "        self.y_score = [self.df_filtered[sim].values for sim in sim_list]\n",
    "        self.title = str(sequenceVectorization.params['vectorizationType'])\n",
    "        pass\n",
    "    \n",
    "    def Compute_precision_recall_gain(self):\n",
    "        '''One might choose PRG if there is little interest in identifying false negatives '''\n",
    "        for count,sim in enumerate(self.sim_list):\n",
    "            prg_curve = prg.create_prg_curve(self.y_test, self.y_score[count])\n",
    "            auprg = prg.calc_auprg(prg_curve)\n",
    "            prg.plot_prg(prg_curve)\n",
    "            logging.info('auprg:  %.3f' %  auprg)\n",
    "            logging.info(\"compute_precision_recall_gain Complete: \"+str(sim))\n",
    "        pass\n",
    "    \n",
    "    def Compute_avg_precision(self):\n",
    "        '''Generated precision-recall curve'''\n",
    "        \n",
    "        # calculate the no skill line as the proportion of the positive class\n",
    "        no_skill = len(self.y_test[self.y_test==1]) / len(self.y_test)\n",
    "        \n",
    "        for count,sim in enumerate(self.sim_list):\n",
    "            plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill') #reference curve\n",
    "            precision, recall, _ = precision_recall_curve(self.y_test, self.y_score[count]) #compute precision-recall curve\n",
    "            plt.plot(recall, precision, marker='.', label = str(sim)) #plot model curve\n",
    "            plt.title(self.label[count])\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.legend() #show the legend\n",
    "            plt.show() #show the plot\n",
    "\n",
    "            average_precision = average_precision_score(self.y_test, self.y_score[count])\n",
    "            auc_score = auc(recall, precision)\n",
    "            logging.info('Average precision-recall score: {0:0.2f}'.format(average_precision))\n",
    "            logging.info('Precision-Recall AUC: %.3f' % auc_score)\n",
    "        pass\n",
    "    \n",
    "    def Compute_avg_precision_same_plot(self):\n",
    "        '''Generated precision-recall curve'''\n",
    "\n",
    "        # calculate the no skill line as the proportion of the positive class\n",
    "        no_skill = len(self.y_test[self.y_test==1]) / len(self.y_test)\n",
    "        plt.plot([0, 1], [no_skill, no_skill], linewidth=0.5, linestyle='--', label='No Skill [{0:0.2f}]'.format(no_skill)) #reference curve\n",
    "        \n",
    "        for count,sim in enumerate(self.sim_list):\n",
    "            precision, recall, _ = precision_recall_curve(self.y_test, self.y_score[count]) #compute precision-recall curve\n",
    "            average_precision = average_precision_score(self.y_test, self.y_score[count])\n",
    "            auc_score = auc(recall, precision)\n",
    "            logging.info('Average precision-recall score: {0:0.2f}'.format(average_precision))\n",
    "            logging.info('Precision-Recall AUC: %.2f' % auc_score)\n",
    "            \n",
    "            #plt.plot(recall, precision, linewidth=0.4, marker='.', label = str(sim)) #plot model curve\n",
    "            plt.plot(recall, precision, linewidth=1, label = str(sim)+  ' [auc:{0:0.2f}]'.format(auc_score)) #plot model curve\n",
    "            pass\n",
    "        \n",
    "        plt.title(self.title)\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.legend(fontsize=9) #show the legend\n",
    "        plt.show() #show the plot\n",
    "        pass\n",
    "    \n",
    "    def Compute_roc_curve(self):\n",
    "\n",
    "        plt.plot([0, 1], [0, 1],  linewidth=0.5, linestyle='--', label='No Skill') #reference curve\n",
    "        for count,sim in enumerate(self.sim_list):\n",
    "            fpr, tpr, _ = roc_curve(self.y_test, self.y_score[count]) #compute roc curve\n",
    "            roc_auc = roc_auc_score(self.y_test, self.y_score[count])\n",
    "            logging.info('ROC AUC %.2f' % roc_auc)\n",
    "            \n",
    "            plt.plot(fpr, tpr,  linewidth=1, label = str(sim)+  ' [auc:{0:0.2f}]'.format(roc_auc)) #plot model curve\n",
    "            pass\n",
    "        plt.title(self.title)\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(fontsize=9) #show the legend\n",
    "        plt.show() #show the plot\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = [SimilarityMetric.SCM_sim, SimilarityMetric.WMD_sim]\n",
    "supevisedEval = SupervisedVectorEvaluation(word2vec, sim_list = similarities ) #<---- Parameter \n",
    "#supevisedEval = SupervisedVectorEvaluation(word2vec, similarity=SimilarityMetric.WMD_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supevisedEval.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supevisedEval.y_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO Move the confusion matrix to SupervisedVectorEvaluation\n",
    "y_score_threshold = [0 if elem<=0.8 else 1 for elem in supevisedEval.y_score] #Hardcoded 0.7 Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO a Variation threshold analysis\n",
    "tn, fp, fn, tp = confusion_matrix(supevisedEval.y_test, y_score_threshold).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Racall-Gain\n",
    "Based on the library here: [link](https://github.com/meeliskull/prg/tree/master/Python_package). \n",
    "The area under traditional PR curves can easily favour models with lower expected F1 score than others, and so the use of Precision-Recall-Gain curves will result in better model selection [(Flach & Kull, 2015)](http://people.cs.bris.ac.uk/~flach//PRGcurves/).\n",
    "One might choose PRG if there is little interest in identifying false negatives [(from Blog)](https://medium.com/@alexabate/i-did-something-boring-so-you-dont-have-to-9140ca46c84d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supevisedEval.Compute_precision_recall_gain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the average precision score¶\n",
    "Precision is a metric that quantifies the number of correct positive predictions made.\n",
    "\n",
    "Recall is a metric that quantifies the number of correct positive predictions made out of all positive predictions that could have been made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supevisedEval.Compute_avg_precision_same_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute ROC Curve\n",
    "An ROC curve (or receiver operating characteristic curve) is a plot that summarizes the performance of a binary classification model on the positive class [(see Blog)](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/).\n",
    "\n",
    "Use ROC when both classes detection is equally important — When we want to give equal weight to both classes prediction ability we should look at the ROC curve [link](https://towardsdatascience.com/what-metrics-should-we-use-on-imbalanced-data-set-precision-recall-roc-e2e79252aeba)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supevisedEval.Compute_roc_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute distribution of similarities word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Statistics\n",
    "filter_metrics = supevisedEval.df_filtered #word2vec.df_ground_link\n",
    "filter_metrics.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(filter_metrics, alpha=0.2, figsize=(12, 12), diagonal='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lag plots are used to check if a data set or time series is random. Random data should not exhibit any structure in the lag plot. Non-random structure implies that the underlying data are not random. The lag argument may be passed, and when lag=1 the plot is essentially data[:-1] vs. data[1:]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot(filter_metrics[[SimilarityMetric.WMD_sim]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot(filter_metrics[DistanceMetric.WMD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate model precision-recall curve\n",
    "sim = np.array(filter_metrics[SimilarityMetric.SCM_sim]) #SimilarityMetric.SCM_sim #SimilarityMetric.WMD_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics.hist(column=[SimilarityMetric.WMD_sim,DistanceMetric.WMD,SimilarityMetric.SCM_sim,\n",
    "                            DistanceMetric.SCM],color='k',bins=50,figsize=[10,5],alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = filter_metrics[[SimilarityMetric.WMD_sim,DistanceMetric.WMD,SimilarityMetric.SCM_sim,\n",
    "                            DistanceMetric.SCM]].std()\n",
    "print(errors)\n",
    "filter_metrics[[SimilarityMetric.WMD_sim,DistanceMetric.WMD,SimilarityMetric.SCM_sim,\n",
    "                            DistanceMetric.SCM]].plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics[SimilarityMetric.WMD_sim].plot.kde()\n",
    "filter_metrics[SimilarityMetric.WMD_sim].plot.hist(density=True) # Histogram will now be normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics[SimilarityMetric.SCM_sim].plot.kde()\n",
    "filter_metrics[SimilarityMetric.SCM_sim].plot.hist(density=True) # Histogram will now be normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics[DistanceMetric.WMD].plot.kde()\n",
    "filter_metrics[DistanceMetric.WMD].plot.hist(density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics[DistanceMetric.SCM].plot.kde()\n",
    "filter_metrics[DistanceMetric.SCM].plot.hist(density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics.hist(by='Linked?',column=SimilarityMetric.WMD_sim ,figsize=[10, 5],bins=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics.hist(by='Linked?',column=SimilarityMetric.SCM_sim ,figsize=[10, 5],bins=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics.hist(by='Linked?',column=DistanceMetric.WMD,figsize=[10, 5],bins=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics.hist(by='Linked?',column=DistanceMetric.SCM,figsize=[10, 5],bins=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = filter_metrics.boxplot(by='Linked?',column=[SimilarityMetric.WMD_sim,DistanceMetric.WMD,SimilarityMetric.SCM_sim,\n",
    "                            DistanceMetric.SCM],figsize=[7, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics_01 = filter_metrics.copy()\n",
    "filter_metrics_01.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics_01[EntropyMetric.MSI_I]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spearman_corr(filter_metrics_01, columns = [EntropyMetric.MSI_I,SimilarityMetric.SCM_sim] ):\n",
    "    df_correlation = filter_metrics_01.copy() \n",
    "    correlation = df_correlation[columns].corr(method='spearman')\n",
    "    #correlation = df_correlation.corr(method='spearman')\n",
    "    return correlation[columns[0]].values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum Shared Entropy and Word Distance\n",
    "x1 = filter_metrics_01.plot.scatter(\n",
    "    x=EntropyMetric.MSI_I,\n",
    "    y=SimilarityMetric.WMD_sim, \n",
    "    c='DarkBlue', \n",
    "    s=1,\n",
    "    title = 'SCM-Entropy Correlation {%.2f}' % compute_spearman_corr(filter_metrics_01)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = filter_metrics_01.plot.scatter(\n",
    "    x=EntropyMetric.MSI_X,\n",
    "    y=SimilarityMetric.WMD_sim, \n",
    "    c='DarkBlue', \n",
    "    s=1,\n",
    "    title = 'SCM-Extropy Correlation {%.2f}' % compute_spearman_corr(filter_metrics_01,[EntropyMetric.MSI_X,SimilarityMetric.SCM_sim] )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics_linked = filter_metrics_01[filter_metrics_01['Linked?'] == 1].copy()\n",
    "filter_metrics_nonlinked = filter_metrics_01[filter_metrics_01['Linked?'] == 0].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = filter_metrics_01[filter_metrics_01['Linked?'] == 1].plot.scatter(\n",
    "    x=EntropyMetric.MSI_I,\n",
    "    y=SimilarityMetric.SCM_sim, \n",
    "    c='Red',\n",
    "    s=1,\n",
    "    title = 'Liked SCM-Entropy Correlation {%.2f}' % compute_spearman_corr(filter_metrics_linked)\n",
    ")\n",
    "#x2.text(0,0,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_ = filter_metrics_nonlinked.plot.scatter(\n",
    "    x=EntropyMetric.MSI_I,\n",
    "    y=SimilarityMetric.SCM_sim, \n",
    "    c='DarkBlue',\n",
    "    s=1,\n",
    "    title = 'non-Linked SCM-Entropy Correlation {%.2f}' % compute_spearman_corr(filter_metrics_nonlinked)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Information levels vs semantics\n",
    "fig, ax = plt.subplots()\n",
    "filter_metrics_01.plot.scatter(\n",
    "    x = EntropyMetric.MSI_I,\n",
    "    y = EntropyMetric.MSI_X,\n",
    "    c = SimilarityMetric.SCM_sim,\n",
    "    #figsize = [12, 6],\n",
    "    title = 'Information-Semantic Interactions SCM',\n",
    "    colormap = 'viridis',\n",
    "    ax = ax,\n",
    "    s=1\n",
    ")\n",
    "ax.set_xlabel(\"Minimum Shared Entropy\")\n",
    "ax.set_ylabel(\"Minimum Shared Extropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separated by ground truth Links!\n",
    "fig, ax = plt.subplots()\n",
    "filter_metrics_01[filter_metrics_01['Linked?'] == 1].plot.scatter(\n",
    "    x = EntropyMetric.MSI_I,\n",
    "    y = EntropyMetric.MSI_X,\n",
    "    c = SimilarityMetric.SCM_sim,\n",
    "    #figsize = [12, 6],\n",
    "    title = 'Information-Semantic Interactions SCM Linked',\n",
    "    colormap = 'viridis',\n",
    "    ax = ax,\n",
    "    s=1\n",
    ")\n",
    "ax.set_xlabel(\"Minimum Shared Entropy\")\n",
    "ax.set_ylabel(\"Minimum Shared Extropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separated by ground truth NonLinked!\n",
    "fig, ax = plt.subplots()\n",
    "filter_metrics_01[filter_metrics_01['Linked?'] == 0].plot.scatter(\n",
    "    x = EntropyMetric.MSI_I,\n",
    "    y = EntropyMetric.MSI_X,\n",
    "    c = SimilarityMetric.SCM_sim,\n",
    "    #figsize = [6, 5],\n",
    "    title = 'Information-Semantic Interactions SCM non-Linked',\n",
    "    colormap = 'viridis',\n",
    "    ax = ax,\n",
    "    s=1\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Minimum Shared Entropy\")\n",
    "ax.set_ylabel(\"Minimum Shared Extropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax7 = filter_metrics_01.plot.scatter(\n",
    "    x = EntropyMetric.MSI_X,\n",
    "    y = EntropyMetric.MSI_I,\n",
    "    c = SimilarityMetric.SCM_sim,\n",
    "    #figsize = [12, 6],\n",
    "    title = 'Information-Semantic Interactions SCM',\n",
    "    colormap = 'viridis',\n",
    "    s=1\n",
    ")\n",
    "ax7.set_xlabel(\"Minimum Shared Extropy\")\n",
    "ax7.set_ylabel(\"Minimum Shared Entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "filter_metrics_01.plot.scatter(\n",
    "    x = EntropyMetric.MSI_I,\n",
    "    y = EntropyMetric.MSI_X,\n",
    "    c = SimilarityMetric.WMD_sim,\n",
    "    #figsize = [12, 6],\n",
    "    title = 'Information-Semantic Interactions WMD',\n",
    "    colormap = 'viridis',\n",
    "    ax = ax\n",
    ")\n",
    "ax.set_xlabel(\"Minimum Shared Entropy\")\n",
    "ax.set_ylabel(\"Minimum Shared Extropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "filter_metrics_01[filter_metrics_01['Linked?'] == 1].plot.scatter(\n",
    "    x = EntropyMetric.MSI_I,\n",
    "    y = EntropyMetric.MSI_X,\n",
    "    c = SimilarityMetric.WMD_sim,\n",
    "    #figsize = [12, 6],\n",
    "    title = 'Information-Semantic Interactions WMD Linked',\n",
    "    colormap = 'viridis',\n",
    "    ax = ax\n",
    ")\n",
    "ax.set_xlabel(\"Minimum Shared Entropy\")\n",
    "ax.set_ylabel(\"Minimum Shared Extropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "filter_metrics_01[filter_metrics_01['Linked?'] == 0].plot.scatter(\n",
    "    x = EntropyMetric.MSI_I,\n",
    "    y = EntropyMetric.MSI_X,\n",
    "    c = SimilarityMetric.WMD_sim,\n",
    "    #figsize = [12, 6],\n",
    "    title = 'Information-Semantic Interactions WMD non-Linked',\n",
    "    colormap = 'viridis',\n",
    "    ax = ax\n",
    ")\n",
    "ax.set_xlabel(\"Minimum Shared Entropy\")\n",
    "ax.set_ylabel(\"Minimum Shared Extropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_metrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Similarity with Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to reproduce the same empirical evaluation like here: [link](https://arxiv.org/pdf/1507.07998.pdf). Pay attention to:\n",
    "- Accuracy vs. Dimensionality (we can replace accuracy for false positive rate or true positive rate)\n",
    "- Visualize paragraph vectors using t-sne\n",
    "- Computing Cosine Distance and Similarity. More about similarity [link](https://www.kdnuggets.com/2017/08/comparing-distance-measurements-python-scipy.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_to_trained_model\": 'test_data/models/pv/conv/[doc2vec-Py-Java-PVDBOW-500-20E-1592609630.689167].model',\n",
    "#\"path_to_trained_model\": 'test_data/models/pv/conv/[doc2vec-Py-Java-Wiki-PVDBOW-500-20E[15]-1592941134.367976].model',\n",
    "path_to_trained_model = 'test_data/models/[doc2vec-Py-Java-PVDBOW-500-20E-8k-1594572857.17191].model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec_params():\n",
    "    return {\n",
    "        \"vectorizationType\": VectorizationType.doc2vec,\n",
    "        \"linkType\": LinkType.req2tc,\n",
    "        \"system\": 'libest',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_path\": '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-req].csv',\n",
    "        \"target_path\": '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-tc].csv',\n",
    "        \"system_path\": '/tf/main/benchmarking/traceability/testbeds/nltk/[libest-pre-all].csv',\n",
    "        \"saving_path\": 'test_data/',\n",
    "        \"names\": ['Source','Target','Linked?']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_params = doc2vec_params()\n",
    "doc2vec_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "class Doc2VecSeqVect(BasicSequenceVectorization):\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        super().__init__(params)\n",
    "        self.new_model = gensim.models.Doc2Vec.load( params['path_to_trained_model'] )\n",
    "        self.new_model.init_sims(replace=True)  # Normalizes the vectors in the word2vec class.\n",
    "        self.df_inferred_src = None\n",
    "        self.df_inferred_trg = None\n",
    "        \n",
    "        self.dict_distance_dispatcher = {\n",
    "            DistanceMetric.COS: self.cos_scipy,\n",
    "            SimilarityMetric.Pearson: self.pearson_abs_scipy,\n",
    "            DistanceMetric.EUC: self.euclidean_scipy,\n",
    "            DistanceMetric.MAN: self.manhattan_scipy\n",
    "        }\n",
    "    \n",
    "    def distance(self, metric_list, link):\n",
    "        '''Iterate on the metrics'''\n",
    "        ν_inferredSource = list(self.df_inferred_src[self.df_inferred_src['ids'].str.contains(link[0])]['inf-doc2vec'])\n",
    "        w_inferredTarget = list(self.df_inferred_trg[self.df_inferred_trg['ids'].str.contains(link[1])]['inf-doc2vec'])\n",
    "        \n",
    "        dist = [ self.dict_distance_dispatcher[metric](ν_inferredSource,w_inferredTarget) for metric in metric_list]\n",
    "        logging.info(\"Computed distances or similarities \"+ str(link) + str(dist))    \n",
    "        return functools.reduce(lambda a,b : a+b, dist) #Always return a list\n",
    "    \n",
    "    def computeDistanceMetric(self, links, metric_list):\n",
    "        '''It is computed the cosine similarity'''\n",
    "        \n",
    "        metric_labels = [ self.dict_labels[metric] for metric in metric_list] #tracking of the labels\n",
    "        distSim = [[link[0], link[1], self.distance( metric_list, link )] for link in links] #Return the link with metrics\n",
    "        distSim = [[elem[0], elem[1]] + elem[2] for elem in distSim] #Return the link with metrics\n",
    "        \n",
    "        return distSim, functools.reduce(lambda a,b : a+b, metric_labels)\n",
    "\n",
    "    \n",
    "    def InferDoc2Vec(self, steps=200):\n",
    "        '''Activate Inference on Target and Source Corpus'''\n",
    "        self.df_inferred_src = self.df_source.copy()\n",
    "        self.df_inferred_trg = self.df_target.copy()\n",
    "        \n",
    "        self.df_inferred_src['inf-doc2vec'] =  [self.new_model.infer_vector(artifact.split(),steps=steps) for artifact in self.df_inferred_src['text'].values]\n",
    "        self.df_inferred_trg['inf-doc2vec'] =  [self.new_model.infer_vector(artifact.split(),steps=steps) for artifact in self.df_inferred_trg['text'].values]\n",
    "        \n",
    "        logging.info(\"Infer Doc2Vec on Source and Target Complete\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Doc2Vec SequenceVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec = Doc2VecSeqVect(params = doc2vec_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step1]Apply Doc2Vec Inference\n",
    "doc2vec.InferDoc2Vec(steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec.df_inferred_src.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_inferDoc2Vec_trg = inferDoc2Vec(df_target)\n",
    "#test_inferDoc2Vec_trg.head()\n",
    "doc2vec.df_inferred_trg.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(doc2vec.df_inferred_trg['inf-doc2vec'][0], doc2vec.df_inferred_trg['inf-doc2vec'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 2]NonGroundTruth Computation\n",
    "metric_l = [DistanceMetric.EUC,DistanceMetric.COS,DistanceMetric.MAN]# , SimilarityMetric.Pearson]\n",
    "doc2vec.ComputeDistanceArtifacts( sampling=False, samples = 50, metric_list = metric_l )\n",
    "doc2vec.df_nonground_link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 3]Saving Non-GroundTruth Links\n",
    "doc2vec.SaveLinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_nonglinks_doc2vec = LoadLinks(timestamp=1594653325.258415, params=doc2vec_params)\n",
    "df_nonglinks_doc2vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 4]GroundTruthMatching Testing\n",
    "path_to_ground_truth = '/tf/main/benchmarking/traceability/testbeds/groundtruth/english/[libest-ground-req-to-tc].txt'\n",
    "doc2vec.MatchWithGroundTruth(path_to_ground_truth)\n",
    "doc2vec.df_ground_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 5]Saving GroundTruth Links\n",
    "doc2vec.SaveLinks(grtruth = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_glinks_doc2vec = LoadLinks(timestamp=1594653350.19946, params=doc2vec_params, grtruth = True)\n",
    "df_glinks_doc2vec.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach Evaluation and Interpretation (doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supervisedEvalDoc2vec = SupervisedVectorEvaluation(doc2vec, similarity=SimilarityMetric.EUC_sim)\n",
    "#supervisedEvalDoc2vec = SupervisedVectorEvaluation(doc2vec, similarity=SimilarityMetric.COS_sim)\n",
    "supervisedEvalDoc2vec = SupervisedVectorEvaluation(doc2vec, similarity=SimilarityMetric.MAN_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisedEvalDoc2vec.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisedEvalDoc2vec.y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisedEvalDoc2vec.Compute_precision_recall_gain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisedEvalDoc2vec.Compute_avg_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisedEvalDoc2vec.Compute_roc_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute distribution of similarities doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Statistics\n",
    "filter_doc2vec = doc2vec.df_ground_link\n",
    "filter_doc2vec.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot(filter_doc2vec[[SimilarityMetric.EUC_sim]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot(filter_doc2vec[DistanceMetric.EUC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_doc2vec.hist(column=[SimilarityMetric.EUC_sim,DistanceMetric.EUC],color='k',bins=50,figsize=[10,5],alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate distance from similarity analysis here\n",
    "errors = filter_doc2vec[[SimilarityMetric.EUC_sim,DistanceMetric.EUC]].std()\n",
    "print(errors)\n",
    "filter_doc2vec[[SimilarityMetric.EUC_sim,DistanceMetric.EUC]].plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_doc2vec.hist(by='Linked?',column=SimilarityMetric.EUC_sim,figsize=[10, 5],bins=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_doc2vec.hist(by='Linked?',column=DistanceMetric.EUC,figsize=[10, 5],bins=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate the distance from the similarity plot\n",
    "boxplot = filter_doc2vec.boxplot(by='Linked?',column=[SimilarityMetric.EUC_sim,DistanceMetric.EUC],figsize=[10, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = filter_doc2vec.boxplot(by='Linked?',column=[SimilarityMetric.EUC_sim],figsize=[10, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Doc2vec and Word2vec\n",
    "Please check this post for futher detatils [link](https://stats.stackexchange.com/questions/217614/intepreting-doc2vec-cosine-similarity-between-doc-vectors-and-word-vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_build_docs #<-------- [Activate when stable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
