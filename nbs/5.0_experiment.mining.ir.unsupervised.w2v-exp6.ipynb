{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiment.mining.ir.unsupervised.w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting Neural Unsupervised Approaches for Software Information Retrieval [w2v]\n",
    "\n",
    "> Just Paper. Full Experimentation. This module is dedicated to experiment with word2vec. Consider to Copy the entire notebook for a new and separeted empirical evaluation. \n",
    "> Implementing mutual information analysis\n",
    "> Author: @danaderp April 2020\n",
    "> Author: @danielrc Nov 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This copy is for Cisco purposes. It was adapted to process private github data from cisco. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds4se.mining.ir import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prg import prg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ds4se as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Similarity with BasicSequenceVectorization\n",
    "\n",
    "We test diferent similarities based on [blog](https://www.kdnuggets.com/2017/08/comparing-distance-measurements-python-scipy.html) and [blog2](https://www.kdnuggets.com/2019/01/comparison-text-distance-metrics.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experients Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../dvc-ds4se/' #dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiments 1.2.2 <<-- word2vec\n",
    "path_model_prefix = path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_128k'\n",
    "path_to_trained_model = path_data+'/models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model'\n",
    "def sacp_params():\n",
    "        return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.issue2src,\n",
    "        \"system\": 'sacp-python-common',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_type\": SoftwareArtifacts.PR.value,\n",
    "        \"target_type\": SoftwareArtifacts.PY.value,\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": '/tf/data/cisco/sacp_data/[sacp-python-common-all-corpus-1609224778.517111].csv',\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','bpe128k'],\n",
    "            \"prep\": Preprocessing.bpe\n",
    "        },\n",
    "        \"path_mappings\": \"/tf/data/cisco/sacp_data/sacp-pr-mappings.csv\",\n",
    "        \"saving_path\": path_data + 'metrics/traceability/experiments1.2.x/',\n",
    "        \"names\": ['Source','Target','Linked?'],\n",
    "        \"model_prefix\": path_model_prefix\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectorizationType': <VectorizationType.word2vec: 1>,\n",
       " 'linkType': <LinkType.issue2src: 3>,\n",
       " 'system': 'sacp-python-common',\n",
       " 'path_to_trained_model': '../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model',\n",
       " 'source_type': 'pr',\n",
       " 'target_type': 'py',\n",
       " 'system_path_config': {'system_path': '/tf/data/cisco/sacp_data/[sacp-python-common-all-corpus-1609224778.517111].csv',\n",
       "  'sep': '~',\n",
       "  'names': ['ids', 'bpe128k'],\n",
       "  'prep': <Preprocessing.bpe: 2>},\n",
       " 'path_mappings': '/tf/data/cisco/sacp_data/sacp-pr-mappings.csv',\n",
       " 'saving_path': '../dvc-ds4se/metrics/traceability/experiments1.2.x/',\n",
       " 'names': ['Source', 'Target', 'Linked?'],\n",
       " 'model_prefix': '../dvc-ds4se/models/bpe/sentencepiece/wiki_py_java_bpe_128k'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = sacp_params()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Similarity with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-16 15:15:12,473 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-16 15:15:12,622 : INFO : built Dictionary(3580 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 362 documents (total 149985 corpus positions)\n",
      "2021-01-16 15:15:13,272 : INFO : Ignored vocab by BPE{'\\t', '`', '^', '\\r\\n\\r\\n', '\\r\\n', 'Î³', '\\r\\n\\r\\n@', '```', '\\r\\n\\r\\n\\r\\n', '@', '\\\\'}\n",
      "2021-01-16 15:15:13,282 : INFO : bpe preprocessing documents, dictionary, and vocab for the test corpus\n",
      "2021-01-16 15:15:13,283 : INFO : loading Word2Vec object from ../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model\n",
      "2021-01-16 15:15:13,346 : INFO : loading wv recursively from ../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.wv.* with mmap=None\n",
      "2021-01-16 15:15:13,347 : INFO : loading vectors from ../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.wv.vectors.npy with mmap=None\n",
      "2021-01-16 15:15:13,367 : INFO : setting ignored attribute vectors_norm to None\n",
      "2021-01-16 15:15:13,369 : INFO : loading vocabulary recursively from ../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.vocabulary.* with mmap=None\n",
      "2021-01-16 15:15:13,370 : INFO : loading trainables recursively from ../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.trainables.* with mmap=None\n",
      "2021-01-16 15:15:13,371 : INFO : loading syn1neg from ../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.trainables.syn1neg.npy with mmap=None\n",
      "2021-01-16 15:15:13,413 : INFO : setting ignored attribute cum_table to None\n",
      "2021-01-16 15:15:13,415 : INFO : loaded ../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model\n",
      "2021-01-16 15:15:13,488 : INFO : precomputing L2-norms of word weight vectors\n",
      "2021-01-16 15:15:13,557 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fc7170e64e0>\n",
      "2021-01-16 15:15:13,558 : INFO : iterating over columns in dictionary order\n",
      "2021-01-16 15:15:13,562 : INFO : PROGRESS: at 0.03% columns (1 / 3580, 0.027933% density, 0.027933% projected density)\n",
      "2021-01-16 15:15:24,971 : INFO : PROGRESS: at 27.96% columns (1001 / 3580, 0.509160% density, 1.749005% projected density)\n",
      "2021-01-16 15:15:35,040 : INFO : PROGRESS: at 55.89% columns (2001 / 3580, 0.777597% density, 1.369162% projected density)\n",
      "2021-01-16 15:15:44,682 : INFO : PROGRESS: at 83.83% columns (3001 / 3580, 1.015808% density, 1.206404% projected density)\n",
      "2021-01-16 15:15:49,631 : INFO : constructed a sparse term similarity matrix with 1.097687% density\n"
     ]
    }
   ],
   "source": [
    "#[step 1]Creating the Vectorization Class\n",
    "word2vec = ds.mining.ir.Word2VecSeqVect( params = parameters, logging = logging )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-16 15:15:49,811 : INFO : Removed 1 and 6 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-16 15:15:49,813 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-16 15:15:49,816 : INFO : built Dictionary(261 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 2 documents (total 1208 corpus positions)\n",
      "2021-01-16 15:15:50,005 : INFO : token count processed\n",
      "2021-01-16 15:15:50,188 : INFO : frequencies processed\n",
      "2021-01-16 15:27:04,818 : INFO : scalar_distribution processed\n",
      "2021-01-16 15:27:04,820 : INFO : entropies processed\n",
      "2021-01-16 15:27:04,822 : INFO : extropies processed\n",
      "2021-01-16 15:27:04,912 : INFO : token count processed\n",
      "2021-01-16 15:27:04,913 : INFO : vocab #128011\n",
      "2021-01-16 15:27:04,953 : INFO : alphabet_source #128011\n",
      "2021-01-16 15:27:04,981 : INFO : alphabet_target #128011\n",
      "2021-01-16 15:27:05,040 : INFO : diff src2tgt #set()\n",
      "2021-01-16 15:27:05,132 : INFO : diff tgt2src #set()\n",
      "2021-01-16 15:49:01,200 : INFO : alphabet #128011\n",
      "2021-01-16 16:00:10,461 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.248352530937981, 0.44477010888626706], [0.9333877861499786, 0.066612214], [0.9182958340544896, 0.9182958340544896], [4.328599539040249, 6.786873156936814, 6.8783434490672155, 4.237129246909848, 2.549743910026966, 0.09147029213040181]]\n",
      "2021-01-16 16:00:10,466 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-16 16:00:10,478 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-16 16:00:10,480 : INFO : built Dictionary(358 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 2 documents (total 2013 corpus positions)\n",
      "2021-01-16 16:00:11,054 : INFO : token count processed\n",
      "2021-01-16 16:00:11,392 : INFO : frequencies processed\n",
      "2021-01-16 16:11:06,520 : INFO : scalar_distribution processed\n",
      "2021-01-16 16:11:06,530 : INFO : entropies processed\n",
      "2021-01-16 16:11:06,531 : INFO : extropies processed\n",
      "2021-01-16 16:11:06,609 : INFO : token count processed\n",
      "2021-01-16 16:11:06,622 : INFO : vocab #128011\n",
      "2021-01-16 16:11:06,653 : INFO : alphabet_source #128011\n",
      "2021-01-16 16:11:06,680 : INFO : alphabet_target #128011\n",
      "2021-01-16 16:11:06,735 : INFO : diff src2tgt #set()\n",
      "2021-01-16 16:11:06,793 : INFO : diff tgt2src #set()\n",
      "2021-01-16 16:33:09,271 : INFO : alphabet #128011\n",
      "2021-01-16 16:44:10,492 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.2472553644112103, 0.44498725682739837], [0.9242691695690155, 0.07573083], [0.9182958340544896, 0.9182958340544896], [4.328599539040249, 7.022418290988289, 7.083135727621986, 4.267882102406553, 2.7545361885817368, 0.06071743663369755]]\n",
      "2021-01-16 16:44:10,498 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-16 16:44:10,510 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-16 16:44:10,512 : INFO : built Dictionary(282 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 2 documents (total 1857 corpus positions)\n",
      "2021-01-16 16:44:10,884 : INFO : token count processed\n",
      "2021-01-16 16:44:11,237 : INFO : frequencies processed\n",
      "2021-01-16 16:55:03,225 : INFO : scalar_distribution processed\n",
      "2021-01-16 16:55:03,230 : INFO : entropies processed\n",
      "2021-01-16 16:55:03,231 : INFO : extropies processed\n",
      "2021-01-16 16:55:03,299 : INFO : token count processed\n",
      "2021-01-16 16:55:03,312 : INFO : vocab #128011\n",
      "2021-01-16 16:55:03,330 : INFO : alphabet_source #128011\n",
      "2021-01-16 16:55:03,378 : INFO : alphabet_target #128011\n",
      "2021-01-16 16:55:03,426 : INFO : diff src2tgt #set()\n",
      "2021-01-16 16:55:03,485 : INFO : diff tgt2src #set()\n",
      "2021-01-16 17:27:16,696 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2498384683188444, 0.444476354227881], [0.9322489202022552, 0.06775108], [1.9182958340544893, 1.2183406773511978], [4.328599539040249, 6.442520552013249, 6.511964794825376, 4.259155296228123, 2.1833652557851266, 0.06944424281212669]]\n",
      "2021-01-16 17:27:16,717 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-16 17:27:16,718 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-16 17:27:16,719 : INFO : built Dictionary(153 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 2 documents (total 561 corpus positions)\n",
      "2021-01-16 17:27:16,879 : INFO : token count processed\n",
      "2021-01-16 17:27:17,196 : INFO : frequencies processed\n",
      "2021-01-16 17:37:06,483 : INFO : scalar_distribution processed\n",
      "2021-01-16 17:37:06,485 : INFO : entropies processed\n",
      "2021-01-16 17:37:06,486 : INFO : extropies processed\n",
      "2021-01-16 17:37:06,545 : INFO : token count processed\n",
      "2021-01-16 17:37:06,558 : INFO : vocab #128011\n",
      "2021-01-16 17:37:06,591 : INFO : alphabet_source #128011\n",
      "2021-01-16 17:37:06,615 : INFO : alphabet_target #128011\n",
      "2021-01-16 17:37:06,674 : INFO : diff src2tgt #set()\n",
      "2021-01-16 17:37:06,724 : INFO : diff tgt2src #set()\n",
      "2021-01-16 17:56:49,072 : INFO : alphabet #128011\n",
      "2021-01-16 18:06:51,318 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.2471384784196713, 0.4450104030541378], [0.9349607676267624, 0.06503923], [0.0, 0.0], [4.328599539040249, 5.855292654715939, 6.050165914126657, 4.133726279629531, 1.7215663750864074, 0.1948732594107181]]\n",
      "2021-01-16 18:06:51,322 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-16 18:06:51,335 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-16 18:06:51,337 : INFO : built Dictionary(130 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 2 documents (total 353 corpus positions)\n",
      "2021-01-16 18:06:51,546 : INFO : token count processed\n",
      "2021-01-16 18:06:51,857 : INFO : frequencies processed\n",
      "2021-01-16 18:16:27,353 : INFO : scalar_distribution processed\n",
      "2021-01-16 18:16:27,354 : INFO : entropies processed\n",
      "2021-01-16 18:16:27,355 : INFO : extropies processed\n",
      "2021-01-16 18:16:27,440 : INFO : token count processed\n",
      "2021-01-16 18:16:27,451 : INFO : vocab #128011\n",
      "2021-01-16 18:16:27,476 : INFO : alphabet_source #128011\n",
      "2021-01-16 18:16:27,497 : INFO : alphabet_target #128011\n",
      "2021-01-16 18:16:27,525 : INFO : diff src2tgt #set()\n",
      "2021-01-16 18:16:27,552 : INFO : diff tgt2src #set()\n",
      "2021-01-16 18:35:43,106 : INFO : alphabet #128011\n",
      "2021-01-16 18:45:17,873 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.2564126497563755, 0.44318134810490883], [0.945445828139782, 0.05455417], [0.0, 0.0], [4.328599539040249, 5.781790887408139, 6.054206773925545, 4.056183652522844, 1.7256072348852953, 0.2724158865174058]]\n",
      "2021-01-16 18:45:17,895 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-16 18:45:17,898 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-16 18:45:17,900 : INFO : built Dictionary(232 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 2 documents (total 1812 corpus positions)\n",
      "2021-01-16 18:45:18,220 : INFO : token count processed\n",
      "2021-01-16 18:45:18,506 : INFO : frequencies processed\n",
      "2021-01-16 18:54:53,427 : INFO : scalar_distribution processed\n",
      "2021-01-16 18:54:53,438 : INFO : entropies processed\n",
      "2021-01-16 18:54:53,439 : INFO : extropies processed\n",
      "2021-01-16 18:54:53,502 : INFO : token count processed\n",
      "2021-01-16 18:54:53,514 : INFO : vocab #128011\n",
      "2021-01-16 18:54:53,542 : INFO : alphabet_source #128011\n",
      "2021-01-16 18:54:53,570 : INFO : alphabet_target #128011\n",
      "2021-01-16 18:54:53,616 : INFO : diff src2tgt #set()\n",
      "2021-01-16 18:54:53,667 : INFO : diff tgt2src #set()\n",
      "2021-01-16 19:14:22,710 : INFO : alphabet #128011\n",
      "2021-01-16 19:24:10,406 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.2553964712265058, 0.44338102535745766], [0.938563771545887, 0.06143623], [1.4591479170272448, 1.1091703386755989], [4.328599539040249, 6.321670150661742, 6.397080411051959, 4.253189278650033, 2.068480872011709, 0.0754102603902167]]\n",
      "2021-01-16 19:24:10,414 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-16 19:24:10,415 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-16 19:24:10,417 : INFO : built Dictionary(193 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 2 documents (total 1180 corpus positions)\n",
      "2021-01-16 19:24:10,525 : INFO : token count processed\n",
      "2021-01-16 19:24:10,817 : INFO : frequencies processed\n",
      "2021-01-16 19:33:45,065 : INFO : scalar_distribution processed\n",
      "2021-01-16 19:33:45,067 : INFO : entropies processed\n",
      "2021-01-16 19:33:45,068 : INFO : extropies processed\n",
      "2021-01-16 19:33:45,139 : INFO : token count processed\n",
      "2021-01-16 19:33:45,141 : INFO : vocab #128011\n",
      "2021-01-16 19:33:45,168 : INFO : alphabet_source #128011\n",
      "2021-01-16 19:33:45,196 : INFO : alphabet_target #128011\n",
      "2021-01-16 19:33:45,255 : INFO : diff src2tgt #set()\n",
      "2021-01-16 19:33:45,304 : INFO : diff tgt2src #set()\n",
      "2021-01-16 19:53:03,130 : INFO : alphabet #128011\n",
      "2021-01-16 20:02:44,509 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.2567729100517449, 0.4431106007813038], [0.9385907016694546, 0.0614093], [0.8112781244591328, 0.8112781244591328], [4.328599539040249, 6.075950464980766, 6.184362509765048, 4.220187494255967, 1.8557629707247987, 0.10841204478428246]]\n",
      "2021-01-16 20:02:44,519 : INFO : Removed 1 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-16 20:02:44,530 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-16 20:02:44,533 : INFO : built Dictionary(424 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 2 documents (total 5315 corpus positions)\n",
      "2021-01-16 20:02:45,169 : INFO : token count processed\n",
      "2021-01-16 20:02:45,341 : INFO : frequencies processed\n",
      "2021-01-16 20:12:23,854 : INFO : scalar_distribution processed\n",
      "2021-01-16 20:12:23,866 : INFO : entropies processed\n",
      "2021-01-16 20:12:23,867 : INFO : extropies processed\n",
      "2021-01-16 20:12:23,940 : INFO : token count processed\n",
      "2021-01-16 20:12:23,940 : INFO : vocab #128011\n",
      "2021-01-16 20:12:23,967 : INFO : alphabet_source #128011\n",
      "2021-01-16 20:12:23,992 : INFO : alphabet_target #128011\n",
      "2021-01-16 20:12:24,034 : INFO : diff src2tgt #set()\n",
      "2021-01-16 20:12:24,098 : INFO : diff tgt2src #set()\n",
      "2021-01-16 20:31:32,327 : INFO : alphabet #128011\n",
      "2021-01-16 20:41:14,868 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.2382078567351642, 0.44678602882695756], [0.922771543264389, 0.07722846], [2.1556390622295662, 1.2407663947533207], [4.328599539040249, 6.821527467875317, 6.848756452649715, 4.301370554265851, 2.5201569136094655, 0.027228984774398057]]\n",
      "2021-01-16 20:41:14,886 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-16 20:41:14,897 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-16 20:41:14,899 : INFO : built Dictionary(304 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 2 documents (total 2388 corpus positions)\n",
      "2021-01-16 20:41:15,313 : INFO : token count processed\n",
      "2021-01-16 20:41:15,643 : INFO : frequencies processed\n",
      "2021-01-16 20:50:45,036 : INFO : scalar_distribution processed\n",
      "2021-01-16 20:50:45,037 : INFO : entropies processed\n",
      "2021-01-16 20:50:45,040 : INFO : extropies processed\n",
      "2021-01-16 20:50:45,113 : INFO : token count processed\n",
      "2021-01-16 20:50:45,115 : INFO : vocab #128011\n",
      "2021-01-16 20:50:45,147 : INFO : alphabet_source #128011\n",
      "2021-01-16 20:50:45,186 : INFO : alphabet_target #128011\n",
      "2021-01-16 20:50:45,234 : INFO : diff src2tgt #set()\n",
      "2021-01-16 20:50:45,290 : INFO : diff tgt2src #set()\n",
      "2021-01-16 21:10:13,787 : INFO : alphabet #128011\n",
      "2021-01-16 21:20:00,902 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.2498712607069946, 0.44446987588337045], [0.9381374716758728, 0.06186253], [1.5, 1.1225562489182657], [4.328599539040249, 6.363229521952993, 6.4238176629687995, 4.268011398024443, 2.09521812392855, 0.06058814101580623]]\n",
      "2021-01-16 21:20:00,906 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-16 21:20:00,908 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-16 21:20:00,910 : INFO : built Dictionary(196 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 2 documents (total 573 corpus positions)\n",
      "2021-01-16 21:20:01,102 : INFO : token count processed\n",
      "2021-01-16 21:20:01,380 : INFO : frequencies processed\n",
      "2021-01-16 21:29:39,422 : INFO : scalar_distribution processed\n",
      "2021-01-16 21:29:39,423 : INFO : entropies processed\n",
      "2021-01-16 21:29:39,424 : INFO : extropies processed\n",
      "2021-01-16 21:29:39,484 : INFO : token count processed\n",
      "2021-01-16 21:29:39,495 : INFO : vocab #128011\n",
      "2021-01-16 21:29:39,523 : INFO : alphabet_source #128011\n",
      "2021-01-16 21:29:39,562 : INFO : alphabet_target #128011\n",
      "2021-01-16 21:29:39,616 : INFO : diff src2tgt #set()\n",
      "2021-01-16 21:29:39,653 : INFO : diff tgt2src #set()\n",
      "2021-01-16 21:46:21,340 : INFO : alphabet #128011\n",
      "2021-01-16 21:54:42,834 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.2498898920918147, 0.4444661952191176], [0.9305902123451233, 0.06940979], [0.9182958340544896, 0.9182958340544896], [4.328599539040249, 6.693181005345745, 6.845182298543749, 4.176598245842245, 2.5165827595034997, 0.15200129319800393]]\n",
      "2021-01-16 21:54:42,841 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-16 21:54:42,850 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-16 21:54:42,852 : INFO : built Dictionary(422 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 2 documents (total 2391 corpus positions)\n",
      "2021-01-16 21:54:43,494 : INFO : token count processed\n",
      "2021-01-16 21:54:43,805 : INFO : frequencies processed\n",
      "2021-01-16 22:02:49,227 : INFO : scalar_distribution processed\n",
      "2021-01-16 22:02:49,229 : INFO : entropies processed\n",
      "2021-01-16 22:02:49,231 : INFO : extropies processed\n",
      "2021-01-16 22:02:49,299 : INFO : token count processed\n",
      "2021-01-16 22:02:49,300 : INFO : vocab #128011\n",
      "2021-01-16 22:02:49,316 : INFO : alphabet_source #128011\n",
      "2021-01-16 22:02:49,332 : INFO : alphabet_target #128011\n",
      "2021-01-16 22:02:49,358 : INFO : diff src2tgt #set()\n",
      "2021-01-16 22:02:49,384 : INFO : diff tgt2src #set()\n",
      "2021-01-16 22:19:44,918 : INFO : alphabet #128011\n",
      "2021-01-16 22:28:42,545 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.228505767411177, 0.4487311698374851], [0.9034347161650658, 0.096565284], [2.1556390622295662, 1.2407663947533207], [4.328599539040249, 7.199944187821455, 7.246178842948281, 4.282364883913423, 2.917579303908032, 0.046234655126826674]]\n",
      "2021-01-16 22:28:42,548 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-16 22:28:42,549 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-16 22:28:42,550 : INFO : built Dictionary(64 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 2 documents (total 109 corpus positions)\n",
      "2021-01-16 22:28:42,590 : INFO : token count processed\n",
      "2021-01-16 22:28:42,759 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-16 22:28:42,760 : INFO : frequencies processed\n",
      "2021-01-16 22:28:42,761 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-16 22:28:42,813 : INFO : token count processed\n",
      "2021-01-16 22:28:42,825 : INFO : vocab #128011\n",
      "2021-01-16 22:28:42,851 : INFO : alphabet_source #128011\n",
      "2021-01-16 22:28:42,868 : INFO : alphabet_target #128011\n",
      "2021-01-16 22:28:42,894 : INFO : diff src2tgt #set()\n",
      "2021-01-16 22:28:42,929 : INFO : diff tgt2src #set()\n",
      "2021-01-16 22:45:01,892 : INFO : alphabet #128011\n",
      "2021-01-16 22:52:43,427 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2633890515804782, 0.441815338508296], [0.9516591168940067, 0.048340883], [nan, nan], [4.328599539040249, 4.991331776835099, 5.632655074171856, 3.6872762417034917, 1.3040555351316065, 0.6413232973367569]]\n",
      "2021-01-16 22:52:43,431 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-16 22:52:43,432 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-16 22:52:43,433 : INFO : built Dictionary(147 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 2 documents (total 435 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-16 22:52:43,583 : INFO : token count processed\n",
      "2021-01-16 22:52:43,867 : INFO : frequencies processed\n",
      "2021-01-16 22:59:32,689 : INFO : scalar_distribution processed\n",
      "2021-01-16 22:59:32,690 : INFO : entropies processed\n",
      "2021-01-16 22:59:32,691 : INFO : extropies processed\n",
      "2021-01-16 22:59:32,725 : INFO : token count processed\n",
      "2021-01-16 22:59:32,730 : INFO : vocab #128011\n",
      "2021-01-16 22:59:32,745 : INFO : alphabet_source #128011\n",
      "2021-01-16 22:59:32,762 : INFO : alphabet_target #128011\n",
      "2021-01-16 22:59:32,788 : INFO : diff src2tgt #set()\n",
      "2021-01-16 22:59:32,814 : INFO : diff tgt2src #set()\n",
      "2021-01-16 23:13:13,412 : INFO : alphabet #128011\n",
      "2021-01-16 23:20:03,937 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.2565980627302673, 0.4431449341891643], [0.940634161233902, 0.05936584], [0.0, 0.0], [4.328599539040249, 6.252206315030337, 6.456994940367824, 4.123810913702762, 2.128395401327575, 0.20478862533748732]]\n",
      "2021-01-16 23:20:03,944 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-16 23:20:03,955 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-16 23:20:03,958 : INFO : built Dictionary(361 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 2 documents (total 2196 corpus positions)\n",
      "2021-01-16 23:20:04,338 : INFO : token count processed\n",
      "2021-01-16 23:20:04,497 : INFO : frequencies processed\n",
      "2021-01-16 23:26:57,469 : INFO : scalar_distribution processed\n",
      "2021-01-16 23:26:57,471 : INFO : entropies processed\n",
      "2021-01-16 23:26:57,473 : INFO : extropies processed\n",
      "2021-01-16 23:26:57,508 : INFO : token count processed\n",
      "2021-01-16 23:26:57,510 : INFO : vocab #128011\n",
      "2021-01-16 23:26:57,538 : INFO : alphabet_source #128011\n",
      "2021-01-16 23:26:57,567 : INFO : alphabet_target #128011\n",
      "2021-01-16 23:26:57,616 : INFO : diff src2tgt #set()\n",
      "2021-01-16 23:26:57,652 : INFO : diff tgt2src #set()\n",
      "2021-01-16 23:40:36,946 : INFO : alphabet #128011\n",
      "2021-01-16 23:47:29,812 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.2506171441060785, 0.44432257286353766], [0.9290515631437302, 0.07094844], [1.9182958340544893, 1.2183406773511978], [4.328599539040249, 6.847583194047557, 6.904192136386496, 4.27199059670131, 2.5755925973462466, 0.056608942338939094]]\n",
      "2021-01-16 23:47:29,819 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-16 23:47:29,821 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-16 23:47:29,824 : INFO : built Dictionary(283 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 2 documents (total 2622 corpus positions)\n",
      "2021-01-16 23:47:30,045 : INFO : token count processed\n",
      "2021-01-16 23:47:30,307 : INFO : frequencies processed\n",
      "2021-01-16 23:54:21,383 : INFO : scalar_distribution processed\n",
      "2021-01-16 23:54:21,386 : INFO : entropies processed\n",
      "2021-01-16 23:54:21,387 : INFO : extropies processed\n",
      "2021-01-16 23:54:21,473 : INFO : token count processed\n",
      "2021-01-16 23:54:21,474 : INFO : vocab #128011\n",
      "2021-01-16 23:54:21,490 : INFO : alphabet_source #128011\n",
      "2021-01-16 23:54:21,507 : INFO : alphabet_target #128011\n",
      "2021-01-16 23:54:21,546 : INFO : diff src2tgt #set()\n",
      "2021-01-16 23:54:21,588 : INFO : diff tgt2src #set()\n",
      "2021-01-17 00:08:06,423 : INFO : alphabet #128011\n",
      "2021-01-17 00:15:02,830 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.2203129986725987, 0.4503869502173091], [0.9176509901881218, 0.08234901], [1.4591479170272448, 1.1091703386755989], [4.328599539040249, 6.44651664457804, 6.499999342867929, 4.275116840750361, 2.171399803827679, 0.05348269828988883]]\n",
      "2021-01-17 00:15:02,836 : INFO : Removed 1 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-17 00:15:02,838 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-17 00:15:02,840 : INFO : built Dictionary(298 unique tokens: ['287', '29', '293', '4)', 'bom']...) from 2 documents (total 1283 corpus positions)\n",
      "2021-01-17 00:15:03,245 : INFO : token count processed\n",
      "2021-01-17 00:15:03,484 : INFO : frequencies processed\n",
      "2021-01-17 00:22:15,471 : INFO : scalar_distribution processed\n",
      "2021-01-17 00:22:15,482 : INFO : entropies processed\n",
      "2021-01-17 00:22:15,483 : INFO : extropies processed\n",
      "2021-01-17 00:22:15,541 : INFO : token count processed\n",
      "2021-01-17 00:22:15,541 : INFO : vocab #128011\n",
      "2021-01-17 00:22:15,572 : INFO : alphabet_source #128011\n",
      "2021-01-17 00:22:15,608 : INFO : alphabet_target #128011\n",
      "2021-01-17 00:22:15,645 : INFO : diff src2tgt #set()\n",
      "2021-01-17 00:22:15,682 : INFO : diff tgt2src #set()\n"
     ]
    }
   ],
   "source": [
    "#[step 2]NonGroundTruth Computation\n",
    "metric_list = [DistanceMetric.WMD,DistanceMetric.SCM,EntropyMetric.MSI_I,EntropyMetric.MI]\n",
    "#metric_list = [EntropyMetric.MSI_I,EntropyMetric.MI]\n",
    "word2vec.ComputeDistanceArtifacts( sampling=False, samples = 100, metric_list = metric_list )\n",
    "word2vec.df_nonground_link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_nonground_link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_nonground_link['Target'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 3]Saving Non-GroundTruth Links\n",
    "word2vec.SaveLinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_nonglinks = ds.mining.ir.LoadLinks(timestamp=1610579170.341825, params=parameters, logging=logging)\n",
    "df_nonglinks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_nonground_link = df_nonglinks # Only to load links from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 4]GroundTruthMatching Testing\n",
    "#TODO change the path for a param\n",
    "path_to_ground_truth =  parameters['path_mappings']\n",
    "word2vec.MatchWithGroundTruth(path_to_ground_truth, semeru_format=True)\n",
    "word2vec.df_ground_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_ground_link[word2vec.df_ground_link ['Linked?']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_ground_link['Source'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Only SACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 4.1]GroundTruthMatching Testing For CISCO Mappings\n",
    "word2vec.MatchWithGroundTruth(from_mappings=True)\n",
    "word2vec.df_ground_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[optional]GroundTruth Direct Processing\n",
    "ground_links = word2vec.ground_truth_processing(path_to_ground_truth)\n",
    "ground_links # A tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 5]Saving GroundTruth Links\n",
    "word2vec.SaveLinks(grtruth = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_glinks = ds.mining.ir.LoadLinks(timestamp=1610579318.97542, params=parameters,grtruth = True, logging=logging)\n",
    "df_glinks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_build_docs #<-------- [Activate when stable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
