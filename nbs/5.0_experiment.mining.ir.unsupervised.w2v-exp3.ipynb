{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiment.mining.ir.unsupervised.w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting Neural Unsupervised Approaches for Software Information Retrieval [w2v]\n",
    "\n",
    "> Just Paper. Full Experimentation. This module is dedicated to experiment with word2vec. Consider to Copy the entire notebook for a new and separeted empirical evaluation. \n",
    "> Implementing mutual information analysis\n",
    "> Author: @danaderp April 2020\n",
    "> Author: @danielrc Nov 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This copy is for Cisco purposes. It was adapted to process private github data from cisco. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds4se.mining.ir import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prg import prg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ds4se as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Similarity with BasicSequenceVectorization\n",
    "\n",
    "We test diferent similarities based on [blog](https://www.kdnuggets.com/2017/08/comparing-distance-measurements-python-scipy.html) and [blog2](https://www.kdnuggets.com/2019/01/comparison-text-distance-metrics.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experients Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../dvc-ds4se/' #dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiments 1.2.0 <<-- word2vec\n",
    "path_model_prefix = path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_128k'\n",
    "path_to_trained_model = path_data+'/models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model'\n",
    "def libest_params():\n",
    "        return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.req2tc,\n",
    "        \"system\": 'sacp-python-common',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_type\": SoftwareArtifacts.REQ.value,\n",
    "        \"target_type\": SoftwareArtifacts.TC.value,\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": path_data + 'se-benchmarking/traceability/testbeds/processed/[libest-all-corpus-1596063103.098236].csv',\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','bpe128k'],\n",
    "            \"prep\": Preprocessing.bpe\n",
    "        },\n",
    "        \"path_mappings\": path_data + \"se-benchmarking/traceability/testbeds/groundtruth/english/[libest-ground-req-to-tc].txt\",\n",
    "        \"saving_path\": path_data + 'metrics/traceability/experiments1.2.x/',\n",
    "        \"names\": ['Source','Target','Linked?'],\n",
    "        \"model_prefix\": path_model_prefix\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectorizationType': <VectorizationType.word2vec: 1>,\n",
       " 'linkType': <LinkType.req2tc: 1>,\n",
       " 'system': 'sacp-python-common',\n",
       " 'path_to_trained_model': '../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model',\n",
       " 'source_type': 'req',\n",
       " 'target_type': 'tc',\n",
       " 'system_path_config': {'system_path': '../dvc-ds4se/se-benchmarking/traceability/testbeds/processed/[libest-all-corpus-1596063103.098236].csv',\n",
       "  'sep': '~',\n",
       "  'names': ['ids', 'bpe128k'],\n",
       "  'prep': <Preprocessing.bpe: 2>},\n",
       " 'path_mappings': '../dvc-ds4se/se-benchmarking/traceability/testbeds/groundtruth/english/[libest-ground-req-to-tc].txt',\n",
       " 'saving_path': '../dvc-ds4se/metrics/traceability/experiments1.2.x/',\n",
       " 'names': ['Source', 'Target', 'Linked?'],\n",
       " 'model_prefix': '../dvc-ds4se/models/bpe/sentencepiece/wiki_py_java_bpe_128k'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = libest_params()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Similarity with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 12:45:11,807 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:45:11,943 : INFO : built Dictionary(6117 unique tokens: ['\\n', '\\n\\n', '\"', '\");', '\",']...) from 87 documents (total 366503 corpus positions)\n",
      "2021-01-14 12:45:12,120 : INFO : bpe preprocessing documents, dictionary, and vocab for the test corpus\n",
      "2021-01-14 12:45:12,121 : INFO : loading Word2Vec object from ../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model\n",
      "2021-01-14 12:45:12,209 : INFO : loading wv recursively from ../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.wv.* with mmap=None\n",
      "2021-01-14 12:45:12,210 : INFO : loading vectors from ../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.wv.vectors.npy with mmap=None\n",
      "2021-01-14 12:45:12,619 : INFO : setting ignored attribute vectors_norm to None\n",
      "2021-01-14 12:45:12,620 : INFO : loading vocabulary recursively from ../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.vocabulary.* with mmap=None\n",
      "2021-01-14 12:45:12,621 : INFO : loading trainables recursively from ../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.trainables.* with mmap=None\n",
      "2021-01-14 12:45:12,622 : INFO : loading syn1neg from ../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model.trainables.syn1neg.npy with mmap=None\n",
      "2021-01-14 12:45:13,032 : INFO : setting ignored attribute cum_table to None\n",
      "2021-01-14 12:45:13,033 : INFO : loaded ../dvc-ds4se//models/wv/bpe128k/[word2vec-Java-Py-SK-500-20E-128k-1594873397.267055].model\n",
      "2021-01-14 12:45:13,112 : INFO : precomputing L2-norms of word weight vectors\n",
      "2021-01-14 12:45:13,152 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7fbc9dab25f8>\n",
      "2021-01-14 12:45:13,154 : INFO : iterating over columns in dictionary order\n",
      "2021-01-14 12:45:13,163 : INFO : PROGRESS: at 0.02% columns (1 / 6117, 0.016348% density, 0.016348% projected density)\n",
      "2021-01-14 12:45:20,668 : INFO : PROGRESS: at 16.36% columns (1001 / 6117, 0.177443% density, 1.000781% projected density)\n",
      "2021-01-14 12:45:28,116 : INFO : PROGRESS: at 32.71% columns (2001 / 6117, 0.334048% density, 0.987548% projected density)\n",
      "2021-01-14 12:45:35,028 : INFO : PROGRESS: at 49.06% columns (3001 / 6117, 0.432183% density, 0.863954% projected density)\n",
      "2021-01-14 12:45:41,695 : INFO : PROGRESS: at 65.41% columns (4001 / 6117, 0.522109% density, 0.789589% projected density)\n",
      "2021-01-14 12:45:48,031 : INFO : PROGRESS: at 81.76% columns (5001 / 6117, 0.589243% density, 0.717087% projected density)\n",
      "2021-01-14 12:45:53,896 : INFO : PROGRESS: at 98.10% columns (6001 / 6117, 0.629213% density, 0.641060% projected density)\n",
      "2021-01-14 12:45:54,400 : INFO : constructed a sparse term similarity matrix with 0.632228% density\n"
     ]
    }
   ],
   "source": [
    "#[step 1]Creating the Vectorization Class\n",
    "word2vec = ds.mining.ir.Word2VecSeqVect( params = parameters, logging = logging )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 12:45:54,593 : INFO : Removed 0 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 12:45:54,594 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:45:54,603 : INFO : built Dictionary(1284 unique tokens: ['\\n', '\\n\\n', '\\n\\n\\n', '\"', '\".']...) from 2 documents (total 8429 corpus positions)\n",
      "2021-01-14 12:46:25,668 : INFO : token count processed\n",
      "2021-01-14 12:46:25,816 : INFO : frequencies processed\n",
      "2021-01-14 12:51:27,392 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:51:27,394 : INFO : entropies processed\n",
      "2021-01-14 12:51:27,394 : INFO : extropies processed\n",
      "2021-01-14 12:51:27,438 : INFO : token count processed\n",
      "2021-01-14 12:51:27,454 : INFO : alphabet_source #128008\n",
      "2021-01-14 12:51:27,470 : INFO : alphabet_target #128008\n",
      "2021-01-14 12:51:27,471 : INFO : vocab #128005\n",
      "2021-01-14 12:51:27,497 : INFO : diff #{'\\n\\n\\n'}\n",
      "2021-01-14 13:01:47,997 : INFO : alphabet #128005\n",
      "2021-01-14 13:06:52,037 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ17.txt', 'test_data/LibEST_semeru_format/test/us903.c')[[0.9483599007781748, 0.5132521971944711], [0.45325911045074463, 0.5467409], [5.765700165905343, 1.4217121343050214], [6.793575819668119, 8.295908249700595, 8.659862933770523, 6.429621135598191, 1.8662871141024038, 0.3639546840699275]]\n",
      "2021-01-14 13:06:52,051 : INFO : Removed 0 and 12 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 13:06:52,052 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:06:52,056 : INFO : built Dictionary(927 unique tokens: ['\\n', '\\n\\n', '\\n\\n\\n', '\"', '\".']...) from 2 documents (total 8298 corpus positions)\n",
      "2021-01-14 13:07:06,388 : INFO : token count processed\n",
      "2021-01-14 13:07:06,547 : INFO : frequencies processed\n",
      "2021-01-14 13:12:06,504 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:12:06,505 : INFO : entropies processed\n",
      "2021-01-14 13:12:06,506 : INFO : extropies processed\n",
      "2021-01-14 13:12:06,544 : INFO : token count processed\n",
      "2021-01-14 13:12:06,559 : INFO : alphabet_source #128008\n",
      "2021-01-14 13:12:06,575 : INFO : alphabet_target #128014\n",
      "2021-01-14 13:12:06,576 : INFO : vocab #128005\n",
      "2021-01-14 13:12:06,600 : INFO : diff #set()\n",
      "2021-01-14 13:22:09,054 : INFO : alphabet #128005\n",
      "2021-01-14 13:27:21,079 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ17.txt', 'test_data/LibEST_semeru_format/test/us3496.c')[[0.9437482701936489, 0.5144699112197144], [0.5066628158092499, 0.49333718], [5.583072671136669, 1.4203953874511304], [6.793575819668119, 7.336067584176797, 7.745779705142594, 6.3838636987023225, 0.952203885474475, 0.40971212096579723]]\n",
      "2021-01-14 13:27:21,104 : INFO : Removed 0 and 15 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 13:27:21,105 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:27:21,111 : INFO : built Dictionary(1069 unique tokens: ['\\n', '\\n\\n', '\\n\\n\\n', '\"', '\".']...) from 2 documents (total 15752 corpus positions)\n",
      "2021-01-14 13:27:50,226 : INFO : token count processed\n",
      "2021-01-14 13:27:50,381 : INFO : frequencies processed\n",
      "2021-01-14 13:32:46,726 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:32:46,727 : INFO : entropies processed\n",
      "2021-01-14 13:32:46,728 : INFO : extropies processed\n",
      "2021-01-14 13:32:46,768 : INFO : token count processed\n",
      "2021-01-14 13:32:46,784 : INFO : alphabet_source #128008\n",
      "2021-01-14 13:32:46,800 : INFO : alphabet_target #128015\n",
      "2021-01-14 13:32:46,801 : INFO : vocab #128005\n",
      "2021-01-14 13:32:46,826 : INFO : diff #set()\n",
      "2021-01-14 13:42:45,466 : INFO : alphabet #128005\n",
      "2021-01-14 13:47:44,307 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ17.txt', 'test_data/LibEST_semeru_format/test/us899.c')[[0.9496699784173231, 0.5129073181973939], [0.496221661567688, 0.50377834], [5.780943661774689, 1.4233195379680867], [6.793575819668119, 7.454116702352194, 7.727473196800052, 6.5202193252202605, 0.9338973771319328, 0.2733564944478575]]\n",
      "2021-01-14 13:47:44,326 : INFO : Removed 0 and 33 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 13:47:44,327 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:47:44,331 : INFO : built Dictionary(714 unique tokens: ['\\n', '\\n\\n', '\\n\\n\\n', '\"', '\".']...) from 2 documents (total 11342 corpus positions)\n",
      "2021-01-14 13:47:55,295 : INFO : token count processed\n",
      "2021-01-14 13:47:55,435 : INFO : frequencies processed\n",
      "2021-01-14 13:52:56,849 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:52:56,850 : INFO : entropies processed\n",
      "2021-01-14 13:52:56,851 : INFO : extropies processed\n",
      "2021-01-14 13:52:56,891 : INFO : token count processed\n",
      "2021-01-14 13:52:56,907 : INFO : alphabet_source #128008\n",
      "2021-01-14 13:52:56,923 : INFO : alphabet_target #128014\n",
      "2021-01-14 13:52:56,924 : INFO : vocab #128005\n",
      "2021-01-14 13:52:56,954 : INFO : diff #set()\n",
      "2021-01-14 14:02:53,695 : INFO : alphabet #128005\n",
      "2021-01-14 14:07:49,108 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ17.txt', 'test_data/LibEST_semeru_format/test/us4020.c')[[0.9940851902965027, 0.5014830885190562], [0.5308358669281006, 0.46916413], [5.423559628288567, 1.4173511157313556], [6.793575819668119, 6.856454382698361, 7.255097711602719, 6.394932490763761, 0.46152189193460025, 0.39864332890435783]]\n",
      "2021-01-14 14:07:49,129 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 14:07:49,130 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:07:49,137 : INFO : built Dictionary(705 unique tokens: ['\\n', '\\n\\n', '\\n\\n\\n', '\"', '\".']...) from 2 documents (total 12746 corpus positions)\n",
      "2021-01-14 14:07:59,499 : INFO : token count processed\n",
      "2021-01-14 14:07:59,680 : INFO : frequencies processed\n",
      "2021-01-14 14:12:55,119 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:12:55,120 : INFO : entropies processed\n",
      "2021-01-14 14:12:55,121 : INFO : extropies processed\n",
      "2021-01-14 14:12:55,159 : INFO : token count processed\n",
      "2021-01-14 14:12:55,175 : INFO : alphabet_source #128008\n",
      "2021-01-14 14:12:55,191 : INFO : alphabet_target #128015\n",
      "2021-01-14 14:12:55,192 : INFO : vocab #128005\n",
      "2021-01-14 14:12:55,217 : INFO : diff #set()\n",
      "2021-01-14 14:22:59,123 : INFO : alphabet #128005\n",
      "2021-01-14 14:27:58,715 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ17.txt', 'test_data/LibEST_semeru_format/test/us897.c')[[0.9447447148066938, 0.5142063081011634], [0.5000310838222504, 0.49996892], [5.468329852746622, 1.418656537785687], [6.793575819668119, 6.88886771674332, 7.248111420373331, 6.434332116038108, 0.4545356007052117, 0.35924370363001046]]\n",
      "2021-01-14 14:27:58,730 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 14:27:58,731 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:27:58,735 : INFO : built Dictionary(946 unique tokens: ['\\n', '\\n\\n', '\\n\\n\\n', '\"', '\".']...) from 2 documents (total 8250 corpus positions)\n",
      "2021-01-14 14:28:13,553 : INFO : token count processed\n",
      "2021-01-14 14:28:13,699 : INFO : frequencies processed\n",
      "2021-01-14 14:33:48,457 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:33:48,458 : INFO : entropies processed\n",
      "2021-01-14 14:33:48,459 : INFO : extropies processed\n",
      "2021-01-14 14:33:48,501 : INFO : token count processed\n",
      "2021-01-14 14:33:48,518 : INFO : alphabet_source #128008\n",
      "2021-01-14 14:33:48,534 : INFO : alphabet_target #128008\n",
      "2021-01-14 14:33:48,535 : INFO : vocab #128005\n",
      "2021-01-14 14:33:48,560 : INFO : diff #{'\\n\\n\\n'}\n",
      "2021-01-14 14:44:29,335 : INFO : alphabet #128005\n",
      "2021-01-14 14:49:34,590 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ17.txt', 'test_data/LibEST_semeru_format/test/us1060.c')[[0.9881684059150345, 0.5029755009811455], [0.5477555096149445, 0.4522445], [5.747619766585081, 1.422352119295666], [6.793575819668119, 7.08964724033698, 7.44032583421375, 6.442897225791349, 0.6467500145456313, 0.35067859387677025]]\n",
      "2021-01-14 14:49:34,612 : INFO : Removed 0 and 76 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 14:49:34,613 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 14:49:34,622 : INFO : built Dictionary(871 unique tokens: ['\\n', '\\n\\n', '\\n\\n\\n', '\"', '\".']...) from 2 documents (total 14458 corpus positions)\n",
      "2021-01-14 14:49:47,280 : INFO : token count processed\n",
      "2021-01-14 14:49:47,424 : INFO : frequencies processed\n",
      "2021-01-14 14:54:42,574 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:54:42,576 : INFO : entropies processed\n",
      "2021-01-14 14:54:42,576 : INFO : extropies processed\n",
      "2021-01-14 14:54:42,620 : INFO : token count processed\n",
      "2021-01-14 14:54:42,636 : INFO : alphabet_source #128008\n",
      "2021-01-14 14:54:42,651 : INFO : alphabet_target #128008\n",
      "2021-01-14 14:54:42,652 : INFO : vocab #128005\n",
      "2021-01-14 14:54:42,677 : INFO : diff #{'\\n\\n\\n'}\n",
      "2021-01-14 15:04:35,039 : INFO : alphabet #128005\n",
      "2021-01-14 15:09:29,994 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ17.txt', 'test_data/LibEST_semeru_format/test/us900.c')[[1.0656583305169893, 0.48410716584950514], [0.6237243115901947, 0.3762757], [5.435400650947243, 1.4177287733854755], [6.793575819668119, 7.345749245906533, 7.569746470186149, 6.569578595388504, 0.7761706505180301, 0.223997224279616]]\n",
      "2021-01-14 15:09:30,003 : INFO : Removed 0 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 15:09:30,004 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:09:30,007 : INFO : built Dictionary(712 unique tokens: ['\\n', '\\n\\n', '\\n\\n\\n', '\"', '\".']...) from 2 documents (total 4777 corpus positions)\n",
      "2021-01-14 15:09:37,303 : INFO : token count processed\n",
      "2021-01-14 15:09:37,454 : INFO : frequencies processed\n",
      "2021-01-14 15:14:31,901 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:14:31,902 : INFO : entropies processed\n",
      "2021-01-14 15:14:31,903 : INFO : extropies processed\n",
      "2021-01-14 15:14:31,940 : INFO : token count processed\n",
      "2021-01-14 15:14:31,955 : INFO : alphabet_source #128008\n",
      "2021-01-14 15:14:31,971 : INFO : alphabet_target #128009\n",
      "2021-01-14 15:14:31,972 : INFO : vocab #128005\n",
      "2021-01-14 15:14:31,996 : INFO : diff #set()\n",
      "2021-01-14 15:24:32,481 : INFO : alphabet #128005\n",
      "2021-01-14 15:29:32,670 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ17.txt', 'test_data/LibEST_semeru_format/test/us896.c')[[0.9932683462263385, 0.501688596968493], [0.5341168344020844, 0.46588317], [5.29182429928481, 1.4144424814994343], [6.793575819668119, 7.557266299124576, 7.982731825383279, 6.368110293409416, 1.1891560057151604, 0.42546552625870326]]\n",
      "2021-01-14 15:29:32,691 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 15:29:32,692 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:29:32,701 : INFO : built Dictionary(1023 unique tokens: ['\\n', '\\n\\n', '\\n\\n\\n', '\"', '\".']...) from 2 documents (total 12810 corpus positions)\n",
      "2021-01-14 15:29:50,985 : INFO : token count processed\n",
      "2021-01-14 15:29:51,169 : INFO : frequencies processed\n",
      "2021-01-14 15:34:51,089 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:34:51,090 : INFO : entropies processed\n",
      "2021-01-14 15:34:51,091 : INFO : extropies processed\n",
      "2021-01-14 15:34:51,130 : INFO : token count processed\n",
      "2021-01-14 15:34:51,146 : INFO : alphabet_source #128008\n",
      "2021-01-14 15:34:51,162 : INFO : alphabet_target #128009\n",
      "2021-01-14 15:34:51,163 : INFO : vocab #128005\n",
      "2021-01-14 15:34:51,188 : INFO : diff #set()\n",
      "2021-01-14 15:44:54,440 : INFO : alphabet #128005\n",
      "2021-01-14 15:49:53,906 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ17.txt', 'test_data/LibEST_semeru_format/test/us894.c')[[0.9559282022064168, 0.5112662105244629], [0.4969521760940552, 0.5030478], [5.801781044084321, 1.4226767907960582], [6.793575819668119, 7.35286280835418, 7.74971323873702, 6.396725389285279, 0.9561374190689014, 0.3968504303828402]]\n",
      "2021-01-14 15:49:53,917 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 15:49:53,918 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:49:53,925 : INFO : built Dictionary(677 unique tokens: ['\\n', '\\n\\n', '\\n\\n\\n', '\"', '\".']...) from 2 documents (total 6635 corpus positions)\n",
      "2021-01-14 15:50:01,045 : INFO : token count processed\n",
      "2021-01-14 15:50:01,231 : INFO : frequencies processed\n",
      "2021-01-14 15:55:04,982 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:55:04,983 : INFO : entropies processed\n",
      "2021-01-14 15:55:04,984 : INFO : extropies processed\n",
      "2021-01-14 15:55:05,022 : INFO : token count processed\n",
      "2021-01-14 15:55:05,038 : INFO : alphabet_source #128008\n",
      "2021-01-14 15:55:05,054 : INFO : alphabet_target #128008\n",
      "2021-01-14 15:55:05,055 : INFO : vocab #128005\n",
      "2021-01-14 15:55:05,080 : INFO : diff #{'\\n\\n\\n'}\n",
      "2021-01-14 16:05:01,085 : INFO : alphabet #128005\n",
      "2021-01-14 16:09:59,668 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ17.txt', 'test_data/LibEST_semeru_format/test/us1005.c')[[0.9248351817165431, 0.5195250011526769], [0.46077919006347656, 0.5392208], [5.637266789795468, 1.4212100853125493], [6.793575819668119, 7.213303475711673, 7.734288139381414, 6.272591155998379, 0.9407123197132954, 0.5209846636697408]]\n",
      "2021-01-14 16:09:59,688 : INFO : Removed 0 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:09:59,689 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:09:59,694 : INFO : built Dictionary(826 unique tokens: ['\\n', '\\n\\n', '\\n\\n\\n', '\"', '\".']...) from 2 documents (total 12166 corpus positions)\n",
      "2021-01-14 16:10:15,908 : INFO : token count processed\n",
      "2021-01-14 16:10:16,049 : INFO : frequencies processed\n",
      "2021-01-14 16:15:12,708 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:15:12,710 : INFO : entropies processed\n",
      "2021-01-14 16:15:12,710 : INFO : extropies processed\n",
      "2021-01-14 16:15:12,749 : INFO : token count processed\n",
      "2021-01-14 16:15:12,766 : INFO : alphabet_source #128008\n",
      "2021-01-14 16:15:12,782 : INFO : alphabet_target #128014\n",
      "2021-01-14 16:15:12,783 : INFO : vocab #128005\n",
      "2021-01-14 16:15:12,809 : INFO : diff #set()\n",
      "2021-01-14 16:25:09,605 : INFO : alphabet #128005\n",
      "2021-01-14 16:30:06,339 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ17.txt', 'test_data/LibEST_semeru_format/test/us898.c')[[0.9571247994232108, 0.5109536194598896], [0.5144844353199005, 0.48551556], [5.627687956198736, 1.420358272534896], [6.793575819668119, 6.9849761902830245, 7.3479270770553295, 6.430624932895814, 0.5543512573872107, 0.362950886772305]]\n",
      "2021-01-14 16:30:06,358 : INFO : Removed 0 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:30:06,359 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:30:06,368 : INFO : built Dictionary(1235 unique tokens: ['\\n', '\\n\\n', '\\n\\n\\n', '\"', '\".']...) from 2 documents (total 11391 corpus positions)\n",
      "2021-01-14 16:30:35,667 : INFO : token count processed\n",
      "2021-01-14 16:30:35,839 : INFO : frequencies processed\n",
      "2021-01-14 16:35:36,335 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:35:36,337 : INFO : entropies processed\n",
      "2021-01-14 16:35:36,338 : INFO : extropies processed\n",
      "2021-01-14 16:35:36,380 : INFO : token count processed\n",
      "2021-01-14 16:35:36,396 : INFO : alphabet_source #128008\n",
      "2021-01-14 16:35:36,412 : INFO : alphabet_target #128008\n",
      "2021-01-14 16:35:36,413 : INFO : vocab #128005\n",
      "2021-01-14 16:35:36,439 : INFO : diff #{'\\n\\n\\n'}\n",
      "2021-01-14 16:45:32,041 : INFO : alphabet #128005\n",
      "2021-01-14 16:50:32,013 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ17.txt', 'test_data/LibEST_semeru_format/test/us3512.c')[[0.8992389997419225, 0.5265266773354405], [0.47945636510849, 0.52054363], [5.950771478991496, 1.4258614163586405], [6.793575819668119, 7.822176838778024, 8.159242455003078, 6.456510203443065, 1.365666635334959, 0.33706561622505404]]\n",
      "2021-01-14 16:50:32,034 : INFO : Removed 0 and 67 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:50:32,035 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:50:32,042 : INFO : built Dictionary(1502 unique tokens: ['\\n', '\\n\\n', '\\n\\n\\n', '\"', '\".']...) from 2 documents (total 12358 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 16:51:16,421 : INFO : token count processed\n",
      "2021-01-14 16:51:16,568 : INFO : frequencies processed\n",
      "2021-01-14 16:56:15,500 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:56:15,501 : INFO : entropies processed\n",
      "2021-01-14 16:56:15,502 : INFO : extropies processed\n",
      "2021-01-14 16:56:15,542 : INFO : token count processed\n",
      "2021-01-14 16:56:15,557 : INFO : alphabet_source #128008\n",
      "2021-01-14 16:56:15,572 : INFO : alphabet_target #128015\n",
      "2021-01-14 16:56:15,573 : INFO : vocab #128005\n",
      "2021-01-14 16:56:15,598 : INFO : diff #set()\n",
      "2021-01-14 17:06:16,606 : INFO : alphabet #128005\n",
      "2021-01-14 17:11:15,898 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ17.txt', 'test_data/LibEST_semeru_format/test/us1883.c')[[0.9403919749750873, 0.5153597896181976], [0.4461361765861511, 0.5538638], [5.7890523584609195, 1.4220690160660838], [6.793575819668119, 8.359479410422157, 8.671599229668622, 6.4814560004216535, 1.8780234100005035, 0.31211981924646537]]\n",
      "2021-01-14 17:11:15,913 : INFO : Removed 0 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:11:15,914 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:11:15,922 : INFO : built Dictionary(1268 unique tokens: ['\\n', '\\n\\n', '\\n\\n\\n', '\"', '\".']...) from 2 documents (total 8273 corpus positions)\n",
      "2021-01-14 17:11:52,799 : INFO : token count processed\n",
      "2021-01-14 17:11:52,944 : INFO : frequencies processed\n",
      "2021-01-14 17:16:56,901 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:16:56,902 : INFO : entropies processed\n",
      "2021-01-14 17:16:56,903 : INFO : extropies processed\n",
      "2021-01-14 17:16:56,941 : INFO : token count processed\n",
      "2021-01-14 17:16:56,956 : INFO : alphabet_source #128008\n",
      "2021-01-14 17:16:56,972 : INFO : alphabet_target #128008\n",
      "2021-01-14 17:16:56,973 : INFO : vocab #128005\n",
      "2021-01-14 17:16:56,997 : INFO : diff #{'\\n\\n\\n'}\n",
      "2021-01-14 17:26:56,675 : INFO : alphabet #128005\n",
      "2021-01-14 17:31:52,257 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ17.txt', 'test_data/LibEST_semeru_format/test/us748.c')[[0.9549346585394571, 0.511526048009761], [0.45856136083602905, 0.54143864], [5.79754310247398, 1.4227982422460332], [6.793575819668119, 8.317341209722748, 8.670385245125846, 6.440531784265021, 1.8768094254577274, 0.3530440354030979]]\n",
      "2021-01-14 17:31:52,268 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:31:52,269 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:31:52,275 : INFO : built Dictionary(687 unique tokens: ['\\n', '\\n\\n', '\\n\\n\\n', '\"', '\".']...) from 2 documents (total 5506 corpus positions)\n",
      "2021-01-14 17:32:02,001 : INFO : token count processed\n",
      "2021-01-14 17:32:02,142 : INFO : frequencies processed\n",
      "2021-01-14 17:36:58,644 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:36:58,644 : INFO : entropies processed\n",
      "2021-01-14 17:36:58,645 : INFO : extropies processed\n",
      "2021-01-14 17:36:58,679 : INFO : token count processed\n",
      "2021-01-14 17:36:58,694 : INFO : alphabet_source #128008\n",
      "2021-01-14 17:36:58,710 : INFO : alphabet_target #128010\n",
      "2021-01-14 17:36:58,711 : INFO : vocab #128005\n",
      "2021-01-14 17:36:58,739 : INFO : diff #set()\n",
      "2021-01-14 17:45:50,767 : INFO : alphabet #128005\n",
      "2021-01-14 17:49:57,912 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ17.txt', 'test_data/LibEST_semeru_format/test/us3612.c')[[0.970476153673697, 0.507491551286033], [0.5188913345336914, 0.48110867], [5.513631642856708, 1.4180559717527113], [6.793575819668119, 7.124835351141783, 7.664651875947774, 6.253759294862128, 0.8710760562796551, 0.5398165248059907]]\n",
      "2021-01-14 17:49:57,928 : INFO : Removed 0 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:49:57,929 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:49:57,934 : INFO : built Dictionary(965 unique tokens: ['\\n', '\\n\\n', '\\n\\n\\n', '\"', '\".']...) from 2 documents (total 9690 corpus positions)\n",
      "2021-01-14 17:50:18,066 : INFO : token count processed\n",
      "2021-01-14 17:50:18,198 : INFO : frequencies processed\n",
      "2021-01-14 17:54:25,503 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:54:25,505 : INFO : entropies processed\n",
      "2021-01-14 17:54:25,505 : INFO : extropies processed\n",
      "2021-01-14 17:54:25,541 : INFO : token count processed\n",
      "2021-01-14 17:54:25,556 : INFO : alphabet_source #128008\n",
      "2021-01-14 17:54:25,572 : INFO : alphabet_target #128008\n",
      "2021-01-14 17:54:25,573 : INFO : vocab #128005\n",
      "2021-01-14 17:54:25,597 : INFO : diff #{'\\n\\n\\n'}\n",
      "2021-01-14 18:02:41,133 : INFO : alphabet #128005\n",
      "2021-01-14 18:06:49,212 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ17.txt', 'test_data/LibEST_semeru_format/test/us901.c')[[0.9428361904538475, 0.5147114331684337], [0.46758508682250977, 0.5324149], [5.684507837117203, 1.4212294528039855], [6.793575819668119, 7.508450572503415, 7.941508482894466, 6.360517909277068, 1.1479326632263476, 0.43305791039105124]]\n",
      "2021-01-14 18:06:49,222 : INFO : Removed 0 and 31 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 18:06:49,223 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:06:49,227 : INFO : built Dictionary(1116 unique tokens: ['\\n', '\\n\\n', '\\n\\n\\n', '\"', '\".']...) from 2 documents (total 5075 corpus positions)\n",
      "2021-01-14 18:07:10,145 : INFO : token count processed\n",
      "2021-01-14 18:07:10,297 : INFO : frequencies processed\n",
      "2021-01-14 18:11:18,616 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:11:18,618 : INFO : entropies processed\n",
      "2021-01-14 18:11:18,618 : INFO : extropies processed\n",
      "2021-01-14 18:11:18,652 : INFO : token count processed\n",
      "2021-01-14 18:11:18,666 : INFO : alphabet_source #128008\n",
      "2021-01-14 18:11:18,681 : INFO : alphabet_target #128008\n",
      "2021-01-14 18:11:18,682 : INFO : vocab #128005\n",
      "2021-01-14 18:11:18,705 : INFO : diff #{'\\n\\n\\n'}\n",
      "2021-01-14 18:19:34,353 : INFO : alphabet #128005\n",
      "2021-01-14 18:23:48,730 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ17.txt', 'test_data/LibEST_semeru_format/test/us1864.c')[[0.9713317359669195, 0.507271293691982], [0.47551077604293823, 0.5244892], [5.526078131195592, 1.4189447956480998], [6.793575819668119, 8.355941620556992, 8.686796553791694, 6.4627208864334165, 1.8932207341235756, 0.3308549332347024]]\n",
      "2021-01-14 18:23:48,747 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 18:23:48,748 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:23:48,753 : INFO : built Dictionary(878 unique tokens: ['\\n', '\\n\\n', '\\n\\n\\n', '\"', '\".']...) from 2 documents (total 10283 corpus positions)\n",
      "2021-01-14 18:24:04,301 : INFO : token count processed\n",
      "2021-01-14 18:24:04,432 : INFO : frequencies processed\n",
      "2021-01-14 18:28:11,878 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:28:11,879 : INFO : entropies processed\n",
      "2021-01-14 18:28:11,880 : INFO : extropies processed\n",
      "2021-01-14 18:28:11,920 : INFO : token count processed\n",
      "2021-01-14 18:28:11,935 : INFO : alphabet_source #128008\n",
      "2021-01-14 18:28:11,950 : INFO : alphabet_target #128008\n",
      "2021-01-14 18:28:11,951 : INFO : vocab #128005\n",
      "2021-01-14 18:28:11,975 : INFO : diff #{'\\n\\n\\n'}\n",
      "2021-01-14 18:36:32,800 : INFO : alphabet #128005\n",
      "2021-01-14 18:40:42,593 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ17.txt', 'test_data/LibEST_semeru_format/test/us1159.c')[[0.9630231582977244, 0.5094183406716253], [0.5102879703044891, 0.48971203], [5.5954038325932896, 1.4197440183475736], [6.793575819668119, 7.14584941974052, 7.58148279756057, 6.357942441848069, 0.787906977892451, 0.43563337782005007]]\n",
      "2021-01-14 18:40:42,611 : INFO : Removed 0 and 80 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 18:40:42,612 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:40:42,624 : INFO : built Dictionary(1416 unique tokens: ['\\n', '\\n\\n', '\\n\\n\\n', '\"', '\".']...) from 2 documents (total 10891 corpus positions)\n",
      "2021-01-14 18:41:32,841 : INFO : token count processed\n",
      "2021-01-14 18:41:32,990 : INFO : frequencies processed\n",
      "2021-01-14 18:45:53,640 : INFO : scalar_distribution processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 18:45:53,642 : INFO : entropies processed\n",
      "2021-01-14 18:45:53,642 : INFO : extropies processed\n",
      "2021-01-14 18:45:53,678 : INFO : token count processed\n",
      "2021-01-14 18:45:53,694 : INFO : alphabet_source #128008\n",
      "2021-01-14 18:45:53,709 : INFO : alphabet_target #128008\n",
      "2021-01-14 18:45:53,710 : INFO : vocab #128005\n",
      "2021-01-14 18:45:53,734 : INFO : diff #{'\\n\\n\\n'}\n",
      "2021-01-14 18:54:19,463 : INFO : alphabet #128005\n",
      "2021-01-14 18:58:28,921 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ17.txt', 'test_data/LibEST_semeru_format/test/us2174.c')[[0.9837201686591849, 0.5041033588300458], [0.48848122358322144, 0.5115188], [5.756497020470084, 1.4216222442048667], [6.793575819668119, 8.413564762277948, 8.727476658665003, 6.479663923281064, 1.933900838996884, 0.3139118963870544]]\n",
      "2021-01-14 18:58:28,940 : INFO : Removed 0 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 18:58:28,941 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:58:28,954 : INFO : built Dictionary(1049 unique tokens: ['\\n', '\\n\\n', '\\n\\n\\n', '\"', '\".']...) from 2 documents (total 11784 corpus positions)\n",
      "2021-01-14 18:58:46,465 : INFO : token count processed\n",
      "2021-01-14 18:58:46,620 : INFO : frequencies processed\n",
      "2021-01-14 19:02:55,442 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:02:55,443 : INFO : entropies processed\n",
      "2021-01-14 19:02:55,445 : INFO : extropies processed\n",
      "2021-01-14 19:02:55,486 : INFO : token count processed\n",
      "2021-01-14 19:02:55,505 : INFO : alphabet_source #128008\n",
      "2021-01-14 19:02:55,520 : INFO : alphabet_target #128009\n",
      "2021-01-14 19:02:55,521 : INFO : vocab #128005\n",
      "2021-01-14 19:02:55,545 : INFO : diff #set()\n",
      "2021-01-14 19:11:13,705 : INFO : alphabet #128005\n",
      "2021-01-14 19:15:23,096 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ17.txt', 'test_data/LibEST_semeru_format/test/us893.c')[[0.9612008578804014, 0.5098916798765659], [0.49207502603530884, 0.507925], [5.742798165603919, 1.4215691294637562], [6.793575819668119, 7.576231765520081, 7.972674162584654, 6.397133422603546, 1.179098342916535, 0.39644239706457274]]\n",
      "2021-01-14 19:15:23,116 : INFO : Removed 0 and 77 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 19:15:23,117 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:15:23,122 : INFO : built Dictionary(862 unique tokens: ['\\n', '\\n\\n', '\\n\\n\\n', '\"', '\".']...) from 2 documents (total 13198 corpus positions)\n",
      "2021-01-14 19:15:39,973 : INFO : token count processed\n",
      "2021-01-14 19:15:40,129 : INFO : frequencies processed\n",
      "2021-01-14 19:19:48,898 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:19:48,899 : INFO : entropies processed\n",
      "2021-01-14 19:19:48,900 : INFO : extropies processed\n",
      "2021-01-14 19:19:48,936 : INFO : token count processed\n",
      "2021-01-14 19:19:48,951 : INFO : alphabet_source #128008\n",
      "2021-01-14 19:19:48,967 : INFO : alphabet_target #128008\n",
      "2021-01-14 19:19:48,968 : INFO : vocab #128005\n",
      "2021-01-14 19:19:48,992 : INFO : diff #{'\\n\\n\\n'}\n",
      "2021-01-14 19:28:08,446 : INFO : alphabet #128005\n",
      "2021-01-14 19:32:18,061 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ17.txt', 'test_data/LibEST_semeru_format/test/us895.c')[[1.072284080743027, 0.4825593215199749], [0.6301409304141998, 0.36985907], [5.385574162297012, 1.416495568922654], [6.793575819668119, 7.386680070401629, 7.621833995836901, 6.558421894232848, 0.8282581761687817, 0.23515392543527192]]\n",
      "2021-01-14 19:32:18,075 : INFO : Removed 1 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 19:32:18,076 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:32:18,082 : INFO : built Dictionary(1316 unique tokens: ['\\n', '\\n\\n', '\"', '\".', \"'\"]...) from 2 documents (total 8595 corpus positions)\n",
      "2021-01-14 19:32:58,864 : INFO : token count processed\n",
      "2021-01-14 19:32:59,013 : INFO : frequencies processed\n",
      "2021-01-14 19:37:08,041 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:37:08,043 : INFO : entropies processed\n",
      "2021-01-14 19:37:08,043 : INFO : extropies processed\n",
      "2021-01-14 19:37:08,079 : INFO : token count processed\n",
      "2021-01-14 19:37:08,101 : INFO : alphabet_source #128007\n",
      "2021-01-14 19:37:08,116 : INFO : alphabet_target #128008\n",
      "2021-01-14 19:37:08,117 : INFO : vocab #128005\n",
      "2021-01-14 19:37:08,141 : INFO : diff #set()\n",
      "2021-01-14 19:45:27,208 : INFO : alphabet #128005\n",
      "2021-01-14 19:49:37,142 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us903.c')[[0.9851949505309607, 0.5037288653855078], [0.5016779601573944, 0.49832204], [5.461687766944176, 1.403433470211006], [7.042123584554741, 8.295908249700595, 8.692586796002171, 6.645445038253165, 1.6504632114474305, 0.39667854630157606]]\n",
      "2021-01-14 19:49:37,156 : INFO : Removed 1 and 12 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 19:49:37,157 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:49:37,162 : INFO : built Dictionary(957 unique tokens: ['\\n', '\\n\\n', '\"', '\".', \"'\"]...) from 2 documents (total 8464 corpus positions)\n",
      "2021-01-14 19:49:53,068 : INFO : token count processed\n",
      "2021-01-14 19:49:53,221 : INFO : frequencies processed\n",
      "2021-01-14 19:54:02,146 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:54:02,147 : INFO : entropies processed\n",
      "2021-01-14 19:54:02,148 : INFO : extropies processed\n",
      "2021-01-14 19:54:02,183 : INFO : token count processed\n",
      "2021-01-14 19:54:02,205 : INFO : alphabet_source #128007\n",
      "2021-01-14 19:54:02,220 : INFO : alphabet_target #128014\n",
      "2021-01-14 19:54:02,221 : INFO : vocab #128005\n",
      "2021-01-14 19:54:02,245 : INFO : diff #set()\n",
      "2021-01-14 20:02:21,252 : INFO : alphabet #128005\n",
      "2021-01-14 20:06:30,727 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us3496.c')[[1.0077890131088287, 0.49806030089367603], [0.5761754214763641, 0.42382458], [5.355059637284342, 1.410746448961275], [7.042123584554741, 7.336067584176797, 7.812167112857124, 6.566024055874412, 0.7700435283023834, 0.47609952868032757]]\n",
      "2021-01-14 20:06:30,752 : INFO : Removed 1 and 15 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 20:06:30,753 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:06:30,759 : INFO : built Dictionary(1095 unique tokens: ['\\n', '\\n\\n', '\"', '\".', \"'\"]...) from 2 documents (total 15918 corpus positions)\n",
      "2021-01-14 20:07:00,686 : INFO : token count processed\n",
      "2021-01-14 20:07:00,818 : INFO : frequencies processed\n",
      "2021-01-14 20:11:08,544 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:11:08,545 : INFO : entropies processed\n",
      "2021-01-14 20:11:08,546 : INFO : extropies processed\n",
      "2021-01-14 20:11:08,584 : INFO : token count processed\n",
      "2021-01-14 20:11:08,599 : INFO : alphabet_source #128007\n",
      "2021-01-14 20:11:08,614 : INFO : alphabet_target #128015\n",
      "2021-01-14 20:11:08,615 : INFO : vocab #128005\n",
      "2021-01-14 20:11:08,639 : INFO : diff #set()\n",
      "2021-01-14 20:19:27,346 : INFO : alphabet #128005\n",
      "2021-01-14 20:23:36,353 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us899.c')[[0.9819703397327959, 0.5045484182850171], [0.5352509319782257, 0.46474907], [5.5203587773466465, 1.4090596988568016], [7.042123584554741, 7.454116702352194, 7.756889165988931, 6.739351120918005, 0.7147655814341904, 0.30277246363673704]]\n",
      "2021-01-14 20:23:36,371 : INFO : Removed 1 and 33 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 20:23:36,372 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:23:36,377 : INFO : built Dictionary(739 unique tokens: ['\\n', '\\n\\n', '\"', '\".', \"'\"]...) from 2 documents (total 11508 corpus positions)\n",
      "2021-01-14 20:23:49,449 : INFO : token count processed\n",
      "2021-01-14 20:23:49,580 : INFO : frequencies processed\n",
      "2021-01-14 20:27:58,074 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:27:58,075 : INFO : entropies processed\n",
      "2021-01-14 20:27:58,076 : INFO : extropies processed\n",
      "2021-01-14 20:27:58,112 : INFO : token count processed\n",
      "2021-01-14 20:27:58,133 : INFO : alphabet_source #128007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 20:27:58,148 : INFO : alphabet_target #128014\n",
      "2021-01-14 20:27:58,149 : INFO : vocab #128005\n",
      "2021-01-14 20:27:58,178 : INFO : diff #set()\n",
      "2021-01-14 20:36:17,441 : INFO : alphabet #128005\n",
      "2021-01-14 20:40:26,567 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us4020.c')[[1.0239105089297453, 0.4940929925448163], [0.6008836627006531, 0.39911634], [5.302251132207655, 1.407528730555946], [7.042123584554741, 6.856454382698361, 7.311404076155075, 6.587173891098027, 0.26928049160033396, 0.45494969345671343]]\n",
      "2021-01-14 20:40:26,587 : INFO : Removed 1 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 20:40:26,588 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:40:26,599 : INFO : built Dictionary(734 unique tokens: ['\\n', '\\n\\n', '\"', '\".', \"'\"]...) from 2 documents (total 12912 corpus positions)\n",
      "2021-01-14 20:40:37,936 : INFO : token count processed\n",
      "2021-01-14 20:40:38,066 : INFO : frequencies processed\n",
      "2021-01-14 20:44:47,287 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:44:47,288 : INFO : entropies processed\n",
      "2021-01-14 20:44:47,289 : INFO : extropies processed\n",
      "2021-01-14 20:44:47,326 : INFO : token count processed\n",
      "2021-01-14 20:44:47,346 : INFO : alphabet_source #128007\n",
      "2021-01-14 20:44:47,361 : INFO : alphabet_target #128015\n",
      "2021-01-14 20:44:47,362 : INFO : vocab #128005\n",
      "2021-01-14 20:44:47,386 : INFO : diff #set()\n",
      "2021-01-14 20:53:05,903 : INFO : alphabet #128005\n",
      "2021-01-14 20:57:15,485 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us897.c')[[0.9963261203815105, 0.5009201601834944], [0.5563740730285645, 0.44362593], [5.086704214557991, 1.3975293518114806], [7.042123584554741, 6.88886771674332, 7.295420210293454, 6.635571091004607, 0.25329662573871303, 0.4065524935501337]]\n",
      "2021-01-14 20:57:15,499 : INFO : Removed 1 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 20:57:15,500 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:57:15,504 : INFO : built Dictionary(976 unique tokens: ['\\n', '\\n\\n', '\"', '\".', \"'\"]...) from 2 documents (total 8416 corpus positions)\n",
      "2021-01-14 20:57:32,247 : INFO : token count processed\n",
      "2021-01-14 20:57:32,374 : INFO : frequencies processed\n",
      "2021-01-14 21:01:41,032 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:01:41,033 : INFO : entropies processed\n",
      "2021-01-14 21:01:41,034 : INFO : extropies processed\n",
      "2021-01-14 21:01:41,069 : INFO : token count processed\n",
      "2021-01-14 21:01:41,084 : INFO : alphabet_source #128007\n",
      "2021-01-14 21:01:41,099 : INFO : alphabet_target #128008\n",
      "2021-01-14 21:01:41,100 : INFO : vocab #128005\n",
      "2021-01-14 21:01:41,124 : INFO : diff #set()\n",
      "2021-01-14 21:09:57,843 : INFO : alphabet #128005\n",
      "2021-01-14 21:14:07,200 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us1060.c')[[1.014269085574327, 0.4964579991629425], [0.5902795791625977, 0.40972042], [5.444814655245267, 1.406637441013271], [7.042123584554741, 7.08964724033698, 7.5096135651091505, 6.62215725978257, 0.4674899805544097, 0.41996632477217055]]\n",
      "2021-01-14 21:14:07,222 : INFO : Removed 1 and 76 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 21:14:07,223 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:14:07,229 : INFO : built Dictionary(900 unique tokens: ['\\n', '\\n\\n', '\"', '\".', \"'\"]...) from 2 documents (total 14624 corpus positions)\n",
      "2021-01-14 21:14:20,743 : INFO : token count processed\n",
      "2021-01-14 21:14:20,873 : INFO : frequencies processed\n",
      "2021-01-14 21:18:31,754 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:18:31,755 : INFO : entropies processed\n",
      "2021-01-14 21:18:31,756 : INFO : extropies processed\n",
      "2021-01-14 21:18:31,798 : INFO : token count processed\n",
      "2021-01-14 21:18:31,818 : INFO : alphabet_source #128007\n",
      "2021-01-14 21:18:31,833 : INFO : alphabet_target #128008\n",
      "2021-01-14 21:18:31,834 : INFO : vocab #128005\n",
      "2021-01-14 21:18:31,858 : INFO : diff #set()\n",
      "2021-01-14 21:26:49,953 : INFO : alphabet #128005\n",
      "2021-01-14 21:30:58,671 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us900.c')[[1.0988219729229656, 0.4764577524444965], [0.6888113617897034, 0.31118864], [5.362612421947037, 1.4103570946124526], [7.042123584554741, 7.345749245906533, 7.619455860953474, 6.768416969507799, 0.5773322763987334, 0.27370661504694116]]\n",
      "2021-01-14 21:30:58,680 : INFO : Removed 1 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 21:30:58,681 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:30:58,685 : INFO : built Dictionary(740 unique tokens: ['\\n', '\\n\\n', '\"', '\".', \"'\"]...) from 2 documents (total 4943 corpus positions)\n",
      "2021-01-14 21:31:06,936 : INFO : token count processed\n",
      "2021-01-14 21:31:07,065 : INFO : frequencies processed\n",
      "2021-01-14 21:35:16,138 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:35:16,139 : INFO : entropies processed\n",
      "2021-01-14 21:35:16,139 : INFO : extropies processed\n",
      "2021-01-14 21:35:16,173 : INFO : token count processed\n",
      "2021-01-14 21:35:16,196 : INFO : alphabet_source #128007\n",
      "2021-01-14 21:35:16,211 : INFO : alphabet_target #128009\n",
      "2021-01-14 21:35:16,212 : INFO : vocab #128005\n",
      "2021-01-14 21:35:16,237 : INFO : diff #set()\n",
      "2021-01-14 21:43:36,780 : INFO : alphabet #128005\n",
      "2021-01-14 21:47:46,797 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us896.c')[[1.0422991924952196, 0.48964422239144606], [0.5966615080833435, 0.4033385], [5.385880795372274, 1.415028181191125], [7.042123584554741, 7.557266299124576, 8.055743050357716, 6.543646833321601, 1.0136194658029751, 0.49847675123313984]]\n",
      "2021-01-14 21:47:46,818 : INFO : Removed 1 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 21:47:46,818 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:47:46,824 : INFO : built Dictionary(1048 unique tokens: ['\\n', '\\n\\n', '\"', '\".', \"'\"]...) from 2 documents (total 12976 corpus positions)\n",
      "2021-01-14 21:48:05,928 : INFO : token count processed\n",
      "2021-01-14 21:48:06,085 : INFO : frequencies processed\n",
      "2021-01-14 21:52:15,624 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:52:15,625 : INFO : entropies processed\n",
      "2021-01-14 21:52:15,625 : INFO : extropies processed\n",
      "2021-01-14 21:52:15,662 : INFO : token count processed\n",
      "2021-01-14 21:52:15,683 : INFO : alphabet_source #128007\n",
      "2021-01-14 21:52:15,698 : INFO : alphabet_target #128009\n",
      "2021-01-14 21:52:15,699 : INFO : vocab #128005\n",
      "2021-01-14 21:52:15,723 : INFO : diff #set()\n",
      "2021-01-14 22:00:40,610 : INFO : alphabet #128005\n",
      "2021-01-14 22:04:48,912 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us894.c')[[0.989325686638927, 0.5026828973839642], [0.5584036111831665, 0.4415964], [5.56227288543644, 1.4090683792306344], [7.042123584554741, 7.35286280835418, 7.788698206627535, 6.606288186281386, 0.746574622072794, 0.4358353982733547]]\n",
      "2021-01-14 22:04:48,923 : INFO : Removed 1 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 22:04:48,925 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:04:48,933 : INFO : built Dictionary(715 unique tokens: ['\\n', '\\n\\n', '\"', '\".', \"'\"]...) from 2 documents (total 6801 corpus positions)\n",
      "2021-01-14 22:04:56,613 : INFO : token count processed\n",
      "2021-01-14 22:04:56,785 : INFO : frequencies processed\n",
      "2021-01-14 22:09:04,848 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:09:04,849 : INFO : entropies processed\n",
      "2021-01-14 22:09:04,850 : INFO : extropies processed\n",
      "2021-01-14 22:09:04,882 : INFO : token count processed\n",
      "2021-01-14 22:09:04,903 : INFO : alphabet_source #128007\n",
      "2021-01-14 22:09:04,917 : INFO : alphabet_target #128008\n",
      "2021-01-14 22:09:04,918 : INFO : vocab #128005\n",
      "2021-01-14 22:09:04,942 : INFO : diff #set()\n",
      "2021-01-14 22:17:24,343 : INFO : alphabet #128005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 22:21:33,629 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us1005.c')[[0.9591795698570794, 0.510417735763215], [0.5015999972820282, 0.4984], [5.08130338057509, 1.3934087062938274], [7.042123584554741, 7.213303475711673, 7.79764241687267, 6.457784643393743, 0.7555188323179296, 0.5843389411609969]]\n",
      "2021-01-14 22:21:33,648 : INFO : Removed 1 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 22:21:33,649 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:21:33,654 : INFO : built Dictionary(852 unique tokens: ['\\n', '\\n\\n', '\"', '\".', \"'\"]...) from 2 documents (total 12332 corpus positions)\n",
      "2021-01-14 22:21:51,331 : INFO : token count processed\n",
      "2021-01-14 22:21:51,460 : INFO : frequencies processed\n",
      "2021-01-14 22:26:01,149 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:26:01,150 : INFO : entropies processed\n",
      "2021-01-14 22:26:01,151 : INFO : extropies processed\n",
      "2021-01-14 22:26:01,187 : INFO : token count processed\n",
      "2021-01-14 22:26:01,209 : INFO : alphabet_source #128007\n",
      "2021-01-14 22:26:01,224 : INFO : alphabet_target #128014\n",
      "2021-01-14 22:26:01,225 : INFO : vocab #128005\n",
      "2021-01-14 22:26:01,249 : INFO : diff #set()\n",
      "2021-01-14 22:34:26,238 : INFO : alphabet #128005\n",
      "2021-01-14 22:38:35,410 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us898.c')[[0.9890873638829601, 0.5027431263993696], [0.5659234821796417, 0.43407652], [5.294813013852129, 1.4026097850091857], [7.042123584554741, 6.9849761902830245, 7.390318546094354, 6.636781228743411, 0.3481949615396136, 0.4053423558113298]]\n",
      "2021-01-14 22:38:35,429 : INFO : Removed 1 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 22:38:35,430 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:38:35,436 : INFO : built Dictionary(1271 unique tokens: ['\\n', '\\n\\n', '\"', '\".', \"'\"]...) from 2 documents (total 11557 corpus positions)\n",
      "2021-01-14 22:39:10,026 : INFO : token count processed\n",
      "2021-01-14 22:39:10,157 : INFO : frequencies processed\n",
      "2021-01-14 22:43:18,040 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:43:18,041 : INFO : entropies processed\n",
      "2021-01-14 22:43:18,042 : INFO : extropies processed\n",
      "2021-01-14 22:43:18,078 : INFO : token count processed\n",
      "2021-01-14 22:43:18,093 : INFO : alphabet_source #128007\n",
      "2021-01-14 22:43:18,108 : INFO : alphabet_target #128008\n",
      "2021-01-14 22:43:18,109 : INFO : vocab #128005\n",
      "2021-01-14 22:43:18,133 : INFO : diff #set()\n",
      "2021-01-14 22:51:33,160 : INFO : alphabet #128005\n",
      "2021-01-14 22:55:40,699 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us3512.c')[[0.9868392362418594, 0.5033119850660475], [0.5472579896450043, 0.452742], [5.468358267985782, 1.406601935825291], [7.042123584554741, 7.822176838778024, 8.21310430476466, 6.651196118568105, 1.1709807202099185, 0.3909274659866355]]\n",
      "2021-01-14 22:55:40,719 : INFO : Removed 1 and 67 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 22:55:40,720 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:55:40,727 : INFO : built Dictionary(1528 unique tokens: ['\\n', '\\n\\n', '\"', '\".', \"'\"]...) from 2 documents (total 12524 corpus positions)\n",
      "2021-01-14 22:56:36,604 : INFO : token count processed\n",
      "2021-01-14 22:56:36,735 : INFO : frequencies processed\n",
      "2021-01-14 23:00:44,320 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:00:44,321 : INFO : entropies processed\n",
      "2021-01-14 23:00:44,322 : INFO : extropies processed\n",
      "2021-01-14 23:00:44,358 : INFO : token count processed\n",
      "2021-01-14 23:00:44,379 : INFO : alphabet_source #128007\n",
      "2021-01-14 23:00:44,394 : INFO : alphabet_target #128015\n",
      "2021-01-14 23:00:44,395 : INFO : vocab #128005\n",
      "2021-01-14 23:00:44,419 : INFO : diff #set()\n",
      "2021-01-14 23:09:01,491 : INFO : alphabet #128005\n",
      "2021-01-14 23:13:10,243 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us1883.c')[[0.9754029730412251, 0.5062258251340248], [0.47789788246154785, 0.5221021], [5.625178026694333, 1.409901575885705], [7.042123584554741, 8.359479410422157, 8.689151737743966, 6.712451257232932, 1.6470281531892255, 0.3296723273218092]]\n",
      "2021-01-14 23:13:10,258 : INFO : Removed 1 and 45 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 23:13:10,259 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:13:10,264 : INFO : built Dictionary(1300 unique tokens: ['\\n', '\\n\\n', '\"', '\".', \"'\"]...) from 2 documents (total 8439 corpus positions)\n",
      "2021-01-14 23:13:53,783 : INFO : token count processed\n",
      "2021-01-14 23:13:53,915 : INFO : frequencies processed\n",
      "2021-01-14 23:18:01,646 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:18:01,647 : INFO : entropies processed\n",
      "2021-01-14 23:18:01,649 : INFO : extropies processed\n",
      "2021-01-14 23:18:01,689 : INFO : token count processed\n",
      "2021-01-14 23:18:01,704 : INFO : alphabet_source #128007\n",
      "2021-01-14 23:18:01,719 : INFO : alphabet_target #128008\n",
      "2021-01-14 23:18:01,720 : INFO : vocab #128005\n",
      "2021-01-14 23:18:01,744 : INFO : diff #set()\n",
      "2021-01-14 23:26:25,704 : INFO : alphabet #128005\n",
      "2021-01-14 23:30:34,408 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us748.c')[[0.9908225508892301, 0.5023049390079167], [0.5122222304344177, 0.48777777], [5.460765943474564, 1.4019504096078421], [7.042123584554741, 8.317341209722748, 8.706537217934303, 6.6529275763431865, 1.6644136333795618, 0.3891960082115542]]\n",
      "2021-01-14 23:30:34,418 : INFO : Removed 1 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 23:30:34,419 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:30:34,422 : INFO : built Dictionary(717 unique tokens: ['\\n', '\\n\\n', '\"', '\".', \"'\"]...) from 2 documents (total 5672 corpus positions)\n",
      "2021-01-14 23:30:44,469 : INFO : token count processed\n",
      "2021-01-14 23:30:44,593 : INFO : frequencies processed\n",
      "2021-01-14 23:34:52,225 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:34:52,226 : INFO : entropies processed\n",
      "2021-01-14 23:34:52,227 : INFO : extropies processed\n",
      "2021-01-14 23:34:52,259 : INFO : token count processed\n",
      "2021-01-14 23:34:52,280 : INFO : alphabet_source #128007\n",
      "2021-01-14 23:34:52,295 : INFO : alphabet_target #128010\n",
      "2021-01-14 23:34:52,296 : INFO : vocab #128005\n",
      "2021-01-14 23:34:52,319 : INFO : diff #set()\n",
      "2021-01-14 23:43:09,899 : INFO : alphabet #128005\n",
      "2021-01-14 23:47:18,658 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us3612.c')[[0.9819187202080831, 0.5045615593635491], [0.5635806620121002, 0.43641934], [5.415272433641595, 1.4134663163403216], [7.042123584554741, 7.124835351141783, 7.721820956980887, 6.445137978715637, 0.6796973724261459, 0.5969856058391034]]\n",
      "2021-01-14 23:47:18,674 : INFO : Removed 1 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 23:47:18,675 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:47:18,680 : INFO : built Dictionary(985 unique tokens: ['\\n', '\\n\\n', '\"', '\".', \"'\"]...) from 2 documents (total 9856 corpus positions)\n",
      "2021-01-14 23:47:43,074 : INFO : token count processed\n",
      "2021-01-14 23:47:43,206 : INFO : frequencies processed\n",
      "2021-01-14 23:51:51,812 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:51:51,813 : INFO : entropies processed\n",
      "2021-01-14 23:51:51,814 : INFO : extropies processed\n",
      "2021-01-14 23:51:51,850 : INFO : token count processed\n",
      "2021-01-14 23:51:51,866 : INFO : alphabet_source #128007\n",
      "2021-01-14 23:51:51,881 : INFO : alphabet_target #128008\n",
      "2021-01-14 23:51:51,882 : INFO : vocab #128005\n",
      "2021-01-14 23:51:51,906 : INFO : diff #set()\n",
      "2021-01-15 00:00:09,691 : INFO : alphabet #128005\n",
      "2021-01-15 00:04:18,813 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us901.c')[[0.9732594101294391, 0.5067757411248851], [0.52211132645607, 0.47788867], [5.472230070490738, 1.404681234499236], [7.042123584554741, 7.508450572503415, 7.984371855884586, 6.56620230117357, 0.942248271329845, 0.47592128338117057]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 00:04:18,823 : INFO : Removed 1 and 31 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 00:04:18,824 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:04:18,828 : INFO : built Dictionary(1141 unique tokens: ['\\n', '\\n\\n', '\"', '\".', \"'\"]...) from 2 documents (total 5241 corpus positions)\n",
      "2021-01-15 00:04:46,995 : INFO : token count processed\n",
      "2021-01-15 00:04:47,128 : INFO : frequencies processed\n",
      "2021-01-15 00:08:55,020 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:08:55,021 : INFO : entropies processed\n",
      "2021-01-15 00:08:55,022 : INFO : extropies processed\n",
      "2021-01-15 00:08:55,060 : INFO : token count processed\n",
      "2021-01-15 00:08:55,075 : INFO : alphabet_source #128007\n",
      "2021-01-15 00:08:55,090 : INFO : alphabet_target #128008\n",
      "2021-01-15 00:08:55,091 : INFO : vocab #128005\n",
      "2021-01-15 00:08:55,115 : INFO : diff #set()\n",
      "2021-01-15 00:17:44,222 : INFO : alphabet #128005\n",
      "2021-01-15 00:21:53,775 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us1864.c')[[1.017884030168357, 0.4955686179431072], [0.5434313714504242, 0.45656863], [5.612073605354201, 1.4173579974804495], [7.042123584554741, 8.355941620556992, 8.732868915772464, 6.665196289339269, 1.6907453312177232, 0.37692729521547186]]\n",
      "2021-01-15 00:21:53,792 : INFO : Removed 1 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 00:21:53,793 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:21:53,804 : INFO : built Dictionary(898 unique tokens: ['\\n', '\\n\\n', '\"', '\".', \"'\"]...) from 2 documents (total 10449 corpus positions)\n",
      "2021-01-15 00:22:14,757 : INFO : token count processed\n",
      "2021-01-15 00:22:14,906 : INFO : frequencies processed\n",
      "2021-01-15 00:26:21,944 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:26:21,945 : INFO : entropies processed\n",
      "2021-01-15 00:26:21,946 : INFO : extropies processed\n",
      "2021-01-15 00:26:21,987 : INFO : token count processed\n",
      "2021-01-15 00:26:22,008 : INFO : alphabet_source #128007\n",
      "2021-01-15 00:26:22,023 : INFO : alphabet_target #128008\n",
      "2021-01-15 00:26:22,024 : INFO : vocab #128005\n",
      "2021-01-15 00:26:22,048 : INFO : diff #set()\n",
      "2021-01-15 00:34:39,899 : INFO : alphabet #128005\n",
      "2021-01-15 00:38:51,235 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us1159.c')[[0.9881504213938521, 0.5029800508247863], [0.5544028580188751, 0.44559714], [5.347421318575366, 1.4020319889463162], [7.042123584554741, 7.14584941974052, 7.618667315060091, 6.569305689235169, 0.5765437305053505, 0.4728178953195714]]\n",
      "2021-01-15 00:38:51,253 : INFO : Removed 1 and 80 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 00:38:51,255 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:38:51,261 : INFO : built Dictionary(1444 unique tokens: ['\\n', '\\n\\n', '\"', '\".', \"'\"]...) from 2 documents (total 11057 corpus positions)\n",
      "2021-01-15 00:39:47,231 : INFO : token count processed\n",
      "2021-01-15 00:39:47,356 : INFO : frequencies processed\n",
      "2021-01-15 00:43:54,861 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:43:54,862 : INFO : entropies processed\n",
      "2021-01-15 00:43:54,863 : INFO : extropies processed\n",
      "2021-01-15 00:43:54,899 : INFO : token count processed\n",
      "2021-01-15 00:43:54,914 : INFO : alphabet_source #128007\n",
      "2021-01-15 00:43:54,929 : INFO : alphabet_target #128008\n",
      "2021-01-15 00:43:54,930 : INFO : vocab #128005\n",
      "2021-01-15 00:43:54,954 : INFO : diff #set()\n",
      "2021-01-15 00:52:11,483 : INFO : alphabet #128005\n",
      "2021-01-15 00:56:20,653 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us2174.c')[[1.0177705786941165, 0.4955964818592961], [0.5444092154502869, 0.45559078], [5.550629996494589, 1.4065031247036044], [7.042123584554741, 8.413564762277948, 8.752539510479675, 6.703148836353014, 1.7104159259249343, 0.3389747482017267]]\n",
      "2021-01-15 00:56:20,673 : INFO : Removed 1 and 29 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 00:56:20,674 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:56:20,686 : INFO : built Dictionary(1078 unique tokens: ['\\n', '\\n\\n', '\"', '\".', \"'\"]...) from 2 documents (total 11950 corpus positions)\n",
      "2021-01-15 00:56:42,475 : INFO : token count processed\n",
      "2021-01-15 00:56:42,606 : INFO : frequencies processed\n",
      "2021-01-15 01:00:51,357 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:00:51,358 : INFO : entropies processed\n",
      "2021-01-15 01:00:51,359 : INFO : extropies processed\n",
      "2021-01-15 01:00:51,396 : INFO : token count processed\n",
      "2021-01-15 01:00:51,411 : INFO : alphabet_source #128007\n",
      "2021-01-15 01:00:51,426 : INFO : alphabet_target #128009\n",
      "2021-01-15 01:00:51,427 : INFO : vocab #128005\n",
      "2021-01-15 01:00:51,451 : INFO : diff #set()\n",
      "2021-01-15 01:09:05,246 : INFO : alphabet #128005\n",
      "2021-01-15 01:13:12,793 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us893.c')[[0.9903842321139658, 0.5024155556828898], [0.543521910905838, 0.4564781], [5.4470848551998925, 1.405965914891513], [7.042123584554741, 7.576231765520081, 8.006133070810394, 6.612222279264428, 0.9640094862556534, 0.4299013052903131]]\n",
      "2021-01-15 01:13:12,814 : INFO : Removed 1 and 77 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:13:12,815 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:13:12,820 : INFO : built Dictionary(891 unique tokens: ['\\n', '\\n\\n', '\"', '\".', \"'\"]...) from 2 documents (total 13364 corpus positions)\n",
      "2021-01-15 01:13:25,889 : INFO : token count processed\n",
      "2021-01-15 01:13:26,017 : INFO : frequencies processed\n",
      "2021-01-15 01:17:34,990 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:17:34,991 : INFO : entropies processed\n",
      "2021-01-15 01:17:34,992 : INFO : extropies processed\n",
      "2021-01-15 01:17:35,029 : INFO : token count processed\n",
      "2021-01-15 01:17:35,051 : INFO : alphabet_source #128007\n",
      "2021-01-15 01:17:35,066 : INFO : alphabet_target #128008\n",
      "2021-01-15 01:17:35,067 : INFO : vocab #128005\n",
      "2021-01-15 01:17:35,093 : INFO : diff #set()\n",
      "2021-01-15 01:25:49,627 : INFO : alphabet #128005\n",
      "2021-01-15 01:29:57,402 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ46.txt', 'test_data/LibEST_semeru_format/test/us895.c')[[1.1011845730232441, 0.47592201696073355], [0.6938909590244293, 0.30610904], [5.435813890786668, 1.4153350979202262], [7.042123584554741, 7.386680070401629, 7.674807401249664, 6.753996253706704, 0.6326838166949234, 0.28812733084803543]]\n",
      "2021-01-15 01:29:57,417 : INFO : Removed 3 and 43 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:29:57,418 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:29:57,423 : INFO : built Dictionary(1274 unique tokens: ['\\n', '\\n\\n', '\"', '\")', '\".']...) from 2 documents (total 8137 corpus positions)\n",
      "2021-01-15 01:30:20,841 : INFO : token count processed\n",
      "2021-01-15 01:30:20,991 : INFO : frequencies processed\n",
      "2021-01-15 01:34:28,013 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:34:28,014 : INFO : entropies processed\n",
      "2021-01-15 01:34:28,015 : INFO : extropies processed\n",
      "2021-01-15 01:34:28,051 : INFO : token count processed\n",
      "2021-01-15 01:34:28,070 : INFO : alphabet_source #128007\n",
      "2021-01-15 01:34:28,085 : INFO : alphabet_target #128008\n",
      "2021-01-15 01:34:28,086 : INFO : vocab #128005\n",
      "2021-01-15 01:34:28,110 : INFO : diff #set()\n",
      "2021-01-15 01:42:42,669 : INFO : alphabet #128005\n",
      "2021-01-15 01:46:50,910 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ18.txt', 'test_data/LibEST_semeru_format/test/us903.c')[[1.0256396476535874, 0.49367122190679685], [0.5332062840461731, 0.46679372], [5.497750114815891, 1.4183002616359424], [6.808240665536412, 8.295908249700595, 8.630599190999755, 6.4735497242372535, 1.8223585254633425, 0.33469094129915966]]\n",
      "2021-01-15 01:46:50,924 : INFO : Removed 3 and 12 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:46:50,925 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 01:46:50,929 : INFO : built Dictionary(918 unique tokens: ['\\n', '\\n\\n', '\"', '\")', '\".']...) from 2 documents (total 8006 corpus positions)\n",
      "2021-01-15 01:47:02,195 : INFO : token count processed\n",
      "2021-01-15 01:47:02,326 : INFO : frequencies processed\n",
      "2021-01-15 01:51:10,624 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:51:10,625 : INFO : entropies processed\n",
      "2021-01-15 01:51:10,626 : INFO : extropies processed\n",
      "2021-01-15 01:51:10,665 : INFO : token count processed\n",
      "2021-01-15 01:51:10,681 : INFO : alphabet_source #128007\n",
      "2021-01-15 01:51:10,696 : INFO : alphabet_target #128014\n",
      "2021-01-15 01:51:10,697 : INFO : vocab #128005\n",
      "2021-01-15 01:51:10,721 : INFO : diff #set()\n",
      "2021-01-15 01:59:28,101 : INFO : alphabet #128005\n",
      "2021-01-15 02:03:37,321 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ18.txt', 'test_data/LibEST_semeru_format/test/us3496.c')[[1.0265195056471956, 0.4934568836931263], [0.6033020615577698, 0.39669794], [5.02095313758977, 1.4070716021195742], [6.808240665536412, 7.336067584176797, 7.690385800416793, 6.453922449296416, 0.882145134880381, 0.3543182162399967]]\n",
      "2021-01-15 02:03:37,345 : INFO : Removed 3 and 15 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 02:03:37,346 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:03:37,352 : INFO : built Dictionary(1052 unique tokens: ['\\n', '\\n\\n', '\"', '\")', '\".']...) from 2 documents (total 15460 corpus positions)\n",
      "2021-01-15 02:03:59,605 : INFO : token count processed\n",
      "2021-01-15 02:03:59,734 : INFO : frequencies processed\n",
      "2021-01-15 02:08:07,550 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:08:07,551 : INFO : entropies processed\n",
      "2021-01-15 02:08:07,552 : INFO : extropies processed\n",
      "2021-01-15 02:08:07,594 : INFO : token count processed\n",
      "2021-01-15 02:08:07,609 : INFO : alphabet_source #128007\n",
      "2021-01-15 02:08:07,624 : INFO : alphabet_target #128015\n",
      "2021-01-15 02:08:07,625 : INFO : vocab #128005\n",
      "2021-01-15 02:08:07,650 : INFO : diff #set()\n",
      "2021-01-15 02:16:22,680 : INFO : alphabet #128005\n",
      "2021-01-15 02:20:30,905 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ18.txt', 'test_data/LibEST_semeru_format/test/us899.c')[[1.0163852676173357, 0.4959369700125072], [0.5666414201259613, 0.43335858], [5.461556769931053, 1.4166400271529], [6.808240665536412, 7.454116702352194, 7.6815506156335625, 6.580806752255044, 0.8733099500971502, 0.2274339132813683]]\n",
      "2021-01-15 02:20:30,923 : INFO : Removed 3 and 33 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 02:20:30,924 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:20:30,934 : INFO : built Dictionary(698 unique tokens: ['\\n', '\\n\\n', '\"', '\")', '\".']...) from 2 documents (total 11050 corpus positions)\n",
      "2021-01-15 02:20:37,936 : INFO : token count processed\n",
      "2021-01-15 02:20:38,064 : INFO : frequencies processed\n",
      "2021-01-15 02:24:45,969 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:24:45,970 : INFO : entropies processed\n",
      "2021-01-15 02:24:45,971 : INFO : extropies processed\n",
      "2021-01-15 02:24:46,007 : INFO : token count processed\n",
      "2021-01-15 02:24:46,022 : INFO : alphabet_source #128007\n",
      "2021-01-15 02:24:46,037 : INFO : alphabet_target #128014\n",
      "2021-01-15 02:24:46,038 : INFO : vocab #128005\n",
      "2021-01-15 02:24:46,062 : INFO : diff #set()\n",
      "2021-01-15 02:33:03,209 : INFO : alphabet #128005\n",
      "2021-01-15 02:37:10,280 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ18.txt', 'test_data/LibEST_semeru_format/test/us4020.c')[[1.0330623785690254, 0.4918688233775945], [0.6127859652042389, 0.38721403], [5.042428575844076, 1.4083250998984034], [6.808240665536412, 6.856454382698361, 7.182612635868077, 6.482082412366696, 0.3743719703316648, 0.3261582531697158]]\n",
      "2021-01-15 02:37:10,299 : INFO : Removed 3 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 02:37:10,300 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:37:10,311 : INFO : built Dictionary(691 unique tokens: ['\\n', '\\n\\n', '\"', '\")', '\".']...) from 2 documents (total 12454 corpus positions)\n",
      "2021-01-15 02:37:17,369 : INFO : token count processed\n",
      "2021-01-15 02:37:17,499 : INFO : frequencies processed\n",
      "2021-01-15 02:41:25,160 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:41:25,161 : INFO : entropies processed\n",
      "2021-01-15 02:41:25,161 : INFO : extropies processed\n",
      "2021-01-15 02:41:25,198 : INFO : token count processed\n",
      "2021-01-15 02:41:25,219 : INFO : alphabet_source #128007\n",
      "2021-01-15 02:41:25,234 : INFO : alphabet_target #128015\n",
      "2021-01-15 02:41:25,235 : INFO : vocab #128005\n",
      "2021-01-15 02:41:25,259 : INFO : diff #set()\n",
      "2021-01-15 02:49:43,357 : INFO : alphabet #128005\n",
      "2021-01-15 02:53:52,347 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ18.txt', 'test_data/LibEST_semeru_format/test/us897.c')[[1.0081436717362178, 0.49797233837129373], [0.5827970504760742, 0.41720295], [5.035045110872062, 1.4084837842247913], [6.808240665536412, 6.88886771674332, 7.1816081262905715, 6.515500255989162, 0.3733674607541593, 0.29274040954725145]]\n",
      "2021-01-15 02:53:52,360 : INFO : Removed 3 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 02:53:52,361 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:53:52,366 : INFO : built Dictionary(934 unique tokens: ['\\n', '\\n\\n', '\"', '\")', '\".']...) from 2 documents (total 7958 corpus positions)\n",
      "2021-01-15 02:54:03,121 : INFO : token count processed\n",
      "2021-01-15 02:54:03,266 : INFO : frequencies processed\n",
      "2021-01-15 02:58:11,482 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:58:11,483 : INFO : entropies processed\n",
      "2021-01-15 02:58:11,484 : INFO : extropies processed\n",
      "2021-01-15 02:58:11,518 : INFO : token count processed\n",
      "2021-01-15 02:58:11,541 : INFO : alphabet_source #128007\n",
      "2021-01-15 02:58:11,556 : INFO : alphabet_target #128008\n",
      "2021-01-15 02:58:11,557 : INFO : vocab #128005\n",
      "2021-01-15 02:58:11,581 : INFO : diff #set()\n",
      "2021-01-15 03:06:29,202 : INFO : alphabet #128005\n",
      "2021-01-15 03:10:37,605 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ18.txt', 'test_data/LibEST_semeru_format/test/us1060.c')[[1.0358942093112178, 0.49118465754579616], [0.6021830141544342, 0.397817], [5.37823181046479, 1.4158525001922744], [6.808240665536412, 7.08964724033698, 7.366485167225081, 6.531402738648312, 0.5582445016886686, 0.27683792688810094]]\n",
      "2021-01-15 03:10:37,627 : INFO : Removed 3 and 76 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 03:10:37,628 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:10:37,640 : INFO : built Dictionary(862 unique tokens: ['\\n', '\\n\\n', '\"', '\")', '\".']...) from 2 documents (total 14166 corpus positions)\n",
      "2021-01-15 03:10:52,521 : INFO : token count processed\n",
      "2021-01-15 03:10:52,671 : INFO : frequencies processed\n",
      "2021-01-15 03:15:00,634 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:15:00,636 : INFO : entropies processed\n",
      "2021-01-15 03:15:00,637 : INFO : extropies processed\n",
      "2021-01-15 03:15:00,677 : INFO : token count processed\n",
      "2021-01-15 03:15:00,692 : INFO : alphabet_source #128007\n",
      "2021-01-15 03:15:00,706 : INFO : alphabet_target #128008\n",
      "2021-01-15 03:15:00,707 : INFO : vocab #128005\n",
      "2021-01-15 03:15:00,730 : INFO : diff #set()\n",
      "2021-01-15 03:25:41,659 : INFO : alphabet #128005\n",
      "2021-01-15 03:31:00,413 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ18.txt', 'test_data/LibEST_semeru_format/test/us900.c')[[1.1287495381398889, 0.469759350305628], [0.7150512933731079, 0.2849487], [4.8567714328342655, 1.4027184600330744], [6.808240665536412, 7.345749245906533, 7.518157721242322, 6.635832190200623, 0.70991705570591, 0.1724084753357893]]\n",
      "2021-01-15 03:31:00,422 : INFO : Removed 3 and 21 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 03:31:00,423 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:31:00,426 : INFO : built Dictionary(699 unique tokens: ['\\n', '\\n\\n', '\"', '\")', '\".']...) from 2 documents (total 4485 corpus positions)\n",
      "2021-01-15 03:31:07,294 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 03:31:07,439 : INFO : frequencies processed\n",
      "2021-01-15 03:36:27,071 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:36:27,072 : INFO : entropies processed\n",
      "2021-01-15 03:36:27,073 : INFO : extropies processed\n",
      "2021-01-15 03:36:27,113 : INFO : token count processed\n",
      "2021-01-15 03:36:27,129 : INFO : alphabet_source #128007\n",
      "2021-01-15 03:36:27,145 : INFO : alphabet_target #128009\n",
      "2021-01-15 03:36:27,146 : INFO : vocab #128005\n",
      "2021-01-15 03:36:27,172 : INFO : diff #set()\n",
      "2021-01-15 03:47:04,835 : INFO : alphabet #128005\n",
      "2021-01-15 03:52:23,281 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ18.txt', 'test_data/LibEST_semeru_format/test/us896.c')[[1.0749355656448603, 0.48194267646533606], [0.6241728663444519, 0.37582713], [4.830044878754637, 1.4018332057063791], [6.808240665536412, 7.557266299124576, 7.919661890450423, 6.445845074210564, 1.111421224914011, 0.3623955913258472]]\n",
      "2021-01-15 03:52:23,301 : INFO : Removed 3 and 17 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 03:52:23,302 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:52:23,310 : INFO : built Dictionary(1005 unique tokens: ['\\n', '\\n\\n', '\"', '\")', '\".']...) from 2 documents (total 12518 corpus positions)\n",
      "2021-01-15 03:52:37,940 : INFO : token count processed\n",
      "2021-01-15 03:52:38,072 : INFO : frequencies processed\n",
      "2021-01-15 03:57:58,051 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:57:58,052 : INFO : entropies processed\n",
      "2021-01-15 03:57:58,053 : INFO : extropies processed\n",
      "2021-01-15 03:57:58,092 : INFO : token count processed\n",
      "2021-01-15 03:57:58,108 : INFO : alphabet_source #128007\n",
      "2021-01-15 03:57:58,125 : INFO : alphabet_target #128009\n",
      "2021-01-15 03:57:58,126 : INFO : vocab #128005\n",
      "2021-01-15 03:57:58,151 : INFO : diff #set()\n",
      "2021-01-15 04:08:36,080 : INFO : alphabet #128005\n",
      "2021-01-15 04:13:55,947 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ18.txt', 'test_data/LibEST_semeru_format/test/us894.c')[[1.0167505333081037, 0.4958471479165479], [0.5803686082363129, 0.4196314], [5.564084629405143, 1.4194297457302636], [6.808240665536412, 7.35286280835418, 7.69513527894407, 6.465968194946521, 0.8868946134076579, 0.3422724705898901]]\n",
      "2021-01-15 04:13:55,959 : INFO : Removed 3 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 04:13:55,960 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:13:55,966 : INFO : built Dictionary(668 unique tokens: ['\\n', '\\n\\n', '\"', '\")', '\".']...) from 2 documents (total 6343 corpus positions)\n",
      "2021-01-15 04:14:04,681 : INFO : token count processed\n",
      "2021-01-15 04:14:04,866 : INFO : frequencies processed\n",
      "2021-01-15 04:19:24,922 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:19:24,923 : INFO : entropies processed\n",
      "2021-01-15 04:19:24,924 : INFO : extropies processed\n",
      "2021-01-15 04:19:24,961 : INFO : token count processed\n",
      "2021-01-15 04:19:24,977 : INFO : alphabet_source #128007\n",
      "2021-01-15 04:19:24,993 : INFO : alphabet_target #128008\n",
      "2021-01-15 04:19:24,994 : INFO : vocab #128005\n",
      "2021-01-15 04:19:25,019 : INFO : diff #set()\n",
      "2021-01-15 04:30:03,519 : INFO : alphabet #128005\n",
      "2021-01-15 04:35:20,949 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ18.txt', 'test_data/LibEST_semeru_format/test/us1005.c')[[0.9925285835006366, 0.501874858047516], [0.5314827561378479, 0.46851724], [5.128824275973964, 1.4102635326538766], [6.808240665536412, 7.213303475711673, 7.664668894854596, 6.35687524639349, 0.8564282293181833, 0.45136541914292216]]\n",
      "2021-01-15 04:35:20,976 : INFO : Removed 3 and 7 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 04:35:20,977 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:35:20,984 : INFO : built Dictionary(815 unique tokens: ['\\n', '\\n\\n', '\"', '\")', '\".']...) from 2 documents (total 11874 corpus positions)\n",
      "2021-01-15 04:35:36,093 : INFO : token count processed\n",
      "2021-01-15 04:35:36,240 : INFO : frequencies processed\n",
      "2021-01-15 04:40:54,400 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:40:54,401 : INFO : entropies processed\n",
      "2021-01-15 04:40:54,402 : INFO : extropies processed\n",
      "2021-01-15 04:40:54,444 : INFO : token count processed\n",
      "2021-01-15 04:40:54,460 : INFO : alphabet_source #128007\n",
      "2021-01-15 04:40:54,476 : INFO : alphabet_target #128014\n",
      "2021-01-15 04:40:54,477 : INFO : vocab #128005\n",
      "2021-01-15 04:40:54,503 : INFO : diff #set()\n",
      "2021-01-15 04:51:33,926 : INFO : alphabet #128005\n",
      "2021-01-15 04:56:52,515 : INFO : Computed distances or similarities ('test_data/LibEST_semeru_format/requirements/RQ18.txt', 'test_data/LibEST_semeru_format/test/us898.c')[[1.0205817859804138, 0.49490696537917483], [0.5981183052062988, 0.4018817], [5.220201244570422, 1.413197228104655], [6.808240665536412, 6.9849761902830245, 7.283093720991983, 6.510123134827455, 0.4748530554555703, 0.2981175307089581]]\n",
      "2021-01-15 04:56:52,534 : INFO : Removed 3 and 25 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 04:56:52,535 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:56:52,544 : INFO : built Dictionary(1234 unique tokens: ['\\n', '\\n\\n', '\"', '\")', '\".']...) from 2 documents (total 11099 corpus positions)\n",
      "2021-01-15 04:57:16,624 : INFO : token count processed\n",
      "2021-01-15 04:57:16,769 : INFO : frequencies processed\n",
      "2021-01-15 05:02:37,616 : INFO : scalar_distribution processed\n",
      "2021-01-15 05:02:37,618 : INFO : entropies processed\n",
      "2021-01-15 05:02:37,619 : INFO : extropies processed\n",
      "2021-01-15 05:02:37,661 : INFO : token count processed\n",
      "2021-01-15 05:02:37,677 : INFO : alphabet_source #128007\n",
      "2021-01-15 05:02:37,693 : INFO : alphabet_target #128008\n",
      "2021-01-15 05:02:37,694 : INFO : vocab #128005\n",
      "2021-01-15 05:02:37,720 : INFO : diff #set()\n"
     ]
    }
   ],
   "source": [
    "#[step 2]NonGroundTruth Computation\n",
    "metric_list = [DistanceMetric.WMD,DistanceMetric.SCM,EntropyMetric.MSI_I,EntropyMetric.MI]\n",
    "#metric_list = [EntropyMetric.MSI_I,EntropyMetric.MI]\n",
    "word2vec.ComputeDistanceArtifacts( sampling=False, samples = 100, metric_list = metric_list )\n",
    "word2vec.df_nonground_link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_nonground_link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_nonground_link['Target'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 3]Saving Non-GroundTruth Links\n",
    "word2vec.SaveLinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_nonglinks = ds.mining.ir.LoadLinks(timestamp=1610579170.341825, params=parameters, logging=logging)\n",
    "df_nonglinks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_nonground_link = df_nonglinks # Only to load links from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 4]GroundTruthMatching Testing\n",
    "#TODO change the path for a param\n",
    "path_to_ground_truth =  parameters['path_mappings']\n",
    "word2vec.MatchWithGroundTruth(path_to_ground_truth, semeru_format=True)\n",
    "word2vec.df_ground_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_ground_link[word2vec.df_ground_link ['Linked?']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_ground_link['Source'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Only SACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 4.1]GroundTruthMatching Testing For CISCO Mappings\n",
    "word2vec.MatchWithGroundTruth(from_mappings=True)\n",
    "word2vec.df_ground_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[optional]GroundTruth Direct Processing\n",
    "ground_links = word2vec.ground_truth_processing(path_to_ground_truth)\n",
    "ground_links # A tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 5]Saving GroundTruth Links\n",
    "word2vec.SaveLinks(grtruth = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_glinks = ds.mining.ir.LoadLinks(timestamp=1610579318.97542, params=parameters,grtruth = True, logging=logging)\n",
    "df_glinks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_build_docs #<-------- [Activate when stable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
