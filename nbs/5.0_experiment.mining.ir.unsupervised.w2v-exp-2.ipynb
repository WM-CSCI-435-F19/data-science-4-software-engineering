{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp experiment.mining.ir.unsupervised.w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting Neural Unsupervised Approaches for Software Information Retrieval [w2v]\n",
    "\n",
    "> Just Paper. Full Experimentation. This module is dedicated to experiment with word2vec. Consider to Copy the entire notebook for a new and separeted empirical evaluation. \n",
    "> Implementing mutual information analysis\n",
    "> Author: @danaderp April 2020\n",
    "> Author: @danielrc Nov 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This copy is for Cisco purposes. It was adapted to process private github data from cisco. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds4se.mining.ir import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prg import prg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ds4se as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Similarity with BasicSequenceVectorization\n",
    "\n",
    "We test diferent similarities based on [blog](https://www.kdnuggets.com/2017/08/comparing-distance-measurements-python-scipy.html) and [blog2](https://www.kdnuggets.com/2019/01/comparison-text-distance-metrics.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experients Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../dvc-ds4se/' #dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiments 1.1.2 <<-- word2vec\n",
    "path_model_prefix = path_data+'models/bpe/sentencepiece/wiki_py_java_bpe_32k'\n",
    "path_to_trained_model = path_data+'/models/wv/bpe32k/[word2vec-Py-Java-SK-500-20E-32k-1593748814.350487].model'\n",
    "def sacp_params():\n",
    "        return {\n",
    "        \"vectorizationType\": VectorizationType.word2vec,\n",
    "        \"linkType\": LinkType.issue2src,\n",
    "        \"system\": 'sacp-python-common',\n",
    "        \"path_to_trained_model\": path_to_trained_model,\n",
    "        \"source_type\": SoftwareArtifacts.PR.value,\n",
    "        \"target_type\": SoftwareArtifacts.PY.value,\n",
    "        \"system_path_config\": {\n",
    "            \"system_path\": '/tf/data/cisco/sacp_data/[sacp-python-common-all-corpus-1609224778.517111].csv',\n",
    "            \"sep\": '~',\n",
    "            \"names\": ['ids','bpe32k'],\n",
    "            \"prep\": Preprocessing.bpe\n",
    "        },\n",
    "        \"path_mappings\": \"/tf/data/cisco/sacp_data/sacp-pr-mappings.csv\",\n",
    "        \"saving_path\": path_data + 'metrics/traceability/experiments1.1.x/',\n",
    "        \"names\": ['Source','Target','Linked?'],\n",
    "        \"model_prefix\": path_model_prefix\n",
    "\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectorizationType': <VectorizationType.word2vec: 1>,\n",
       " 'linkType': <LinkType.issue2src: 3>,\n",
       " 'system': 'sacp-python-common',\n",
       " 'path_to_trained_model': '../dvc-ds4se//models/wv/bpe32k/[word2vec-Py-Java-SK-500-20E-32k-1593748814.350487].model',\n",
       " 'source_type': 'pr',\n",
       " 'target_type': 'py',\n",
       " 'system_path_config': {'system_path': '/tf/data/cisco/sacp_data/[sacp-python-common-all-corpus-1609224778.517111].csv',\n",
       "  'sep': '~',\n",
       "  'names': ['ids', 'bpe32k'],\n",
       "  'prep': <Preprocessing.bpe: 2>},\n",
       " 'path_mappings': '/tf/data/cisco/sacp_data/sacp-pr-mappings.csv',\n",
       " 'saving_path': '../dvc-ds4se/metrics/traceability/experiments1.1.x/',\n",
       " 'names': ['Source', 'Target', 'Linked?'],\n",
       " 'model_prefix': '../dvc-ds4se/models/bpe/sentencepiece/wiki_py_java_bpe_32k'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = sacp_params()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artifacts Similarity with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 02:39:56,815 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:39:56,888 : INFO : built Dictionary(3010 unique tokens: ['28', '29', '3', '4)', '7']...) from 362 documents (total 171602 corpus positions)\n",
      "2021-01-14 02:39:56,941 : INFO : bpe preprocessing documents, dictionary, and vocab for the test corpus\n",
      "2021-01-14 02:39:56,942 : INFO : loading Word2Vec object from ../dvc-ds4se//models/wv/bpe32k/[word2vec-Py-Java-SK-500-20E-32k-1593748814.350487].model\n",
      "2021-01-14 02:39:57,524 : INFO : loading wv recursively from ../dvc-ds4se//models/wv/bpe32k/[word2vec-Py-Java-SK-500-20E-32k-1593748814.350487].model.wv.* with mmap=None\n",
      "2021-01-14 02:39:57,525 : INFO : setting ignored attribute vectors_norm to None\n",
      "2021-01-14 02:39:57,526 : INFO : loading vocabulary recursively from ../dvc-ds4se//models/wv/bpe32k/[word2vec-Py-Java-SK-500-20E-32k-1593748814.350487].model.vocabulary.* with mmap=None\n",
      "2021-01-14 02:39:57,527 : INFO : loading trainables recursively from ../dvc-ds4se//models/wv/bpe32k/[word2vec-Py-Java-SK-500-20E-32k-1593748814.350487].model.trainables.* with mmap=None\n",
      "2021-01-14 02:39:57,528 : INFO : setting ignored attribute cum_table to None\n",
      "2021-01-14 02:39:57,529 : INFO : loaded ../dvc-ds4se//models/wv/bpe32k/[word2vec-Py-Java-SK-500-20E-32k-1593748814.350487].model\n",
      "2021-01-14 02:39:57,568 : INFO : precomputing L2-norms of word weight vectors\n",
      "2021-01-14 02:39:57,590 : INFO : constructing a sparse term similarity matrix using <gensim.models.keyedvectors.WordEmbeddingSimilarityIndex object at 0x7f1299f72550>\n",
      "2021-01-14 02:39:57,591 : INFO : iterating over columns in dictionary order\n",
      "2021-01-14 02:39:57,594 : INFO : PROGRESS: at 0.03% columns (1 / 3010, 0.033223% density, 0.033223% projected density)\n",
      "2021-01-14 02:40:02,516 : INFO : PROGRESS: at 33.26% columns (1001 / 3010, 0.810101% density, 2.369292% projected density)\n",
      "2021-01-14 02:40:06,405 : INFO : PROGRESS: at 66.48% columns (2001 / 3010, 1.217051% density, 1.813993% projected density)\n",
      "2021-01-14 02:40:10,192 : INFO : PROGRESS: at 99.70% columns (3001 / 3010, 1.483935% density, 1.488286% projected density)\n",
      "2021-01-14 02:40:10,225 : INFO : constructed a sparse term similarity matrix with 1.486452% density\n"
     ]
    }
   ],
   "source": [
    "#[step 1]Creating the Vectorization Class\n",
    "word2vec = ds.mining.ir.Word2VecSeqVect( params = parameters, logging = logging )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 02:40:10,333 : INFO : Removed 1 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:40:10,334 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:40:10,337 : INFO : built Dictionary(279 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 1421 corpus positions)\n",
      "2021-01-14 02:40:10,509 : INFO : token count processed\n",
      "2021-01-14 02:40:10,542 : INFO : frequencies processed\n",
      "2021-01-14 02:40:19,914 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:40:19,915 : INFO : entropies processed\n",
      "2021-01-14 02:40:19,916 : INFO : extropies processed\n",
      "2021-01-14 02:40:19,927 : INFO : token count processed\n",
      "2021-01-14 02:40:19,933 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:40:19,939 : INFO : alphabet_target #32010\n",
      "2021-01-14 02:40:19,940 : INFO : vocab #32006\n",
      "2021-01-14 02:40:19,946 : INFO : diff #set()\n",
      "2021-01-14 02:40:38,809 : INFO : alphabet #32006\n",
      "2021-01-14 02:40:47,978 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.2517016912617411, 0.44410856192928916], [0.9405163638293743, 0.059483636], [2.251629167387823, 1.2667563532600834], [4.518397116698909, 6.905617163738059, 7.074862872578713, 4.349151407858255, 2.5564657558798034, 0.16924570884065382]]\n",
      "2021-01-14 02:40:47,984 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:40:47,985 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:40:47,988 : INFO : built Dictionary(370 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 2311 corpus positions)\n",
      "2021-01-14 02:40:48,235 : INFO : token count processed\n",
      "2021-01-14 02:40:48,263 : INFO : frequencies processed\n",
      "2021-01-14 02:40:57,692 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:40:57,693 : INFO : entropies processed\n",
      "2021-01-14 02:40:57,694 : INFO : extropies processed\n",
      "2021-01-14 02:40:57,707 : INFO : token count processed\n",
      "2021-01-14 02:40:57,711 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:40:57,715 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:40:57,716 : INFO : vocab #32006\n",
      "2021-01-14 02:40:57,722 : INFO : diff #set()\n",
      "2021-01-14 02:41:16,433 : INFO : alphabet #32006\n",
      "2021-01-14 02:41:25,758 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.260697426318442, 0.4423413714538992], [0.9482670277357101, 0.051732972], [2.2516291673878226, 1.2667563532600834], [4.518397116698909, 7.1219284286457345, 7.362819865646176, 4.277505679698469, 2.8444227489472667, 0.24089143700044158]]\n",
      "2021-01-14 02:41:25,764 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:41:25,765 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:41:25,767 : INFO : built Dictionary(288 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 2294 corpus positions)\n",
      "2021-01-14 02:41:25,948 : INFO : token count processed\n",
      "2021-01-14 02:41:25,982 : INFO : frequencies processed\n",
      "2021-01-14 02:41:35,471 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:41:35,472 : INFO : entropies processed\n",
      "2021-01-14 02:41:35,473 : INFO : extropies processed\n",
      "2021-01-14 02:41:35,480 : INFO : token count processed\n",
      "2021-01-14 02:41:35,486 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:41:35,492 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:41:35,493 : INFO : vocab #32006\n",
      "2021-01-14 02:41:35,500 : INFO : diff #set()\n",
      "2021-01-14 02:41:53,994 : INFO : alphabet #32006\n",
      "2021-01-14 02:42:03,157 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2539581464575604, 0.443663961361329], [0.9449014738202095, 0.055098526], [2.5, 1.2968140217166515], [4.518397116698909, 6.41099024988467, 6.507248009703224, 4.422139356880356, 1.9888508930043143, 0.09625775981855345]]\n",
      "2021-01-14 02:42:03,161 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:42:03,162 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:42:03,166 : INFO : built Dictionary(168 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 693 corpus positions)\n",
      "2021-01-14 02:42:03,264 : INFO : token count processed\n",
      "2021-01-14 02:42:03,326 : INFO : frequencies processed\n",
      "2021-01-14 02:42:12,625 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:42:12,626 : INFO : entropies processed\n",
      "2021-01-14 02:42:12,627 : INFO : extropies processed\n",
      "2021-01-14 02:42:12,634 : INFO : token count processed\n",
      "2021-01-14 02:42:12,638 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:42:12,642 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:42:12,643 : INFO : vocab #32006\n",
      "2021-01-14 02:42:12,649 : INFO : diff #set()\n",
      "2021-01-14 02:42:31,193 : INFO : alphabet #32006\n",
      "2021-01-14 02:42:40,488 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.224534716579882, 0.4495322066888006], [0.9330300539731979, 0.066969946], [1.9219280948873623, 1.2148067842293933], [4.518397116698909, 6.077866832717642, 6.2899344817648934, 4.306329467651658, 1.771537365065984, 0.21206764904725173]]\n",
      "2021-01-14 02:42:40,492 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:42:40,492 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:42:40,494 : INFO : built Dictionary(144 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 458 corpus positions)\n",
      "2021-01-14 02:42:40,566 : INFO : token count processed\n",
      "2021-01-14 02:42:40,596 : INFO : frequencies processed\n",
      "2021-01-14 02:42:49,768 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:42:49,769 : INFO : entropies processed\n",
      "2021-01-14 02:42:49,770 : INFO : extropies processed\n",
      "2021-01-14 02:42:49,776 : INFO : token count processed\n",
      "2021-01-14 02:42:49,780 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:42:49,787 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:42:49,788 : INFO : vocab #32006\n",
      "2021-01-14 02:42:49,795 : INFO : diff #set()\n",
      "2021-01-14 02:43:08,362 : INFO : alphabet #32006\n",
      "2021-01-14 02:43:17,821 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.2257713872199287, 0.4492824401202484], [0.9229012802243233, 0.07709872], [2.0, 1.2451124978365313], [4.518397116698909, 5.977547459003844, 6.2072257071540395, 4.288718868548715, 1.6888285904551301, 0.22967824815019533]]\n",
      "2021-01-14 02:43:17,827 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:43:17,828 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:43:17,831 : INFO : built Dictionary(247 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 2169 corpus positions)\n",
      "2021-01-14 02:43:17,986 : INFO : token count processed\n",
      "2021-01-14 02:43:18,042 : INFO : frequencies processed\n",
      "2021-01-14 02:43:27,222 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:43:27,223 : INFO : entropies processed\n",
      "2021-01-14 02:43:27,223 : INFO : extropies processed\n",
      "2021-01-14 02:43:27,230 : INFO : token count processed\n",
      "2021-01-14 02:43:27,234 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:43:27,238 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:43:27,239 : INFO : vocab #32006\n",
      "2021-01-14 02:43:27,245 : INFO : diff #set()\n",
      "2021-01-14 02:43:45,735 : INFO : alphabet #32006\n",
      "2021-01-14 02:43:54,934 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.2429469617829099, 0.4458420181300693], [0.9368654191493988, 0.06313458], [2.4193819456463714, 1.2761517340193214], [4.518397116698909, 6.4614394051846435, 6.5903237400762515, 4.3895127818073005, 2.071926623377342, 0.128884334891608]]\n",
      "2021-01-14 02:43:54,939 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:43:54,940 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:43:54,942 : INFO : built Dictionary(210 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 1393 corpus positions)\n",
      "2021-01-14 02:43:55,059 : INFO : token count processed\n",
      "2021-01-14 02:43:55,090 : INFO : frequencies processed\n",
      "2021-01-14 02:44:04,403 : INFO : scalar_distribution processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 02:44:04,404 : INFO : entropies processed\n",
      "2021-01-14 02:44:04,405 : INFO : extropies processed\n",
      "2021-01-14 02:44:04,412 : INFO : token count processed\n",
      "2021-01-14 02:44:04,416 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:44:04,420 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:44:04,421 : INFO : vocab #32006\n",
      "2021-01-14 02:44:04,427 : INFO : diff #set()\n",
      "2021-01-14 02:44:27,686 : INFO : alphabet #32006\n",
      "2021-01-14 02:44:36,865 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.2624122042042383, 0.4420061022220889], [0.956292163580656, 0.043707836], [2.1280852788913944, 1.2238339714721664], [4.518397116698909, 6.327195724598159, 6.478612943640626, 4.366979897656443, 1.9602158269417167, 0.1514172190424672]]\n",
      "2021-01-14 02:44:36,877 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:44:36,878 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:44:36,880 : INFO : built Dictionary(423 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 6285 corpus positions)\n",
      "2021-01-14 02:44:37,187 : INFO : token count processed\n",
      "2021-01-14 02:44:37,215 : INFO : frequencies processed\n",
      "2021-01-14 02:44:46,375 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:44:46,376 : INFO : entropies processed\n",
      "2021-01-14 02:44:46,377 : INFO : extropies processed\n",
      "2021-01-14 02:44:46,385 : INFO : token count processed\n",
      "2021-01-14 02:44:46,392 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:44:46,396 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:44:46,397 : INFO : vocab #32006\n",
      "2021-01-14 02:44:46,406 : INFO : diff #set()\n",
      "2021-01-14 02:45:05,354 : INFO : alphabet #32006\n",
      "2021-01-14 02:45:14,530 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.225169165141252, 0.4494040343833907], [0.9272142425179482, 0.07278576], [2.8453509366224363, 1.3210203571681218], [4.518397116698909, 6.9079058562486315, 7.0235163370439135, 4.402786635903627, 2.505119220345004, 0.11561048079528202]]\n",
      "2021-01-14 02:45:14,536 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:45:14,537 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:45:14,539 : INFO : built Dictionary(328 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 2679 corpus positions)\n",
      "2021-01-14 02:45:14,748 : INFO : token count processed\n",
      "2021-01-14 02:45:14,776 : INFO : frequencies processed\n",
      "2021-01-14 02:45:24,106 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:45:24,107 : INFO : entropies processed\n",
      "2021-01-14 02:45:24,107 : INFO : extropies processed\n",
      "2021-01-14 02:45:24,114 : INFO : token count processed\n",
      "2021-01-14 02:45:24,119 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:45:24,122 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:45:24,123 : INFO : vocab #32006\n",
      "2021-01-14 02:45:24,128 : INFO : diff #set()\n",
      "2021-01-14 02:45:42,567 : INFO : alphabet #32006\n",
      "2021-01-14 02:45:51,915 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.2534162563904045, 0.4437706514116626], [0.9466579183936119, 0.05334208], [2.2516291673878226, 1.2667563532600834], [4.518397116698909, 6.61034830706307, 6.789460466443839, 4.339284957318141, 2.2710633497449297, 0.17911215938076896]]\n",
      "2021-01-14 02:45:51,919 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:45:51,920 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:45:51,921 : INFO : built Dictionary(217 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 726 corpus positions)\n",
      "2021-01-14 02:45:52,038 : INFO : token count processed\n",
      "2021-01-14 02:45:52,068 : INFO : frequencies processed\n",
      "2021-01-14 02:46:01,549 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:46:01,550 : INFO : entropies processed\n",
      "2021-01-14 02:46:01,550 : INFO : extropies processed\n",
      "2021-01-14 02:46:01,557 : INFO : token count processed\n",
      "2021-01-14 02:46:01,562 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:46:01,566 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:46:01,567 : INFO : vocab #32006\n",
      "2021-01-14 02:46:01,573 : INFO : diff #set()\n",
      "2021-01-14 02:46:20,151 : INFO : alphabet #32006\n",
      "2021-01-14 02:46:29,321 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.263065780362598, 0.4418784503205098], [0.9554195515811443, 0.04458045], [0.9182958340544896, 0.9182958340544896], [4.518397116698909, 6.616715366949855, 6.825126769602441, 4.309985714046324, 2.306729652903532, 0.20841140265258673]]\n",
      "2021-01-14 02:46:29,328 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:46:29,329 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:46:29,331 : INFO : built Dictionary(430 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 2760 corpus positions)\n",
      "2021-01-14 02:46:29,678 : INFO : token count processed\n",
      "2021-01-14 02:46:29,707 : INFO : frequencies processed\n",
      "2021-01-14 02:46:38,994 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:46:38,995 : INFO : entropies processed\n",
      "2021-01-14 02:46:38,996 : INFO : extropies processed\n",
      "2021-01-14 02:46:39,003 : INFO : token count processed\n",
      "2021-01-14 02:46:39,010 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:46:39,014 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:46:39,015 : INFO : vocab #32006\n",
      "2021-01-14 02:46:39,023 : INFO : diff #set()\n",
      "2021-01-14 02:46:57,770 : INFO : alphabet #32006\n",
      "2021-01-14 02:47:07,102 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.2262265960626093, 0.44919057285931213], [0.9120099246501923, 0.087990075], [3.0220552088742, 1.3359632893587228], [4.518397116698909, 7.32185870753746, 7.533861415098355, 4.306394409138014, 3.0154642983994453, 0.21200270756089434]]\n",
      "2021-01-14 02:47:07,105 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:47:07,106 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:47:07,107 : INFO : built Dictionary(72 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 125 corpus positions)\n",
      "2021-01-14 02:47:07,135 : INFO : token count processed\n",
      "2021-01-14 02:47:07,159 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 02:47:07,160 : INFO : frequencies processed\n",
      "2021-01-14 02:47:07,161 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 02:47:07,166 : INFO : token count processed\n",
      "2021-01-14 02:47:07,173 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:47:07,178 : INFO : alphabet_target #32008\n",
      "2021-01-14 02:47:07,179 : INFO : vocab #32006\n",
      "2021-01-14 02:47:07,186 : INFO : diff #set()\n",
      "2021-01-14 02:47:25,901 : INFO : alphabet #32006\n",
      "2021-01-14 02:47:35,201 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2792609973798637, 0.43873869695026374], [0.9767377376556396, 0.023262262], [nan, nan], [4.518397116698909, 5.176618657501385, 5.887157228830312, 3.8078585453699825, 1.3687601121314028, 0.710538571328927]]\n",
      "2021-01-14 02:47:35,205 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:47:35,206 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:47:35,207 : INFO : built Dictionary(165 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 518 corpus positions)\n",
      "2021-01-14 02:47:35,298 : INFO : token count processed\n",
      "2021-01-14 02:47:35,325 : INFO : frequencies processed\n",
      "2021-01-14 02:47:44,632 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:47:44,633 : INFO : entropies processed\n",
      "2021-01-14 02:47:44,634 : INFO : extropies processed\n",
      "2021-01-14 02:47:44,647 : INFO : token count processed\n",
      "2021-01-14 02:47:44,651 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:47:44,655 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:47:44,656 : INFO : vocab #32006\n",
      "2021-01-14 02:47:44,663 : INFO : diff #set()\n",
      "2021-01-14 02:48:03,331 : INFO : alphabet #32006\n",
      "2021-01-14 02:48:12,629 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.260308408411219, 0.4424175020889758], [0.9491332545876503, 0.050866745], [1.5, 1.1225562489182657], [4.518397116698909, 6.468846789852156, 6.7297658506872295, 4.257478055863835, 2.21136873398832, 0.2609190608350733]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 02:48:12,635 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:48:12,636 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:48:12,638 : INFO : built Dictionary(378 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 2567 corpus positions)\n",
      "2021-01-14 02:48:12,878 : INFO : token count processed\n",
      "2021-01-14 02:48:12,906 : INFO : frequencies processed\n",
      "2021-01-14 02:48:22,080 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:48:22,081 : INFO : entropies processed\n",
      "2021-01-14 02:48:22,082 : INFO : extropies processed\n",
      "2021-01-14 02:48:22,089 : INFO : token count processed\n",
      "2021-01-14 02:48:22,096 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:48:22,101 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:48:22,102 : INFO : vocab #32006\n",
      "2021-01-14 02:48:22,110 : INFO : diff #set()\n",
      "2021-01-14 02:48:40,457 : INFO : alphabet #32006\n",
      "2021-01-14 02:48:49,734 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.2521369836332763, 0.44402272475750687], [0.93662528693676, 0.06337471], [2.725480556997868, 1.3192201298976014], [4.518397116698909, 6.957796704012729, 7.116996176325046, 4.359197644386593, 2.598599059626137, 0.15919947231231735]]\n",
      "2021-01-14 02:48:49,741 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:48:49,742 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:48:49,744 : INFO : built Dictionary(289 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 3064 corpus positions)\n",
      "2021-01-14 02:48:49,938 : INFO : token count processed\n",
      "2021-01-14 02:48:49,970 : INFO : frequencies processed\n",
      "2021-01-14 02:48:59,134 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:48:59,136 : INFO : entropies processed\n",
      "2021-01-14 02:48:59,137 : INFO : extropies processed\n",
      "2021-01-14 02:48:59,150 : INFO : token count processed\n",
      "2021-01-14 02:48:59,156 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:48:59,160 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:48:59,161 : INFO : vocab #32006\n",
      "2021-01-14 02:48:59,166 : INFO : diff #set()\n",
      "2021-01-14 02:49:22,212 : INFO : alphabet #32006\n",
      "2021-01-14 02:49:31,388 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.2270416122860872, 0.4490261854485453], [0.9224307164549828, 0.07756928], [2.1556390622295662, 1.2407663947533207], [4.518397116698909, 6.441859572014148, 6.608521483463866, 4.3517352052491916, 2.0901243667649565, 0.16666191144971787]]\n",
      "2021-01-14 02:49:31,393 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:49:31,394 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:49:31,396 : INFO : built Dictionary(314 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 1540 corpus positions)\n",
      "2021-01-14 02:49:31,597 : INFO : token count processed\n",
      "2021-01-14 02:49:31,633 : INFO : frequencies processed\n",
      "2021-01-14 02:49:40,815 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:49:40,816 : INFO : entropies processed\n",
      "2021-01-14 02:49:40,817 : INFO : extropies processed\n",
      "2021-01-14 02:49:40,823 : INFO : token count processed\n",
      "2021-01-14 02:49:40,827 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:49:40,830 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:49:40,831 : INFO : vocab #32006\n",
      "2021-01-14 02:49:40,837 : INFO : diff #set()\n",
      "2021-01-14 02:49:59,290 : INFO : alphabet #32006\n",
      "2021-01-14 02:50:08,450 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.256567928140858, 0.4431508520214946], [0.9442671686410904, 0.05573283], [2.5216406363433186, 1.2998438251349493], [4.518397116698909, 6.998955278238291, 7.191536152710699, 4.325816242226501, 2.67313903601179, 0.19258087447240868]]\n",
      "2021-01-14 02:50:08,455 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:50:08,456 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:50:08,458 : INFO : built Dictionary(230 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 1359 corpus positions)\n",
      "2021-01-14 02:50:08,587 : INFO : token count processed\n",
      "2021-01-14 02:50:08,617 : INFO : frequencies processed\n",
      "2021-01-14 02:50:17,790 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:50:17,791 : INFO : entropies processed\n",
      "2021-01-14 02:50:17,792 : INFO : extropies processed\n",
      "2021-01-14 02:50:17,798 : INFO : token count processed\n",
      "2021-01-14 02:50:17,805 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:50:17,811 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:50:17,812 : INFO : vocab #32006\n",
      "2021-01-14 02:50:17,818 : INFO : diff #set()\n",
      "2021-01-14 02:50:36,350 : INFO : alphabet #32006\n",
      "2021-01-14 02:50:45,520 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.255646441174205, 0.4433318900276931], [0.9478200748562813, 0.052179925], [2.2516291673878226, 1.2667563532600834], [4.518397116698909, 6.492983191376071, 6.646556154854237, 4.364824153220742, 2.128159038155328, 0.15357296347816618]]\n",
      "2021-01-14 02:50:45,527 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:50:45,528 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:50:45,530 : INFO : built Dictionary(439 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 3298 corpus positions)\n",
      "2021-01-14 02:50:45,852 : INFO : token count processed\n",
      "2021-01-14 02:50:45,895 : INFO : frequencies processed\n",
      "2021-01-14 02:50:55,072 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:50:55,073 : INFO : entropies processed\n",
      "2021-01-14 02:50:55,073 : INFO : extropies processed\n",
      "2021-01-14 02:50:55,081 : INFO : token count processed\n",
      "2021-01-14 02:50:55,085 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:50:55,090 : INFO : alphabet_target #32008\n",
      "2021-01-14 02:50:55,092 : INFO : vocab #32006\n",
      "2021-01-14 02:50:55,100 : INFO : diff #set()\n",
      "2021-01-14 02:51:13,588 : INFO : alphabet #32006\n",
      "2021-01-14 02:51:22,757 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.2559904270300557, 0.4432642922676185], [0.9429375045001507, 0.057062495], [2.321928094887362, 1.2877123795494492], [4.518397116698909, 6.560342487747443, 6.768631335118499, 4.310108269327852, 2.25023421841959, 0.20828884737105646]]\n",
      "2021-01-14 02:51:22,765 : INFO : Removed 1 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:51:22,766 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:51:22,768 : INFO : built Dictionary(453 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 3503 corpus positions)\n",
      "2021-01-14 02:51:23,118 : INFO : token count processed\n",
      "2021-01-14 02:51:23,146 : INFO : frequencies processed\n",
      "2021-01-14 02:51:32,430 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:51:32,431 : INFO : entropies processed\n",
      "2021-01-14 02:51:32,432 : INFO : extropies processed\n",
      "2021-01-14 02:51:32,439 : INFO : token count processed\n",
      "2021-01-14 02:51:32,445 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:51:32,451 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:51:32,452 : INFO : vocab #32006\n",
      "2021-01-14 02:51:32,458 : INFO : diff #set()\n",
      "2021-01-14 02:51:50,832 : INFO : alphabet #32006\n",
      "2021-01-14 02:52:00,098 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.2447589706695685, 0.4454821266185737], [0.9369791373610497, 0.06302086], [3.121928094887362, 1.3519647487142497], [4.518397116698909, 7.046173750105238, 7.245651915986279, 4.318918950817867, 2.72725479928737, 0.1994781658810414]]\n",
      "2021-01-14 02:52:00,108 : INFO : Removed 1 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:52:00,109 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:52:00,112 : INFO : built Dictionary(507 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 5612 corpus positions)\n",
      "2021-01-14 02:52:00,517 : INFO : token count processed\n",
      "2021-01-14 02:52:00,551 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 02:52:09,730 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:52:09,731 : INFO : entropies processed\n",
      "2021-01-14 02:52:09,732 : INFO : extropies processed\n",
      "2021-01-14 02:52:09,744 : INFO : token count processed\n",
      "2021-01-14 02:52:09,749 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:52:09,753 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:52:09,754 : INFO : vocab #32006\n",
      "2021-01-14 02:52:09,760 : INFO : diff #set()\n",
      "2021-01-14 02:52:28,172 : INFO : alphabet #32006\n",
      "2021-01-14 02:52:37,443 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.2145942533521752, 0.45154998415006514], [0.9245785549283028, 0.075421445], [3.121928094887362, 1.3519647487142497], [4.518397116698909, 7.009229588004272, 7.126022895418843, 4.401603809284338, 2.6076257787199335, 0.11679330741457061]]\n",
      "2021-01-14 02:52:37,456 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:52:37,457 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:52:37,460 : INFO : built Dictionary(586 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 6553 corpus positions)\n",
      "2021-01-14 02:52:37,968 : INFO : token count processed\n",
      "2021-01-14 02:52:38,001 : INFO : frequencies processed\n",
      "2021-01-14 02:52:47,182 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:52:47,183 : INFO : entropies processed\n",
      "2021-01-14 02:52:47,184 : INFO : extropies processed\n",
      "2021-01-14 02:52:47,193 : INFO : token count processed\n",
      "2021-01-14 02:52:47,199 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:52:47,204 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:52:47,205 : INFO : vocab #32006\n",
      "2021-01-14 02:52:47,213 : INFO : diff #set()\n",
      "2021-01-14 02:53:05,576 : INFO : alphabet #32006\n",
      "2021-01-14 02:53:14,866 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.250394494971063, 0.4443665331721577], [0.9341150522232056, 0.06588495], [2.8453509366224363, 1.3210203571681218], [4.518397116698909, 7.376088004590871, 7.548735536877647, 4.3457495844121325, 3.0303384201787376, 0.17264753228677598]]\n",
      "2021-01-14 02:53:14,870 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:53:14,870 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:53:14,872 : INFO : built Dictionary(141 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 339 corpus positions)\n",
      "2021-01-14 02:53:14,944 : INFO : token count processed\n",
      "2021-01-14 02:53:15,000 : INFO : frequencies processed\n",
      "2021-01-14 02:53:24,140 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:53:24,141 : INFO : entropies processed\n",
      "2021-01-14 02:53:24,142 : INFO : extropies processed\n",
      "2021-01-14 02:53:24,154 : INFO : token count processed\n",
      "2021-01-14 02:53:24,161 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:53:24,166 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:53:24,167 : INFO : vocab #32006\n",
      "2021-01-14 02:53:24,176 : INFO : diff #set()\n",
      "2021-01-14 02:53:42,679 : INFO : alphabet #32006\n",
      "2021-01-14 02:53:51,830 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.2504533433966067, 0.4443549131708286], [0.9434799812734127, 0.05652002], [1.5, 1.1225562489182657], [4.518397116698909, 6.2993628166120885, 6.580358101492301, 4.237401831818698, 2.0619609847933917, 0.28099528488021264]]\n",
      "2021-01-14 02:53:51,833 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:53:51,834 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:53:51,835 : INFO : built Dictionary(41 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 55 corpus positions)\n",
      "2021-01-14 02:53:51,850 : INFO : token count processed\n",
      "2021-01-14 02:53:51,875 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 02:53:51,876 : INFO : frequencies processed\n",
      "2021-01-14 02:53:51,877 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 02:53:51,882 : INFO : token count processed\n",
      "2021-01-14 02:53:51,886 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:53:51,890 : INFO : alphabet_target #32008\n",
      "2021-01-14 02:53:51,891 : INFO : vocab #32006\n",
      "2021-01-14 02:53:51,897 : INFO : diff #set()\n",
      "2021-01-14 02:54:10,377 : INFO : alphabet #32006\n",
      "2021-01-14 02:54:19,892 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2874751737218986, 0.4371632144854813], [0.9742732681334019, 0.025726732], [nan, nan], [4.518397116698909, 3.8936606896881862, 5.157393752043275, 3.254664054343822, 0.6389966353443652, 1.2637330623550884]]\n",
      "2021-01-14 02:54:19,915 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:54:19,916 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:54:19,920 : INFO : built Dictionary(741 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 12497 corpus positions)\n",
      "2021-01-14 02:54:20,622 : INFO : token count processed\n",
      "2021-01-14 02:54:20,650 : INFO : frequencies processed\n",
      "2021-01-14 02:54:29,796 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:54:29,797 : INFO : entropies processed\n",
      "2021-01-14 02:54:29,798 : INFO : extropies processed\n",
      "2021-01-14 02:54:29,809 : INFO : token count processed\n",
      "2021-01-14 02:54:29,813 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:54:29,817 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:54:29,818 : INFO : vocab #32006\n",
      "2021-01-14 02:54:29,824 : INFO : diff #set()\n",
      "2021-01-14 02:54:48,303 : INFO : alphabet #32006\n",
      "2021-01-14 02:54:57,470 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.2375289920220809, 0.4469215833919937], [0.9310728758573532, 0.068927124], [3.5465935642949384, 1.3764678303056375], [4.518397116698909, 7.434393313070278, 7.643251625310084, 4.309538804459104, 3.1248545086111745, 0.20885831223980578]]\n",
      "2021-01-14 02:54:57,478 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:54:57,479 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:54:57,482 : INFO : built Dictionary(493 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 4124 corpus positions)\n",
      "2021-01-14 02:54:57,857 : INFO : token count processed\n",
      "2021-01-14 02:54:57,884 : INFO : frequencies processed\n",
      "2021-01-14 02:55:07,034 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:55:07,035 : INFO : entropies processed\n",
      "2021-01-14 02:55:07,036 : INFO : extropies processed\n",
      "2021-01-14 02:55:07,044 : INFO : token count processed\n",
      "2021-01-14 02:55:07,048 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:55:07,052 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:55:07,053 : INFO : vocab #32006\n",
      "2021-01-14 02:55:07,063 : INFO : diff #set()\n",
      "2021-01-14 02:55:25,753 : INFO : alphabet #32006\n",
      "2021-01-14 02:55:34,918 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.235893713613514, 0.4472484509936125], [0.9285945519804955, 0.07140545], [3.3927474104487847, 1.3672090515720436], [4.518397116698909, 7.2991514951718255, 7.496627758609847, 4.320920853260887, 2.9782306419109377, 0.1974762634380216]]\n",
      "2021-01-14 02:55:34,926 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:55:34,927 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:55:34,929 : INFO : built Dictionary(462 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 3541 corpus positions)\n",
      "2021-01-14 02:55:35,278 : INFO : token count processed\n",
      "2021-01-14 02:55:35,336 : INFO : frequencies processed\n",
      "2021-01-14 02:55:44,495 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:55:44,496 : INFO : entropies processed\n",
      "2021-01-14 02:55:44,496 : INFO : extropies processed\n",
      "2021-01-14 02:55:44,504 : INFO : token count processed\n",
      "2021-01-14 02:55:44,508 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:55:44,515 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:55:44,517 : INFO : vocab #32006\n",
      "2021-01-14 02:55:44,525 : INFO : diff #set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 02:56:03,198 : INFO : alphabet #32006\n",
      "2021-01-14 02:56:12,392 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.2518240972758314, 0.4440844208078956], [0.9409961178898811, 0.059003882], [2.9219280948873623, 1.3359016564230495], [4.518397116698909, 7.170319527000998, 7.367292985990579, 4.321423657709328, 2.8488958692916695, 0.1969734589895813]]\n",
      "2021-01-14 02:56:12,396 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:56:12,397 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:56:12,398 : INFO : built Dictionary(173 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 577 corpus positions)\n",
      "2021-01-14 02:56:12,492 : INFO : token count processed\n",
      "2021-01-14 02:56:12,546 : INFO : frequencies processed\n",
      "2021-01-14 02:56:21,828 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:56:21,829 : INFO : entropies processed\n",
      "2021-01-14 02:56:21,830 : INFO : extropies processed\n",
      "2021-01-14 02:56:21,836 : INFO : token count processed\n",
      "2021-01-14 02:56:21,843 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:56:21,848 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:56:21,850 : INFO : vocab #32006\n",
      "2021-01-14 02:56:21,857 : INFO : diff #set()\n",
      "2021-01-14 02:56:40,225 : INFO : alphabet #32006\n",
      "2021-01-14 02:56:49,431 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.2195665122762476, 0.4505384247190065], [0.9346564635634422, 0.06534354], [2.251629167387823, 1.2667563532600834], [4.518397116698909, 6.353654804387375, 6.599087112805812, 4.272964808280474, 2.0806899961069023, 0.24543230841843666]]\n",
      "2021-01-14 02:56:49,435 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:56:49,436 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:56:49,437 : INFO : built Dictionary(169 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 705 corpus positions)\n",
      "2021-01-14 02:56:49,524 : INFO : token count processed\n",
      "2021-01-14 02:56:49,552 : INFO : frequencies processed\n",
      "2021-01-14 02:56:58,961 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:56:58,962 : INFO : entropies processed\n",
      "2021-01-14 02:56:58,963 : INFO : extropies processed\n",
      "2021-01-14 02:56:58,970 : INFO : token count processed\n",
      "2021-01-14 02:56:58,976 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:56:58,980 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:56:58,981 : INFO : vocab #32006\n",
      "2021-01-14 02:56:58,988 : INFO : diff #set()\n",
      "2021-01-14 02:57:17,348 : INFO : alphabet #32006\n",
      "2021-01-14 02:57:26,568 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.2204801877329954, 0.45035303873661325], [0.9372644126415253, 0.06273559], [1.9219280948873623, 1.2148067842293933], [4.518397116698909, 6.245180322479091, 6.454777217945869, 4.308800221232131, 1.93638010124696, 0.2095968954667784]]\n",
      "2021-01-14 02:57:26,574 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:57:26,575 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:57:26,579 : INFO : built Dictionary(394 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 1777 corpus positions)\n",
      "2021-01-14 02:57:26,857 : INFO : token count processed\n",
      "2021-01-14 02:57:26,893 : INFO : frequencies processed\n",
      "2021-01-14 02:57:36,163 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:57:36,164 : INFO : entropies processed\n",
      "2021-01-14 02:57:36,165 : INFO : extropies processed\n",
      "2021-01-14 02:57:36,179 : INFO : token count processed\n",
      "2021-01-14 02:57:36,184 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:57:36,188 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:57:36,189 : INFO : vocab #32006\n",
      "2021-01-14 02:57:36,195 : INFO : diff #set()\n",
      "2021-01-14 02:57:54,572 : INFO : alphabet #32006\n",
      "2021-01-14 02:58:04,006 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.239500146673005, 0.44652821366660644], [0.9275302365422249, 0.07246976], [2.94770277922009, 1.3393100707180505], [4.518397116698909, 7.2691387000368, 7.491547171127479, 4.2959886456082295, 2.9731500544285696, 0.22240847109067907]]\n",
      "2021-01-14 02:58:04,012 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:58:04,013 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:58:04,014 : INFO : built Dictionary(320 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 1614 corpus positions)\n",
      "2021-01-14 02:58:04,218 : INFO : token count processed\n",
      "2021-01-14 02:58:04,246 : INFO : frequencies processed\n",
      "2021-01-14 02:58:13,435 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:58:13,436 : INFO : entropies processed\n",
      "2021-01-14 02:58:13,437 : INFO : extropies processed\n",
      "2021-01-14 02:58:13,443 : INFO : token count processed\n",
      "2021-01-14 02:58:13,447 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:58:13,451 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:58:13,452 : INFO : vocab #32006\n",
      "2021-01-14 02:58:13,462 : INFO : diff #set()\n",
      "2021-01-14 02:58:32,101 : INFO : alphabet #32006\n",
      "2021-01-14 02:58:41,332 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.2274237106348966, 0.4489491582699206], [0.9192129299044609, 0.08078707], [2.94770277922009, 1.3393100707180505], [4.518397116698909, 7.08857858466988, 7.275007883506275, 4.331967817862514, 2.7566107668073654, 0.18642929883639514]]\n",
      "2021-01-14 02:58:41,336 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:58:41,337 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:58:41,340 : INFO : built Dictionary(149 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 537 corpus positions)\n",
      "2021-01-14 02:58:41,410 : INFO : token count processed\n",
      "2021-01-14 02:58:41,439 : INFO : frequencies processed\n",
      "2021-01-14 02:58:50,596 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:58:50,597 : INFO : entropies processed\n",
      "2021-01-14 02:58:50,598 : INFO : extropies processed\n",
      "2021-01-14 02:58:50,604 : INFO : token count processed\n",
      "2021-01-14 02:58:50,608 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:58:50,612 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:58:50,613 : INFO : vocab #32006\n",
      "2021-01-14 02:58:50,619 : INFO : diff #set()\n",
      "2021-01-14 02:59:09,332 : INFO : alphabet #32006\n",
      "2021-01-14 02:59:18,505 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.2621279635725873, 0.44206164111984897], [0.9598655253648758, 0.040134475], [1.5, 1.1225562489182657], [4.518397116698909, 6.0479231618016716, 6.314814134957322, 4.251506143543258, 1.7964170182584125, 0.26689097315565036]]\n",
      "2021-01-14 02:59:18,509 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:59:18,510 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:59:18,512 : INFO : built Dictionary(152 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 656 corpus positions)\n",
      "2021-01-14 02:59:18,594 : INFO : token count processed\n",
      "2021-01-14 02:59:18,626 : INFO : frequencies processed\n",
      "2021-01-14 02:59:27,792 : INFO : scalar_distribution processed\n",
      "2021-01-14 02:59:27,794 : INFO : entropies processed\n",
      "2021-01-14 02:59:27,794 : INFO : extropies processed\n",
      "2021-01-14 02:59:27,801 : INFO : token count processed\n",
      "2021-01-14 02:59:27,808 : INFO : alphabet_source #32006\n",
      "2021-01-14 02:59:27,813 : INFO : alphabet_target #32009\n",
      "2021-01-14 02:59:27,814 : INFO : vocab #32006\n",
      "2021-01-14 02:59:27,822 : INFO : diff #set()\n",
      "2021-01-14 02:59:46,333 : INFO : alphabet #32006\n",
      "2021-01-14 02:59:55,562 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.2619483848365882, 0.442096736912166], [0.9596125110983849, 0.04038749], [0.9182958340544896, 0.9182958340544896], [4.518397116698909, 6.036583168403119, 6.270317841294261, 4.284662443807768, 1.7519207245953519, 0.23373467289114203]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 02:59:55,575 : INFO : Removed 1 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 02:59:55,575 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 02:59:55,579 : INFO : built Dictionary(561 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 7014 corpus positions)\n",
      "2021-01-14 02:59:56,055 : INFO : token count processed\n",
      "2021-01-14 02:59:56,083 : INFO : frequencies processed\n",
      "2021-01-14 03:00:05,266 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:00:05,267 : INFO : entropies processed\n",
      "2021-01-14 03:00:05,268 : INFO : extropies processed\n",
      "2021-01-14 03:00:05,277 : INFO : token count processed\n",
      "2021-01-14 03:00:05,284 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:00:05,290 : INFO : alphabet_target #32010\n",
      "2021-01-14 03:00:05,291 : INFO : vocab #32006\n",
      "2021-01-14 03:00:05,298 : INFO : diff #set()\n",
      "2021-01-14 03:00:23,791 : INFO : alphabet #32006\n",
      "2021-01-14 03:00:33,108 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.2516425824687936, 0.44412022040530025], [0.9369109347462654, 0.063089065], [2.8453509366224363, 1.3210203571681218], [4.518397116698909, 7.29352035514053, 7.458355528923279, 4.353561942916161, 2.939958412224369, 0.16483517378274826]]\n",
      "2021-01-14 03:00:33,115 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:00:33,116 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:00:33,118 : INFO : built Dictionary(372 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 3257 corpus positions)\n",
      "2021-01-14 03:00:33,364 : INFO : token count processed\n",
      "2021-01-14 03:00:33,392 : INFO : frequencies processed\n",
      "2021-01-14 03:00:42,654 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:00:42,655 : INFO : entropies processed\n",
      "2021-01-14 03:00:42,656 : INFO : extropies processed\n",
      "2021-01-14 03:00:42,663 : INFO : token count processed\n",
      "2021-01-14 03:00:42,670 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:00:42,675 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:00:42,676 : INFO : vocab #32006\n",
      "2021-01-14 03:00:42,684 : INFO : diff #set()\n",
      "2021-01-14 03:01:01,022 : INFO : alphabet #32006\n",
      "2021-01-14 03:01:10,334 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.2562903394328022, 0.4432053723420121], [0.9514435939490795, 0.048556406], [2.521640636343318, 1.2998438251349493], [4.518397116698909, 6.8153433747477745, 6.9731539377553275, 4.3605865536913555, 2.454756821056418, 0.15781056300755303]]\n",
      "2021-01-14 03:01:10,337 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:01:10,338 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:01:10,339 : INFO : built Dictionary(126 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 278 corpus positions)\n",
      "2021-01-14 03:01:10,412 : INFO : token count processed\n",
      "2021-01-14 03:01:10,443 : INFO : frequencies processed\n",
      "2021-01-14 03:01:19,726 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:01:19,727 : INFO : entropies processed\n",
      "2021-01-14 03:01:19,728 : INFO : extropies processed\n",
      "2021-01-14 03:01:19,734 : INFO : token count processed\n",
      "2021-01-14 03:01:19,741 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:01:19,746 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:01:19,747 : INFO : vocab #32006\n",
      "2021-01-14 03:01:19,753 : INFO : diff #set()\n",
      "2021-01-14 03:01:38,124 : INFO : alphabet #32006\n",
      "2021-01-14 03:01:47,296 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.2503821484738784, 0.4443689711448169], [0.9480689130723476, 0.051931087], [1.584962500721156, 1.1699250014423124], [4.518397116698909, 6.150121915859574, 6.448278595084084, 4.220240437474399, 1.929881478385175, 0.29815667922451006]]\n",
      "2021-01-14 03:01:47,300 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:01:47,301 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:01:47,304 : INFO : built Dictionary(291 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 989 corpus positions)\n",
      "2021-01-14 03:01:47,502 : INFO : token count processed\n",
      "2021-01-14 03:01:47,529 : INFO : frequencies processed\n",
      "2021-01-14 03:01:56,957 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:01:56,959 : INFO : entropies processed\n",
      "2021-01-14 03:01:56,960 : INFO : extropies processed\n",
      "2021-01-14 03:01:56,972 : INFO : token count processed\n",
      "2021-01-14 03:01:56,978 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:01:56,982 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:01:56,983 : INFO : vocab #32006\n",
      "2021-01-14 03:01:56,989 : INFO : diff #set()\n",
      "2021-01-14 03:02:15,450 : INFO : alphabet #32006\n",
      "2021-01-14 03:02:24,749 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.2421132014292888, 0.44600781056127137], [0.9220177084207535, 0.07798229], [1.5, 1.1225562489182657], [4.518397116698909, 7.0391145208191315, 7.3201401137354, 4.23737152378264, 2.8017429970364907, 0.2810255929162686]]\n",
      "2021-01-14 03:02:24,758 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:02:24,759 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:02:24,762 : INFO : built Dictionary(589 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 4364 corpus positions)\n",
      "2021-01-14 03:02:25,273 : INFO : token count processed\n",
      "2021-01-14 03:02:25,300 : INFO : frequencies processed\n",
      "2021-01-14 03:02:34,623 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:02:34,624 : INFO : entropies processed\n",
      "2021-01-14 03:02:34,625 : INFO : extropies processed\n",
      "2021-01-14 03:02:34,633 : INFO : token count processed\n",
      "2021-01-14 03:02:34,637 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:02:34,641 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:02:34,642 : INFO : vocab #32006\n",
      "2021-01-14 03:02:34,651 : INFO : diff #set()\n",
      "2021-01-14 03:02:53,347 : INFO : alphabet #32006\n",
      "2021-01-14 03:03:02,502 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.2376976756611098, 0.4468878932470437], [0.9209005534648895, 0.07909945], [3.2776134368191157, 1.3618978811135465], [4.518397116698909, 7.482466367279176, 7.761226457413258, 4.239637026564827, 3.2428293407143487, 0.27876009013408254]]\n",
      "2021-01-14 03:03:02,506 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:03:02,507 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:03:02,508 : INFO : built Dictionary(175 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 586 corpus positions)\n",
      "2021-01-14 03:03:02,601 : INFO : token count processed\n",
      "2021-01-14 03:03:02,656 : INFO : frequencies processed\n",
      "2021-01-14 03:03:11,814 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:03:11,815 : INFO : entropies processed\n",
      "2021-01-14 03:03:11,816 : INFO : extropies processed\n",
      "2021-01-14 03:03:11,823 : INFO : token count processed\n",
      "2021-01-14 03:03:11,830 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:03:11,834 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:03:11,835 : INFO : vocab #32006\n",
      "2021-01-14 03:03:11,843 : INFO : diff #set()\n",
      "2021-01-14 03:03:30,576 : INFO : alphabet #32006\n",
      "2021-01-14 03:03:39,758 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.2572300211613099, 0.4430208665599421], [0.9547254554927349, 0.045274545], [1.5, 1.1225562489182657], [4.518397116698909, 6.372162341197667, 6.630194539173884, 4.260364918722692, 2.1117974224749743, 0.2580321979762168]]\n",
      "2021-01-14 03:03:39,764 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:03:39,765 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:03:39,766 : INFO : built Dictionary(322 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 2007 corpus positions)\n",
      "2021-01-14 03:03:39,983 : INFO : token count processed\n",
      "2021-01-14 03:03:40,029 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 03:03:49,358 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:03:49,359 : INFO : entropies processed\n",
      "2021-01-14 03:03:49,360 : INFO : extropies processed\n",
      "2021-01-14 03:03:49,374 : INFO : token count processed\n",
      "2021-01-14 03:03:49,377 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:03:49,381 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:03:49,382 : INFO : vocab #32006\n",
      "2021-01-14 03:03:49,387 : INFO : diff #set()\n",
      "2021-01-14 03:04:08,046 : INFO : alphabet #32006\n",
      "2021-01-14 03:04:17,363 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.258804523993203, 0.4427120582493614], [0.9500152207911015, 0.04998478], [2.251629167387823, 1.2667563532600834], [4.518397116698909, 6.798155919669889, 7.009581894035486, 4.306971142333312, 2.491184777336577, 0.2114259743655973]]\n",
      "2021-01-14 03:04:17,367 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:04:17,368 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:04:17,369 : INFO : built Dictionary(180 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 723 corpus positions)\n",
      "2021-01-14 03:04:17,463 : INFO : token count processed\n",
      "2021-01-14 03:04:17,490 : INFO : frequencies processed\n",
      "2021-01-14 03:04:26,657 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:04:26,659 : INFO : entropies processed\n",
      "2021-01-14 03:04:26,659 : INFO : extropies processed\n",
      "2021-01-14 03:04:26,666 : INFO : token count processed\n",
      "2021-01-14 03:04:26,670 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:04:26,675 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:04:26,676 : INFO : vocab #32006\n",
      "2021-01-14 03:04:26,687 : INFO : diff #set()\n",
      "2021-01-14 03:04:45,163 : INFO : alphabet #32006\n",
      "2021-01-14 03:04:54,455 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.258858760790938, 0.44270142841947796], [0.9537583142518997, 0.046241686], [1.5, 1.1225562489182657], [4.518397116698909, 6.271631856729336, 6.517268283249218, 4.272760690179028, 1.998871166550309, 0.2456364265198827]]\n",
      "2021-01-14 03:04:54,463 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:04:54,464 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:04:54,466 : INFO : built Dictionary(352 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 3238 corpus positions)\n",
      "2021-01-14 03:04:54,701 : INFO : token count processed\n",
      "2021-01-14 03:04:54,729 : INFO : frequencies processed\n",
      "2021-01-14 03:05:03,999 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:05:04,000 : INFO : entropies processed\n",
      "2021-01-14 03:05:04,001 : INFO : extropies processed\n",
      "2021-01-14 03:05:04,008 : INFO : token count processed\n",
      "2021-01-14 03:05:04,012 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:05:04,016 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:05:04,017 : INFO : vocab #32006\n",
      "2021-01-14 03:05:04,027 : INFO : diff #set()\n",
      "2021-01-14 03:05:22,395 : INFO : alphabet #32006\n",
      "2021-01-14 03:05:31,704 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.1923303384598183, 0.45613563907642296], [0.9075978323817253, 0.09240217], [3.121928094887362, 1.3519647487142497], [4.518397116698909, 6.873598627629562, 7.034882612453957, 4.357113131874515, 2.516485495755048, 0.16128398482439543]]\n",
      "2021-01-14 03:05:31,707 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:05:31,708 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:05:31,709 : INFO : built Dictionary(108 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 186 corpus positions)\n",
      "2021-01-14 03:05:31,766 : INFO : token count processed\n",
      "2021-01-14 03:05:31,793 : INFO : frequencies processed\n",
      "2021-01-14 03:05:41,571 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:05:41,572 : INFO : entropies processed\n",
      "2021-01-14 03:05:41,573 : INFO : extropies processed\n",
      "2021-01-14 03:05:41,579 : INFO : token count processed\n",
      "2021-01-14 03:05:41,587 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:05:41,594 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:05:41,595 : INFO : vocab #32006\n",
      "2021-01-14 03:05:41,601 : INFO : diff #set()\n",
      "2021-01-14 03:05:59,957 : INFO : alphabet #32006\n",
      "2021-01-14 03:06:09,128 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.239530834420623, 0.4465220949988414], [0.9423963166773319, 0.057603683], [1.0, 1.0], [4.518397116698909, 6.049830202851529, 6.459199126494109, 4.109028193056329, 1.9408020097952, 0.40936892364257993]]\n",
      "2021-01-14 03:06:09,133 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:06:09,134 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:06:09,137 : INFO : built Dictionary(255 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 1266 corpus positions)\n",
      "2021-01-14 03:06:09,287 : INFO : token count processed\n",
      "2021-01-14 03:06:09,315 : INFO : frequencies processed\n",
      "2021-01-14 03:06:18,741 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:06:18,742 : INFO : entropies processed\n",
      "2021-01-14 03:06:18,743 : INFO : extropies processed\n",
      "2021-01-14 03:06:18,749 : INFO : token count processed\n",
      "2021-01-14 03:06:18,757 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:06:18,762 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:06:18,763 : INFO : vocab #32006\n",
      "2021-01-14 03:06:18,771 : INFO : diff #set()\n",
      "2021-01-14 03:06:37,261 : INFO : alphabet #32006\n",
      "2021-01-14 03:06:46,550 : INFO : Computed distances or similarities ('295', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.2389303106666416, 0.44664186072957757], [0.9348544254899025, 0.065145575], [1.5, 1.1225562489182657], [4.518397116698909, 6.778844940588858, 7.007254493302148, 4.28998756398562, 2.4888573766032387, 0.22840955271329033]]\n",
      "2021-01-14 03:06:46,553 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:06:46,554 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:06:46,555 : INFO : built Dictionary(155 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 299 corpus positions)\n",
      "2021-01-14 03:06:46,628 : INFO : token count processed\n",
      "2021-01-14 03:06:46,656 : INFO : frequencies processed\n",
      "2021-01-14 03:06:56,088 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:06:56,089 : INFO : entropies processed\n",
      "2021-01-14 03:06:56,090 : INFO : extropies processed\n",
      "2021-01-14 03:06:56,096 : INFO : token count processed\n",
      "2021-01-14 03:06:56,104 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:06:56,109 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:06:56,110 : INFO : vocab #32006\n",
      "2021-01-14 03:06:56,117 : INFO : diff #set()\n",
      "2021-01-14 03:07:14,922 : INFO : alphabet #32006\n",
      "2021-01-14 03:07:24,094 : INFO : Computed distances or similarities ('295', 'sacp-python-common/setup.py')[[1.2587524046891914, 0.44272227355419325], [0.9524871148169041, 0.047512885], [1.0, 1.0], [4.518397116698909, 6.469677430851302, 6.846061182517849, 4.142013365032364, 2.327664065818939, 0.3763837516665465]]\n",
      "2021-01-14 03:07:24,099 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:07:24,100 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:07:24,101 : INFO : built Dictionary(223 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 1201 corpus positions)\n",
      "2021-01-14 03:07:24,233 : INFO : token count processed\n",
      "2021-01-14 03:07:24,288 : INFO : frequencies processed\n",
      "2021-01-14 03:07:33,458 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:07:33,459 : INFO : entropies processed\n",
      "2021-01-14 03:07:33,460 : INFO : extropies processed\n",
      "2021-01-14 03:07:33,466 : INFO : token count processed\n",
      "2021-01-14 03:07:33,474 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:07:33,478 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:07:33,479 : INFO : vocab #32006\n",
      "2021-01-14 03:07:33,487 : INFO : diff #set()\n",
      "2021-01-14 03:07:52,289 : INFO : alphabet #32006\n",
      "2021-01-14 03:08:01,460 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.251089626665283, 0.44422931373078167], [0.9414647221565247, 0.058535278], [1.9219280948873623, 1.2148067842293933], [4.518397116698909, 6.459180448028249, 6.669407551464258, 4.3081700132629015, 2.1510104347653485, 0.2102271034360088]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 03:08:01,463 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:08:01,464 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:08:01,465 : INFO : built Dictionary(134 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 351 corpus positions)\n",
      "2021-01-14 03:08:01,527 : INFO : token count processed\n",
      "2021-01-14 03:08:01,555 : INFO : frequencies processed\n",
      "2021-01-14 03:08:10,702 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:08:10,704 : INFO : entropies processed\n",
      "2021-01-14 03:08:10,704 : INFO : extropies processed\n",
      "2021-01-14 03:08:10,711 : INFO : token count processed\n",
      "2021-01-14 03:08:10,718 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:08:10,722 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:08:10,723 : INFO : vocab #32006\n",
      "2021-01-14 03:08:10,730 : INFO : diff #set()\n",
      "2021-01-14 03:08:29,670 : INFO : alphabet #32006\n",
      "2021-01-14 03:08:38,870 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.2489142057521145, 0.44465902587225], [0.9378410838544369, 0.062158916], [1.5, 1.1225562489182657], [4.518397116698909, 6.097125733496388, 6.400490680811378, 4.215032169383919, 1.882093564112469, 0.3033649473149902]]\n",
      "2021-01-14 03:08:38,873 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:08:38,874 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:08:38,875 : INFO : built Dictionary(125 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 347 corpus positions)\n",
      "2021-01-14 03:08:38,943 : INFO : token count processed\n",
      "2021-01-14 03:08:38,973 : INFO : frequencies processed\n",
      "2021-01-14 03:08:48,109 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:08:48,110 : INFO : entropies processed\n",
      "2021-01-14 03:08:48,111 : INFO : extropies processed\n",
      "2021-01-14 03:08:48,117 : INFO : token count processed\n",
      "2021-01-14 03:08:48,124 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:08:48,129 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:08:48,130 : INFO : vocab #32006\n",
      "2021-01-14 03:08:48,138 : INFO : diff #set()\n",
      "2021-01-14 03:09:06,591 : INFO : alphabet #32006\n",
      "2021-01-14 03:09:15,917 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.2391907555978552, 0.44658991088635674], [0.9338368847966194, 0.066163115], [1.9219280948873623, 1.2148067842293933], [4.518397116698909, 6.0695858597523715, 6.359261263252444, 4.228721713198837, 1.8408641465535345, 0.28967540350007237]]\n",
      "2021-01-14 03:09:15,921 : INFO : Removed 1 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:09:15,922 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:09:15,924 : INFO : built Dictionary(127 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 443 corpus positions)\n",
      "2021-01-14 03:09:15,989 : INFO : token count processed\n",
      "2021-01-14 03:09:16,038 : INFO : frequencies processed\n",
      "2021-01-14 03:09:25,324 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:09:25,325 : INFO : entropies processed\n",
      "2021-01-14 03:09:25,326 : INFO : extropies processed\n",
      "2021-01-14 03:09:25,338 : INFO : token count processed\n",
      "2021-01-14 03:09:25,342 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:09:25,346 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:09:25,347 : INFO : vocab #32006\n",
      "2021-01-14 03:09:25,353 : INFO : diff #set()\n",
      "2021-01-14 03:09:44,116 : INFO : alphabet #32006\n",
      "2021-01-14 03:09:53,354 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.2302934168571402, 0.4483714978673831], [0.9272855371236801, 0.07271446], [1.9219280948873623, 1.2148067842293933], [4.518397116698909, 6.104787343210121, 6.343919414967391, 4.279265044941639, 1.8255222982684813, 0.23913207175727003]]\n",
      "2021-01-14 03:09:53,370 : INFO : Removed 1 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:09:53,371 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:09:53,374 : INFO : built Dictionary(412 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 9128 corpus positions)\n",
      "2021-01-14 03:09:53,692 : INFO : token count processed\n",
      "2021-01-14 03:09:53,737 : INFO : frequencies processed\n",
      "2021-01-14 03:10:03,236 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:10:03,237 : INFO : entropies processed\n",
      "2021-01-14 03:10:03,238 : INFO : extropies processed\n",
      "2021-01-14 03:10:03,247 : INFO : token count processed\n",
      "2021-01-14 03:10:03,251 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:10:03,255 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:10:03,256 : INFO : vocab #32006\n",
      "2021-01-14 03:10:03,267 : INFO : diff #set()\n",
      "2021-01-14 03:10:21,606 : INFO : alphabet #32006\n",
      "2021-01-14 03:10:30,867 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.2581644695041758, 0.44283754062412006], [0.9493218846619129, 0.050678115], [2.75, 1.3226647836567116], [4.518397116698909, 6.89087415148015, 7.05501839561524, 4.35425287256382, 2.5366212789163303, 0.16414424413508932]]\n",
      "2021-01-14 03:10:30,873 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:10:30,874 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:10:30,875 : INFO : built Dictionary(273 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 2288 corpus positions)\n",
      "2021-01-14 03:10:31,034 : INFO : token count processed\n",
      "2021-01-14 03:10:31,061 : INFO : frequencies processed\n",
      "2021-01-14 03:10:40,350 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:10:40,351 : INFO : entropies processed\n",
      "2021-01-14 03:10:40,352 : INFO : extropies processed\n",
      "2021-01-14 03:10:40,359 : INFO : token count processed\n",
      "2021-01-14 03:10:40,363 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:10:40,367 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:10:40,369 : INFO : vocab #32006\n",
      "2021-01-14 03:10:40,380 : INFO : diff #set()\n",
      "2021-01-14 03:10:59,169 : INFO : alphabet #32006\n",
      "2021-01-14 03:11:08,533 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.249466229945493, 0.44454990552324547], [0.9367147237062454, 0.06328528], [1.9219280948873623, 1.2148067842293933], [4.518397116698909, 6.655493573668506, 6.819971535074007, 4.353919155293408, 2.3015744183750977, 0.16447796140550075]]\n",
      "2021-01-14 03:11:08,538 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:11:08,539 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:11:08,540 : INFO : built Dictionary(249 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 1295 corpus positions)\n",
      "2021-01-14 03:11:08,688 : INFO : token count processed\n",
      "2021-01-14 03:11:08,716 : INFO : frequencies processed\n",
      "2021-01-14 03:11:17,875 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:11:17,876 : INFO : entropies processed\n",
      "2021-01-14 03:11:17,877 : INFO : extropies processed\n",
      "2021-01-14 03:11:17,883 : INFO : token count processed\n",
      "2021-01-14 03:11:17,891 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:11:17,896 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:11:17,897 : INFO : vocab #32006\n",
      "2021-01-14 03:11:17,904 : INFO : diff #set()\n",
      "2021-01-14 03:11:36,411 : INFO : alphabet #32006\n",
      "2021-01-14 03:11:45,665 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.225776417401218, 0.4492814247567527], [0.9293845891952515, 0.07061541], [2.2516291673878226, 1.2667563532600834], [4.518397116698909, 6.6236746347295465, 6.782310638097917, 4.35976111333054, 2.263913521399007, 0.15863600336837003]]\n",
      "2021-01-14 03:11:45,669 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:11:45,670 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:11:45,672 : INFO : built Dictionary(250 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 1234 corpus positions)\n",
      "2021-01-14 03:11:45,817 : INFO : token count processed\n",
      "2021-01-14 03:11:45,855 : INFO : frequencies processed\n",
      "2021-01-14 03:11:55,422 : INFO : scalar_distribution processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 03:11:55,423 : INFO : entropies processed\n",
      "2021-01-14 03:11:55,424 : INFO : extropies processed\n",
      "2021-01-14 03:11:55,431 : INFO : token count processed\n",
      "2021-01-14 03:11:55,437 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:11:55,442 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:11:55,444 : INFO : vocab #32006\n",
      "2021-01-14 03:11:55,450 : INFO : diff #set()\n",
      "2021-01-14 03:12:14,075 : INFO : alphabet #32006\n",
      "2021-01-14 03:12:23,254 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.2436361167503447, 0.4457050733558291], [0.9290293008089066, 0.0709707], [2.2516291673878226, 1.2667563532600834], [4.518397116698909, 6.75472436518627, 6.912705806461513, 4.360415675423668, 2.394308689762603, 0.1579814412752425]]\n",
      "2021-01-14 03:12:23,258 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:12:23,259 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:12:23,261 : INFO : built Dictionary(204 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 1029 corpus positions)\n",
      "2021-01-14 03:12:23,369 : INFO : token count processed\n",
      "2021-01-14 03:12:23,398 : INFO : frequencies processed\n",
      "2021-01-14 03:12:32,577 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:12:32,578 : INFO : entropies processed\n",
      "2021-01-14 03:12:32,579 : INFO : extropies processed\n",
      "2021-01-14 03:12:32,585 : INFO : token count processed\n",
      "2021-01-14 03:12:32,589 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:12:32,594 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:12:32,595 : INFO : vocab #32006\n",
      "2021-01-14 03:12:32,603 : INFO : diff #set()\n",
      "2021-01-14 03:12:51,233 : INFO : alphabet #32006\n",
      "2021-01-14 03:13:00,391 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.2486552010466767, 0.44471024260835196], [0.9392974711954594, 0.06070253], [1.584962500721156, 1.1699250014423124], [4.518397116698909, 6.597313085495733, 6.762363903190086, 4.3533462990045555, 2.2439667864911765, 0.16505081769435304]]\n",
      "2021-01-14 03:13:00,396 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:13:00,397 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:13:00,398 : INFO : built Dictionary(228 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 995 corpus positions)\n",
      "2021-01-14 03:13:00,543 : INFO : token count processed\n",
      "2021-01-14 03:13:00,575 : INFO : frequencies processed\n",
      "2021-01-14 03:13:09,744 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:13:09,745 : INFO : entropies processed\n",
      "2021-01-14 03:13:09,746 : INFO : extropies processed\n",
      "2021-01-14 03:13:09,753 : INFO : token count processed\n",
      "2021-01-14 03:13:09,759 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:13:09,765 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:13:09,766 : INFO : vocab #32006\n",
      "2021-01-14 03:13:09,773 : INFO : diff #set()\n",
      "2021-01-14 03:13:28,441 : INFO : alphabet #32006\n",
      "2021-01-14 03:13:37,610 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.2427636755050284, 0.4458784538566324], [0.9354904219508171, 0.06450958], [1.9219280948873623, 1.2148067842293933], [4.518397116698909, 6.659481538516613, 6.835079959093074, 4.342798696122449, 2.3166828423941643, 0.1755984205764607]]\n",
      "2021-01-14 03:13:37,615 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:13:37,616 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:13:37,617 : INFO : built Dictionary(251 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 1157 corpus positions)\n",
      "2021-01-14 03:13:37,778 : INFO : token count processed\n",
      "2021-01-14 03:13:37,837 : INFO : frequencies processed\n",
      "2021-01-14 03:13:47,111 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:13:47,112 : INFO : entropies processed\n",
      "2021-01-14 03:13:47,112 : INFO : extropies processed\n",
      "2021-01-14 03:13:47,119 : INFO : token count processed\n",
      "2021-01-14 03:13:47,123 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:13:47,127 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:13:47,128 : INFO : vocab #32006\n",
      "2021-01-14 03:13:47,135 : INFO : diff #set()\n",
      "2021-01-14 03:14:05,571 : INFO : alphabet #32006\n",
      "2021-01-14 03:14:15,034 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.2335216720656395, 0.44772343716511376], [0.9269338101148605, 0.07306619], [2.5216406363433186, 1.2998438251349493], [4.518397116698909, 6.774682571479102, 6.9122691716596245, 4.380810516518387, 2.393872054960715, 0.13758660018052282]]\n",
      "2021-01-14 03:14:15,048 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:14:15,049 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:14:15,052 : INFO : built Dictionary(426 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 7894 corpus positions)\n",
      "2021-01-14 03:14:15,359 : INFO : token count processed\n",
      "2021-01-14 03:14:15,387 : INFO : frequencies processed\n",
      "2021-01-14 03:14:24,678 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:14:24,679 : INFO : entropies processed\n",
      "2021-01-14 03:14:24,680 : INFO : extropies processed\n",
      "2021-01-14 03:14:24,688 : INFO : token count processed\n",
      "2021-01-14 03:14:24,696 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:14:24,702 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:14:24,703 : INFO : vocab #32006\n",
      "2021-01-14 03:14:24,710 : INFO : diff #set()\n",
      "2021-01-14 03:14:43,056 : INFO : alphabet #32006\n",
      "2021-01-14 03:14:52,311 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.251440444199993, 0.4441600942970229], [0.9463778510689735, 0.05362215], [3.121928094887362, 1.3519647487142497], [4.518397116698909, 6.839453716525233, 6.995665150966096, 4.362185682258045, 2.477268034267187, 0.1562114344408636]]\n",
      "2021-01-14 03:14:52,317 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:14:52,318 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:14:52,320 : INFO : built Dictionary(331 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 2336 corpus positions)\n",
      "2021-01-14 03:14:52,546 : INFO : token count processed\n",
      "2021-01-14 03:14:52,611 : INFO : frequencies processed\n",
      "2021-01-14 03:15:01,906 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:15:01,907 : INFO : entropies processed\n",
      "2021-01-14 03:15:01,908 : INFO : extropies processed\n",
      "2021-01-14 03:15:01,915 : INFO : token count processed\n",
      "2021-01-14 03:15:01,921 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:15:01,928 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:15:01,929 : INFO : vocab #32006\n",
      "2021-01-14 03:15:01,935 : INFO : diff #set()\n",
      "2021-01-14 03:15:20,425 : INFO : alphabet #32006\n",
      "2021-01-14 03:15:29,698 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.2455327138586791, 0.4453286268457963], [0.9313657656311989, 0.068634234], [1.9219280948873623, 1.2148067842293933], [4.518397116698909, 6.86432793886027, 7.00248077916984, 4.380244276389339, 2.4840836624709306, 0.1381528403095702]]\n",
      "2021-01-14 03:15:29,702 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:15:29,703 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:15:29,704 : INFO : built Dictionary(166 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 538 corpus positions)\n",
      "2021-01-14 03:15:29,795 : INFO : token count processed\n",
      "2021-01-14 03:15:29,847 : INFO : frequencies processed\n",
      "2021-01-14 03:15:39,115 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:15:39,116 : INFO : entropies processed\n",
      "2021-01-14 03:15:39,117 : INFO : extropies processed\n",
      "2021-01-14 03:15:39,123 : INFO : token count processed\n",
      "2021-01-14 03:15:39,127 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:15:39,131 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:15:39,132 : INFO : vocab #32006\n",
      "2021-01-14 03:15:39,138 : INFO : diff #set()\n",
      "2021-01-14 03:15:58,009 : INFO : alphabet #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 03:16:07,236 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.2517512295124256, 0.4440987915954336], [0.9383876137435436, 0.061612386], [1.0, 1.0], [4.518397116698909, 6.431978396403875, 6.658425676261, 4.291949836841784, 2.140028559562091, 0.22644727985712532]]\n",
      "2021-01-14 03:16:07,240 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:16:07,241 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:16:07,242 : INFO : built Dictionary(223 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 692 corpus positions)\n",
      "2021-01-14 03:16:07,377 : INFO : token count processed\n",
      "2021-01-14 03:16:07,436 : INFO : frequencies processed\n",
      "2021-01-14 03:16:16,622 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:16:16,623 : INFO : entropies processed\n",
      "2021-01-14 03:16:16,624 : INFO : extropies processed\n",
      "2021-01-14 03:16:16,635 : INFO : token count processed\n",
      "2021-01-14 03:16:16,639 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:16:16,643 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:16:16,644 : INFO : vocab #32006\n",
      "2021-01-14 03:16:16,651 : INFO : diff #set()\n",
      "2021-01-14 03:16:35,194 : INFO : alphabet #32006\n",
      "2021-01-14 03:16:44,586 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/test_auth_utility.py')[[1.2595338268446756, 0.44256916542667973], [0.9401875622570515, 0.059812438], [2.0, 1.2451124978365313], [4.518397116698909, 6.911818353685893, 7.096095919933984, 4.334119550450818, 2.5776988032350747, 0.18427756624809088]]\n",
      "2021-01-14 03:16:44,599 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:16:44,600 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:16:44,602 : INFO : built Dictionary(310 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 7222 corpus positions)\n",
      "2021-01-14 03:16:44,799 : INFO : token count processed\n",
      "2021-01-14 03:16:44,828 : INFO : frequencies processed\n",
      "2021-01-14 03:16:53,993 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:16:53,994 : INFO : entropies processed\n",
      "2021-01-14 03:16:53,995 : INFO : extropies processed\n",
      "2021-01-14 03:16:54,003 : INFO : token count processed\n",
      "2021-01-14 03:16:54,007 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:16:54,011 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:16:54,012 : INFO : vocab #32006\n",
      "2021-01-14 03:16:54,018 : INFO : diff #set()\n",
      "2021-01-14 03:17:12,491 : INFO : alphabet #32006\n",
      "2021-01-14 03:17:21,863 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.2323242894894912, 0.44796358876186815], [0.9191559925675392, 0.08084401], [2.94770277922009, 1.3393100707180505], [4.518397116698909, 6.363791471162389, 6.405848539041624, 4.476340048819674, 1.887451422342715, 0.042057067879235355]]\n",
      "2021-01-14 03:17:21,868 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:17:21,868 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:17:21,870 : INFO : built Dictionary(213 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 1292 corpus positions)\n",
      "2021-01-14 03:17:21,985 : INFO : token count processed\n",
      "2021-01-14 03:17:22,040 : INFO : frequencies processed\n",
      "2021-01-14 03:17:31,226 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:17:31,228 : INFO : entropies processed\n",
      "2021-01-14 03:17:31,228 : INFO : extropies processed\n",
      "2021-01-14 03:17:31,235 : INFO : token count processed\n",
      "2021-01-14 03:17:31,239 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:17:31,246 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:17:31,247 : INFO : vocab #32006\n",
      "2021-01-14 03:17:31,256 : INFO : diff #set()\n",
      "2021-01-14 03:17:49,761 : INFO : alphabet #32006\n",
      "2021-01-14 03:17:58,924 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.2326925475625836, 0.44788970209610524], [0.9414359368383884, 0.058564063], [2.521640636343318, 1.2998438251349493], [4.518397116698909, 6.29000629755059, 6.537767051848237, 4.270636362401263, 2.019369935149328, 0.247760754297647]]\n",
      "2021-01-14 03:17:58,928 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:17:58,929 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:17:58,931 : INFO : built Dictionary(223 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 1245 corpus positions)\n",
      "2021-01-14 03:17:59,052 : INFO : token count processed\n",
      "2021-01-14 03:17:59,080 : INFO : frequencies processed\n",
      "2021-01-14 03:18:08,588 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:18:08,589 : INFO : entropies processed\n",
      "2021-01-14 03:18:08,589 : INFO : extropies processed\n",
      "2021-01-14 03:18:08,596 : INFO : token count processed\n",
      "2021-01-14 03:18:08,601 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:18:08,606 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:18:08,607 : INFO : vocab #32006\n",
      "2021-01-14 03:18:08,615 : INFO : diff #set()\n",
      "2021-01-14 03:18:26,974 : INFO : alphabet #32006\n",
      "2021-01-14 03:18:36,138 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.2453346746668466, 0.44536790496426815], [0.9408875778317451, 0.059112422], [2.521640636343318, 1.2998438251349493], [4.518397116698909, 6.361621244785958, 6.598464245483502, 4.281554116001367, 2.0800671287845924, 0.23684300069754372]]\n",
      "2021-01-14 03:18:36,143 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:18:36,144 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:18:36,147 : INFO : built Dictionary(231 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 1268 corpus positions)\n",
      "2021-01-14 03:18:36,282 : INFO : token count processed\n",
      "2021-01-14 03:18:36,309 : INFO : frequencies processed\n",
      "2021-01-14 03:18:45,581 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:18:45,582 : INFO : entropies processed\n",
      "2021-01-14 03:18:45,582 : INFO : extropies processed\n",
      "2021-01-14 03:18:45,589 : INFO : token count processed\n",
      "2021-01-14 03:18:45,595 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:18:45,599 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:18:45,600 : INFO : vocab #32006\n",
      "2021-01-14 03:18:45,607 : INFO : diff #set()\n",
      "2021-01-14 03:19:04,132 : INFO : alphabet #32006\n",
      "2021-01-14 03:19:13,295 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.1900961983356726, 0.4566009478304804], [0.8946545943617821, 0.105345406], [2.521640636343318, 1.2998438251349493], [4.518397116698909, 6.620594433343389, 6.85428764709048, 4.284703902951819, 2.3358905303915707, 0.23369321374709084]]\n",
      "2021-01-14 03:19:13,300 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:19:13,301 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:19:13,303 : INFO : built Dictionary(205 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 1361 corpus positions)\n",
      "2021-01-14 03:19:13,422 : INFO : token count processed\n",
      "2021-01-14 03:19:13,454 : INFO : frequencies processed\n",
      "2021-01-14 03:19:22,723 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:19:22,724 : INFO : entropies processed\n",
      "2021-01-14 03:19:22,725 : INFO : extropies processed\n",
      "2021-01-14 03:19:22,732 : INFO : token count processed\n",
      "2021-01-14 03:19:22,739 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:19:22,744 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:19:22,745 : INFO : vocab #32006\n",
      "2021-01-14 03:19:22,752 : INFO : diff #set()\n",
      "2021-01-14 03:19:41,560 : INFO : alphabet #32006\n",
      "2021-01-14 03:19:50,857 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.1894876691249285, 0.4567278519543659], [0.8989906162023544, 0.101009384], [1.9219280948873623, 1.2148067842293933], [4.518397116698909, 6.207411496248084, 6.367059148295397, 4.358749464651597, 1.8486620315964872, 0.15964765204731268]]\n",
      "2021-01-14 03:19:50,860 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 03:19:50,861 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:19:50,862 : INFO : built Dictionary(148 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 265 corpus positions)\n",
      "2021-01-14 03:19:50,931 : INFO : token count processed\n",
      "2021-01-14 03:19:50,958 : INFO : frequencies processed\n",
      "2021-01-14 03:20:00,302 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:20:00,303 : INFO : entropies processed\n",
      "2021-01-14 03:20:00,304 : INFO : extropies processed\n",
      "2021-01-14 03:20:00,310 : INFO : token count processed\n",
      "2021-01-14 03:20:00,314 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:20:00,318 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:20:00,319 : INFO : vocab #32006\n",
      "2021-01-14 03:20:00,325 : INFO : diff #set()\n",
      "2021-01-14 03:20:18,816 : INFO : alphabet #32006\n",
      "2021-01-14 03:20:28,176 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.252443441598421, 0.44396231289623916], [0.9295104071497917, 0.07048959], [0.0, 0.0], [4.518397116698909, 6.5805228788529595, 6.874342607120076, 4.2245773884317925, 2.355945490421167, 0.2938197282671169]]\n",
      "2021-01-14 03:20:28,180 : INFO : Removed 1 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:20:28,181 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:20:28,183 : INFO : built Dictionary(224 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 1226 corpus positions)\n",
      "2021-01-14 03:20:28,318 : INFO : token count processed\n",
      "2021-01-14 03:20:28,367 : INFO : frequencies processed\n",
      "2021-01-14 03:20:37,579 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:20:37,581 : INFO : entropies processed\n",
      "2021-01-14 03:20:37,582 : INFO : extropies processed\n",
      "2021-01-14 03:20:37,594 : INFO : token count processed\n",
      "2021-01-14 03:20:37,598 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:20:37,602 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:20:37,603 : INFO : vocab #32006\n",
      "2021-01-14 03:20:37,609 : INFO : diff #set()\n",
      "2021-01-14 03:20:56,090 : INFO : alphabet #32006\n",
      "2021-01-14 03:21:05,418 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.2371418495023505, 0.446998924195374], [0.9373434484004974, 0.06265655], [2.94770277922009, 1.3393100707180505], [4.518397116698909, 6.422089779976135, 6.5474740250144325, 4.393012871660611, 2.029076908315523, 0.1253842450382976]]\n",
      "2021-01-14 03:21:05,423 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:21:05,423 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:21:05,425 : INFO : built Dictionary(239 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 1508 corpus positions)\n",
      "2021-01-14 03:21:05,578 : INFO : token count processed\n",
      "2021-01-14 03:21:05,632 : INFO : frequencies processed\n",
      "2021-01-14 03:21:14,829 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:21:14,830 : INFO : entropies processed\n",
      "2021-01-14 03:21:14,831 : INFO : extropies processed\n",
      "2021-01-14 03:21:14,837 : INFO : token count processed\n",
      "2021-01-14 03:21:14,841 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:21:14,845 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:21:14,846 : INFO : vocab #32006\n",
      "2021-01-14 03:21:14,856 : INFO : diff #set()\n",
      "2021-01-14 03:21:33,330 : INFO : alphabet #32006\n",
      "2021-01-14 03:21:42,506 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.2451411185143828, 0.4454063006345469], [0.9373412802815437, 0.06265872], [2.2516291673878226, 1.2667563532600834], [4.518397116698909, 6.485445644653597, 6.752873686541229, 4.250969074811276, 2.23447656984232, 0.2674280418876327]]\n",
      "2021-01-14 03:21:42,511 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:21:42,512 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:21:42,514 : INFO : built Dictionary(216 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 1394 corpus positions)\n",
      "2021-01-14 03:21:42,636 : INFO : token count processed\n",
      "2021-01-14 03:21:42,668 : INFO : frequencies processed\n",
      "2021-01-14 03:21:52,029 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:21:52,030 : INFO : entropies processed\n",
      "2021-01-14 03:21:52,031 : INFO : extropies processed\n",
      "2021-01-14 03:21:52,038 : INFO : token count processed\n",
      "2021-01-14 03:21:52,042 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:21:52,049 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:21:52,050 : INFO : vocab #32006\n",
      "2021-01-14 03:21:52,060 : INFO : diff #set()\n",
      "2021-01-14 03:22:10,555 : INFO : alphabet #32006\n",
      "2021-01-14 03:22:19,718 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.2395145760111748, 0.446525336656264], [0.9280669391155243, 0.07193306], [1.5, 1.1225562489182657], [4.518397116698909, 6.2276600107346916, 6.392013281687529, 4.354043845746073, 1.8736161649886194, 0.16435327095283725]]\n",
      "2021-01-14 03:22:19,723 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:22:19,724 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:22:19,725 : INFO : built Dictionary(205 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 1592 corpus positions)\n",
      "2021-01-14 03:22:19,847 : INFO : token count processed\n",
      "2021-01-14 03:22:19,875 : INFO : frequencies processed\n",
      "2021-01-14 03:22:29,197 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:22:29,198 : INFO : entropies processed\n",
      "2021-01-14 03:22:29,199 : INFO : extropies processed\n",
      "2021-01-14 03:22:29,206 : INFO : token count processed\n",
      "2021-01-14 03:22:29,214 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:22:29,219 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:22:29,221 : INFO : vocab #32006\n",
      "2021-01-14 03:22:29,228 : INFO : diff #set()\n",
      "2021-01-14 03:22:47,622 : INFO : alphabet #32006\n",
      "2021-01-14 03:22:56,772 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.2562382012179767, 0.4432156141404634], [0.9514220207929611, 0.04857798], [1.9219280948873623, 1.2148067842293933], [4.518397116698909, 6.253918170574241, 6.4631605404199455, 4.3091547468532045, 1.944763423721036, 0.20924236984570488]]\n",
      "2021-01-14 03:22:56,776 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:22:56,777 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:22:56,778 : INFO : built Dictionary(179 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 689 corpus positions)\n",
      "2021-01-14 03:22:56,875 : INFO : token count processed\n",
      "2021-01-14 03:22:56,926 : INFO : frequencies processed\n",
      "2021-01-14 03:23:06,212 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:23:06,213 : INFO : entropies processed\n",
      "2021-01-14 03:23:06,214 : INFO : extropies processed\n",
      "2021-01-14 03:23:06,226 : INFO : token count processed\n",
      "2021-01-14 03:23:06,230 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:23:06,233 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:23:06,234 : INFO : vocab #32006\n",
      "2021-01-14 03:23:06,242 : INFO : diff #set()\n",
      "2021-01-14 03:23:24,685 : INFO : alphabet #32006\n",
      "2021-01-14 03:23:33,854 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.2412666041672513, 0.44617628181344926], [0.9235898330807686, 0.07641017], [1.0, 1.0], [4.518397116698909, 6.374522245625576, 6.6401850549578025, 4.252734307366682, 2.121787938258893, 0.26566280933222686]]\n",
      "2021-01-14 03:23:33,859 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:23:33,860 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:23:33,862 : INFO : built Dictionary(296 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 1971 corpus positions)\n",
      "2021-01-14 03:23:34,047 : INFO : token count processed\n",
      "2021-01-14 03:23:34,080 : INFO : frequencies processed\n",
      "2021-01-14 03:23:43,354 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:23:43,355 : INFO : entropies processed\n",
      "2021-01-14 03:23:43,356 : INFO : extropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 03:23:43,362 : INFO : token count processed\n",
      "2021-01-14 03:23:43,370 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:23:43,374 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:23:43,375 : INFO : vocab #32006\n",
      "2021-01-14 03:23:43,383 : INFO : diff #set()\n",
      "2021-01-14 03:24:02,054 : INFO : alphabet #32006\n",
      "2021-01-14 03:24:11,373 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.2151246578227646, 0.451441861959541], [0.8757209330797195, 0.12427907], [2.75, 1.3226647836567116], [4.518397116698909, 6.731238669067808, 7.013031327060045, 4.2366044587066725, 2.494634210361135, 0.28179265799223696]]\n",
      "2021-01-14 03:24:11,378 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:24:11,379 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:24:11,380 : INFO : built Dictionary(220 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 1363 corpus positions)\n",
      "2021-01-14 03:24:11,508 : INFO : token count processed\n",
      "2021-01-14 03:24:11,536 : INFO : frequencies processed\n",
      "2021-01-14 03:24:20,696 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:24:20,698 : INFO : entropies processed\n",
      "2021-01-14 03:24:20,698 : INFO : extropies processed\n",
      "2021-01-14 03:24:20,705 : INFO : token count processed\n",
      "2021-01-14 03:24:20,712 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:24:20,717 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:24:20,718 : INFO : vocab #32006\n",
      "2021-01-14 03:24:20,725 : INFO : diff #set()\n",
      "2021-01-14 03:24:39,281 : INFO : alphabet #32006\n",
      "2021-01-14 03:24:48,473 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.1532127207719076, 0.46442229806329066], [0.868255615234375, 0.13174438], [2.94770277922009, 1.3393100707180505], [4.518397116698909, 6.503741451859337, 6.701759622860386, 4.320378945697861, 2.183362506161477, 0.19801817100104913]]\n",
      "2021-01-14 03:24:48,478 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:24:48,479 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:24:48,480 : INFO : built Dictionary(243 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 1685 corpus positions)\n",
      "2021-01-14 03:24:48,622 : INFO : token count processed\n",
      "2021-01-14 03:24:48,653 : INFO : frequencies processed\n",
      "2021-01-14 03:24:57,802 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:24:57,803 : INFO : entropies processed\n",
      "2021-01-14 03:24:57,804 : INFO : extropies processed\n",
      "2021-01-14 03:24:57,811 : INFO : token count processed\n",
      "2021-01-14 03:24:57,815 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:24:57,819 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:24:57,820 : INFO : vocab #32006\n",
      "2021-01-14 03:24:57,826 : INFO : diff #set()\n",
      "2021-01-14 03:25:16,284 : INFO : alphabet #32006\n",
      "2021-01-14 03:25:25,611 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.2420897559827715, 0.44601247444782677], [0.947108581662178, 0.05289142], [2.75, 1.3226647836567116], [4.518397116698909, 6.334729224484471, 6.488706594338742, 4.3644197468446375, 1.9703094776398329, 0.15397736985427102]]\n",
      "2021-01-14 03:25:25,616 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:25:25,617 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:25:25,619 : INFO : built Dictionary(242 unique tokens: ['28', '29', '3', '4)', '7']...) from 2 documents (total 2034 corpus positions)\n",
      "2021-01-14 03:25:25,754 : INFO : token count processed\n",
      "2021-01-14 03:25:25,783 : INFO : frequencies processed\n",
      "2021-01-14 03:25:34,956 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:25:34,957 : INFO : entropies processed\n",
      "2021-01-14 03:25:34,957 : INFO : extropies processed\n",
      "2021-01-14 03:25:34,964 : INFO : token count processed\n",
      "2021-01-14 03:25:34,968 : INFO : alphabet_source #32006\n",
      "2021-01-14 03:25:34,972 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:25:34,973 : INFO : vocab #32006\n",
      "2021-01-14 03:25:34,979 : INFO : diff #set()\n",
      "2021-01-14 03:25:53,457 : INFO : alphabet #32006\n",
      "2021-01-14 03:26:02,737 : INFO : Computed distances or similarities ('295', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.2355411145533053, 0.4473189929230244], [0.9365982338786125, 0.063401766], [2.2516291673878226, 1.2667563532600834], [4.518397116698909, 6.21319712067992, 6.429283246364788, 4.302310991014041, 1.9108861296658786, 0.21608612568486762]]\n",
      "2021-01-14 03:26:02,742 : INFO : Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:26:02,743 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:26:02,745 : INFO : built Dictionary(399 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1712 corpus positions)\n",
      "2021-01-14 03:26:04,189 : INFO : token count processed\n",
      "2021-01-14 03:26:04,220 : INFO : frequencies processed\n",
      "2021-01-14 03:26:13,476 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:26:13,477 : INFO : entropies processed\n",
      "2021-01-14 03:26:13,477 : INFO : extropies processed\n",
      "2021-01-14 03:26:13,484 : INFO : token count processed\n",
      "2021-01-14 03:26:13,491 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:26:13,496 : INFO : alphabet_target #32010\n",
      "2021-01-14 03:26:13,497 : INFO : vocab #32006\n",
      "2021-01-14 03:26:13,504 : INFO : diff #{'```'}\n",
      "2021-01-14 03:26:32,000 : INFO : alphabet #32006\n",
      "2021-01-14 03:26:41,158 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/auth_utility.py')[[0.9127231923922128, 0.5228148035102328], [0.38151001930236816, 0.61849], [4.8844091241711, 1.4003158473601292], [7.0153361396447735, 6.905617163738059, 7.444780620737308, 6.476172682645524, 0.42944448109253486, 0.5391634569992494]]\n",
      "2021-01-14 03:26:41,165 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:26:41,166 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:26:41,168 : INFO : built Dictionary(470 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 2602 corpus positions)\n",
      "2021-01-14 03:26:43,172 : INFO : token count processed\n",
      "2021-01-14 03:26:43,197 : INFO : frequencies processed\n",
      "2021-01-14 03:26:52,588 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:26:52,590 : INFO : entropies processed\n",
      "2021-01-14 03:26:52,590 : INFO : extropies processed\n",
      "2021-01-14 03:26:52,597 : INFO : token count processed\n",
      "2021-01-14 03:26:52,605 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:26:52,611 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:26:52,612 : INFO : vocab #32006\n",
      "2021-01-14 03:26:52,620 : INFO : diff #{'```'}\n",
      "2021-01-14 03:27:11,408 : INFO : alphabet #32006\n",
      "2021-01-14 03:27:20,591 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[0.8920845030909786, 0.5285176208390076], [0.3712965250015259, 0.6287035], [5.487254599785193, 1.41420338737372], [7.0153361396447735, 7.1219284286457345, 7.59568026440585, 6.541584303884658, 0.5803441247610763, 0.47375183576011537]]\n",
      "2021-01-14 03:27:20,598 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:27:20,599 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:27:20,601 : INFO : built Dictionary(383 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 2585 corpus positions)\n",
      "2021-01-14 03:27:21,884 : INFO : token count processed\n",
      "2021-01-14 03:27:21,911 : INFO : frequencies processed\n",
      "2021-01-14 03:27:31,186 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:27:31,187 : INFO : entropies processed\n",
      "2021-01-14 03:27:31,188 : INFO : extropies processed\n",
      "2021-01-14 03:27:31,195 : INFO : token count processed\n",
      "2021-01-14 03:27:31,199 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:27:31,206 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:27:31,208 : INFO : vocab #32006\n",
      "2021-01-14 03:27:31,216 : INFO : diff #{'```'}\n",
      "2021-01-14 03:27:49,615 : INFO : alphabet #32006\n",
      "2021-01-14 03:27:59,011 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[0.9029594946424635, 0.5254972598288985], [0.38057684898376465, 0.61942315], [5.601352600084602, 1.4157485543204897], [7.0153361396447735, 6.41099024988467, 6.842021009572892, 6.584305379956551, -0.17331513007188182, 0.43103075968822147]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 03:27:59,016 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:27:59,017 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:27:59,020 : INFO : built Dictionary(285 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 984 corpus positions)\n",
      "2021-01-14 03:27:59,670 : INFO : token count processed\n",
      "2021-01-14 03:27:59,702 : INFO : frequencies processed\n",
      "2021-01-14 03:28:09,071 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:28:09,072 : INFO : entropies processed\n",
      "2021-01-14 03:28:09,073 : INFO : extropies processed\n",
      "2021-01-14 03:28:09,086 : INFO : token count processed\n",
      "2021-01-14 03:28:09,090 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:28:09,094 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:28:09,095 : INFO : vocab #32006\n",
      "2021-01-14 03:28:09,101 : INFO : diff #{'```'}\n",
      "2021-01-14 03:28:27,471 : INFO : alphabet #32006\n",
      "2021-01-14 03:28:36,750 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[0.8491269348481367, 0.5407957567186306], [0.3376085162162781, 0.6623915], [5.040522695771716, 1.400185786461847], [7.0153361396447735, 6.077866832717642, 6.963807700694609, 6.129395271667807, -0.05152843895016446, 0.8859408679769674]]\n",
      "2021-01-14 03:28:36,754 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:28:36,755 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:28:36,756 : INFO : built Dictionary(265 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 749 corpus positions)\n",
      "2021-01-14 03:28:37,277 : INFO : token count processed\n",
      "2021-01-14 03:28:37,306 : INFO : frequencies processed\n",
      "2021-01-14 03:28:46,629 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:28:46,630 : INFO : entropies processed\n",
      "2021-01-14 03:28:46,631 : INFO : extropies processed\n",
      "2021-01-14 03:28:46,638 : INFO : token count processed\n",
      "2021-01-14 03:28:46,645 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:28:46,651 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:28:46,652 : INFO : vocab #32006\n",
      "2021-01-14 03:28:46,658 : INFO : diff #{'```'}\n",
      "2021-01-14 03:29:05,180 : INFO : alphabet #32006\n",
      "2021-01-14 03:29:14,505 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[0.8606518711340296, 0.5374460507706476], [0.3439008593559265, 0.65609914], [4.940336540814903, 1.3942112093419592], [7.0153361396447735, 5.977547459003844, 7.008424332757547, 5.984459265891071, -0.006911806887226923, 1.0308768737537024]]\n",
      "2021-01-14 03:29:14,511 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:29:14,511 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:29:14,513 : INFO : built Dictionary(355 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 2460 corpus positions)\n",
      "2021-01-14 03:29:15,572 : INFO : token count processed\n",
      "2021-01-14 03:29:15,599 : INFO : frequencies processed\n",
      "2021-01-14 03:29:24,829 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:29:24,830 : INFO : entropies processed\n",
      "2021-01-14 03:29:24,831 : INFO : extropies processed\n",
      "2021-01-14 03:29:24,845 : INFO : token count processed\n",
      "2021-01-14 03:29:24,849 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:29:24,853 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:29:24,854 : INFO : vocab #32006\n",
      "2021-01-14 03:29:24,861 : INFO : diff #{'```'}\n",
      "2021-01-14 03:29:43,498 : INFO : alphabet #32006\n",
      "2021-01-14 03:29:52,651 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[0.8820948406784305, 0.5313228528056186], [0.3548870086669922, 0.645113], [5.309259905459978, 1.4096134097785504], [7.0153361396447735, 6.4614394051846435, 6.938972363103973, 6.537803181725444, -0.07636377654080029, 0.4775329579193297]]\n",
      "2021-01-14 03:29:52,656 : INFO : Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:29:52,657 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:29:52,659 : INFO : built Dictionary(346 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1684 corpus positions)\n",
      "2021-01-14 03:29:53,633 : INFO : token count processed\n",
      "2021-01-14 03:29:53,675 : INFO : frequencies processed\n",
      "2021-01-14 03:30:02,831 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:30:02,832 : INFO : entropies processed\n",
      "2021-01-14 03:30:02,833 : INFO : extropies processed\n",
      "2021-01-14 03:30:02,840 : INFO : token count processed\n",
      "2021-01-14 03:30:02,847 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:30:02,851 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:30:02,852 : INFO : vocab #32006\n",
      "2021-01-14 03:30:02,859 : INFO : diff #{'```'}\n",
      "2021-01-14 03:30:21,620 : INFO : alphabet #32006\n",
      "2021-01-14 03:30:30,815 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[0.9751836462202428, 0.5062820370721595], [0.40970391035079956, 0.5902961], [4.468683767539514, 1.3785027598578041], [7.0153361396447735, 6.327195724598159, 7.02983491833956, 6.312696945903372, 0.014498778694786552, 0.7026391937414012]]\n",
      "2021-01-14 03:30:30,827 : INFO : Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:30:30,828 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:30:30,831 : INFO : built Dictionary(507 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 6576 corpus positions)\n",
      "2021-01-14 03:30:33,065 : INFO : token count processed\n",
      "2021-01-14 03:30:33,093 : INFO : frequencies processed\n",
      "2021-01-14 03:30:42,244 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:30:42,245 : INFO : entropies processed\n",
      "2021-01-14 03:30:42,246 : INFO : extropies processed\n",
      "2021-01-14 03:30:42,254 : INFO : token count processed\n",
      "2021-01-14 03:30:42,261 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:30:42,266 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:30:42,268 : INFO : vocab #32006\n",
      "2021-01-14 03:30:42,274 : INFO : diff #{'```'}\n",
      "2021-01-14 03:31:00,908 : INFO : alphabet #32006\n",
      "2021-01-14 03:31:10,100 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[0.8408511040329623, 0.5432269876739004], [0.3074772357940674, 0.69252276], [5.870093411883781, 1.4210401054090567], [7.0153361396447735, 6.9079058562486315, 7.150450338882769, 6.772791657010637, 0.1351141992379956, 0.24254448263413764]]\n",
      "2021-01-14 03:31:10,106 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:31:10,107 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:31:10,109 : INFO : built Dictionary(444 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 2970 corpus positions)\n",
      "2021-01-14 03:31:11,785 : INFO : token count processed\n",
      "2021-01-14 03:31:11,824 : INFO : frequencies processed\n",
      "2021-01-14 03:31:21,095 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:31:21,096 : INFO : entropies processed\n",
      "2021-01-14 03:31:21,097 : INFO : extropies processed\n",
      "2021-01-14 03:31:21,104 : INFO : token count processed\n",
      "2021-01-14 03:31:21,108 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:31:21,112 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:31:21,113 : INFO : vocab #32006\n",
      "2021-01-14 03:31:21,123 : INFO : diff #{'```'}\n",
      "2021-01-14 03:31:39,550 : INFO : alphabet #32006\n",
      "2021-01-14 03:31:48,914 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[0.9183410213083841, 0.5212837492876843], [0.3866700530052185, 0.61332995], [5.14236228298175, 1.4084609727294628], [7.0153361396447735, 6.61034830706307, 7.083935181815498, 6.541749264892346, 0.06859904217072454, 0.4735868747524279]]\n",
      "2021-01-14 03:31:48,918 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:31:48,919 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:31:48,921 : INFO : built Dictionary(349 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1017 corpus positions)\n",
      "2021-01-14 03:31:49,911 : INFO : token count processed\n",
      "2021-01-14 03:31:49,944 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 03:31:59,412 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:31:59,413 : INFO : entropies processed\n",
      "2021-01-14 03:31:59,414 : INFO : extropies processed\n",
      "2021-01-14 03:31:59,420 : INFO : token count processed\n",
      "2021-01-14 03:31:59,424 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:31:59,428 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:31:59,429 : INFO : vocab #32006\n",
      "2021-01-14 03:31:59,436 : INFO : diff #{'```'}\n",
      "2021-01-14 03:32:17,831 : INFO : alphabet #32006\n",
      "2021-01-14 03:32:26,993 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[0.9872402406687041, 0.5032104219384674], [0.5190221965312958, 0.4809778], [4.4564794265302154, 1.3794327932427706], [7.0153361396447735, 6.616715366949855, 7.44456437889213, 6.187487127702498, 0.4292282392473563, 0.8278490119422752]]\n",
      "2021-01-14 03:32:27,000 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:32:27,001 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:32:27,003 : INFO : built Dictionary(525 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 3051 corpus positions)\n",
      "2021-01-14 03:32:29,562 : INFO : token count processed\n",
      "2021-01-14 03:32:29,620 : INFO : frequencies processed\n",
      "2021-01-14 03:32:38,955 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:32:38,956 : INFO : entropies processed\n",
      "2021-01-14 03:32:38,957 : INFO : extropies processed\n",
      "2021-01-14 03:32:38,968 : INFO : token count processed\n",
      "2021-01-14 03:32:38,973 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:32:38,978 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:32:38,979 : INFO : vocab #32006\n",
      "2021-01-14 03:32:38,985 : INFO : diff #{'```'}\n",
      "2021-01-14 03:32:57,570 : INFO : alphabet #32006\n",
      "2021-01-14 03:33:06,854 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[0.8550674997145435, 0.5390639425001404], [0.3712587356567383, 0.62874126], [5.707027662365066, 1.4186486145961972], [7.0153361396447735, 7.32185870753746, 7.716570767404685, 6.620624079777549, 0.7012346277599111, 0.39471205986722424]]\n",
      "2021-01-14 03:33:06,858 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:33:06,859 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:33:06,861 : INFO : built Dictionary(229 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 416 corpus positions)\n",
      "2021-01-14 03:33:07,125 : INFO : token count processed\n",
      "2021-01-14 03:33:07,153 : INFO : frequencies processed\n",
      "2021-01-14 03:33:16,311 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:33:16,312 : INFO : entropies processed\n",
      "2021-01-14 03:33:16,313 : INFO : extropies processed\n",
      "2021-01-14 03:33:16,320 : INFO : token count processed\n",
      "2021-01-14 03:33:16,324 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:33:16,328 : INFO : alphabet_target #32008\n",
      "2021-01-14 03:33:16,329 : INFO : vocab #32006\n",
      "2021-01-14 03:33:16,335 : INFO : diff #{'```'}\n",
      "2021-01-14 03:33:35,122 : INFO : alphabet #32006\n",
      "2021-01-14 03:33:44,298 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/fireException.py')[[1.0952970817630197, 0.47725929115435145], [0.5747200846672058, 0.42527992], [3.2089873002587996, 1.333515948269913], [7.0153361396447735, 5.176618657501385, 7.219252913625067, 4.9727018835210925, 0.20391677398029362, 2.042634256123682]]\n",
      "2021-01-14 03:33:44,302 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:33:44,303 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:33:44,306 : INFO : built Dictionary(303 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 809 corpus positions)\n",
      "2021-01-14 03:33:45,049 : INFO : token count processed\n",
      "2021-01-14 03:33:45,075 : INFO : frequencies processed\n",
      "2021-01-14 03:33:54,213 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:33:54,214 : INFO : entropies processed\n",
      "2021-01-14 03:33:54,215 : INFO : extropies processed\n",
      "2021-01-14 03:33:54,221 : INFO : token count processed\n",
      "2021-01-14 03:33:54,228 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:33:54,233 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:33:54,234 : INFO : vocab #32006\n",
      "2021-01-14 03:33:54,241 : INFO : diff #{'```'}\n",
      "2021-01-14 03:34:12,899 : INFO : alphabet #32006\n",
      "2021-01-14 03:34:22,068 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[0.9730609108620332, 0.5068267251633395], [0.46249961853027344, 0.5375004], [4.316388570616585, 1.3828925671466095], [7.0153361396447735, 6.468846789852156, 7.419828144647079, 6.064354784849851, 0.40449200500230553, 0.9509813547949229]]\n",
      "2021-01-14 03:34:22,075 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:34:22,076 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:34:22,078 : INFO : built Dictionary(466 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 2858 corpus positions)\n",
      "2021-01-14 03:34:24,256 : INFO : token count processed\n",
      "2021-01-14 03:34:24,289 : INFO : frequencies processed\n",
      "2021-01-14 03:34:33,434 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:34:33,435 : INFO : entropies processed\n",
      "2021-01-14 03:34:33,436 : INFO : extropies processed\n",
      "2021-01-14 03:34:33,449 : INFO : token count processed\n",
      "2021-01-14 03:34:33,455 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:34:33,459 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:34:33,460 : INFO : vocab #32006\n",
      "2021-01-14 03:34:33,466 : INFO : diff #{'```'}\n",
      "2021-01-14 03:34:52,037 : INFO : alphabet #32006\n",
      "2021-01-14 03:35:01,208 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[0.8677839995123491, 0.5353938144138108], [0.37070131301879883, 0.6292987], [5.763181548182598, 1.419080867235623], [7.0153361396447735, 6.957796704012729, 7.347923108054116, 6.625209735603387, 0.3325869684093421, 0.39012640404138654]]\n",
      "2021-01-14 03:35:01,215 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:35:01,216 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:35:01,218 : INFO : built Dictionary(394 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 3355 corpus positions)\n",
      "2021-01-14 03:35:02,538 : INFO : token count processed\n",
      "2021-01-14 03:35:02,565 : INFO : frequencies processed\n",
      "2021-01-14 03:35:11,723 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:35:11,724 : INFO : entropies processed\n",
      "2021-01-14 03:35:11,725 : INFO : extropies processed\n",
      "2021-01-14 03:35:11,741 : INFO : token count processed\n",
      "2021-01-14 03:35:11,745 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:35:11,749 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:35:11,750 : INFO : vocab #32006\n",
      "2021-01-14 03:35:11,756 : INFO : diff #{'```'}\n",
      "2021-01-14 03:35:30,230 : INFO : alphabet #32006\n",
      "2021-01-14 03:35:39,569 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[0.8491274453019514, 0.5407956074313234], [0.3727009892463684, 0.627299], [5.284224252493628, 1.4104113739337019], [7.0153361396447735, 6.441859572014148, 6.869468725005542, 6.58772698665338, -0.14586741463923136, 0.4276091529913941]]\n",
      "2021-01-14 03:35:39,574 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:35:39,575 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:35:39,577 : INFO : built Dictionary(421 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1831 corpus positions)\n",
      "2021-01-14 03:35:41,181 : INFO : token count processed\n",
      "2021-01-14 03:35:41,209 : INFO : frequencies processed\n",
      "2021-01-14 03:35:50,479 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:35:50,480 : INFO : entropies processed\n",
      "2021-01-14 03:35:50,481 : INFO : extropies processed\n",
      "2021-01-14 03:35:50,488 : INFO : token count processed\n",
      "2021-01-14 03:35:50,492 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:35:50,496 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:35:50,497 : INFO : vocab #32006\n",
      "2021-01-14 03:35:50,503 : INFO : diff #{'```'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 03:36:09,221 : INFO : alphabet #32006\n",
      "2021-01-14 03:36:18,593 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[0.8883541801317842, 0.5295616736105152], [0.36182093620300293, 0.63817906], [5.370835038779332, 1.4123490540038415], [7.0153361396447735, 6.998955278238291, 7.506186035893072, 6.508105381989993, 0.49084989624829856, 0.5072307576547814]]\n",
      "2021-01-14 03:36:18,598 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:36:18,599 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:36:18,600 : INFO : built Dictionary(340 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1650 corpus positions)\n",
      "2021-01-14 03:36:19,582 : INFO : token count processed\n",
      "2021-01-14 03:36:19,610 : INFO : frequencies processed\n",
      "2021-01-14 03:36:28,914 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:36:28,915 : INFO : entropies processed\n",
      "2021-01-14 03:36:28,916 : INFO : extropies processed\n",
      "2021-01-14 03:36:28,923 : INFO : token count processed\n",
      "2021-01-14 03:36:28,927 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:36:28,936 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:36:28,937 : INFO : vocab #32006\n",
      "2021-01-14 03:36:28,945 : INFO : diff #{'```'}\n",
      "2021-01-14 03:36:47,306 : INFO : alphabet #32006\n",
      "2021-01-14 03:36:56,530 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[0.8695901433896932, 0.5348765896823421], [0.3375178575515747, 0.66248214], [5.1944808645901706, 1.4059914189084797], [7.0153361396447735, 6.492983191376071, 7.080445617602578, 6.427873713418267, 0.06510947795780453, 0.5874624262265069]]\n",
      "2021-01-14 03:36:56,537 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:36:56,538 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:36:56,540 : INFO : built Dictionary(526 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 3589 corpus positions)\n",
      "2021-01-14 03:36:59,014 : INFO : token count processed\n",
      "2021-01-14 03:36:59,041 : INFO : frequencies processed\n",
      "2021-01-14 03:37:08,295 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:37:08,296 : INFO : entropies processed\n",
      "2021-01-14 03:37:08,296 : INFO : extropies processed\n",
      "2021-01-14 03:37:08,304 : INFO : token count processed\n",
      "2021-01-14 03:37:08,311 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:37:08,316 : INFO : alphabet_target #32008\n",
      "2021-01-14 03:37:08,317 : INFO : vocab #32006\n",
      "2021-01-14 03:37:08,324 : INFO : diff #{'```'}\n",
      "2021-01-14 03:37:26,696 : INFO : alphabet #32006\n",
      "2021-01-14 03:37:36,176 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[0.8340443383568122, 0.5452430887771975], [0.2838398814201355, 0.7161601], [5.7160525380684515, 1.4162036484978222], [7.0153361396447735, 6.560342487747443, 6.997319775657104, 6.578358851735112, -0.018016363987669592, 0.43697728790966117]]\n",
      "2021-01-14 03:37:36,184 : INFO : Removed 3 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:37:36,184 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:37:36,187 : INFO : built Dictionary(543 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 3794 corpus positions)\n",
      "2021-01-14 03:37:38,914 : INFO : token count processed\n",
      "2021-01-14 03:37:38,942 : INFO : frequencies processed\n",
      "2021-01-14 03:37:48,089 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:37:48,090 : INFO : entropies processed\n",
      "2021-01-14 03:37:48,091 : INFO : extropies processed\n",
      "2021-01-14 03:37:48,099 : INFO : token count processed\n",
      "2021-01-14 03:37:48,103 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:37:48,107 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:37:48,108 : INFO : vocab #32006\n",
      "2021-01-14 03:37:48,118 : INFO : diff #{'```'}\n",
      "2021-01-14 03:38:06,603 : INFO : alphabet #32006\n",
      "2021-01-14 03:38:15,810 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[0.8836394692003771, 0.5308871556107866], [0.3604954481124878, 0.63950455], [5.775125379573469, 1.4195827266668424], [7.0153361396447735, 7.046173750105238, 7.429244426890683, 6.632265462859328, 0.4139082872459099, 0.3830706767854455]]\n",
      "2021-01-14 03:38:15,821 : INFO : Removed 3 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:38:15,822 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:38:15,825 : INFO : built Dictionary(583 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 5903 corpus positions)\n",
      "2021-01-14 03:38:19,580 : INFO : token count processed\n",
      "2021-01-14 03:38:19,607 : INFO : frequencies processed\n",
      "2021-01-14 03:38:28,761 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:38:28,762 : INFO : entropies processed\n",
      "2021-01-14 03:38:28,763 : INFO : extropies processed\n",
      "2021-01-14 03:38:28,771 : INFO : token count processed\n",
      "2021-01-14 03:38:28,775 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:38:28,779 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:38:28,780 : INFO : vocab #32006\n",
      "2021-01-14 03:38:28,791 : INFO : diff #{'```'}\n",
      "2021-01-14 03:38:47,293 : INFO : alphabet #32006\n",
      "2021-01-14 03:38:56,522 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[0.8061602149934867, 0.5536607393401178], [0.2924255132675171, 0.7075745], [5.960931410515296, 1.4214359453151593], [7.0153361396447735, 7.009229588004272, 7.2471127486546045, 6.777452978994441, 0.231776609009831, 0.23788316065033221]]\n",
      "2021-01-14 03:38:56,535 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:38:56,536 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:38:56,539 : INFO : built Dictionary(658 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 6844 corpus positions)\n",
      "2021-01-14 03:39:00,986 : INFO : token count processed\n",
      "2021-01-14 03:39:01,014 : INFO : frequencies processed\n",
      "2021-01-14 03:39:10,179 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:39:10,180 : INFO : entropies processed\n",
      "2021-01-14 03:39:10,181 : INFO : extropies processed\n",
      "2021-01-14 03:39:10,189 : INFO : token count processed\n",
      "2021-01-14 03:39:10,193 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:39:10,198 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:39:10,200 : INFO : vocab #32006\n",
      "2021-01-14 03:39:10,207 : INFO : diff #{'```'}\n",
      "2021-01-14 03:39:28,692 : INFO : alphabet #32006\n",
      "2021-01-14 03:39:38,236 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[0.8572783806242763, 0.5384222475382908], [0.34985727071762085, 0.6501427], [5.995313050665305, 1.422600938775939], [7.0153361396447735, 7.376088004590871, 7.641000517634805, 6.750423626600839, 0.6256643779900317, 0.26491251304393426]]\n",
      "2021-01-14 03:39:38,239 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:39:38,240 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:39:38,242 : INFO : built Dictionary(288 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 630 corpus positions)\n",
      "2021-01-14 03:39:38,860 : INFO : token count processed\n",
      "2021-01-14 03:39:38,903 : INFO : frequencies processed\n",
      "2021-01-14 03:39:48,078 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:39:48,080 : INFO : entropies processed\n",
      "2021-01-14 03:39:48,081 : INFO : extropies processed\n",
      "2021-01-14 03:39:48,087 : INFO : token count processed\n",
      "2021-01-14 03:39:48,091 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:39:48,098 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:39:48,099 : INFO : vocab #32006\n",
      "2021-01-14 03:39:48,108 : INFO : diff #{'```'}\n",
      "2021-01-14 03:40:06,599 : INFO : alphabet #32006\n",
      "2021-01-14 03:40:15,809 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[0.9762601438324506, 0.5060062578911073], [0.4264499545097351, 0.57355005], [4.0500815934786285, 1.3778014083355132], [7.0153361396447735, 6.2993628166120885, 7.387411242927538, 5.927287713329324, 0.3720751032827643, 1.0880484263154493]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 03:40:15,813 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:40:15,814 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:40:15,815 : INFO : built Dictionary(204 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 346 corpus positions)\n",
      "2021-01-14 03:40:15,921 : INFO : token count processed\n",
      "2021-01-14 03:40:15,970 : INFO : frequencies processed\n",
      "2021-01-14 03:40:25,655 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:40:25,656 : INFO : entropies processed\n",
      "2021-01-14 03:40:25,657 : INFO : extropies processed\n",
      "2021-01-14 03:40:25,667 : INFO : token count processed\n",
      "2021-01-14 03:40:25,672 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:40:25,677 : INFO : alphabet_target #32008\n",
      "2021-01-14 03:40:25,678 : INFO : vocab #32006\n",
      "2021-01-14 03:40:25,684 : INFO : diff #{'```'}\n",
      "2021-01-14 03:40:44,065 : INFO : alphabet #32006\n",
      "2021-01-14 03:40:53,292 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2012076511347567, 0.45429607673973166], [0.706338495016098, 0.2936615], [2.2810361125534233, 1.2263316211630273], [7.0153361396447735, 3.8936606896881862, 7.072882196191371, 3.836114633141589, 0.05754605654659706, 3.1792215065031844]]\n",
      "2021-01-14 03:40:53,315 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:40:53,316 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:40:53,326 : INFO : built Dictionary(830 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 12788 corpus positions)\n",
      "2021-01-14 03:41:04,574 : INFO : token count processed\n",
      "2021-01-14 03:41:04,602 : INFO : frequencies processed\n",
      "2021-01-14 03:41:13,895 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:41:13,896 : INFO : entropies processed\n",
      "2021-01-14 03:41:13,897 : INFO : extropies processed\n",
      "2021-01-14 03:41:13,914 : INFO : token count processed\n",
      "2021-01-14 03:41:13,918 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:41:13,922 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:41:13,923 : INFO : vocab #32006\n",
      "2021-01-14 03:41:13,929 : INFO : diff #{'```'}\n",
      "2021-01-14 03:41:32,297 : INFO : alphabet #32006\n",
      "2021-01-14 03:41:41,610 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[0.8481669058902022, 0.5410766726819688], [0.3125492334365845, 0.68745077], [5.752096487689866, 1.4189004193538828], [7.0153361396447735, 7.434393313070278, 7.703509721709599, 6.746219731005452, 0.6881735820648256, 0.26911640863932096]]\n",
      "2021-01-14 03:41:41,619 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:41:41,619 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:41:41,622 : INFO : built Dictionary(598 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 4415 corpus positions)\n",
      "2021-01-14 03:41:44,809 : INFO : token count processed\n",
      "2021-01-14 03:41:44,867 : INFO : frequencies processed\n",
      "2021-01-14 03:41:54,172 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:41:54,173 : INFO : entropies processed\n",
      "2021-01-14 03:41:54,174 : INFO : extropies processed\n",
      "2021-01-14 03:41:54,187 : INFO : token count processed\n",
      "2021-01-14 03:41:54,191 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:41:54,195 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:41:54,196 : INFO : vocab #32006\n",
      "2021-01-14 03:41:54,202 : INFO : diff #{'```'}\n",
      "2021-01-14 03:42:12,564 : INFO : alphabet #32006\n",
      "2021-01-14 03:42:22,067 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[0.8913280440369239, 0.5287290077217706], [0.35582590103149414, 0.6441741], [5.451649907598967, 1.4135818655470855], [7.0153361396447735, 7.2991514951718255, 7.6606351855905475, 6.653852449226052, 0.6452990459457739, 0.36148369041872197]]\n",
      "2021-01-14 03:42:22,076 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:42:22,077 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:42:22,081 : INFO : built Dictionary(562 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 3832 corpus positions)\n",
      "2021-01-14 03:42:25,362 : INFO : token count processed\n",
      "2021-01-14 03:42:25,433 : INFO : frequencies processed\n",
      "2021-01-14 03:42:34,648 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:42:34,649 : INFO : entropies processed\n",
      "2021-01-14 03:42:34,650 : INFO : extropies processed\n",
      "2021-01-14 03:42:34,657 : INFO : token count processed\n",
      "2021-01-14 03:42:34,663 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:42:34,667 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:42:34,668 : INFO : vocab #32006\n",
      "2021-01-14 03:42:34,675 : INFO : diff #{'```'}\n",
      "2021-01-14 03:42:53,185 : INFO : alphabet #32006\n",
      "2021-01-14 03:43:02,353 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[0.8945140851620158, 0.5278398338825133], [0.3609257936477661, 0.6390742], [5.547389492548522, 1.4158218345361144], [7.0153361396447735, 7.170319527000998, 7.552456186433309, 6.633199480212463, 0.5371200467885355, 0.38213665943231145]]\n",
      "2021-01-14 03:43:02,357 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:43:02,358 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:43:02,360 : INFO : built Dictionary(309 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 868 corpus positions)\n",
      "2021-01-14 03:43:03,166 : INFO : token count processed\n",
      "2021-01-14 03:43:03,216 : INFO : frequencies processed\n",
      "2021-01-14 03:43:12,722 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:43:12,723 : INFO : entropies processed\n",
      "2021-01-14 03:43:12,724 : INFO : extropies processed\n",
      "2021-01-14 03:43:12,738 : INFO : token count processed\n",
      "2021-01-14 03:43:12,743 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:43:12,747 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:43:12,748 : INFO : vocab #32006\n",
      "2021-01-14 03:43:12,754 : INFO : diff #{'```'}\n",
      "2021-01-14 03:43:31,236 : INFO : alphabet #32006\n",
      "2021-01-14 03:43:40,563 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[0.9219488928710441, 0.5203051983896309], [0.3682880401611328, 0.63171196], [4.419485323863603, 1.3836683608653093], [7.0153361396447735, 6.353654804387375, 7.28277634337983, 6.086214600652319, 0.26744020373505606, 0.9291215389924545]]\n",
      "2021-01-14 03:43:40,567 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:43:40,568 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:43:40,569 : INFO : built Dictionary(307 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 996 corpus positions)\n",
      "2021-01-14 03:43:41,331 : INFO : token count processed\n",
      "2021-01-14 03:43:41,359 : INFO : frequencies processed\n",
      "2021-01-14 03:43:50,555 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:43:50,556 : INFO : entropies processed\n",
      "2021-01-14 03:43:50,556 : INFO : extropies processed\n",
      "2021-01-14 03:43:50,563 : INFO : token count processed\n",
      "2021-01-14 03:43:50,566 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:43:50,570 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:43:50,571 : INFO : vocab #32006\n",
      "2021-01-14 03:43:50,576 : INFO : diff #{'```'}\n",
      "2021-01-14 03:44:09,046 : INFO : alphabet #32006\n",
      "2021-01-14 03:44:18,220 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[0.9541022314664218, 0.5117439527457924], [0.39455825090408325, 0.60544175], [4.280226099537215, 1.3763710958418514], [7.0153361396447735, 6.245180322479091, 7.1671177149869925, 6.093398747136871, 0.15178157534221892, 0.9219373925079015]]\n",
      "2021-01-14 03:44:18,225 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:44:18,226 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:44:18,228 : INFO : built Dictionary(502 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 2068 corpus positions)\n",
      "2021-01-14 03:44:20,688 : INFO : token count processed\n",
      "2021-01-14 03:44:20,716 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 03:44:29,912 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:44:29,913 : INFO : entropies processed\n",
      "2021-01-14 03:44:29,914 : INFO : extropies processed\n",
      "2021-01-14 03:44:29,921 : INFO : token count processed\n",
      "2021-01-14 03:44:29,929 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:44:29,936 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:44:29,937 : INFO : vocab #32006\n",
      "2021-01-14 03:44:29,945 : INFO : diff #{'```'}\n",
      "2021-01-14 03:44:48,523 : INFO : alphabet #32006\n",
      "2021-01-14 03:44:57,693 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[0.9040778359309178, 0.5251886142097193], [0.3704284429550171, 0.62957156], [5.396641490392236, 1.4125923997563725], [7.0153361396447735, 7.2691387000368, 7.754461165438181, 6.530013674243392, 0.7391250257934079, 0.48532246540138146]]\n",
      "2021-01-14 03:44:57,698 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:44:57,699 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:44:57,701 : INFO : built Dictionary(437 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1905 corpus positions)\n",
      "2021-01-14 03:44:59,449 : INFO : token count processed\n",
      "2021-01-14 03:44:59,495 : INFO : frequencies processed\n",
      "2021-01-14 03:45:08,757 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:45:08,758 : INFO : entropies processed\n",
      "2021-01-14 03:45:08,759 : INFO : extropies processed\n",
      "2021-01-14 03:45:08,765 : INFO : token count processed\n",
      "2021-01-14 03:45:08,773 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:45:08,777 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:45:08,778 : INFO : vocab #32006\n",
      "2021-01-14 03:45:08,786 : INFO : diff #{'```'}\n",
      "2021-01-14 03:45:27,351 : INFO : alphabet #32006\n",
      "2021-01-14 03:45:36,540 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[0.875303893722027, 0.5332469064601795], [0.3461461663246155, 0.65385383], [5.071742512316571, 1.4059747745253797], [7.0153361396447735, 7.08857858466988, 7.577763003476777, 6.526151720837875, 0.5624268638320036, 0.48918441880689745]]\n",
      "2021-01-14 03:45:36,544 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:45:36,545 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:45:36,546 : INFO : built Dictionary(290 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 828 corpus positions)\n",
      "2021-01-14 03:45:37,200 : INFO : token count processed\n",
      "2021-01-14 03:45:37,241 : INFO : frequencies processed\n",
      "2021-01-14 03:45:46,532 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:45:46,533 : INFO : entropies processed\n",
      "2021-01-14 03:45:46,534 : INFO : extropies processed\n",
      "2021-01-14 03:45:46,541 : INFO : token count processed\n",
      "2021-01-14 03:45:46,548 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:45:46,552 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:45:46,553 : INFO : vocab #32006\n",
      "2021-01-14 03:45:46,561 : INFO : diff #{'```'}\n",
      "2021-01-14 03:46:05,086 : INFO : alphabet #32006\n",
      "2021-01-14 03:46:14,276 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[0.9651191505345851, 0.5088749960672679], [0.3848189115524292, 0.6151811], [4.098822601778444, 1.3697932218328184], [7.0153361396447735, 6.0479231618016716, 7.135405785288407, 5.927853516158038, 0.12006964564363365, 1.0874826234867356]]\n",
      "2021-01-14 03:46:14,280 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:46:14,281 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:46:14,284 : INFO : built Dictionary(291 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 947 corpus positions)\n",
      "2021-01-14 03:46:14,926 : INFO : token count processed\n",
      "2021-01-14 03:46:14,954 : INFO : frequencies processed\n",
      "2021-01-14 03:46:24,242 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:46:24,243 : INFO : entropies processed\n",
      "2021-01-14 03:46:24,243 : INFO : extropies processed\n",
      "2021-01-14 03:46:24,250 : INFO : token count processed\n",
      "2021-01-14 03:46:24,254 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:46:24,258 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:46:24,259 : INFO : vocab #32006\n",
      "2021-01-14 03:46:24,265 : INFO : diff #{'```'}\n",
      "2021-01-14 03:46:42,967 : INFO : alphabet #32006\n",
      "2021-01-14 03:46:52,279 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[0.978861531729607, 0.5053410680665253], [0.4121231436729431, 0.58787686], [4.140915571618753, 1.367538852757985], [7.0153361396447735, 6.036583168403119, 7.065700953814503, 5.98621835423339, 0.050364814169729044, 1.0291177854113833]]\n",
      "2021-01-14 03:46:52,293 : INFO : Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:46:52,294 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:46:52,297 : INFO : built Dictionary(659 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 7305 corpus positions)\n",
      "2021-01-14 03:46:57,879 : INFO : token count processed\n",
      "2021-01-14 03:46:57,907 : INFO : frequencies processed\n",
      "2021-01-14 03:47:07,055 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:47:07,056 : INFO : entropies processed\n",
      "2021-01-14 03:47:07,056 : INFO : extropies processed\n",
      "2021-01-14 03:47:07,065 : INFO : token count processed\n",
      "2021-01-14 03:47:07,072 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:47:07,078 : INFO : alphabet_target #32010\n",
      "2021-01-14 03:47:07,079 : INFO : vocab #32006\n",
      "2021-01-14 03:47:07,086 : INFO : diff #{'```'}\n",
      "2021-01-14 03:47:25,812 : INFO : alphabet #32006\n",
      "2021-01-14 03:47:34,991 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[0.8667793693711155, 0.5356819431408661], [0.3557332754135132, 0.6442667], [5.509412299197953, 1.4150301871679891], [7.0153361396447735, 7.29352035514053, 7.565531826739272, 6.743324668046031, 0.5501956870944982, 0.2720114715987414]]\n",
      "2021-01-14 03:47:34,998 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:47:34,999 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:47:35,002 : INFO : built Dictionary(482 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 3548 corpus positions)\n",
      "2021-01-14 03:47:37,030 : INFO : token count processed\n",
      "2021-01-14 03:47:37,064 : INFO : frequencies processed\n",
      "2021-01-14 03:47:46,233 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:47:46,234 : INFO : entropies processed\n",
      "2021-01-14 03:47:46,235 : INFO : extropies processed\n",
      "2021-01-14 03:47:46,242 : INFO : token count processed\n",
      "2021-01-14 03:47:46,250 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:47:46,254 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:47:46,255 : INFO : vocab #32006\n",
      "2021-01-14 03:47:46,263 : INFO : diff #{'```'}\n",
      "2021-01-14 03:48:04,858 : INFO : alphabet #32006\n",
      "2021-01-14 03:48:14,027 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[0.9059539479169834, 0.5246716485951299], [0.35379868745803833, 0.6462013], [5.295458525674118, 1.4113591438272062], [7.0153361396447735, 6.8153433747477745, 7.20807649981413, 6.622603014578418, 0.19274036016935625, 0.39273312506635527]]\n",
      "2021-01-14 03:48:14,031 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:48:14,031 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:48:14,033 : INFO : built Dictionary(274 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 569 corpus positions)\n",
      "2021-01-14 03:48:14,588 : INFO : token count processed\n",
      "2021-01-14 03:48:14,616 : INFO : frequencies processed\n",
      "2021-01-14 03:48:23,926 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:48:23,927 : INFO : entropies processed\n",
      "2021-01-14 03:48:23,928 : INFO : extropies processed\n",
      "2021-01-14 03:48:23,934 : INFO : token count processed\n",
      "2021-01-14 03:48:23,938 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:48:23,943 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:48:23,943 : INFO : vocab #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 03:48:23,950 : INFO : diff #{'```'}\n",
      "2021-01-14 03:48:42,427 : INFO : alphabet #32006\n",
      "2021-01-14 03:48:51,737 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.0039035063939317, 0.49902602436158316], [0.4647488594055176, 0.53525114], [3.96307418942857, 1.366504939709587], [7.0153361396447735, 6.150121915859574, 7.363059907941658, 5.802398147562691, 0.3477237682968841, 1.2129379920820833]]\n",
      "2021-01-14 03:48:51,741 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:48:51,742 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:48:51,744 : INFO : built Dictionary(404 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1280 corpus positions)\n",
      "2021-01-14 03:48:53,299 : INFO : token count processed\n",
      "2021-01-14 03:48:53,330 : INFO : frequencies processed\n",
      "2021-01-14 03:49:02,741 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:49:02,742 : INFO : entropies processed\n",
      "2021-01-14 03:49:02,743 : INFO : extropies processed\n",
      "2021-01-14 03:49:02,750 : INFO : token count processed\n",
      "2021-01-14 03:49:02,756 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:49:02,760 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:49:02,761 : INFO : vocab #32006\n",
      "2021-01-14 03:49:02,769 : INFO : diff #{'```'}\n",
      "2021-01-14 03:49:21,263 : INFO : alphabet #32006\n",
      "2021-01-14 03:49:30,447 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[0.8997129002713341, 0.5263953305034519], [0.40073758363723755, 0.5992624], [5.077188730608714, 1.4040397264157303], [7.0153361396447735, 7.0391145208191315, 7.673244389894879, 6.3812062705690265, 0.657908250250105, 0.6341298690757471]]\n",
      "2021-01-14 03:49:30,457 : INFO : Removed 3 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:49:30,458 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:49:30,461 : INFO : built Dictionary(694 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 4655 corpus positions)\n",
      "2021-01-14 03:49:36,063 : INFO : token count processed\n",
      "2021-01-14 03:49:36,123 : INFO : frequencies processed\n",
      "2021-01-14 03:49:45,571 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:49:45,572 : INFO : entropies processed\n",
      "2021-01-14 03:49:45,573 : INFO : extropies processed\n",
      "2021-01-14 03:49:45,581 : INFO : token count processed\n",
      "2021-01-14 03:49:45,585 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:49:45,589 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:49:45,591 : INFO : vocab #32006\n",
      "2021-01-14 03:49:45,601 : INFO : diff #{'```'}\n",
      "2021-01-14 03:50:04,203 : INFO : alphabet #32006\n",
      "2021-01-14 03:50:13,350 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[0.8647593441363673, 0.536262227694123], [0.33818644285202026, 0.66181356], [5.392264915881306, 1.4129523455990782], [7.0153361396447735, 7.482466367279176, 7.89325214973929, 6.60455035718466, 0.8779160100945163, 0.4107857824601142]]\n",
      "2021-01-14 03:50:13,355 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:50:13,355 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:50:13,357 : INFO : built Dictionary(308 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 877 corpus positions)\n",
      "2021-01-14 03:50:14,137 : INFO : token count processed\n",
      "2021-01-14 03:50:14,163 : INFO : frequencies processed\n",
      "2021-01-14 03:50:23,435 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:50:23,436 : INFO : entropies processed\n",
      "2021-01-14 03:50:23,437 : INFO : extropies processed\n",
      "2021-01-14 03:50:23,443 : INFO : token count processed\n",
      "2021-01-14 03:50:23,450 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:50:23,454 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:50:23,455 : INFO : vocab #32006\n",
      "2021-01-14 03:50:23,461 : INFO : diff #{'```'}\n",
      "2021-01-14 03:50:42,193 : INFO : alphabet #32006\n",
      "2021-01-14 03:50:51,398 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[0.9329670275947366, 0.5173393988227195], [0.37353312969207764, 0.6264669], [4.426951416152235, 1.3822311656163448], [7.0153361396447735, 6.372162341197667, 7.294637066957469, 6.092861413884972, 0.2793009273126952, 0.9224747257598018]]\n",
      "2021-01-14 03:50:51,404 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:50:51,405 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:50:51,408 : INFO : built Dictionary(441 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 2298 corpus positions)\n",
      "2021-01-14 03:50:53,061 : INFO : token count processed\n",
      "2021-01-14 03:50:53,089 : INFO : frequencies processed\n",
      "2021-01-14 03:51:02,390 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:51:02,391 : INFO : entropies processed\n",
      "2021-01-14 03:51:02,392 : INFO : extropies processed\n",
      "2021-01-14 03:51:02,399 : INFO : token count processed\n",
      "2021-01-14 03:51:02,406 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:51:02,411 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:51:02,411 : INFO : vocab #32006\n",
      "2021-01-14 03:51:02,419 : INFO : diff #{'```'}\n",
      "2021-01-14 03:51:20,787 : INFO : alphabet #32006\n",
      "2021-01-14 03:51:30,311 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[0.9318736235439266, 0.5176322031694545], [0.384962797164917, 0.6150372], [5.041190663623751, 1.4064400485350084], [7.0153361396447735, 6.798155919669889, 7.334434171985712, 6.47905788732895, 0.31909803234093825, 0.5362782523158227]]\n",
      "2021-01-14 03:51:30,316 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:51:30,317 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:51:30,318 : INFO : built Dictionary(312 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1014 corpus positions)\n",
      "2021-01-14 03:51:31,120 : INFO : token count processed\n",
      "2021-01-14 03:51:31,149 : INFO : frequencies processed\n",
      "2021-01-14 03:51:40,308 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:51:40,309 : INFO : entropies processed\n",
      "2021-01-14 03:51:40,310 : INFO : extropies processed\n",
      "2021-01-14 03:51:40,316 : INFO : token count processed\n",
      "2021-01-14 03:51:40,324 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:51:40,329 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:51:40,330 : INFO : vocab #32006\n",
      "2021-01-14 03:51:40,338 : INFO : diff #{'```'}\n",
      "2021-01-14 03:51:58,817 : INFO : alphabet #32006\n",
      "2021-01-14 03:52:08,001 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[0.9291121175981141, 0.5183731888248534], [0.3611184358596802, 0.63888156], [4.478374885071764, 1.3872109246601074], [7.0153361396447735, 6.271631856729336, 7.161706430190872, 6.125261566183237, 0.1463702905460984, 0.8900745734615363]]\n",
      "2021-01-14 03:52:08,008 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:52:08,009 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:52:08,015 : INFO : built Dictionary(470 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 3529 corpus positions)\n",
      "2021-01-14 03:52:10,024 : INFO : token count processed\n",
      "2021-01-14 03:52:10,056 : INFO : frequencies processed\n",
      "2021-01-14 03:52:19,320 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:52:19,321 : INFO : entropies processed\n",
      "2021-01-14 03:52:19,322 : INFO : extropies processed\n",
      "2021-01-14 03:52:19,329 : INFO : token count processed\n",
      "2021-01-14 03:52:19,333 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:52:19,337 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:52:19,338 : INFO : vocab #32006\n",
      "2021-01-14 03:52:19,348 : INFO : diff #{'```'}\n",
      "2021-01-14 03:52:37,807 : INFO : alphabet #32006\n",
      "2021-01-14 03:52:46,960 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[0.9135826558178128, 0.5225799873131832], [0.3681989908218384, 0.631801], [5.214987095103389, 1.4101629453773694], [7.0153361396447735, 6.873598627629562, 7.279299454313444, 6.609635312960892, 0.2639633146686702, 0.40570082668388174]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 03:52:46,964 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:52:46,965 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:52:46,966 : INFO : built Dictionary(249 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 477 corpus positions)\n",
      "2021-01-14 03:52:47,365 : INFO : token count processed\n",
      "2021-01-14 03:52:47,420 : INFO : frequencies processed\n",
      "2021-01-14 03:52:56,673 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:52:56,674 : INFO : entropies processed\n",
      "2021-01-14 03:52:56,675 : INFO : extropies processed\n",
      "2021-01-14 03:52:56,686 : INFO : token count processed\n",
      "2021-01-14 03:52:56,692 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:52:56,696 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:52:56,697 : INFO : vocab #32006\n",
      "2021-01-14 03:52:56,703 : INFO : diff #{'```'}\n",
      "2021-01-14 03:53:15,498 : INFO : alphabet #32006\n",
      "2021-01-14 03:53:24,676 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[0.9114409742078946, 0.5231655141296749], [0.37667572498321533, 0.6233243], [4.4591397944489835, 1.3955263259214086], [7.0153361396447735, 6.049830202851529, 7.280275650983695, 5.784890691512608, 0.26493951133892146, 1.2304454481321656]]\n",
      "2021-01-14 03:53:24,680 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:53:24,681 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:53:24,683 : INFO : built Dictionary(384 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1557 corpus positions)\n",
      "2021-01-14 03:53:25,864 : INFO : token count processed\n",
      "2021-01-14 03:53:25,892 : INFO : frequencies processed\n",
      "2021-01-14 03:53:35,072 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:53:35,073 : INFO : entropies processed\n",
      "2021-01-14 03:53:35,074 : INFO : extropies processed\n",
      "2021-01-14 03:53:35,081 : INFO : token count processed\n",
      "2021-01-14 03:53:35,085 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:53:35,092 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:53:35,093 : INFO : vocab #32006\n",
      "2021-01-14 03:53:35,101 : INFO : diff #{'```'}\n",
      "2021-01-14 03:53:53,797 : INFO : alphabet #32006\n",
      "2021-01-14 03:54:02,979 : INFO : Computed distances or similarities ('294', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.0069406881270042, 0.49827082878730167], [0.5015330910682678, 0.4984669], [4.5964362725382815, 1.3885754555756133], [7.0153361396447735, 6.778844940588858, 7.478143167077216, 6.316037913156415, 0.46280702743244273, 0.6992982264883585]]\n",
      "2021-01-14 03:54:02,982 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:54:02,983 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:54:02,985 : INFO : built Dictionary(291 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 590 corpus positions)\n",
      "2021-01-14 03:54:03,633 : INFO : token count processed\n",
      "2021-01-14 03:54:03,661 : INFO : frequencies processed\n",
      "2021-01-14 03:54:12,949 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:54:12,950 : INFO : entropies processed\n",
      "2021-01-14 03:54:12,950 : INFO : extropies processed\n",
      "2021-01-14 03:54:12,957 : INFO : token count processed\n",
      "2021-01-14 03:54:12,963 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:54:12,968 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:54:12,969 : INFO : vocab #32006\n",
      "2021-01-14 03:54:12,976 : INFO : diff #{'```'}\n",
      "2021-01-14 03:54:31,346 : INFO : alphabet #32006\n",
      "2021-01-14 03:54:40,581 : INFO : Computed distances or similarities ('294', 'sacp-python-common/setup.py')[[0.9271189062344706, 0.5189093401371732], [0.4074304699897766, 0.59256953], [4.562583118533521, 1.3913065433032088], [7.0153361396447735, 6.469677430851302, 7.49169357396018, 5.993319996535895, 0.4763574343154069, 1.0220161431088783]]\n",
      "2021-01-14 03:54:40,586 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:54:40,587 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:54:40,589 : INFO : built Dictionary(340 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1492 corpus positions)\n",
      "2021-01-14 03:54:41,591 : INFO : token count processed\n",
      "2021-01-14 03:54:41,617 : INFO : frequencies processed\n",
      "2021-01-14 03:54:50,906 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:54:50,907 : INFO : entropies processed\n",
      "2021-01-14 03:54:50,908 : INFO : extropies processed\n",
      "2021-01-14 03:54:50,914 : INFO : token count processed\n",
      "2021-01-14 03:54:50,922 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:54:50,926 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:54:50,927 : INFO : vocab #32006\n",
      "2021-01-14 03:54:50,934 : INFO : diff #{'```'}\n",
      "2021-01-14 03:55:09,285 : INFO : alphabet #32006\n",
      "2021-01-14 03:55:18,546 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/bandit/test_bandit.py')[[0.8520215826111824, 0.53995051104647], [0.3406563401222229, 0.65934366], [4.975599317865577, 1.4046722899335151], [7.0153361396447735, 6.459180448028249, 7.10481562074951, 6.369700966923513, 0.08947948110473636, 0.6456351727212608]]\n",
      "2021-01-14 03:55:18,550 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:55:18,551 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:55:18,553 : INFO : built Dictionary(267 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 642 corpus positions)\n",
      "2021-01-14 03:55:19,026 : INFO : token count processed\n",
      "2021-01-14 03:55:19,092 : INFO : frequencies processed\n",
      "2021-01-14 03:55:28,608 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:55:28,609 : INFO : entropies processed\n",
      "2021-01-14 03:55:28,611 : INFO : extropies processed\n",
      "2021-01-14 03:55:28,624 : INFO : token count processed\n",
      "2021-01-14 03:55:28,628 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:55:28,632 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:55:28,633 : INFO : vocab #32006\n",
      "2021-01-14 03:55:28,639 : INFO : diff #{'```'}\n",
      "2021-01-14 03:55:47,023 : INFO : alphabet #32006\n",
      "2021-01-14 03:55:56,297 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[0.8776638152089513, 0.5325767008449899], [0.32728588581085205, 0.6727141], [4.493780522247874, 1.3805408567515003], [7.0153361396447735, 6.097125733496388, 7.186136767309078, 5.926325105832083, 0.17080062766430437, 1.0890110338126897]]\n",
      "2021-01-14 03:55:56,300 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:55:56,301 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:55:56,303 : INFO : built Dictionary(257 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 638 corpus positions)\n",
      "2021-01-14 03:55:56,788 : INFO : token count processed\n",
      "2021-01-14 03:55:56,840 : INFO : frequencies processed\n",
      "2021-01-14 03:56:06,048 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:56:06,050 : INFO : entropies processed\n",
      "2021-01-14 03:56:06,051 : INFO : extropies processed\n",
      "2021-01-14 03:56:06,062 : INFO : token count processed\n",
      "2021-01-14 03:56:06,066 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:56:06,070 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:56:06,070 : INFO : vocab #32006\n",
      "2021-01-14 03:56:06,076 : INFO : diff #{'```'}\n",
      "2021-01-14 03:56:24,776 : INFO : alphabet #32006\n",
      "2021-01-14 03:56:33,946 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[0.8567226826880555, 0.5385833917600759], [0.3322905898094177, 0.6677094], [4.601693658263358, 1.3872215595000597], [7.0153361396447735, 6.0695858597523715, 7.149316608097844, 5.935605391299302, 0.13398046845307032, 1.0797307483454723]]\n",
      "2021-01-14 03:56:33,950 : INFO : Removed 3 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:56:33,951 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:56:33,952 : INFO : built Dictionary(258 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 734 corpus positions)\n",
      "2021-01-14 03:56:34,437 : INFO : token count processed\n",
      "2021-01-14 03:56:34,492 : INFO : frequencies processed\n",
      "2021-01-14 03:56:43,661 : INFO : scalar_distribution processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 03:56:43,662 : INFO : entropies processed\n",
      "2021-01-14 03:56:43,663 : INFO : extropies processed\n",
      "2021-01-14 03:56:43,669 : INFO : token count processed\n",
      "2021-01-14 03:56:43,676 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:56:43,681 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:56:43,682 : INFO : vocab #32006\n",
      "2021-01-14 03:56:43,689 : INFO : diff #{'```'}\n",
      "2021-01-14 03:57:02,333 : INFO : alphabet #32006\n",
      "2021-01-14 03:57:11,536 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[0.8428196905368571, 0.5426466871040847], [0.32271915674209595, 0.67728084], [4.565933927250763, 1.3881999438227561], [7.0153361396447735, 6.104787343210121, 7.104654004062152, 6.015469478792741, 0.08931786441737888, 0.9998666608520317]]\n",
      "2021-01-14 03:57:11,553 : INFO : Removed 3 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:57:11,554 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:57:11,562 : INFO : built Dictionary(481 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 9419 corpus positions)\n",
      "2021-01-14 03:57:13,509 : INFO : token count processed\n",
      "2021-01-14 03:57:13,540 : INFO : frequencies processed\n",
      "2021-01-14 03:57:22,709 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:57:22,710 : INFO : entropies processed\n",
      "2021-01-14 03:57:22,711 : INFO : extropies processed\n",
      "2021-01-14 03:57:22,725 : INFO : token count processed\n",
      "2021-01-14 03:57:22,729 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:57:22,732 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:57:22,733 : INFO : vocab #32006\n",
      "2021-01-14 03:57:22,739 : INFO : diff #{'```'}\n",
      "2021-01-14 03:57:41,335 : INFO : alphabet #32006\n",
      "2021-01-14 03:57:50,762 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[0.6645851856439485, 0.6007502701720536], [0.2194257378578186, 0.78057426], [5.999877108748538, 1.4223510011264144], [7.0153361396447735, 6.89087415148015, 7.120693084960531, 6.785517206164394, 0.10535694531575768, 0.22981893348038085]]\n",
      "2021-01-14 03:57:50,768 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:57:50,769 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:57:50,771 : INFO : built Dictionary(396 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 2579 corpus positions)\n",
      "2021-01-14 03:57:52,055 : INFO : token count processed\n",
      "2021-01-14 03:57:52,082 : INFO : frequencies processed\n",
      "2021-01-14 03:58:01,493 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:58:01,494 : INFO : entropies processed\n",
      "2021-01-14 03:58:01,495 : INFO : extropies processed\n",
      "2021-01-14 03:58:01,507 : INFO : token count processed\n",
      "2021-01-14 03:58:01,512 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:58:01,516 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:58:01,517 : INFO : vocab #32006\n",
      "2021-01-14 03:58:01,523 : INFO : diff #{'```'}\n",
      "2021-01-14 03:58:20,038 : INFO : alphabet #32006\n",
      "2021-01-14 03:58:29,217 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[0.8743925066040363, 0.5335061874589798], [0.33724212646484375, 0.6627579], [4.893370125500677, 1.4026491228054494], [7.0153361396447735, 6.655493573668506, 7.131668885112815, 6.5391608282004645, 0.11633274546804184, 0.476175311444309]]\n",
      "2021-01-14 03:58:29,222 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:58:29,223 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:58:29,227 : INFO : built Dictionary(349 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1586 corpus positions)\n",
      "2021-01-14 03:58:30,315 : INFO : token count processed\n",
      "2021-01-14 03:58:30,372 : INFO : frequencies processed\n",
      "2021-01-14 03:58:39,842 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:58:39,843 : INFO : entropies processed\n",
      "2021-01-14 03:58:39,844 : INFO : extropies processed\n",
      "2021-01-14 03:58:39,851 : INFO : token count processed\n",
      "2021-01-14 03:58:39,855 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:58:39,859 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:58:39,860 : INFO : vocab #32006\n",
      "2021-01-14 03:58:39,866 : INFO : diff #{'```'}\n",
      "2021-01-14 03:58:58,650 : INFO : alphabet #32006\n",
      "2021-01-14 03:59:07,951 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[0.8089175364982317, 0.552816797793799], [0.31245166063308716, 0.68754834], [5.4649715753856425, 1.4130573228521857], [7.0153361396447735, 6.6236746347295465, 7.149479015193083, 6.489531759181237, 0.13414287554830917, 0.5258043804635362]]\n",
      "2021-01-14 03:59:07,956 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:59:07,957 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:59:07,958 : INFO : built Dictionary(367 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1525 corpus positions)\n",
      "2021-01-14 03:59:09,160 : INFO : token count processed\n",
      "2021-01-14 03:59:09,203 : INFO : frequencies processed\n",
      "2021-01-14 03:59:18,852 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:59:18,853 : INFO : entropies processed\n",
      "2021-01-14 03:59:18,854 : INFO : extropies processed\n",
      "2021-01-14 03:59:18,861 : INFO : token count processed\n",
      "2021-01-14 03:59:18,868 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:59:18,873 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:59:18,874 : INFO : vocab #32006\n",
      "2021-01-14 03:59:18,882 : INFO : diff #{'```'}\n",
      "2021-01-14 03:59:37,280 : INFO : alphabet #32006\n",
      "2021-01-14 03:59:46,486 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/gosec/test_gosec.py')[[0.8964629004563363, 0.5272974228809723], [0.3485684394836426, 0.65143156], [5.000492508324136, 1.403052907482123], [7.0153361396447735, 6.75472436518627, 7.322350865163219, 6.447709639667824, 0.3070147255184459, 0.5676264999769494]]\n",
      "2021-01-14 03:59:46,490 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 03:59:46,491 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 03:59:46,495 : INFO : built Dictionary(331 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1320 corpus positions)\n",
      "2021-01-14 03:59:47,387 : INFO : token count processed\n",
      "2021-01-14 03:59:47,448 : INFO : frequencies processed\n",
      "2021-01-14 03:59:56,917 : INFO : scalar_distribution processed\n",
      "2021-01-14 03:59:56,918 : INFO : entropies processed\n",
      "2021-01-14 03:59:56,919 : INFO : extropies processed\n",
      "2021-01-14 03:59:56,925 : INFO : token count processed\n",
      "2021-01-14 03:59:56,930 : INFO : alphabet_source #32007\n",
      "2021-01-14 03:59:56,934 : INFO : alphabet_target #32009\n",
      "2021-01-14 03:59:56,935 : INFO : vocab #32006\n",
      "2021-01-14 03:59:56,946 : INFO : diff #{'```'}\n",
      "2021-01-14 04:00:15,554 : INFO : alphabet #32006\n",
      "2021-01-14 04:00:24,826 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[0.9125508486378344, 0.5228619153902363], [0.34170013666152954, 0.65829986], [4.718322061265963, 1.3957016386976049], [7.0153361396447735, 6.597313085495733, 7.261182012279036, 6.35146721286147, 0.24584587263426272, 0.6638689267833033]]\n",
      "2021-01-14 04:00:24,831 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:00:24,832 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:00:24,833 : INFO : built Dictionary(343 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1286 corpus positions)\n",
      "2021-01-14 04:00:25,896 : INFO : token count processed\n",
      "2021-01-14 04:00:25,952 : INFO : frequencies processed\n",
      "2021-01-14 04:00:35,120 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:00:35,121 : INFO : entropies processed\n",
      "2021-01-14 04:00:35,122 : INFO : extropies processed\n",
      "2021-01-14 04:00:35,129 : INFO : token count processed\n",
      "2021-01-14 04:00:35,133 : INFO : alphabet_source #32007\n",
      "2021-01-14 04:00:35,137 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:00:35,138 : INFO : vocab #32006\n",
      "2021-01-14 04:00:35,144 : INFO : diff #{'```'}\n",
      "2021-01-14 04:00:53,877 : INFO : alphabet #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 04:01:03,040 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[0.8634803952524485, 0.5366302766305886], [0.3308536410331726, 0.66914636], [5.019465198812597, 1.4031952886517849], [7.0153361396447735, 6.659481538516613, 7.280759504686718, 6.394058173474668, 0.2654233650419444, 0.6212779661701049]]\n",
      "2021-01-14 04:01:03,045 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:01:03,046 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:01:03,048 : INFO : built Dictionary(370 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1448 corpus positions)\n",
      "2021-01-14 04:01:04,241 : INFO : token count processed\n",
      "2021-01-14 04:01:04,304 : INFO : frequencies processed\n",
      "2021-01-14 04:01:13,475 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:01:13,476 : INFO : entropies processed\n",
      "2021-01-14 04:01:13,476 : INFO : extropies processed\n",
      "2021-01-14 04:01:13,490 : INFO : token count processed\n",
      "2021-01-14 04:01:13,495 : INFO : alphabet_source #32007\n",
      "2021-01-14 04:01:13,499 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:01:13,500 : INFO : vocab #32006\n",
      "2021-01-14 04:01:13,506 : INFO : diff #{'```'}\n",
      "2021-01-14 04:01:32,022 : INFO : alphabet #32006\n",
      "2021-01-14 04:01:41,412 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[0.8761796211036322, 0.5329980076277379], [0.29600220918655396, 0.7039978], [4.955976099040424, 1.4019108177894095], [7.0153361396447735, 6.774682571479102, 7.3303999665169615, 6.459618744606913, 0.3150638268721879, 0.5557173950378598]]\n",
      "2021-01-14 04:01:41,426 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:01:41,427 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:01:41,431 : INFO : built Dictionary(494 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 8185 corpus positions)\n",
      "2021-01-14 04:01:43,966 : INFO : token count processed\n",
      "2021-01-14 04:01:44,024 : INFO : frequencies processed\n",
      "2021-01-14 04:01:53,216 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:01:53,217 : INFO : entropies processed\n",
      "2021-01-14 04:01:53,218 : INFO : extropies processed\n",
      "2021-01-14 04:01:53,233 : INFO : token count processed\n",
      "2021-01-14 04:01:53,237 : INFO : alphabet_source #32007\n",
      "2021-01-14 04:01:53,241 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:01:53,242 : INFO : vocab #32006\n",
      "2021-01-14 04:01:53,248 : INFO : diff #{'```'}\n",
      "2021-01-14 04:02:11,770 : INFO : alphabet #32006\n",
      "2021-01-14 04:02:20,968 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[0.6550656578645311, 0.6042056369475168], [0.22274678945541382, 0.7772532], [6.048552395213648, 1.4229865448294035], [7.0153361396447735, 6.839453716525233, 7.0677447058495275, 6.787045150320479, 0.05240856620475398, 0.2282909893242948]]\n",
      "2021-01-14 04:02:20,974 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:02:20,975 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:02:20,979 : INFO : built Dictionary(428 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 2627 corpus positions)\n",
      "2021-01-14 04:02:22,571 : INFO : token count processed\n",
      "2021-01-14 04:02:22,609 : INFO : frequencies processed\n",
      "2021-01-14 04:02:31,945 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:02:31,946 : INFO : entropies processed\n",
      "2021-01-14 04:02:31,946 : INFO : extropies processed\n",
      "2021-01-14 04:02:31,953 : INFO : token count processed\n",
      "2021-01-14 04:02:31,957 : INFO : alphabet_source #32007\n",
      "2021-01-14 04:02:31,965 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:02:31,966 : INFO : vocab #32006\n",
      "2021-01-14 04:02:31,976 : INFO : diff #{'```'}\n",
      "2021-01-14 04:02:50,554 : INFO : alphabet #32006\n",
      "2021-01-14 04:02:59,731 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[0.8144609556841356, 0.5511278690606786], [0.27054834365844727, 0.72945166], [5.432958840546917, 1.4134127280010937], [7.0153361396447735, 6.86432793886027, 7.23926887020595, 6.640395208299094, 0.22393273056117646, 0.3749409313456802]]\n",
      "2021-01-14 04:02:59,735 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:02:59,736 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:02:59,737 : INFO : built Dictionary(299 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 829 corpus positions)\n",
      "2021-01-14 04:03:00,431 : INFO : token count processed\n",
      "2021-01-14 04:03:00,459 : INFO : frequencies processed\n",
      "2021-01-14 04:03:09,736 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:03:09,737 : INFO : entropies processed\n",
      "2021-01-14 04:03:09,738 : INFO : extropies processed\n",
      "2021-01-14 04:03:09,744 : INFO : token count processed\n",
      "2021-01-14 04:03:09,748 : INFO : alphabet_source #32007\n",
      "2021-01-14 04:03:09,752 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:03:09,753 : INFO : vocab #32006\n",
      "2021-01-14 04:03:09,759 : INFO : diff #{'```'}\n",
      "2021-01-14 04:03:28,286 : INFO : alphabet #32006\n",
      "2021-01-14 04:03:37,440 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[0.9333523826868294, 0.5172362829223477], [0.3913313150405884, 0.6086687], [4.563639511723595, 1.3858666412327272], [7.0153361396447735, 6.431978396403875, 7.322281002063411, 6.125033533985237, 0.3069448624186375, 0.890302605659536]]\n",
      "2021-01-14 04:03:37,444 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:03:37,445 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:03:37,447 : INFO : built Dictionary(343 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 983 corpus positions)\n",
      "2021-01-14 04:03:38,473 : INFO : token count processed\n",
      "2021-01-14 04:03:38,535 : INFO : frequencies processed\n",
      "2021-01-14 04:03:47,825 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:03:47,826 : INFO : entropies processed\n",
      "2021-01-14 04:03:47,827 : INFO : extropies processed\n",
      "2021-01-14 04:03:47,834 : INFO : token count processed\n",
      "2021-01-14 04:03:47,841 : INFO : alphabet_source #32007\n",
      "2021-01-14 04:03:47,846 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:03:47,847 : INFO : vocab #32006\n",
      "2021-01-14 04:03:47,855 : INFO : diff #{'```'}\n",
      "2021-01-14 04:04:06,334 : INFO : alphabet #32006\n",
      "2021-01-14 04:04:15,692 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/test_auth_utility.py')[[0.8788941057648095, 0.5322279722586851], [0.32727891206741333, 0.6727211], [4.978232176591187, 1.4011577255615524], [7.0153361396447735, 6.911818353685893, 7.538448090647966, 6.3887064026827005, 0.5231119510031927, 0.626629736962073]]\n",
      "2021-01-14 04:04:15,705 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:04:15,708 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:04:15,712 : INFO : built Dictionary(424 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 7513 corpus positions)\n",
      "2021-01-14 04:04:17,334 : INFO : token count processed\n",
      "2021-01-14 04:04:17,396 : INFO : frequencies processed\n",
      "2021-01-14 04:04:26,896 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:04:26,897 : INFO : entropies processed\n",
      "2021-01-14 04:04:26,898 : INFO : extropies processed\n",
      "2021-01-14 04:04:26,906 : INFO : token count processed\n",
      "2021-01-14 04:04:26,914 : INFO : alphabet_source #32007\n",
      "2021-01-14 04:04:26,921 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:04:26,922 : INFO : vocab #32006\n",
      "2021-01-14 04:04:26,929 : INFO : diff #{'```'}\n",
      "2021-01-14 04:04:45,347 : INFO : alphabet #32006\n",
      "2021-01-14 04:04:54,628 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[0.8746009270346786, 0.5334468715866056], [0.2785837650299072, 0.72141623], [5.157884088210471, 1.4081477467142567], [7.0153361396447735, 6.363791471162389, 6.5619754739706035, 6.81715213683656, -0.45336066567417, 0.19818400280821447]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 04:04:54,633 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:04:54,634 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:04:54,636 : INFO : built Dictionary(339 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1583 corpus positions)\n",
      "2021-01-14 04:04:55,601 : INFO : token count processed\n",
      "2021-01-14 04:04:55,627 : INFO : frequencies processed\n",
      "2021-01-14 04:05:04,775 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:05:04,776 : INFO : entropies processed\n",
      "2021-01-14 04:05:04,777 : INFO : extropies processed\n",
      "2021-01-14 04:05:04,791 : INFO : token count processed\n",
      "2021-01-14 04:05:04,796 : INFO : alphabet_source #32007\n",
      "2021-01-14 04:05:04,799 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:05:04,800 : INFO : vocab #32006\n",
      "2021-01-14 04:05:04,805 : INFO : diff #{'```'}\n",
      "2021-01-14 04:05:23,623 : INFO : alphabet #32006\n",
      "2021-01-14 04:05:32,786 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[0.816649389754873, 0.5504639506332775], [0.2999362349510193, 0.70006377], [4.8843786631838695, 1.402969967274951], [7.0153361396447735, 6.29000629755059, 6.991000733508933, 6.314341703686431, -0.024335406135840287, 0.7009944359583429]]\n",
      "2021-01-14 04:05:32,791 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:05:32,792 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:05:32,793 : INFO : built Dictionary(346 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1536 corpus positions)\n",
      "2021-01-14 04:05:33,812 : INFO : token count processed\n",
      "2021-01-14 04:05:33,868 : INFO : frequencies processed\n",
      "2021-01-14 04:05:43,039 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:05:43,040 : INFO : entropies processed\n",
      "2021-01-14 04:05:43,041 : INFO : extropies processed\n",
      "2021-01-14 04:05:43,048 : INFO : token count processed\n",
      "2021-01-14 04:05:43,052 : INFO : alphabet_source #32007\n",
      "2021-01-14 04:05:43,056 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:05:43,057 : INFO : vocab #32006\n",
      "2021-01-14 04:05:43,064 : INFO : diff #{'```'}\n",
      "2021-01-14 04:06:01,631 : INFO : alphabet #32006\n",
      "2021-01-14 04:06:11,148 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[0.8079250603991602, 0.5531202713563893], [0.2796509265899658, 0.7203491], [4.921857860714503, 1.4033911920830628], [7.0153361396447735, 6.361621244785958, 7.033215571765728, 6.343741812665003, 0.017879432120954775, 0.6715943269797702]]\n",
      "2021-01-14 04:06:11,153 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:06:11,154 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:06:11,156 : INFO : built Dictionary(358 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1559 corpus positions)\n",
      "2021-01-14 04:06:12,208 : INFO : token count processed\n",
      "2021-01-14 04:06:12,235 : INFO : frequencies processed\n",
      "2021-01-14 04:06:21,408 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:06:21,409 : INFO : entropies processed\n",
      "2021-01-14 04:06:21,410 : INFO : extropies processed\n",
      "2021-01-14 04:06:21,417 : INFO : token count processed\n",
      "2021-01-14 04:06:21,424 : INFO : alphabet_source #32007\n",
      "2021-01-14 04:06:21,429 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:06:21,430 : INFO : vocab #32006\n",
      "2021-01-14 04:06:21,437 : INFO : diff #{'```'}\n",
      "2021-01-14 04:06:40,207 : INFO : alphabet #32006\n",
      "2021-01-14 04:06:49,451 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_bom.py')[[0.8731665362857334, 0.5338553623656341], [0.35370975732803345, 0.64629024], [4.736327227126206, 1.3975145658481825], [7.0153361396447735, 6.620594433343389, 7.279055253516644, 6.356875319471518, 0.2637191138718702, 0.6584608201732545]]\n",
      "2021-01-14 04:06:49,456 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:06:49,457 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:06:49,461 : INFO : built Dictionary(337 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1652 corpus positions)\n",
      "2021-01-14 04:06:50,401 : INFO : token count processed\n",
      "2021-01-14 04:06:50,461 : INFO : frequencies processed\n",
      "2021-01-14 04:06:59,902 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:06:59,903 : INFO : entropies processed\n",
      "2021-01-14 04:06:59,904 : INFO : extropies processed\n",
      "2021-01-14 04:06:59,911 : INFO : token count processed\n",
      "2021-01-14 04:06:59,915 : INFO : alphabet_source #32007\n",
      "2021-01-14 04:06:59,919 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:06:59,920 : INFO : vocab #32006\n",
      "2021-01-14 04:06:59,927 : INFO : diff #{'```'}\n",
      "2021-01-14 04:07:18,433 : INFO : alphabet #32006\n",
      "2021-01-14 04:07:27,605 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[0.88884557180633, 0.5294239057583123], [0.31114518642425537, 0.6888548], [4.478518343768662, 1.3840299740188269], [7.0153361396447735, 6.207411496248084, 6.898703724982636, 6.324043910910222, -0.11663241466213758, 0.691292228734552]]\n",
      "2021-01-14 04:07:27,608 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:07:27,609 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:07:27,610 : INFO : built Dictionary(285 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 556 corpus positions)\n",
      "2021-01-14 04:07:28,231 : INFO : token count processed\n",
      "2021-01-14 04:07:28,283 : INFO : frequencies processed\n",
      "2021-01-14 04:07:37,550 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:07:37,551 : INFO : entropies processed\n",
      "2021-01-14 04:07:37,552 : INFO : extropies processed\n",
      "2021-01-14 04:07:37,565 : INFO : token count processed\n",
      "2021-01-14 04:07:37,571 : INFO : alphabet_source #32007\n",
      "2021-01-14 04:07:37,574 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:07:37,575 : INFO : vocab #32006\n",
      "2021-01-14 04:07:37,581 : INFO : diff #{'```'}\n",
      "2021-01-14 04:07:56,192 : INFO : alphabet #32006\n",
      "2021-01-14 04:08:05,349 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[0.9566965684775488, 0.5110654437228722], [0.4136844873428345, 0.5863155], [4.657546863980589, 1.4037185830354628], [7.0153361396447735, 6.5805228788529595, 7.512460750509488, 6.083398267988246, 0.4971246108647147, 0.9319378716565287]]\n",
      "2021-01-14 04:08:05,353 : INFO : Removed 3 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:08:05,354 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:08:05,356 : INFO : built Dictionary(346 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1517 corpus positions)\n",
      "2021-01-14 04:08:06,355 : INFO : token count processed\n",
      "2021-01-14 04:08:06,413 : INFO : frequencies processed\n",
      "2021-01-14 04:08:15,695 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:08:15,696 : INFO : entropies processed\n",
      "2021-01-14 04:08:15,697 : INFO : extropies processed\n",
      "2021-01-14 04:08:15,703 : INFO : token count processed\n",
      "2021-01-14 04:08:15,707 : INFO : alphabet_source #32007\n",
      "2021-01-14 04:08:15,711 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:08:15,712 : INFO : vocab #32006\n",
      "2021-01-14 04:08:15,718 : INFO : diff #{'```'}\n",
      "2021-01-14 04:08:34,264 : INFO : alphabet #32006\n",
      "2021-01-14 04:08:43,738 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[0.8550386714408268, 0.5390723198364863], [0.26799696683883667, 0.73200303], [5.066307789522732, 1.405047296207047], [7.0153361396447735, 6.422089779976135, 7.023165723504399, 6.414260196116509, 0.007829583859625622, 0.6010759435282642]]\n",
      "2021-01-14 04:08:43,743 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:08:43,744 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:08:43,745 : INFO : built Dictionary(367 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1799 corpus positions)\n",
      "2021-01-14 04:08:44,811 : INFO : token count processed\n",
      "2021-01-14 04:08:44,853 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 04:08:54,140 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:08:54,142 : INFO : entropies processed\n",
      "2021-01-14 04:08:54,142 : INFO : extropies processed\n",
      "2021-01-14 04:08:54,149 : INFO : token count processed\n",
      "2021-01-14 04:08:54,153 : INFO : alphabet_source #32007\n",
      "2021-01-14 04:08:54,157 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:08:54,158 : INFO : vocab #32006\n",
      "2021-01-14 04:08:54,165 : INFO : diff #{'```'}\n",
      "2021-01-14 04:09:12,573 : INFO : alphabet #32006\n",
      "2021-01-14 04:09:21,987 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_cve.py')[[0.8563765060871814, 0.5386838266488149], [0.34984302520751953, 0.650157], [4.69087268167166, 1.3970948278624202], [7.0153361396447735, 6.485445644653597, 7.154294778158457, 6.346487006139913, 0.1389586385136834, 0.6688491335048603]]\n",
      "2021-01-14 04:09:21,993 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:09:21,993 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:09:21,995 : INFO : built Dictionary(345 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1685 corpus positions)\n",
      "2021-01-14 04:09:22,951 : INFO : token count processed\n",
      "2021-01-14 04:09:22,979 : INFO : frequencies processed\n",
      "2021-01-14 04:09:32,198 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:09:32,199 : INFO : entropies processed\n",
      "2021-01-14 04:09:32,199 : INFO : extropies processed\n",
      "2021-01-14 04:09:32,206 : INFO : token count processed\n",
      "2021-01-14 04:09:32,214 : INFO : alphabet_source #32007\n",
      "2021-01-14 04:09:32,219 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:09:32,220 : INFO : vocab #32006\n",
      "2021-01-14 04:09:32,228 : INFO : diff #{'```'}\n",
      "2021-01-14 04:09:50,898 : INFO : alphabet #32006\n",
      "2021-01-14 04:10:00,068 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[0.8877300918564258, 0.5297367480202548], [0.3129751682281494, 0.68702483], [4.585119276844752, 1.3885205227038642], [7.0153361396447735, 6.2276600107346916, 6.898111016451109, 6.344885133928356, -0.11722512319366452, 0.6704510057164175]]\n",
      "2021-01-14 04:10:00,073 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:10:00,074 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:10:00,076 : INFO : built Dictionary(328 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1883 corpus positions)\n",
      "2021-01-14 04:10:01,000 : INFO : token count processed\n",
      "2021-01-14 04:10:01,031 : INFO : frequencies processed\n",
      "2021-01-14 04:10:10,364 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:10:10,365 : INFO : entropies processed\n",
      "2021-01-14 04:10:10,366 : INFO : extropies processed\n",
      "2021-01-14 04:10:10,380 : INFO : token count processed\n",
      "2021-01-14 04:10:10,384 : INFO : alphabet_source #32007\n",
      "2021-01-14 04:10:10,388 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:10:10,389 : INFO : vocab #32006\n",
      "2021-01-14 04:10:10,395 : INFO : diff #{'```'}\n",
      "2021-01-14 04:10:29,073 : INFO : alphabet #32006\n",
      "2021-01-14 04:10:38,266 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[0.8279596401635626, 0.5470580301819584], [0.29488372802734375, 0.7051163], [4.837884147476473, 1.4014295234786647], [7.0153361396447735, 6.253918170574241, 6.876804877080533, 6.3924494331384825, -0.13853126256424098, 0.622886706506292]]\n",
      "2021-01-14 04:10:38,270 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:10:38,271 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:10:38,273 : INFO : built Dictionary(317 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 980 corpus positions)\n",
      "2021-01-14 04:10:39,031 : INFO : token count processed\n",
      "2021-01-14 04:10:39,084 : INFO : frequencies processed\n",
      "2021-01-14 04:10:48,366 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:10:48,367 : INFO : entropies processed\n",
      "2021-01-14 04:10:48,368 : INFO : extropies processed\n",
      "2021-01-14 04:10:48,375 : INFO : token count processed\n",
      "2021-01-14 04:10:48,382 : INFO : alphabet_source #32007\n",
      "2021-01-14 04:10:48,389 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:10:48,390 : INFO : vocab #32006\n",
      "2021-01-14 04:10:48,397 : INFO : diff #{'```'}\n",
      "2021-01-14 04:11:07,057 : INFO : alphabet #32006\n",
      "2021-01-14 04:11:16,410 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[0.9861539423650856, 0.5034856456338995], [0.46322762966156006, 0.5367724], [4.249142087060022, 1.371850073364545], [7.0153361396447735, 6.374522245625576, 7.3123598286504485, 6.077498556619901, 0.297023689005675, 0.9378375830248729]]\n",
      "2021-01-14 04:11:16,416 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:11:16,416 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:11:16,419 : INFO : built Dictionary(420 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 2262 corpus positions)\n",
      "2021-01-14 04:11:18,029 : INFO : token count processed\n",
      "2021-01-14 04:11:18,084 : INFO : frequencies processed\n",
      "2021-01-14 04:11:27,268 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:11:27,270 : INFO : entropies processed\n",
      "2021-01-14 04:11:27,270 : INFO : extropies processed\n",
      "2021-01-14 04:11:27,277 : INFO : token count processed\n",
      "2021-01-14 04:11:27,281 : INFO : alphabet_source #32007\n",
      "2021-01-14 04:11:27,285 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:11:27,286 : INFO : vocab #32006\n",
      "2021-01-14 04:11:27,292 : INFO : diff #{'```'}\n",
      "2021-01-14 04:11:46,098 : INFO : alphabet #32006\n",
      "2021-01-14 04:11:55,434 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_triage.py')[[0.8570412297339508, 0.5384910060092016], [0.34733664989471436, 0.65266335], [4.8511723650615455, 1.400351232244277], [7.0153361396447735, 6.731238669067808, 7.326608365993723, 6.419966442718858, 0.31127222634894913, 0.595369696925915]]\n",
      "2021-01-14 04:11:55,439 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:11:55,440 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:11:55,443 : INFO : built Dictionary(346 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1654 corpus positions)\n",
      "2021-01-14 04:11:56,408 : INFO : token count processed\n",
      "2021-01-14 04:11:56,472 : INFO : frequencies processed\n",
      "2021-01-14 04:12:05,763 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:12:05,764 : INFO : entropies processed\n",
      "2021-01-14 04:12:05,765 : INFO : extropies processed\n",
      "2021-01-14 04:12:05,772 : INFO : token count processed\n",
      "2021-01-14 04:12:05,779 : INFO : alphabet_source #32007\n",
      "2021-01-14 04:12:05,784 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:12:05,785 : INFO : vocab #32006\n",
      "2021-01-14 04:12:05,792 : INFO : diff #{'```'}\n",
      "2021-01-14 04:12:24,275 : INFO : alphabet #32006\n",
      "2021-01-14 04:12:33,466 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[0.8559430007523053, 0.5388096507245377], [0.30276715755462646, 0.69723284], [4.836009434332937, 1.4009486591265745], [7.0153361396447735, 6.503741451859337, 7.138328064904982, 6.3807495265991285, 0.12299192526020875, 0.634586613045645]]\n",
      "2021-01-14 04:12:33,471 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:12:33,472 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:12:33,476 : INFO : built Dictionary(367 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 1976 corpus positions)\n",
      "2021-01-14 04:12:34,669 : INFO : token count processed\n",
      "2021-01-14 04:12:34,697 : INFO : frequencies processed\n",
      "2021-01-14 04:12:43,990 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:12:43,991 : INFO : entropies processed\n",
      "2021-01-14 04:12:43,992 : INFO : extropies processed\n",
      "2021-01-14 04:12:43,999 : INFO : token count processed\n",
      "2021-01-14 04:12:44,006 : INFO : alphabet_source #32007\n",
      "2021-01-14 04:12:44,011 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:12:44,012 : INFO : vocab #32006\n",
      "2021-01-14 04:12:44,020 : INFO : diff #{'```'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 04:13:02,520 : INFO : alphabet #32006\n",
      "2021-01-14 04:13:11,849 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[0.8462310236268596, 0.541644023528287], [0.3033960461616516, 0.69660395], [4.913171093159632, 1.403662735553462], [7.0153361396447735, 6.334729224484471, 6.889710175596275, 6.4603551885329695, -0.12562596404849824, 0.554980951111804]]\n",
      "2021-01-14 04:13:11,855 : INFO : Removed 3 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:13:11,856 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:13:11,858 : INFO : built Dictionary(364 unique tokens: ['\"', '\",', '\"/', '\":', '#']...) from 2 documents (total 2325 corpus positions)\n",
      "2021-01-14 04:13:12,908 : INFO : token count processed\n",
      "2021-01-14 04:13:12,937 : INFO : frequencies processed\n",
      "2021-01-14 04:13:22,245 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:13:22,246 : INFO : entropies processed\n",
      "2021-01-14 04:13:22,247 : INFO : extropies processed\n",
      "2021-01-14 04:13:22,254 : INFO : token count processed\n",
      "2021-01-14 04:13:22,261 : INFO : alphabet_source #32007\n",
      "2021-01-14 04:13:22,267 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:13:22,268 : INFO : vocab #32006\n",
      "2021-01-14 04:13:22,274 : INFO : diff #{'```'}\n",
      "2021-01-14 04:13:40,628 : INFO : alphabet #32006\n",
      "2021-01-14 04:13:50,050 : INFO : Computed distances or similarities ('294', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[0.8718475724240334, 0.5342315339838302], [0.42359286546707153, 0.57640713], [4.867882598590331, 1.4016839146277003], [7.0153361396447735, 6.21319712067992, 6.808857672991561, 6.419675587333133, -0.20647846665321268, 0.5956605523116405]]\n",
      "2021-01-14 04:13:50,055 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:13:50,056 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:13:50,057 : INFO : built Dictionary(337 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 1543 corpus positions)\n",
      "2021-01-14 04:13:50,826 : INFO : token count processed\n",
      "2021-01-14 04:13:50,880 : INFO : frequencies processed\n",
      "2021-01-14 04:14:00,046 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:14:00,047 : INFO : entropies processed\n",
      "2021-01-14 04:14:00,048 : INFO : extropies processed\n",
      "2021-01-14 04:14:00,055 : INFO : token count processed\n",
      "2021-01-14 04:14:00,062 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:14:00,067 : INFO : alphabet_target #32010\n",
      "2021-01-14 04:14:00,068 : INFO : vocab #32006\n",
      "2021-01-14 04:14:00,075 : INFO : diff #set()\n",
      "2021-01-14 04:14:18,556 : INFO : alphabet #32006\n",
      "2021-01-14 04:14:27,699 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.0739131332382614, 0.48218027263204327], [0.6549377739429474, 0.34506223], [4.234064642253351, 1.3933501814223557], [6.389076609206845, 6.905617163738059, 7.301634897018465, 5.9930588759264385, 0.9125582878116196, 0.3960177332804058]]\n",
      "2021-01-14 04:14:27,705 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:14:27,707 : INFO : built Dictionary(427 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 2433 corpus positions)\n",
      "2021-01-14 04:14:28,848 : INFO : token count processed\n",
      "2021-01-14 04:14:28,880 : INFO : frequencies processed\n",
      "2021-01-14 04:14:38,202 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:14:38,204 : INFO : entropies processed\n",
      "2021-01-14 04:14:38,204 : INFO : extropies processed\n",
      "2021-01-14 04:14:38,212 : INFO : token count processed\n",
      "2021-01-14 04:14:38,216 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:14:38,220 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:14:38,220 : INFO : vocab #32006\n",
      "2021-01-14 04:14:38,227 : INFO : diff #set()\n",
      "2021-01-14 04:14:56,747 : INFO : alphabet #32006\n",
      "2021-01-14 04:15:05,940 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.1121946802585714, 0.4734412075489091], [0.6755239963531494, 0.324476], [4.310769595330944, 1.396853569807143], [6.389076609206845, 7.1219284286457345, 7.524653778481747, 5.986351259370833, 1.1355771692749022, 0.4027253498360128]]\n",
      "2021-01-14 04:15:05,946 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:15:05,950 : INFO : built Dictionary(350 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 2416 corpus positions)\n",
      "2021-01-14 04:15:06,703 : INFO : token count processed\n",
      "2021-01-14 04:15:06,731 : INFO : frequencies processed\n",
      "2021-01-14 04:15:15,930 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:15:15,931 : INFO : entropies processed\n",
      "2021-01-14 04:15:15,932 : INFO : extropies processed\n",
      "2021-01-14 04:15:15,939 : INFO : token count processed\n",
      "2021-01-14 04:15:15,946 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:15:15,953 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:15:15,954 : INFO : vocab #32006\n",
      "2021-01-14 04:15:15,960 : INFO : diff #set()\n",
      "2021-01-14 04:15:34,759 : INFO : alphabet #32006\n",
      "2021-01-14 04:15:43,932 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.16124879694155, 0.4626954570964394], [0.7360351979732513, 0.2639648], [4.009281150586607, 1.3827167972420802], [6.389076609206845, 6.41099024988467, 6.732639733542893, 6.0674271255486225, 0.34356312433604774, 0.32164948365822266]]\n",
      "2021-01-14 04:15:43,936 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:15:43,937 : INFO : built Dictionary(235 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 815 corpus positions)\n",
      "2021-01-14 04:15:44,312 : INFO : token count processed\n",
      "2021-01-14 04:15:44,338 : INFO : frequencies processed\n",
      "2021-01-14 04:15:53,495 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:15:53,496 : INFO : entropies processed\n",
      "2021-01-14 04:15:53,496 : INFO : extropies processed\n",
      "2021-01-14 04:15:53,504 : INFO : token count processed\n",
      "2021-01-14 04:15:53,508 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:15:53,512 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:15:53,513 : INFO : vocab #32006\n",
      "2021-01-14 04:15:53,522 : INFO : diff #set()\n",
      "2021-01-14 04:16:12,059 : INFO : alphabet #32006\n",
      "2021-01-14 04:16:21,395 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.1218155212042842, 0.471294506994853], [0.69777050614357, 0.3022295], [3.4556501805795428, 1.36208757872674], [6.389076609206845, 6.077866832717642, 6.774350462085948, 5.692592979838539, 0.3852738528791031, 0.6964836293683065]]\n",
      "2021-01-14 04:16:21,398 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:16:21,400 : INFO : built Dictionary(213 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 580 corpus positions)\n",
      "2021-01-14 04:16:21,726 : INFO : token count processed\n",
      "2021-01-14 04:16:21,759 : INFO : frequencies processed\n",
      "2021-01-14 04:16:31,284 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:16:31,285 : INFO : entropies processed\n",
      "2021-01-14 04:16:31,286 : INFO : extropies processed\n",
      "2021-01-14 04:16:31,292 : INFO : token count processed\n",
      "2021-01-14 04:16:31,296 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:16:31,301 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:16:31,302 : INFO : vocab #32006\n",
      "2021-01-14 04:16:31,308 : INFO : diff #set()\n",
      "2021-01-14 04:16:49,901 : INFO : alphabet #32006\n",
      "2021-01-14 04:16:59,070 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.1309727438940456, 0.4692692587764609], [0.69838747382164, 0.30161253], [3.246439344671016, 1.3515422895234475], [6.389076609206845, 5.977547459003844, 6.806443606652703, 5.560180461557987, 0.4173669974458578, 0.8288961476488588]]\n",
      "2021-01-14 04:16:59,076 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:16:59,078 : INFO : built Dictionary(312 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 2291 corpus positions)\n",
      "2021-01-14 04:16:59,733 : INFO : token count processed\n",
      "2021-01-14 04:16:59,786 : INFO : frequencies processed\n",
      "2021-01-14 04:17:09,233 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:17:09,234 : INFO : entropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 04:17:09,235 : INFO : extropies processed\n",
      "2021-01-14 04:17:09,241 : INFO : token count processed\n",
      "2021-01-14 04:17:09,248 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:17:09,255 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:17:09,257 : INFO : vocab #32006\n",
      "2021-01-14 04:17:09,263 : INFO : diff #set()\n",
      "2021-01-14 04:17:27,902 : INFO : alphabet #32006\n",
      "2021-01-14 04:17:37,081 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.1284051018246821, 0.46983537069268433], [0.7126915752887726, 0.28730842], [3.7152810859814975, 1.3656814977950282], [6.389076609206845, 6.4614394051846435, 6.822922083374266, 6.027593931017222, 0.4338454741674207, 0.3614826781896223]]\n",
      "2021-01-14 04:17:37,085 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:17:37,086 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:17:37,088 : INFO : built Dictionary(276 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 1515 corpus positions)\n",
      "2021-01-14 04:17:37,583 : INFO : token count processed\n",
      "2021-01-14 04:17:37,636 : INFO : frequencies processed\n",
      "2021-01-14 04:17:46,934 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:17:46,935 : INFO : entropies processed\n",
      "2021-01-14 04:17:46,936 : INFO : extropies processed\n",
      "2021-01-14 04:17:46,943 : INFO : token count processed\n",
      "2021-01-14 04:17:46,947 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:17:46,951 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:17:46,952 : INFO : vocab #32006\n",
      "2021-01-14 04:17:46,958 : INFO : diff #set()\n",
      "2021-01-14 04:18:05,356 : INFO : alphabet #32006\n",
      "2021-01-14 04:18:14,774 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.1015906286339026, 0.475830062418022], [0.6622015237808228, 0.33779848], [3.694739663055176, 1.3759976707750858], [6.389076609206845, 6.327195724598159, 6.783466537057004, 5.932805796748001, 0.3943899278501588, 0.45627081245884504]]\n",
      "2021-01-14 04:18:14,786 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:18:14,787 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:18:14,790 : INFO : built Dictionary(478 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 6407 corpus positions)\n",
      "2021-01-14 04:18:16,167 : INFO : token count processed\n",
      "2021-01-14 04:18:16,194 : INFO : frequencies processed\n",
      "2021-01-14 04:18:25,366 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:18:25,367 : INFO : entropies processed\n",
      "2021-01-14 04:18:25,368 : INFO : extropies processed\n",
      "2021-01-14 04:18:25,377 : INFO : token count processed\n",
      "2021-01-14 04:18:25,384 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:18:25,390 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:18:25,391 : INFO : vocab #32006\n",
      "2021-01-14 04:18:25,398 : INFO : diff #set()\n",
      "2021-01-14 04:18:43,914 : INFO : alphabet #32006\n",
      "2021-01-14 04:18:53,131 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.0923633821099474, 0.47792845571193093], [0.6487889885902405, 0.351211], [4.574825092434211, 1.4051211771836847], [6.389076609206845, 6.9079058562486315, 7.108924520931005, 6.188057944524472, 0.7198479117241599, 0.2010186646823735]]\n",
      "2021-01-14 04:18:53,138 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:18:53,140 : INFO : built Dictionary(387 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 2801 corpus positions)\n",
      "2021-01-14 04:18:54,070 : INFO : token count processed\n",
      "2021-01-14 04:18:54,100 : INFO : frequencies processed\n",
      "2021-01-14 04:19:03,396 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:19:03,397 : INFO : entropies processed\n",
      "2021-01-14 04:19:03,398 : INFO : extropies processed\n",
      "2021-01-14 04:19:03,405 : INFO : token count processed\n",
      "2021-01-14 04:19:03,409 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:19:03,413 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:19:03,414 : INFO : vocab #32006\n",
      "2021-01-14 04:19:03,420 : INFO : diff #set()\n",
      "2021-01-14 04:19:21,918 : INFO : alphabet #32006\n",
      "2021-01-14 04:19:31,091 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.1062610023741597, 0.47477496799912666], [0.6432033479213715, 0.35679665], [4.150491098258532, 1.3912426099466986], [6.389076609206845, 6.61034830706307, 6.9624168173531, 6.037008098916816, 0.5733402081462549, 0.35206851029002983]]\n",
      "2021-01-14 04:19:31,095 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:19:31,097 : INFO : built Dictionary(276 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 848 corpus positions)\n",
      "2021-01-14 04:19:31,642 : INFO : token count processed\n",
      "2021-01-14 04:19:31,669 : INFO : frequencies processed\n",
      "2021-01-14 04:19:41,040 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:19:41,041 : INFO : entropies processed\n",
      "2021-01-14 04:19:41,042 : INFO : extropies processed\n",
      "2021-01-14 04:19:41,055 : INFO : token count processed\n",
      "2021-01-14 04:19:41,059 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:19:41,063 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:19:41,064 : INFO : vocab #32006\n",
      "2021-01-14 04:19:41,070 : INFO : diff #set()\n",
      "2021-01-14 04:19:59,773 : INFO : alphabet #32006\n",
      "2021-01-14 04:20:09,312 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.108863908399106, 0.47418896782160125], [0.7340993285179138, 0.26590067], [3.896104175266961, 1.376640576831356], [6.389076609206845, 6.616715366949855, 7.185045653158657, 5.820746322998044, 0.7959690439518115, 0.568330286208802]]\n",
      "2021-01-14 04:20:09,318 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:20:09,321 : INFO : built Dictionary(481 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 2882 corpus positions)\n",
      "2021-01-14 04:20:10,809 : INFO : token count processed\n",
      "2021-01-14 04:20:10,872 : INFO : frequencies processed\n",
      "2021-01-14 04:20:20,064 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:20:20,065 : INFO : entropies processed\n",
      "2021-01-14 04:20:20,066 : INFO : extropies processed\n",
      "2021-01-14 04:20:20,073 : INFO : token count processed\n",
      "2021-01-14 04:20:20,080 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:20:20,084 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:20:20,085 : INFO : vocab #32006\n",
      "2021-01-14 04:20:20,092 : INFO : diff #set()\n",
      "2021-01-14 04:20:38,729 : INFO : alphabet #32006\n",
      "2021-01-14 04:20:47,901 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.0648597474996113, 0.484294393946574], [0.6185154020786285, 0.3814846], [4.794258456178913, 1.4096895473572233], [6.389076609206845, 7.32185870753746, 7.6593608987313315, 6.051574418012973, 1.2702842895244864, 0.33750219119387115]]\n",
      "2021-01-14 04:20:47,905 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:20:47,906 : INFO : built Dictionary(143 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 247 corpus positions)\n",
      "2021-01-14 04:20:48,032 : INFO : token count processed\n",
      "2021-01-14 04:20:48,090 : INFO : frequencies processed\n",
      "2021-01-14 04:20:57,391 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:20:57,392 : INFO : entropies processed\n",
      "2021-01-14 04:20:57,393 : INFO : extropies processed\n",
      "2021-01-14 04:20:57,399 : INFO : token count processed\n",
      "2021-01-14 04:20:57,406 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:20:57,411 : INFO : alphabet_target #32008\n",
      "2021-01-14 04:20:57,412 : INFO : vocab #32006\n",
      "2021-01-14 04:20:57,419 : INFO : diff #set()\n",
      "2021-01-14 04:21:15,814 : INFO : alphabet #32006\n",
      "2021-01-14 04:21:25,123 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/fireException.py')[[1.1609086602317047, 0.46276828743551535], [0.7468847036361694, 0.2531153], [2.0464393446710156, 1.2127889297821894], [6.389076609206845, 5.176618657501385, 6.813246095601442, 4.752449171106789, 0.4241694863945966, 1.6366274381000565]]\n",
      "2021-01-14 04:21:25,126 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:21:25,128 : INFO : built Dictionary(233 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 640 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 04:21:25,502 : INFO : token count processed\n",
      "2021-01-14 04:21:25,531 : INFO : frequencies processed\n",
      "2021-01-14 04:21:35,013 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:21:35,014 : INFO : entropies processed\n",
      "2021-01-14 04:21:35,015 : INFO : extropies processed\n",
      "2021-01-14 04:21:35,022 : INFO : token count processed\n",
      "2021-01-14 04:21:35,028 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:21:35,034 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:21:35,035 : INFO : vocab #32006\n",
      "2021-01-14 04:21:35,042 : INFO : diff #set()\n",
      "2021-01-14 04:21:53,379 : INFO : alphabet #32006\n",
      "2021-01-14 04:22:02,541 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.1309820246708644, 0.46926721503174224], [0.7169994115829468, 0.2830006], [3.203365087509175, 1.3459973708178854], [6.389076609206845, 6.468846789852156, 7.204558927001025, 5.653364472057976, 0.8154823177941797, 0.7357121371488686]]\n",
      "2021-01-14 04:22:02,547 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:22:02,550 : INFO : built Dictionary(436 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 2689 corpus positions)\n",
      "2021-01-14 04:22:03,741 : INFO : token count processed\n",
      "2021-01-14 04:22:03,769 : INFO : frequencies processed\n",
      "2021-01-14 04:22:13,031 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:22:13,032 : INFO : entropies processed\n",
      "2021-01-14 04:22:13,032 : INFO : extropies processed\n",
      "2021-01-14 04:22:13,039 : INFO : token count processed\n",
      "2021-01-14 04:22:13,046 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:22:13,051 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:22:13,052 : INFO : vocab #32006\n",
      "2021-01-14 04:22:13,058 : INFO : diff #set()\n",
      "2021-01-14 04:22:31,851 : INFO : alphabet #32006\n",
      "2021-01-14 04:22:41,149 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.129759598177618, 0.469536562180856], [0.69683638215065, 0.30316362], [4.3244041417367916, 1.3954887915387475], [6.389076609206845, 6.957796704012729, 7.286853943706423, 6.060019369513152, 0.8977773344995779, 0.32905723969369394]]\n",
      "2021-01-14 04:22:41,156 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:22:41,158 : INFO : built Dictionary(351 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 3186 corpus positions)\n",
      "2021-01-14 04:22:41,934 : INFO : token count processed\n",
      "2021-01-14 04:22:41,962 : INFO : frequencies processed\n",
      "2021-01-14 04:22:51,141 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:22:51,142 : INFO : entropies processed\n",
      "2021-01-14 04:22:51,143 : INFO : extropies processed\n",
      "2021-01-14 04:22:51,150 : INFO : token count processed\n",
      "2021-01-14 04:22:51,154 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:22:51,162 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:22:51,163 : INFO : vocab #32006\n",
      "2021-01-14 04:22:51,170 : INFO : diff #set()\n",
      "2021-01-14 04:23:09,867 : INFO : alphabet #32006\n",
      "2021-01-14 04:23:19,017 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.1248063427642232, 0.470631125234251], [0.725008636713028, 0.27499136], [3.8990401366454366, 1.3758359273705238], [6.389076609206845, 6.441859572014148, 6.7897285169217, 6.041207664299293, 0.40065190771485515, 0.34786894490755227]]\n",
      "2021-01-14 04:23:19,022 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:23:19,024 : INFO : built Dictionary(372 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 1662 corpus positions)\n",
      "2021-01-14 04:23:19,923 : INFO : token count processed\n",
      "2021-01-14 04:23:19,951 : INFO : frequencies processed\n",
      "2021-01-14 04:23:29,134 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:23:29,136 : INFO : entropies processed\n",
      "2021-01-14 04:23:29,136 : INFO : extropies processed\n",
      "2021-01-14 04:23:29,143 : INFO : token count processed\n",
      "2021-01-14 04:23:29,150 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:23:29,155 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:23:29,156 : INFO : vocab #32006\n",
      "2021-01-14 04:23:29,163 : INFO : diff #set()\n",
      "2021-01-14 04:23:47,664 : INFO : alphabet #32006\n",
      "2021-01-14 04:23:57,135 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.098916768045291, 0.47643623378705685], [0.667652040719986, 0.33234796], [4.307993543863673, 1.3962340717630748], [6.389076609206845, 6.998955278238291, 7.404165431892307, 5.983866455552829, 1.0150888226854615, 0.405210153654016]]\n",
      "2021-01-14 04:23:57,139 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:23:57,141 : INFO : built Dictionary(295 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 1481 corpus positions)\n",
      "2021-01-14 04:23:57,761 : INFO : token count processed\n",
      "2021-01-14 04:23:57,801 : INFO : frequencies processed\n",
      "2021-01-14 04:24:06,994 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:24:06,995 : INFO : entropies processed\n",
      "2021-01-14 04:24:06,995 : INFO : extropies processed\n",
      "2021-01-14 04:24:07,002 : INFO : token count processed\n",
      "2021-01-14 04:24:07,010 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:24:07,014 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:24:07,015 : INFO : vocab #32006\n",
      "2021-01-14 04:24:07,023 : INFO : diff #set()\n",
      "2021-01-14 04:24:25,567 : INFO : alphabet #32006\n",
      "2021-01-14 04:24:34,769 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.121700813551798, 0.4713199870654556], [0.6933006644248962, 0.30669934], [3.698746484310978, 1.3695447258137], [6.389076609206845, 6.492983191376071, 6.94356337967662, 5.938496420906295, 0.5544867704697749, 0.4505801883005489]]\n",
      "2021-01-14 04:24:34,776 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:24:34,779 : INFO : built Dictionary(507 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 3420 corpus positions)\n",
      "2021-01-14 04:24:36,210 : INFO : token count processed\n",
      "2021-01-14 04:24:36,237 : INFO : frequencies processed\n",
      "2021-01-14 04:24:45,411 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:24:45,412 : INFO : entropies processed\n",
      "2021-01-14 04:24:45,412 : INFO : extropies processed\n",
      "2021-01-14 04:24:45,420 : INFO : token count processed\n",
      "2021-01-14 04:24:45,423 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:24:45,427 : INFO : alphabet_target #32008\n",
      "2021-01-14 04:24:45,428 : INFO : vocab #32006\n",
      "2021-01-14 04:24:45,434 : INFO : diff #set()\n",
      "2021-01-14 04:25:03,944 : INFO : alphabet #32006\n",
      "2021-01-14 04:25:13,123 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.195190129098208, 0.4555414069809086], [0.7808016985654831, 0.2191983], [3.51602764126623, 1.3679893545177264], [6.389076609206845, 6.560342487747443, 6.952694180304077, 5.99672491665021, 0.5636175710972315, 0.39235169255663394]]\n",
      "2021-01-14 04:25:13,130 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:25:13,131 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:25:13,134 : INFO : built Dictionary(512 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 3625 corpus positions)\n",
      "2021-01-14 04:25:14,627 : INFO : token count processed\n",
      "2021-01-14 04:25:14,659 : INFO : frequencies processed\n",
      "2021-01-14 04:25:24,086 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:25:24,088 : INFO : entropies processed\n",
      "2021-01-14 04:25:24,088 : INFO : extropies processed\n",
      "2021-01-14 04:25:24,096 : INFO : token count processed\n",
      "2021-01-14 04:25:24,100 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:25:24,104 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:25:24,105 : INFO : vocab #32006\n",
      "2021-01-14 04:25:24,114 : INFO : diff #set()\n",
      "2021-01-14 04:25:42,531 : INFO : alphabet #32006\n",
      "2021-01-14 04:25:51,697 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.0979127214618154, 0.4766642528880824], [0.6328797340393066, 0.36712027], [4.394504709977216, 1.4007541998074204], [6.389076609206845, 7.046173750105238, 7.372807559881318, 6.062442799430764, 0.9837309506744729, 0.32663380977608014]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 04:25:51,708 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:25:51,709 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:25:51,712 : INFO : built Dictionary(558 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 5734 corpus positions)\n",
      "2021-01-14 04:25:53,694 : INFO : token count processed\n",
      "2021-01-14 04:25:53,720 : INFO : frequencies processed\n",
      "2021-01-14 04:26:03,110 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:26:03,111 : INFO : entropies processed\n",
      "2021-01-14 04:26:03,112 : INFO : extropies processed\n",
      "2021-01-14 04:26:03,126 : INFO : token count processed\n",
      "2021-01-14 04:26:03,130 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:26:03,134 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:26:03,135 : INFO : vocab #32006\n",
      "2021-01-14 04:26:03,141 : INFO : diff #set()\n",
      "2021-01-14 04:26:21,546 : INFO : alphabet #32006\n",
      "2021-01-14 04:26:30,710 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.0793300213652859, 0.480924138893258], [0.63196662068367, 0.36803338], [4.748969352518193, 1.4090111655206408], [6.389076609206845, 7.009229588004272, 7.212006251218927, 6.1862999459921895, 0.822929642012082, 0.20277666321465482]]\n",
      "2021-01-14 04:26:30,722 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:26:30,726 : INFO : built Dictionary(638 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 6675 corpus positions)\n",
      "2021-01-14 04:26:33,411 : INFO : token count processed\n",
      "2021-01-14 04:26:33,473 : INFO : frequencies processed\n",
      "2021-01-14 04:26:42,843 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:26:42,844 : INFO : entropies processed\n",
      "2021-01-14 04:26:42,845 : INFO : extropies processed\n",
      "2021-01-14 04:26:42,854 : INFO : token count processed\n",
      "2021-01-14 04:26:42,861 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:26:42,866 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:26:42,867 : INFO : vocab #32006\n",
      "2021-01-14 04:26:42,876 : INFO : diff #set()\n",
      "2021-01-14 04:27:01,268 : INFO : alphabet #32006\n",
      "2021-01-14 04:27:10,575 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.1056448748975374, 0.47491389071419793], [0.6524707973003387, 0.3475292], [4.714965837224829, 1.4083046346456571], [6.389076609206845, 7.376088004590871, 7.620363758020873, 6.144800855776842, 1.2312871488140278, 0.24427575343000196]]\n",
      "2021-01-14 04:27:10,579 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:27:10,580 : INFO : built Dictionary(211 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 461 corpus positions)\n",
      "2021-01-14 04:27:10,898 : INFO : token count processed\n",
      "2021-01-14 04:27:10,926 : INFO : frequencies processed\n",
      "2021-01-14 04:27:20,253 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:27:20,254 : INFO : entropies processed\n",
      "2021-01-14 04:27:20,255 : INFO : extropies processed\n",
      "2021-01-14 04:27:20,268 : INFO : token count processed\n",
      "2021-01-14 04:27:20,272 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:27:20,276 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:27:20,277 : INFO : vocab #32006\n",
      "2021-01-14 04:27:20,283 : INFO : diff #set()\n",
      "2021-01-14 04:27:38,758 : INFO : alphabet #32006\n",
      "2021-01-14 04:27:47,921 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.1278802246679824, 0.4699512634250982], [0.7057462632656097, 0.29425374], [3.010570934268484, 1.3392141119813181], [6.389076609206845, 6.2993628166120885, 7.1515439366537334, 5.536895489165201, 0.7624673274468883, 0.8521811200416449]]\n",
      "2021-01-14 04:27:47,924 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:27:47,925 : INFO : built Dictionary(115 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 177 corpus positions)\n",
      "2021-01-14 04:27:47,981 : INFO : token count processed\n",
      "2021-01-14 04:27:48,036 : INFO : frequencies processed\n",
      "2021-01-14 04:27:57,214 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:27:57,215 : INFO : entropies processed\n",
      "2021-01-14 04:27:57,216 : INFO : extropies processed\n",
      "2021-01-14 04:27:57,223 : INFO : token count processed\n",
      "2021-01-14 04:27:57,228 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:27:57,232 : INFO : alphabet_target #32008\n",
      "2021-01-14 04:27:57,232 : INFO : vocab #32006\n",
      "2021-01-14 04:27:57,238 : INFO : diff #set()\n",
      "2021-01-14 04:28:15,848 : INFO : alphabet #32006\n",
      "2021-01-14 04:28:24,996 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2373818070855032, 0.4469509838835407], [0.7960778623819351, 0.20392214], [0.8112781244591328, 0.8112781244591328], [6.389076609206845, 3.8936606896881862, 6.547229272412971, 3.7355080264820613, 0.15815266320612587, 2.653568582724785]]\n",
      "2021-01-14 04:28:25,018 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:28:25,023 : INFO : built Dictionary(785 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 12619 corpus positions)\n",
      "2021-01-14 04:28:29,180 : INFO : token count processed\n",
      "2021-01-14 04:28:29,208 : INFO : frequencies processed\n",
      "2021-01-14 04:28:38,637 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:28:38,638 : INFO : entropies processed\n",
      "2021-01-14 04:28:38,639 : INFO : extropies processed\n",
      "2021-01-14 04:28:38,649 : INFO : token count processed\n",
      "2021-01-14 04:28:38,653 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:28:38,657 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:28:38,657 : INFO : vocab #32006\n",
      "2021-01-14 04:28:38,663 : INFO : diff #set()\n",
      "2021-01-14 04:28:57,120 : INFO : alphabet #32006\n",
      "2021-01-14 04:29:06,275 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.0437001688416414, 0.4893085665138418], [0.6125180423259735, 0.38748196], [5.206088878049538, 1.4188692905705946], [6.389076609206845, 7.434393313070278, 7.676647280531926, 6.146822641745198, 1.2875706713250805, 0.2422539674616475]]\n",
      "2021-01-14 04:29:06,283 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:29:06,286 : INFO : built Dictionary(543 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 4246 corpus positions)\n",
      "2021-01-14 04:29:07,924 : INFO : token count processed\n",
      "2021-01-14 04:29:07,952 : INFO : frequencies processed\n",
      "2021-01-14 04:29:17,108 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:29:17,109 : INFO : entropies processed\n",
      "2021-01-14 04:29:17,110 : INFO : extropies processed\n",
      "2021-01-14 04:29:17,118 : INFO : token count processed\n",
      "2021-01-14 04:29:17,122 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:29:17,130 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:29:17,131 : INFO : vocab #32006\n",
      "2021-01-14 04:29:17,141 : INFO : diff #set()\n",
      "2021-01-14 04:29:35,837 : INFO : alphabet #32006\n",
      "2021-01-14 04:29:44,993 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.0560295758311278, 0.4863743263983743], [0.6016254723072052, 0.39837453], [4.9265112870627235, 1.4131911373543107], [6.389076609206845, 7.2991514951718255, 7.585304658680773, 6.102923445697897, 1.1962280494739277, 0.2861531635089474]]\n",
      "2021-01-14 04:29:45,001 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:29:45,003 : INFO : built Dictionary(517 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 3663 corpus positions)\n",
      "2021-01-14 04:29:46,602 : INFO : token count processed\n",
      "2021-01-14 04:29:46,629 : INFO : frequencies processed\n",
      "2021-01-14 04:29:56,051 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:29:56,052 : INFO : entropies processed\n",
      "2021-01-14 04:29:56,053 : INFO : extropies processed\n",
      "2021-01-14 04:29:56,060 : INFO : token count processed\n",
      "2021-01-14 04:29:56,067 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:29:56,072 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:29:56,073 : INFO : vocab #32006\n",
      "2021-01-14 04:29:56,081 : INFO : diff #set()\n",
      "2021-01-14 04:30:14,688 : INFO : alphabet #32006\n",
      "2021-01-14 04:30:23,851 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.0981263963289747, 0.476615709020042], [0.6406302750110626, 0.35936972], [4.556207559196899, 1.403736155759006], [6.389076609206845, 7.170319527000998, 7.484760182580752, 6.074635953627091, 1.0956835733739068, 0.3144406555797543]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 04:30:23,855 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:30:23,856 : INFO : built Dictionary(237 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 699 corpus positions)\n",
      "2021-01-14 04:30:24,244 : INFO : token count processed\n",
      "2021-01-14 04:30:24,281 : INFO : frequencies processed\n",
      "2021-01-14 04:30:33,562 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:30:33,563 : INFO : entropies processed\n",
      "2021-01-14 04:30:33,564 : INFO : extropies processed\n",
      "2021-01-14 04:30:33,571 : INFO : token count processed\n",
      "2021-01-14 04:30:33,575 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:30:33,581 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:30:33,583 : INFO : vocab #32006\n",
      "2021-01-14 04:30:33,592 : INFO : diff #set()\n",
      "2021-01-14 04:30:52,043 : INFO : alphabet #32006\n",
      "2021-01-14 04:31:01,198 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.0672599188362926, 0.48373210881141776], [0.651101142168045, 0.34889886], [3.7593525449170384, 1.370911136747703], [6.389076609206845, 6.353654804387375, 7.033499156752489, 5.709232256841732, 0.644422547545644, 0.6798443523651141]]\n",
      "2021-01-14 04:31:01,202 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:31:01,203 : INFO : built Dictionary(235 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 827 corpus positions)\n",
      "2021-01-14 04:31:01,597 : INFO : token count processed\n",
      "2021-01-14 04:31:01,626 : INFO : frequencies processed\n",
      "2021-01-14 04:31:10,911 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:31:10,912 : INFO : entropies processed\n",
      "2021-01-14 04:31:10,913 : INFO : extropies processed\n",
      "2021-01-14 04:31:10,920 : INFO : token count processed\n",
      "2021-01-14 04:31:10,927 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:31:10,932 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:31:10,933 : INFO : vocab #32006\n",
      "2021-01-14 04:31:10,940 : INFO : diff #set()\n",
      "2021-01-14 04:31:29,548 : INFO : alphabet #32006\n",
      "2021-01-14 04:31:38,821 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.0777108202927397, 0.4812989325719085], [0.6451336145401001, 0.3548664], [3.493344863722962, 1.3577876609222252], [6.389076609206845, 6.245180322479091, 6.8851696321325795, 5.749087299553357, 0.4960930229257343, 0.6399893096534885]]\n",
      "2021-01-14 04:31:38,826 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:31:38,828 : INFO : built Dictionary(444 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 1899 corpus positions)\n",
      "2021-01-14 04:31:40,071 : INFO : token count processed\n",
      "2021-01-14 04:31:40,109 : INFO : frequencies processed\n",
      "2021-01-14 04:31:49,285 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:31:49,286 : INFO : entropies processed\n",
      "2021-01-14 04:31:49,287 : INFO : extropies processed\n",
      "2021-01-14 04:31:49,294 : INFO : token count processed\n",
      "2021-01-14 04:31:49,301 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:31:49,307 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:31:49,308 : INFO : vocab #32006\n",
      "2021-01-14 04:31:49,316 : INFO : diff #set()\n",
      "2021-01-14 04:32:07,816 : INFO : alphabet #32006\n",
      "2021-01-14 04:32:17,203 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.040967328698259, 0.4899637470619414], [0.5841728746891022, 0.41582713], [4.831199282938273, 1.4118291157430054], [6.389076609206845, 7.2691387000368, 7.633904813866282, 6.024310495377362, 1.244828204659437, 0.3647661138294822]]\n",
      "2021-01-14 04:32:17,209 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:32:17,211 : INFO : built Dictionary(373 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 1736 corpus positions)\n",
      "2021-01-14 04:32:18,204 : INFO : token count processed\n",
      "2021-01-14 04:32:18,265 : INFO : frequencies processed\n",
      "2021-01-14 04:32:27,563 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:32:27,564 : INFO : entropies processed\n",
      "2021-01-14 04:32:27,564 : INFO : extropies processed\n",
      "2021-01-14 04:32:27,572 : INFO : token count processed\n",
      "2021-01-14 04:32:27,575 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:32:27,579 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:32:27,580 : INFO : vocab #32006\n",
      "2021-01-14 04:32:27,586 : INFO : diff #set()\n",
      "2021-01-14 04:32:46,250 : INFO : alphabet #32006\n",
      "2021-01-14 04:32:55,588 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.0228400966838198, 0.49435444830234904], [0.568613737821579, 0.43138626], [4.687430890979732, 1.408317533910048], [6.389076609206845, 7.08857858466988, 7.44248131882982, 6.035173875046905, 1.053404709622975, 0.35390273415994056]]\n",
      "2021-01-14 04:32:55,591 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:32:55,593 : INFO : built Dictionary(215 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 659 corpus positions)\n",
      "2021-01-14 04:32:55,927 : INFO : token count processed\n",
      "2021-01-14 04:32:55,982 : INFO : frequencies processed\n",
      "2021-01-14 04:33:05,260 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:33:05,261 : INFO : entropies processed\n",
      "2021-01-14 04:33:05,262 : INFO : extropies processed\n",
      "2021-01-14 04:33:05,275 : INFO : token count processed\n",
      "2021-01-14 04:33:05,279 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:33:05,283 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:33:05,284 : INFO : vocab #32006\n",
      "2021-01-14 04:33:05,291 : INFO : diff #set()\n",
      "2021-01-14 04:33:23,981 : INFO : alphabet #32006\n",
      "2021-01-14 04:33:33,134 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.1094048034093236, 0.47406737596489346], [0.669031411409378, 0.3309686], [3.332429281549887, 1.3442489208079398], [6.389076609206845, 6.0479231618016716, 6.826701586317007, 5.6102981846915085, 0.43762497711016213, 0.7787784245153357]]\n",
      "2021-01-14 04:33:33,138 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:33:33,139 : INFO : built Dictionary(218 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 778 corpus positions)\n",
      "2021-01-14 04:33:33,467 : INFO : token count processed\n",
      "2021-01-14 04:33:33,495 : INFO : frequencies processed\n",
      "2021-01-14 04:33:42,659 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:33:42,660 : INFO : entropies processed\n",
      "2021-01-14 04:33:42,661 : INFO : extropies processed\n",
      "2021-01-14 04:33:42,668 : INFO : token count processed\n",
      "2021-01-14 04:33:42,673 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:33:42,679 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:33:42,679 : INFO : vocab #32006\n",
      "2021-01-14 04:33:42,686 : INFO : diff #set()\n",
      "2021-01-14 04:34:01,270 : INFO : alphabet #32006\n",
      "2021-01-14 04:34:10,445 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.1164808302298648, 0.47248242729956264], [0.6597373187541962, 0.34026268], [3.2232715659654505, 1.3373072121330938], [6.389076609206845, 6.036583168403119, 6.751492175953853, 5.674167601656112, 0.3624155667470079, 0.7149090075507338]]\n",
      "2021-01-14 04:34:10,459 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:34:10,459 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:34:10,463 : INFO : built Dictionary(611 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 7136 corpus positions)\n",
      "2021-01-14 04:34:12,792 : INFO : token count processed\n",
      "2021-01-14 04:34:12,819 : INFO : frequencies processed\n",
      "2021-01-14 04:34:22,095 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:34:22,096 : INFO : entropies processed\n",
      "2021-01-14 04:34:22,096 : INFO : extropies processed\n",
      "2021-01-14 04:34:22,105 : INFO : token count processed\n",
      "2021-01-14 04:34:22,109 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:34:22,113 : INFO : alphabet_target #32010\n",
      "2021-01-14 04:34:22,114 : INFO : vocab #32006\n",
      "2021-01-14 04:34:22,124 : INFO : diff #set()\n",
      "2021-01-14 04:34:40,624 : INFO : alphabet #32006\n",
      "2021-01-14 04:34:49,796 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.0808091052789714, 0.4805822876606123], [0.6845678389072418, 0.31543216], [4.822332431723137, 1.4096943732382823], [6.389076609206845, 7.29352035514053, 7.5286321438447965, 6.153964820502579, 1.1395555346379513, 0.2351117887042662]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 04:34:49,803 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:34:49,805 : INFO : built Dictionary(426 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 3379 corpus positions)\n",
      "2021-01-14 04:34:50,949 : INFO : token count processed\n",
      "2021-01-14 04:34:50,980 : INFO : frequencies processed\n",
      "2021-01-14 04:35:00,527 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:35:00,528 : INFO : entropies processed\n",
      "2021-01-14 04:35:00,529 : INFO : extropies processed\n",
      "2021-01-14 04:35:00,544 : INFO : token count processed\n",
      "2021-01-14 04:35:00,548 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:35:00,552 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:35:00,553 : INFO : vocab #32006\n",
      "2021-01-14 04:35:00,559 : INFO : diff #set()\n",
      "2021-01-14 04:35:19,251 : INFO : alphabet #32006\n",
      "2021-01-14 04:35:28,427 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.1010820081501285, 0.4759452492196807], [0.6320040225982666, 0.36799598], [4.523138012041646, 1.4035260045020395], [6.389076609206845, 6.8153433747477745, 7.109739129558482, 6.094680854396136, 0.7206625203516372, 0.2943957548107079]]\n",
      "2021-01-14 04:35:28,430 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:35:28,432 : INFO : built Dictionary(195 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 400 corpus positions)\n",
      "2021-01-14 04:35:28,697 : INFO : token count processed\n",
      "2021-01-14 04:35:28,725 : INFO : frequencies processed\n",
      "2021-01-14 04:35:37,991 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:35:37,992 : INFO : entropies processed\n",
      "2021-01-14 04:35:37,993 : INFO : extropies processed\n",
      "2021-01-14 04:35:37,999 : INFO : token count processed\n",
      "2021-01-14 04:35:38,006 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:35:38,011 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:35:38,012 : INFO : vocab #32006\n",
      "2021-01-14 04:35:38,019 : INFO : diff #set()\n",
      "2021-01-14 04:35:56,797 : INFO : alphabet #32006\n",
      "2021-01-14 04:36:06,110 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.1170729463578841, 0.47235028047585914], [0.6909676492214203, 0.30903235], [3.1182751607709758, 1.3442614485447226], [6.389076609206845, 6.150121915859574, 7.066682204295402, 5.4725163207710175, 0.6776055950885569, 0.9165602884358277]]\n",
      "2021-01-14 04:36:06,114 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:36:06,116 : INFO : built Dictionary(343 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 1111 corpus positions)\n",
      "2021-01-14 04:36:06,879 : INFO : token count processed\n",
      "2021-01-14 04:36:06,906 : INFO : frequencies processed\n",
      "2021-01-14 04:36:16,080 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:36:16,081 : INFO : entropies processed\n",
      "2021-01-14 04:36:16,081 : INFO : extropies processed\n",
      "2021-01-14 04:36:16,088 : INFO : token count processed\n",
      "2021-01-14 04:36:16,092 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:36:16,099 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:36:16,100 : INFO : vocab #32006\n",
      "2021-01-14 04:36:16,109 : INFO : diff #set()\n",
      "2021-01-14 04:36:34,582 : INFO : alphabet #32006\n",
      "2021-01-14 04:36:43,905 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.0452629092814933, 0.48893469659179556], [0.603350430727005, 0.39664957], [4.402407696223123, 1.397814105363016], [6.389076609206845, 7.0391145208191315, 7.540587323202426, 5.88760380682355, 1.151510713995581, 0.5014728023832946]]\n",
      "2021-01-14 04:36:43,914 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:36:43,915 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:36:43,918 : INFO : built Dictionary(630 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 4486 corpus positions)\n",
      "2021-01-14 04:36:46,466 : INFO : token count processed\n",
      "2021-01-14 04:36:46,497 : INFO : frequencies processed\n",
      "2021-01-14 04:36:56,126 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:36:56,127 : INFO : entropies processed\n",
      "2021-01-14 04:36:56,128 : INFO : extropies processed\n",
      "2021-01-14 04:36:56,136 : INFO : token count processed\n",
      "2021-01-14 04:36:56,143 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:36:56,148 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:36:56,149 : INFO : vocab #32006\n",
      "2021-01-14 04:36:56,156 : INFO : diff #set()\n",
      "2021-01-14 04:37:14,665 : INFO : alphabet #32006\n",
      "2021-01-14 04:37:23,844 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.044947406843242, 0.4890101313381387], [0.5884851217269897, 0.41151488], [5.199035798964532, 1.417204013319995], [6.389076609206845, 7.482466367279176, 7.836483363591566, 6.035059612894455, 1.4474067543847209, 0.3540169963123905]]\n",
      "2021-01-14 04:37:23,848 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:37:23,849 : INFO : built Dictionary(239 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 708 corpus positions)\n",
      "2021-01-14 04:37:24,265 : INFO : token count processed\n",
      "2021-01-14 04:37:24,328 : INFO : frequencies processed\n",
      "2021-01-14 04:37:33,699 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:37:33,700 : INFO : entropies processed\n",
      "2021-01-14 04:37:33,701 : INFO : extropies processed\n",
      "2021-01-14 04:37:33,708 : INFO : token count processed\n",
      "2021-01-14 04:37:33,714 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:37:33,720 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:37:33,721 : INFO : vocab #32006\n",
      "2021-01-14 04:37:33,727 : INFO : diff #set()\n",
      "2021-01-14 04:37:52,216 : INFO : alphabet #32006\n",
      "2021-01-14 04:38:01,384 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.1014993040854526, 0.4758507404955759], [0.6606591045856476, 0.3393409], [3.5336233414528584, 1.3560559642062078], [6.389076609206845, 6.372162341197667, 7.064714643127356, 5.696524307277155, 0.675638033920511, 0.6925523019296893]]\n",
      "2021-01-14 04:38:01,390 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:38:01,392 : INFO : built Dictionary(381 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 2129 corpus positions)\n",
      "2021-01-14 04:38:02,348 : INFO : token count processed\n",
      "2021-01-14 04:38:02,404 : INFO : frequencies processed\n",
      "2021-01-14 04:38:11,600 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:38:11,601 : INFO : entropies processed\n",
      "2021-01-14 04:38:11,602 : INFO : extropies processed\n",
      "2021-01-14 04:38:11,609 : INFO : token count processed\n",
      "2021-01-14 04:38:11,617 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:38:11,623 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:38:11,624 : INFO : vocab #32006\n",
      "2021-01-14 04:38:11,632 : INFO : diff #set()\n",
      "2021-01-14 04:38:30,188 : INFO : alphabet #32006\n",
      "2021-01-14 04:38:39,351 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.1124508906562038, 0.4733837858305732], [0.666551023721695, 0.33344898], [4.143444767289769, 1.3897211803401055], [6.389076609206845, 6.798155919669889, 7.209652349057979, 5.977580179818755, 0.820575739851134, 0.41149642938809006]]\n",
      "2021-01-14 04:38:39,355 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:38:39,356 : INFO : built Dictionary(243 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 845 corpus positions)\n",
      "2021-01-14 04:38:39,781 : INFO : token count processed\n",
      "2021-01-14 04:38:39,809 : INFO : frequencies processed\n",
      "2021-01-14 04:38:49,069 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:38:49,070 : INFO : entropies processed\n",
      "2021-01-14 04:38:49,071 : INFO : extropies processed\n",
      "2021-01-14 04:38:49,077 : INFO : token count processed\n",
      "2021-01-14 04:38:49,081 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:38:49,088 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:38:49,089 : INFO : vocab #32006\n",
      "2021-01-14 04:38:49,097 : INFO : diff #set()\n",
      "2021-01-14 04:39:07,688 : INFO : alphabet #32006\n",
      "2021-01-14 04:39:16,856 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.105704845149486, 0.4749003652166689], [0.6684599220752716, 0.33154008], [3.6748008763572906, 1.3669354456905172], [6.389076609206845, 6.271631856729336, 6.929336959130101, 5.731371506806081, 0.5402603499232557, 0.6577051024007652]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 04:39:16,863 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:39:16,865 : INFO : built Dictionary(404 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 3360 corpus positions)\n",
      "2021-01-14 04:39:17,888 : INFO : token count processed\n",
      "2021-01-14 04:39:17,945 : INFO : frequencies processed\n",
      "2021-01-14 04:39:27,237 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:39:27,238 : INFO : entropies processed\n",
      "2021-01-14 04:39:27,239 : INFO : extropies processed\n",
      "2021-01-14 04:39:27,252 : INFO : token count processed\n",
      "2021-01-14 04:39:27,257 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:39:27,261 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:39:27,262 : INFO : vocab #32006\n",
      "2021-01-14 04:39:27,268 : INFO : diff #set()\n",
      "2021-01-14 04:39:45,810 : INFO : alphabet #32006\n",
      "2021-01-14 04:39:55,152 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.0275254167092367, 0.4932120661762377], [0.5855162441730499, 0.41448376], [4.7485488551867805, 1.409191059264904], [6.389076609206845, 6.873598627629562, 7.155116527584151, 6.107558709252257, 0.7660399183773059, 0.2815178999545891]]\n",
      "2021-01-14 04:39:55,156 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:39:55,157 : INFO : built Dictionary(179 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 308 corpus positions)\n",
      "2021-01-14 04:39:55,372 : INFO : token count processed\n",
      "2021-01-14 04:39:55,399 : INFO : frequencies processed\n",
      "2021-01-14 04:40:04,669 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:40:04,670 : INFO : entropies processed\n",
      "2021-01-14 04:40:04,671 : INFO : extropies processed\n",
      "2021-01-14 04:40:04,677 : INFO : token count processed\n",
      "2021-01-14 04:40:04,685 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:40:04,691 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:40:04,692 : INFO : vocab #32006\n",
      "2021-01-14 04:40:04,698 : INFO : diff #set()\n",
      "2021-01-14 04:40:23,289 : INFO : alphabet #32006\n",
      "2021-01-14 04:40:32,686 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.0917125588537404, 0.4780771601562695], [0.6961250305175781, 0.30387497], [2.6835423624332306, 1.3142075057096265], [6.389076609206845, 6.049830202851529, 7.096655590557294, 5.34225122150108, 0.7075789813504487, 1.0468253877057645]]\n",
      "2021-01-14 04:40:32,691 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:40:32,692 : INFO : built Dictionary(311 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 1388 corpus positions)\n",
      "2021-01-14 04:40:33,355 : INFO : token count processed\n",
      "2021-01-14 04:40:33,395 : INFO : frequencies processed\n",
      "2021-01-14 04:40:42,568 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:40:42,569 : INFO : entropies processed\n",
      "2021-01-14 04:40:42,570 : INFO : extropies processed\n",
      "2021-01-14 04:40:42,576 : INFO : token count processed\n",
      "2021-01-14 04:40:42,583 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:40:42,588 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:40:42,589 : INFO : vocab #32006\n",
      "2021-01-14 04:40:42,596 : INFO : diff #set()\n",
      "2021-01-14 04:41:01,093 : INFO : alphabet #32006\n",
      "2021-01-14 04:41:10,399 : INFO : Computed distances or similarities ('293', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.0655500383387198, 0.48413254650769916], [0.6524181365966797, 0.34758186], [4.228438191111384, 1.3944951818293603], [6.389076609206845, 6.778844940588858, 7.247524017692925, 5.920397532102779, 0.8584474084860796, 0.468679077104067]]\n",
      "2021-01-14 04:41:10,403 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:41:10,404 : INFO : built Dictionary(220 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 421 corpus positions)\n",
      "2021-01-14 04:41:10,753 : INFO : token count processed\n",
      "2021-01-14 04:41:10,781 : INFO : frequencies processed\n",
      "2021-01-14 04:41:19,953 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:41:19,954 : INFO : entropies processed\n",
      "2021-01-14 04:41:19,955 : INFO : extropies processed\n",
      "2021-01-14 04:41:19,961 : INFO : token count processed\n",
      "2021-01-14 04:41:19,965 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:41:19,969 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:41:19,970 : INFO : vocab #32006\n",
      "2021-01-14 04:41:19,982 : INFO : diff #set()\n",
      "2021-01-14 04:41:38,457 : INFO : alphabet #32006\n",
      "2021-01-14 04:41:47,710 : INFO : Computed distances or similarities ('293', 'sacp-python-common/setup.py')[[1.1206626950200824, 0.47155071023236445], [0.688500314950943, 0.31149969], [3.484183719779189, 1.363148481046742], [6.389076609206845, 6.469677430851302, 7.317025242175239, 5.541728797882909, 0.9279486329683939, 0.847347811323937]]\n",
      "2021-01-14 04:41:47,715 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:41:47,716 : INFO : built Dictionary(284 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 1323 corpus positions)\n",
      "2021-01-14 04:41:48,250 : INFO : token count processed\n",
      "2021-01-14 04:41:48,287 : INFO : frequencies processed\n",
      "2021-01-14 04:41:57,604 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:41:57,605 : INFO : entropies processed\n",
      "2021-01-14 04:41:57,606 : INFO : extropies processed\n",
      "2021-01-14 04:41:57,616 : INFO : token count processed\n",
      "2021-01-14 04:41:57,622 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:41:57,626 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:41:57,627 : INFO : vocab #32006\n",
      "2021-01-14 04:41:57,633 : INFO : diff #set()\n",
      "2021-01-14 04:42:16,152 : INFO : alphabet #32006\n",
      "2021-01-14 04:42:25,396 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.1319552249437708, 0.46905300275542816], [0.6961940228939056, 0.30380598], [3.9015785803191068, 1.3790955056709553], [6.389076609206845, 6.459180448028249, 6.977832550709617, 5.870424506525478, 0.5887559415027717, 0.5186521026813677]]\n",
      "2021-01-14 04:42:25,400 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:42:25,401 : INFO : built Dictionary(203 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 473 corpus positions)\n",
      "2021-01-14 04:42:25,694 : INFO : token count processed\n",
      "2021-01-14 04:42:25,739 : INFO : frequencies processed\n",
      "2021-01-14 04:42:34,974 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:42:34,975 : INFO : entropies processed\n",
      "2021-01-14 04:42:34,976 : INFO : extropies processed\n",
      "2021-01-14 04:42:34,983 : INFO : token count processed\n",
      "2021-01-14 04:42:34,988 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:42:34,994 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:42:34,995 : INFO : vocab #32006\n",
      "2021-01-14 04:42:35,002 : INFO : diff #set()\n",
      "2021-01-14 04:42:53,525 : INFO : alphabet #32006\n",
      "2021-01-14 04:43:02,676 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.1511429556938704, 0.46486915123567], [0.7043009400367737, 0.29569906], [3.077819531114783, 1.3342657629881893], [6.389076609206845, 6.097125733496388, 7.019831602438931, 5.466370740264303, 0.6307549932320855, 0.9227058689425425]]\n",
      "2021-01-14 04:43:02,679 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:43:02,680 : INFO : built Dictionary(194 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 469 corpus positions)\n",
      "2021-01-14 04:43:02,946 : INFO : token count processed\n",
      "2021-01-14 04:43:02,976 : INFO : frequencies processed\n",
      "2021-01-14 04:43:12,363 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:43:12,364 : INFO : entropies processed\n",
      "2021-01-14 04:43:12,364 : INFO : extropies processed\n",
      "2021-01-14 04:43:12,371 : INFO : token count processed\n",
      "2021-01-14 04:43:12,377 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:43:12,383 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:43:12,384 : INFO : vocab #32006\n",
      "2021-01-14 04:43:12,390 : INFO : diff #set()\n",
      "2021-01-14 04:43:30,946 : INFO : alphabet #32006\n",
      "2021-01-14 04:43:40,266 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.13428797213709, 0.4685403343198749], [0.6946528553962708, 0.30534714], [3.1841837197791887, 1.3432870681129108], [6.389076609206845, 6.0695858597523715, 6.982200374690218, 5.4764620942689985, 0.593123765483373, 0.9126145149378466]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 04:43:40,270 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:43:40,271 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:43:40,272 : INFO : built Dictionary(196 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 565 corpus positions)\n",
      "2021-01-14 04:43:40,548 : INFO : token count processed\n",
      "2021-01-14 04:43:40,577 : INFO : frequencies processed\n",
      "2021-01-14 04:43:49,922 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:43:49,923 : INFO : entropies processed\n",
      "2021-01-14 04:43:49,924 : INFO : extropies processed\n",
      "2021-01-14 04:43:49,930 : INFO : token count processed\n",
      "2021-01-14 04:43:49,937 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:43:49,943 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:43:49,944 : INFO : vocab #32006\n",
      "2021-01-14 04:43:49,950 : INFO : diff #set()\n",
      "2021-01-14 04:44:08,354 : INFO : alphabet #32006\n",
      "2021-01-14 04:44:17,542 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.1336395668658354, 0.46868272201613165], [0.7122378349304199, 0.28776217], [3.2841837197791888, 1.3555518397614796], [6.389076609206845, 6.104787343210121, 6.931986220934041, 5.561877731482925, 0.5429096117271959, 0.8271988777239203]]\n",
      "2021-01-14 04:44:17,559 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 04:44:17,560 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:44:17,568 : INFO : built Dictionary(470 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 9250 corpus positions)\n",
      "2021-01-14 04:44:18,874 : INFO : token count processed\n",
      "2021-01-14 04:44:18,902 : INFO : frequencies processed\n",
      "2021-01-14 04:44:28,306 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:44:28,307 : INFO : entropies processed\n",
      "2021-01-14 04:44:28,308 : INFO : extropies processed\n",
      "2021-01-14 04:44:28,318 : INFO : token count processed\n",
      "2021-01-14 04:44:28,322 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:44:28,330 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:44:28,331 : INFO : vocab #32006\n",
      "2021-01-14 04:44:28,338 : INFO : diff #set()\n",
      "2021-01-14 04:44:46,789 : INFO : alphabet #32006\n",
      "2021-01-14 04:44:56,061 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.1168711954481583, 0.472395298377279], [0.6876550614833832, 0.31234494], [4.3665208755696945, 1.3984317887690498], [6.389076609206845, 6.89087415148015, 7.123094475770206, 6.156856284916789, 0.7340178665633612, 0.232220324290056]]\n",
      "2021-01-14 04:44:56,067 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:44:56,069 : INFO : built Dictionary(330 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 2410 corpus positions)\n",
      "2021-01-14 04:44:56,783 : INFO : token count processed\n",
      "2021-01-14 04:44:56,820 : INFO : frequencies processed\n",
      "2021-01-14 04:45:05,984 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:45:05,985 : INFO : entropies processed\n",
      "2021-01-14 04:45:05,986 : INFO : extropies processed\n",
      "2021-01-14 04:45:05,993 : INFO : token count processed\n",
      "2021-01-14 04:45:06,000 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:45:06,005 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:45:06,005 : INFO : vocab #32006\n",
      "2021-01-14 04:45:06,013 : INFO : diff #set()\n",
      "2021-01-14 04:45:24,578 : INFO : alphabet #32006\n",
      "2021-01-14 04:45:33,760 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.0898485380089713, 0.47850357660498893], [0.6290857791900635, 0.37091422], [4.228666028586766, 1.3946853415121925], [6.389076609206845, 6.655493573668506, 7.001150720703761, 6.04341946217159, 0.6120741114969155, 0.3456571470352543]]\n",
      "2021-01-14 04:45:33,765 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:45:33,766 : INFO : built Dictionary(305 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 1417 corpus positions)\n",
      "2021-01-14 04:45:34,407 : INFO : token count processed\n",
      "2021-01-14 04:45:34,435 : INFO : frequencies processed\n",
      "2021-01-14 04:45:43,592 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:45:43,593 : INFO : entropies processed\n",
      "2021-01-14 04:45:43,594 : INFO : extropies processed\n",
      "2021-01-14 04:45:43,600 : INFO : token count processed\n",
      "2021-01-14 04:45:43,604 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:45:43,608 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:45:43,609 : INFO : vocab #32006\n",
      "2021-01-14 04:45:43,615 : INFO : diff #set()\n",
      "2021-01-14 04:46:02,206 : INFO : alphabet #32006\n",
      "2021-01-14 04:46:11,373 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.0874370651407943, 0.47905635896742665], [0.6441561877727509, 0.3558438], [4.3592868014426, 1.3964368723534633], [6.389076609206845, 6.6236746347295465, 7.050490411162589, 5.962260832773803, 0.6614138019557441, 0.4268157764330427]]\n",
      "2021-01-14 04:46:11,378 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:46:11,380 : INFO : built Dictionary(304 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 1356 corpus positions)\n",
      "2021-01-14 04:46:12,002 : INFO : token count processed\n",
      "2021-01-14 04:46:12,030 : INFO : frequencies processed\n",
      "2021-01-14 04:46:21,349 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:46:21,350 : INFO : entropies processed\n",
      "2021-01-14 04:46:21,350 : INFO : extropies processed\n",
      "2021-01-14 04:46:21,357 : INFO : token count processed\n",
      "2021-01-14 04:46:21,361 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:46:21,365 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:46:21,366 : INFO : vocab #32006\n",
      "2021-01-14 04:46:21,372 : INFO : diff #set()\n",
      "2021-01-14 04:46:40,053 : INFO : alphabet #32006\n",
      "2021-01-14 04:46:49,240 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.0826330870545071, 0.48016139098909255], [0.596674770116806, 0.40332523], [4.415882824071533, 1.396400551637682], [6.389076609206845, 6.75472436518627, 7.171905244495498, 5.971895729897618, 0.7828286352886531, 0.4171808793092282]]\n",
      "2021-01-14 04:46:49,245 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:46:49,247 : INFO : built Dictionary(265 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 1151 corpus positions)\n",
      "2021-01-14 04:46:49,746 : INFO : token count processed\n",
      "2021-01-14 04:46:49,773 : INFO : frequencies processed\n",
      "2021-01-14 04:46:58,947 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:46:58,948 : INFO : entropies processed\n",
      "2021-01-14 04:46:58,949 : INFO : extropies processed\n",
      "2021-01-14 04:46:58,963 : INFO : token count processed\n",
      "2021-01-14 04:46:58,968 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:46:58,972 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:46:58,973 : INFO : vocab #32006\n",
      "2021-01-14 04:46:58,979 : INFO : diff #set()\n",
      "2021-01-14 04:47:17,534 : INFO : alphabet #32006\n",
      "2021-01-14 04:47:26,953 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.09877682932991, 0.4764680008018177], [0.5995434522628784, 0.40045655], [3.9454649524951084, 1.3873270536953675], [6.389076609206845, 6.597313085495733, 7.07964838195559, 5.906741312746987, 0.6905717727487453, 0.4823352964598575]]\n",
      "2021-01-14 04:47:26,957 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:47:26,959 : INFO : built Dictionary(284 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 1117 corpus positions)\n",
      "2021-01-14 04:47:27,543 : INFO : token count processed\n",
      "2021-01-14 04:47:27,584 : INFO : frequencies processed\n",
      "2021-01-14 04:47:36,938 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:47:36,939 : INFO : entropies processed\n",
      "2021-01-14 04:47:36,940 : INFO : extropies processed\n",
      "2021-01-14 04:47:36,946 : INFO : token count processed\n",
      "2021-01-14 04:47:36,952 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:47:36,956 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:47:36,957 : INFO : vocab #32006\n",
      "2021-01-14 04:47:36,964 : INFO : diff #set()\n",
      "2021-01-14 04:47:55,332 : INFO : alphabet #32006\n",
      "2021-01-14 04:48:04,492 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.0807577711028067, 0.4805941440603139], [0.6254212856292725, 0.3745787], [4.28282260840597, 1.3944341339800175], [6.389076609206845, 6.659481538516613, 7.130423471514713, 5.918134676208745, 0.7413468623078678, 0.47094193299809994]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 04:48:04,496 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:48:04,499 : INFO : built Dictionary(312 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 1279 corpus positions)\n",
      "2021-01-14 04:48:05,191 : INFO : token count processed\n",
      "2021-01-14 04:48:05,216 : INFO : frequencies processed\n",
      "2021-01-14 04:48:14,665 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:48:14,667 : INFO : entropies processed\n",
      "2021-01-14 04:48:14,668 : INFO : extropies processed\n",
      "2021-01-14 04:48:14,680 : INFO : token count processed\n",
      "2021-01-14 04:48:14,686 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:48:14,690 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:48:14,691 : INFO : vocab #32006\n",
      "2021-01-14 04:48:14,697 : INFO : diff #set()\n",
      "2021-01-14 04:48:33,151 : INFO : alphabet #32006\n",
      "2021-01-14 04:48:42,338 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.1069145058527212, 0.4746277066402725], [0.6246148347854614, 0.37538517], [4.076139155817885, 1.385907852410326], [6.389076609206845, 6.774682571479102, 7.197748279842722, 5.966010900843225, 0.808671670635877, 0.42306570836362045]]\n",
      "2021-01-14 04:48:42,353 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:48:42,356 : INFO : built Dictionary(489 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 8016 corpus positions)\n",
      "2021-01-14 04:48:43,892 : INFO : token count processed\n",
      "2021-01-14 04:48:43,918 : INFO : frequencies processed\n",
      "2021-01-14 04:48:53,188 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:48:53,189 : INFO : entropies processed\n",
      "2021-01-14 04:48:53,190 : INFO : extropies processed\n",
      "2021-01-14 04:48:53,199 : INFO : token count processed\n",
      "2021-01-14 04:48:53,208 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:48:53,215 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:48:53,216 : INFO : vocab #32006\n",
      "2021-01-14 04:48:53,224 : INFO : diff #set()\n",
      "2021-01-14 04:49:11,654 : INFO : alphabet #32006\n",
      "2021-01-14 04:49:21,111 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.104264421230483, 0.4752254469118682], [0.6956959962844849, 0.304304], [4.177558761254978, 1.3926849016708724], [6.389076609206845, 6.839453716525233, 7.0735074561222016, 6.155022869609875, 0.6844308469153564, 0.23405373959696885]]\n",
      "2021-01-14 04:49:21,117 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:49:21,120 : INFO : built Dictionary(387 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 2458 corpus positions)\n",
      "2021-01-14 04:49:22,013 : INFO : token count processed\n",
      "2021-01-14 04:49:22,041 : INFO : frequencies processed\n",
      "2021-01-14 04:49:31,195 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:49:31,196 : INFO : entropies processed\n",
      "2021-01-14 04:49:31,197 : INFO : extropies processed\n",
      "2021-01-14 04:49:31,210 : INFO : token count processed\n",
      "2021-01-14 04:49:31,215 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:49:31,219 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:49:31,220 : INFO : vocab #32006\n",
      "2021-01-14 04:49:31,226 : INFO : diff #set()\n",
      "2021-01-14 04:49:49,929 : INFO : alphabet #32006\n",
      "2021-01-14 04:49:59,096 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.0812404148651293, 0.4804826933292102], [0.5947050452232361, 0.40529495], [4.253340441068115, 1.3944829114053237], [6.389076609206845, 6.86432793886027, 7.167747092365261, 6.085657455701854, 0.7786704831584155, 0.3034191535049908]]\n",
      "2021-01-14 04:49:59,100 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:49:59,102 : INFO : built Dictionary(229 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 660 corpus positions)\n",
      "2021-01-14 04:49:59,456 : INFO : token count processed\n",
      "2021-01-14 04:49:59,484 : INFO : frequencies processed\n",
      "2021-01-14 04:50:08,635 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:50:08,636 : INFO : entropies processed\n",
      "2021-01-14 04:50:08,637 : INFO : extropies processed\n",
      "2021-01-14 04:50:08,643 : INFO : token count processed\n",
      "2021-01-14 04:50:08,647 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:50:08,652 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:50:08,653 : INFO : vocab #32006\n",
      "2021-01-14 04:50:08,664 : INFO : diff #set()\n",
      "2021-01-14 04:50:27,407 : INFO : alphabet #32006\n",
      "2021-01-14 04:50:36,589 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.0889512071699627, 0.47870912282090333], [0.5915543138980865, 0.4084457], [3.780394654123195, 1.3837982328389111], [6.389076609206845, 6.431978396403875, 7.099434116640069, 5.7216208889706515, 0.7103575074332236, 0.6674557202361937]]\n",
      "2021-01-14 04:50:36,593 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:50:36,594 : INFO : built Dictionary(280 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 814 corpus positions)\n",
      "2021-01-14 04:50:37,145 : INFO : token count processed\n",
      "2021-01-14 04:50:37,200 : INFO : frequencies processed\n",
      "2021-01-14 04:50:46,393 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:50:46,394 : INFO : entropies processed\n",
      "2021-01-14 04:50:46,395 : INFO : extropies processed\n",
      "2021-01-14 04:50:46,401 : INFO : token count processed\n",
      "2021-01-14 04:50:46,408 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:50:46,412 : INFO : alphabet_target #32009\n",
      "2021-01-14 04:50:46,413 : INFO : vocab #32006\n",
      "2021-01-14 04:50:46,420 : INFO : diff #set()\n",
      "2021-01-14 04:51:05,180 : INFO : alphabet #32006\n",
      "2021-01-14 04:51:14,352 : INFO : Computed distances or similarities ('293', 'sacp-python-common/test/python/test_auth_utility.py')[[1.068501453795755, 0.4834417680321053], [0.5930239260196686, 0.40697607], [4.225689273970116, 1.3914569861719606], [6.389076609206845, 6.911818353685893, 7.390105133006388, 5.910789829886349, 1.001028523799543, 0.478286779320495]]\n",
      "2021-01-14 04:51:14,365 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 04:51:14,368 : INFO : built Dictionary(368 unique tokens: [\"'\", '()', ')', ',', '.']...) from 2 documents (total 7344 corpus positions)\n",
      "2021-01-14 04:51:15,287 : INFO : token count processed\n",
      "2021-01-14 04:51:15,315 : INFO : frequencies processed\n",
      "2021-01-14 04:51:24,647 : INFO : scalar_distribution processed\n",
      "2021-01-14 04:51:24,649 : INFO : entropies processed\n",
      "2021-01-14 04:51:24,649 : INFO : extropies processed\n",
      "2021-01-14 04:51:24,658 : INFO : token count processed\n",
      "2021-01-14 04:51:24,662 : INFO : alphabet_source #32006\n",
      "2021-01-14 04:51:24,669 : INFO : alphabet_target #32009\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "2021-01-14 08:32:30,073 : INFO : extropies processed\n",
      "2021-01-14 08:32:30,079 : INFO : token count processed\n",
      "2021-01-14 08:32:30,083 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:32:30,089 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:32:30,090 : INFO : vocab #32006\n",
      "2021-01-14 08:32:30,098 : INFO : diff #set()\n",
      "2021-01-14 08:32:48,669 : INFO : alphabet #32006\n",
      "2021-01-14 08:32:57,956 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.1913165001696207, 0.4563466755818222], [0.8020265996456146, 0.1979734], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.049830202851529, 6.344276505643918, 3.953481210651197, 2.0963489922003324, 0.29444630279238826]]\n",
      "2021-01-14 08:32:57,960 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:32:57,963 : INFO : built Dictionary(248 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 1255 corpus positions)\n",
      "2021-01-14 08:32:58,084 : INFO : token count processed\n",
      "2021-01-14 08:32:58,112 : INFO : frequencies processed\n",
      "2021-01-14 08:33:07,534 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:33:07,535 : INFO : entropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 08:33:07,536 : INFO : extropies processed\n",
      "2021-01-14 08:33:07,543 : INFO : token count processed\n",
      "2021-01-14 08:33:07,550 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:33:07,555 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:33:07,556 : INFO : vocab #32006\n",
      "2021-01-14 08:33:07,563 : INFO : diff #set()\n",
      "2021-01-14 08:33:25,927 : INFO : alphabet #32006\n",
      "2021-01-14 08:33:35,374 : INFO : Computed distances or similarities ('292', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.1790340667383867, 0.45891893810399026], [0.7751737684011459, 0.22482623], [2.321928094887362, 1.2877123795494492], [4.247927513443585, 6.778844940588858, 6.963773208426213, 4.06299924560623, 2.7158456949826277, 0.18492826783735516]]\n",
      "2021-01-14 08:33:35,378 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:33:35,379 : INFO : built Dictionary(148 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 288 corpus positions)\n",
      "2021-01-14 08:33:35,438 : INFO : token count processed\n",
      "2021-01-14 08:33:35,466 : INFO : frequencies processed\n",
      "2021-01-14 08:33:44,619 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:33:44,620 : INFO : entropies processed\n",
      "2021-01-14 08:33:44,621 : INFO : extropies processed\n",
      "2021-01-14 08:33:44,627 : INFO : token count processed\n",
      "2021-01-14 08:33:44,631 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:33:44,635 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:33:44,636 : INFO : vocab #32006\n",
      "2021-01-14 08:33:44,642 : INFO : diff #set()\n",
      "2021-01-14 08:34:03,118 : INFO : alphabet #32006\n",
      "2021-01-14 08:34:12,293 : INFO : Computed distances or similarities ('292', 'sacp-python-common/setup.py')[[1.1750302110915478, 0.45976372875213806], [0.7757222056388855, 0.2242778], [2.0, 1.2451124978365313], [4.247927513443585, 6.469677430851302, 6.743306047488547, 3.9742988968063404, 2.4953785340449617, 0.2736286166372448]]\n",
      "2021-01-14 08:34:12,297 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:34:12,299 : INFO : built Dictionary(218 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 1190 corpus positions)\n",
      "2021-01-14 08:34:12,404 : INFO : token count processed\n",
      "2021-01-14 08:34:12,437 : INFO : frequencies processed\n",
      "2021-01-14 08:34:21,763 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:34:21,764 : INFO : entropies processed\n",
      "2021-01-14 08:34:21,765 : INFO : extropies processed\n",
      "2021-01-14 08:34:21,772 : INFO : token count processed\n",
      "2021-01-14 08:34:21,776 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:34:21,780 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:34:21,781 : INFO : vocab #32006\n",
      "2021-01-14 08:34:21,789 : INFO : diff #set()\n",
      "2021-01-14 08:34:40,657 : INFO : alphabet #32006\n",
      "2021-01-14 08:34:49,835 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.1856371307915168, 0.4575324906005121], [0.7872826904058456, 0.21271731], [2.0, 1.2451124978365313], [4.247927513443585, 6.459180448028249, 6.6267287284566, 4.080379233015234, 2.3788012150130147, 0.1675482804283508]]\n",
      "2021-01-14 08:34:49,839 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:34:49,840 : INFO : built Dictionary(129 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 340 corpus positions)\n",
      "2021-01-14 08:34:49,898 : INFO : token count processed\n",
      "2021-01-14 08:34:49,927 : INFO : frequencies processed\n",
      "2021-01-14 08:34:59,232 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:34:59,233 : INFO : entropies processed\n",
      "2021-01-14 08:34:59,234 : INFO : extropies processed\n",
      "2021-01-14 08:34:59,247 : INFO : token count processed\n",
      "2021-01-14 08:34:59,252 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:34:59,256 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:34:59,257 : INFO : vocab #32006\n",
      "2021-01-14 08:34:59,263 : INFO : diff #set()\n",
      "2021-01-14 08:35:17,872 : INFO : alphabet #32006\n",
      "2021-01-14 08:35:27,062 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.1658883001998206, 0.4617043269995697], [0.7379921972751617, 0.2620078], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.097125733496388, 6.305986767551803, 4.039066479388169, 2.058059254108218, 0.20886103405541512]]\n",
      "2021-01-14 08:35:27,066 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:35:27,067 : INFO : built Dictionary(121 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 336 corpus positions)\n",
      "2021-01-14 08:35:27,118 : INFO : token count processed\n",
      "2021-01-14 08:35:27,149 : INFO : frequencies processed\n",
      "2021-01-14 08:35:36,321 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:35:36,322 : INFO : entropies processed\n",
      "2021-01-14 08:35:36,323 : INFO : extropies processed\n",
      "2021-01-14 08:35:36,329 : INFO : token count processed\n",
      "2021-01-14 08:35:36,335 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:35:36,339 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:35:36,340 : INFO : vocab #32006\n",
      "2021-01-14 08:35:36,346 : INFO : diff #set()\n",
      "2021-01-14 08:35:55,079 : INFO : alphabet #32006\n",
      "2021-01-14 08:36:04,255 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.1791067810655143, 0.4589036244983972], [0.7492460608482361, 0.25075394], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.0695858597523715, 6.276592903608295, 4.040920469587663, 2.0286653901647096, 0.20700704385592328]]\n",
      "2021-01-14 08:36:04,259 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 08:36:04,260 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:36:04,261 : INFO : built Dictionary(123 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 432 corpus positions)\n",
      "2021-01-14 08:36:04,319 : INFO : token count processed\n",
      "2021-01-14 08:36:04,349 : INFO : frequencies processed\n",
      "2021-01-14 08:36:13,981 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:36:13,982 : INFO : entropies processed\n",
      "2021-01-14 08:36:13,983 : INFO : extropies processed\n",
      "2021-01-14 08:36:13,990 : INFO : token count processed\n",
      "2021-01-14 08:36:13,997 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:36:14,003 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:36:14,004 : INFO : vocab #32006\n",
      "2021-01-14 08:36:14,010 : INFO : diff #set()\n",
      "2021-01-14 08:36:32,399 : INFO : alphabet #32006\n",
      "2021-01-14 08:36:41,840 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.1779758046773672, 0.4591419233640817], [0.7624823153018951, 0.23751768], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.104787343210121, 6.270964248207533, 4.081750608446172, 2.0230367347639477, 0.16617690499741222]]\n",
      "2021-01-14 08:36:41,857 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 08:36:41,858 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:36:41,861 : INFO : built Dictionary(406 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 9117 corpus positions)\n",
      "2021-01-14 08:36:42,105 : INFO : token count processed\n",
      "2021-01-14 08:36:42,136 : INFO : frequencies processed\n",
      "2021-01-14 08:36:51,572 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:36:51,573 : INFO : entropies processed\n",
      "2021-01-14 08:36:51,574 : INFO : extropies processed\n",
      "2021-01-14 08:36:51,583 : INFO : token count processed\n",
      "2021-01-14 08:36:51,587 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:36:51,591 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:36:51,592 : INFO : vocab #32006\n",
      "2021-01-14 08:36:51,598 : INFO : diff #set()\n",
      "2021-01-14 08:37:10,151 : INFO : alphabet #32006\n",
      "2021-01-14 08:37:19,449 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.1715726097032777, 0.46049576953203486], [0.7640213966369629, 0.2359786], [3.0, 1.3485155455967714], [4.247927513443585, 6.89087415148015, 7.045237890331957, 4.09356377459178, 2.7973103768883716, 0.15436373885180643]]\n",
      "2021-01-14 08:37:19,455 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:37:19,457 : INFO : built Dictionary(264 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 2277 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 08:37:19,579 : INFO : token count processed\n",
      "2021-01-14 08:37:19,607 : INFO : frequencies processed\n",
      "2021-01-14 08:37:28,776 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:37:28,777 : INFO : entropies processed\n",
      "2021-01-14 08:37:28,777 : INFO : extropies processed\n",
      "2021-01-14 08:37:28,784 : INFO : token count processed\n",
      "2021-01-14 08:37:28,788 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:37:28,791 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:37:28,792 : INFO : vocab #32006\n",
      "2021-01-14 08:37:28,798 : INFO : diff #set()\n",
      "2021-01-14 08:37:47,545 : INFO : alphabet #32006\n",
      "2021-01-14 08:37:57,069 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.144173373808558, 0.46638019677660864], [0.729208767414093, 0.27079123], [3.0, 1.3485155455967714], [4.247927513443585, 6.655493573668506, 6.784692921443643, 4.118728165668449, 2.5367654080000577, 0.12919934777513653]]\n",
      "2021-01-14 08:37:57,073 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:37:57,075 : INFO : built Dictionary(245 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 1284 corpus positions)\n",
      "2021-01-14 08:37:57,189 : INFO : token count processed\n",
      "2021-01-14 08:37:57,220 : INFO : frequencies processed\n",
      "2021-01-14 08:38:06,651 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:38:06,652 : INFO : entropies processed\n",
      "2021-01-14 08:38:06,652 : INFO : extropies processed\n",
      "2021-01-14 08:38:06,659 : INFO : token count processed\n",
      "2021-01-14 08:38:06,666 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:38:06,672 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:38:06,674 : INFO : vocab #32006\n",
      "2021-01-14 08:38:06,680 : INFO : diff #set()\n",
      "2021-01-14 08:38:25,202 : INFO : alphabet #32006\n",
      "2021-01-14 08:38:34,374 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.1539367177935131, 0.4642661930311477], [0.7128788530826569, 0.28712115], [2.0, 1.2451124978365313], [4.247927513443585, 6.6236746347295465, 6.743431901873401, 4.128170246299731, 2.495504388429816, 0.11975726714385448]]\n",
      "2021-01-14 08:38:34,379 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:38:34,380 : INFO : built Dictionary(246 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 1223 corpus positions)\n",
      "2021-01-14 08:38:34,489 : INFO : token count processed\n",
      "2021-01-14 08:38:34,516 : INFO : frequencies processed\n",
      "2021-01-14 08:38:43,681 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:38:43,682 : INFO : entropies processed\n",
      "2021-01-14 08:38:43,683 : INFO : extropies processed\n",
      "2021-01-14 08:38:43,689 : INFO : token count processed\n",
      "2021-01-14 08:38:43,696 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:38:43,701 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:38:43,702 : INFO : vocab #32006\n",
      "2021-01-14 08:38:43,709 : INFO : diff #set()\n",
      "2021-01-14 08:39:02,420 : INFO : alphabet #32006\n",
      "2021-01-14 08:39:11,806 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.1665838519311236, 0.46155610322151996], [0.7437762320041656, 0.25622377], [2.0, 1.2451124978365313], [4.247927513443585, 6.75472436518627, 6.8737705507744025, 4.128881327855453, 2.6258430373308173, 0.1190461855881324]]\n",
      "2021-01-14 08:39:11,810 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:39:11,813 : INFO : built Dictionary(198 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 1018 corpus positions)\n",
      "2021-01-14 08:39:11,912 : INFO : token count processed\n",
      "2021-01-14 08:39:11,981 : INFO : frequencies processed\n",
      "2021-01-14 08:39:21,240 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:39:21,241 : INFO : entropies processed\n",
      "2021-01-14 08:39:21,242 : INFO : extropies processed\n",
      "2021-01-14 08:39:21,248 : INFO : token count processed\n",
      "2021-01-14 08:39:21,251 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:39:21,258 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:39:21,259 : INFO : vocab #32006\n",
      "2021-01-14 08:39:21,267 : INFO : diff #set()\n",
      "2021-01-14 08:39:39,762 : INFO : alphabet #32006\n",
      "2021-01-14 08:39:48,943 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.167434713378047, 0.46137491193054386], [0.7245962619781494, 0.27540374], [2.0, 1.2451124978365313], [4.247927513443585, 6.597313085495733, 6.709989733121294, 4.1352508658180245, 2.4620622196777084, 0.11267664762556073]]\n",
      "2021-01-14 08:39:48,947 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:39:48,948 : INFO : built Dictionary(222 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 984 corpus positions)\n",
      "2021-01-14 08:39:49,046 : INFO : token count processed\n",
      "2021-01-14 08:39:49,072 : INFO : frequencies processed\n",
      "2021-01-14 08:39:58,466 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:39:58,467 : INFO : entropies processed\n",
      "2021-01-14 08:39:58,468 : INFO : extropies processed\n",
      "2021-01-14 08:39:58,475 : INFO : token count processed\n",
      "2021-01-14 08:39:58,482 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:39:58,489 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:39:58,490 : INFO : vocab #32006\n",
      "2021-01-14 08:39:58,496 : INFO : diff #set()\n",
      "2021-01-14 08:40:17,028 : INFO : alphabet #32006\n",
      "2021-01-14 08:40:26,184 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.1666334569986923, 0.4615455358956933], [0.7370660901069641, 0.2629339], [2.321928094887362, 1.2877123795494492], [4.247927513443585, 6.659481538516613, 6.788358737757573, 4.1190503142026245, 2.5404312243139877, 0.12887719924095986]]\n",
      "2021-01-14 08:40:26,188 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:40:26,191 : INFO : built Dictionary(247 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 1146 corpus positions)\n",
      "2021-01-14 08:40:26,309 : INFO : token count processed\n",
      "2021-01-14 08:40:26,337 : INFO : frequencies processed\n",
      "2021-01-14 08:40:35,608 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:40:35,609 : INFO : entropies processed\n",
      "2021-01-14 08:40:35,610 : INFO : extropies processed\n",
      "2021-01-14 08:40:35,624 : INFO : token count processed\n",
      "2021-01-14 08:40:35,628 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:40:35,632 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:40:35,633 : INFO : vocab #32006\n",
      "2021-01-14 08:40:35,639 : INFO : diff #set()\n",
      "2021-01-14 08:40:54,410 : INFO : alphabet #32006\n",
      "2021-01-14 08:41:03,730 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.1526420068091872, 0.46454542689254563], [0.697131484746933, 0.30286852], [2.321928094887362, 1.2877123795494492], [4.247927513443585, 6.774682571479102, 6.874729731052401, 4.147880353870285, 2.626802217608816, 0.10004715957329946]]\n",
      "2021-01-14 08:41:03,744 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:41:03,747 : INFO : built Dictionary(422 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 7883 corpus positions)\n",
      "2021-01-14 08:41:03,991 : INFO : token count processed\n",
      "2021-01-14 08:41:04,019 : INFO : frequencies processed\n",
      "2021-01-14 08:41:13,175 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:41:13,176 : INFO : entropies processed\n",
      "2021-01-14 08:41:13,177 : INFO : extropies processed\n",
      "2021-01-14 08:41:13,186 : INFO : token count processed\n",
      "2021-01-14 08:41:13,192 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:41:13,198 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:41:13,199 : INFO : vocab #32006\n",
      "2021-01-14 08:41:13,206 : INFO : diff #set()\n",
      "2021-01-14 08:41:31,594 : INFO : alphabet #32006\n",
      "2021-01-14 08:41:41,084 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.1708396859725732, 0.46065124313957945], [0.7811058014631271, 0.2188942], [3.0, 1.3485155455967714], [4.247927513443585, 6.839453716525233, 6.984571531352399, 4.102809698616419, 2.736644017908814, 0.14511781482716657]]\n",
      "2021-01-14 08:41:41,090 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 08:41:41,092 : INFO : built Dictionary(322 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 2325 corpus positions)\n",
      "2021-01-14 08:41:41,271 : INFO : token count processed\n",
      "2021-01-14 08:41:41,306 : INFO : frequencies processed\n",
      "2021-01-14 08:41:50,585 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:41:50,586 : INFO : entropies processed\n",
      "2021-01-14 08:41:50,587 : INFO : extropies processed\n",
      "2021-01-14 08:41:50,594 : INFO : token count processed\n",
      "2021-01-14 08:41:50,602 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:41:50,606 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:41:50,607 : INFO : vocab #32006\n",
      "2021-01-14 08:41:50,615 : INFO : diff #set()\n",
      "2021-01-14 08:42:08,997 : INFO : alphabet #32006\n",
      "2021-01-14 08:42:18,442 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.085288518013602, 0.4795499478185287], [0.6556228399276733, 0.34437716], [3.0, 1.3485155455967714], [4.247927513443585, 6.86432793886027, 6.966444454444111, 4.145810997859745, 2.7185169410005257, 0.10211651558384105]]\n",
      "2021-01-14 08:42:18,447 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:42:18,449 : INFO : built Dictionary(156 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 527 corpus positions)\n",
      "2021-01-14 08:42:18,523 : INFO : token count processed\n",
      "2021-01-14 08:42:18,551 : INFO : frequencies processed\n",
      "2021-01-14 08:42:27,788 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:42:27,789 : INFO : entropies processed\n",
      "2021-01-14 08:42:27,790 : INFO : extropies processed\n",
      "2021-01-14 08:42:27,797 : INFO : token count processed\n",
      "2021-01-14 08:42:27,804 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:42:27,809 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:42:27,811 : INFO : vocab #32006\n",
      "2021-01-14 08:42:27,818 : INFO : diff #set()\n",
      "2021-01-14 08:42:46,406 : INFO : alphabet #32006\n",
      "2021-01-14 08:42:55,581 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.0579484068840443, 0.48592083098628686], [0.6476327478885651, 0.35236725], [2.807354922057604, 1.3343545280186873], [4.247927513443585, 6.431978396403875, 6.545741716490287, 4.134164193357172, 2.297814203046702, 0.1137633200864121]]\n",
      "2021-01-14 08:42:55,585 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:42:55,586 : INFO : built Dictionary(218 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 681 corpus positions)\n",
      "2021-01-14 08:42:55,686 : INFO : token count processed\n",
      "2021-01-14 08:42:55,738 : INFO : frequencies processed\n",
      "2021-01-14 08:43:04,925 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:43:04,926 : INFO : entropies processed\n",
      "2021-01-14 08:43:04,927 : INFO : extropies processed\n",
      "2021-01-14 08:43:04,933 : INFO : token count processed\n",
      "2021-01-14 08:43:04,937 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:43:04,941 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:43:04,942 : INFO : vocab #32006\n",
      "2021-01-14 08:43:04,948 : INFO : diff #set()\n",
      "2021-01-14 08:43:23,700 : INFO : alphabet #32006\n",
      "2021-01-14 08:43:33,105 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/test_auth_utility.py')[[1.1664135372441395, 0.46159238889915927], [0.7101610898971558, 0.2898389], [2.0, 1.2451124978365313], [4.247927513443585, 6.911818353685893, 7.036028716568007, 4.123717150561472, 2.7881012031244214, 0.1242103628821134]]\n",
      "2021-01-14 08:43:33,119 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:43:33,122 : INFO : built Dictionary(309 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 7211 corpus positions)\n",
      "2021-01-14 08:43:33,284 : INFO : token count processed\n",
      "2021-01-14 08:43:33,316 : INFO : frequencies processed\n",
      "2021-01-14 08:43:42,689 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:43:42,690 : INFO : entropies processed\n",
      "2021-01-14 08:43:42,691 : INFO : extropies processed\n",
      "2021-01-14 08:43:42,700 : INFO : token count processed\n",
      "2021-01-14 08:43:42,707 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:43:42,711 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:43:42,712 : INFO : vocab #32006\n",
      "2021-01-14 08:43:42,720 : INFO : diff #set()\n",
      "2021-01-14 08:44:01,228 : INFO : alphabet #32006\n",
      "2021-01-14 08:44:10,410 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.1776889628058165, 0.4592024008385304], [0.7224248349666595, 0.27757517], [2.0, 1.2451124978365313], [4.247927513443585, 6.363791471162389, 6.397295933664042, 4.214423050941933, 2.1493684202204566, 0.03350446250165273]]\n",
      "2021-01-14 08:44:10,415 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:44:10,416 : INFO : built Dictionary(211 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 1281 corpus positions)\n",
      "2021-01-14 08:44:10,511 : INFO : token count processed\n",
      "2021-01-14 08:44:10,538 : INFO : frequencies processed\n",
      "2021-01-14 08:44:19,871 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:44:19,872 : INFO : entropies processed\n",
      "2021-01-14 08:44:19,873 : INFO : extropies processed\n",
      "2021-01-14 08:44:19,880 : INFO : token count processed\n",
      "2021-01-14 08:44:19,887 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:44:19,891 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:44:19,892 : INFO : vocab #32006\n",
      "2021-01-14 08:44:19,900 : INFO : diff #set()\n",
      "2021-01-14 08:44:38,769 : INFO : alphabet #32006\n",
      "2021-01-14 08:44:48,211 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.1929340484361115, 0.45601006592658316], [0.7643859684467316, 0.23561403], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.29000629755059, 6.506064510410003, 4.031869300584172, 2.2581369969664182, 0.21605821285941307]]\n",
      "2021-01-14 08:44:48,215 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:44:48,217 : INFO : built Dictionary(221 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 1234 corpus positions)\n",
      "2021-01-14 08:44:48,315 : INFO : token count processed\n",
      "2021-01-14 08:44:48,343 : INFO : frequencies processed\n",
      "2021-01-14 08:44:57,639 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:44:57,640 : INFO : entropies processed\n",
      "2021-01-14 08:44:57,641 : INFO : extropies processed\n",
      "2021-01-14 08:44:57,647 : INFO : token count processed\n",
      "2021-01-14 08:44:57,651 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:44:57,655 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:44:57,656 : INFO : vocab #32006\n",
      "2021-01-14 08:44:57,662 : INFO : diff #set()\n",
      "2021-01-14 08:45:16,047 : INFO : alphabet #32006\n",
      "2021-01-14 08:45:25,526 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.1822508287737428, 0.4582424654465239], [0.7666793465614319, 0.23332065], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.361621244785958, 6.562438213471622, 4.047110544757921, 2.3145107000280367, 0.2008169686856638]]\n",
      "2021-01-14 08:45:25,531 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:45:25,533 : INFO : built Dictionary(229 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 1257 corpus positions)\n",
      "2021-01-14 08:45:25,646 : INFO : token count processed\n",
      "2021-01-14 08:45:25,681 : INFO : frequencies processed\n",
      "2021-01-14 08:45:34,902 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:45:34,903 : INFO : entropies processed\n",
      "2021-01-14 08:45:34,904 : INFO : extropies processed\n",
      "2021-01-14 08:45:34,910 : INFO : token count processed\n",
      "2021-01-14 08:45:34,914 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:45:34,917 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:45:34,918 : INFO : vocab #32006\n",
      "2021-01-14 08:45:34,927 : INFO : diff #set()\n",
      "2021-01-14 08:45:53,481 : INFO : alphabet #32006\n",
      "2021-01-14 08:46:02,758 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.172096744723051, 0.4603846501908473], [0.7717169672250748, 0.22828303], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.620594433343389, 6.825536874931166, 4.042985071855807, 2.577609361487581, 0.20494244158777697]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 08:46:02,762 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:46:02,765 : INFO : built Dictionary(199 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 1350 corpus positions)\n",
      "2021-01-14 08:46:02,857 : INFO : token count processed\n",
      "2021-01-14 08:46:02,883 : INFO : frequencies processed\n",
      "2021-01-14 08:46:12,656 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:46:12,657 : INFO : entropies processed\n",
      "2021-01-14 08:46:12,658 : INFO : extropies processed\n",
      "2021-01-14 08:46:12,665 : INFO : token count processed\n",
      "2021-01-14 08:46:12,671 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:46:12,676 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:46:12,677 : INFO : vocab #32006\n",
      "2021-01-14 08:46:12,684 : INFO : diff #set()\n",
      "2021-01-14 08:46:31,227 : INFO : alphabet #32006\n",
      "2021-01-14 08:46:40,506 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.1628935036506138, 0.46234361438145793], [0.7427048683166504, 0.25729513], [2.321928094887362, 1.2877123795494492], [4.247927513443585, 6.207411496248084, 6.329145364385791, 4.126193645305879, 2.081217850942206, 0.12173386813770737]]\n",
      "2021-01-14 08:46:40,509 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:46:40,510 : INFO : built Dictionary(141 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 254 corpus positions)\n",
      "2021-01-14 08:46:40,567 : INFO : token count processed\n",
      "2021-01-14 08:46:40,595 : INFO : frequencies processed\n",
      "2021-01-14 08:46:49,812 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:46:49,813 : INFO : entropies processed\n",
      "2021-01-14 08:46:49,814 : INFO : extropies processed\n",
      "2021-01-14 08:46:49,820 : INFO : token count processed\n",
      "2021-01-14 08:46:49,827 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:46:49,832 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:46:49,833 : INFO : vocab #32006\n",
      "2021-01-14 08:46:49,840 : INFO : diff #set()\n",
      "2021-01-14 08:47:08,697 : INFO : alphabet #32006\n",
      "2021-01-14 08:47:17,915 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.1901886996434097, 0.4565816635629672], [0.7668814808130264, 0.23311852], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.5805228788529595, 6.775027174558456, 4.053423217738089, 2.527099661114871, 0.19450429570549677]]\n",
      "2021-01-14 08:47:17,920 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 08:47:17,921 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:47:17,922 : INFO : built Dictionary(222 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 1215 corpus positions)\n",
      "2021-01-14 08:47:18,023 : INFO : token count processed\n",
      "2021-01-14 08:47:18,051 : INFO : frequencies processed\n",
      "2021-01-14 08:47:27,364 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:47:27,365 : INFO : entropies processed\n",
      "2021-01-14 08:47:27,366 : INFO : extropies processed\n",
      "2021-01-14 08:47:27,372 : INFO : token count processed\n",
      "2021-01-14 08:47:27,376 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:47:27,380 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:47:27,380 : INFO : vocab #32006\n",
      "2021-01-14 08:47:27,386 : INFO : diff #set()\n",
      "2021-01-14 08:47:45,905 : INFO : alphabet #32006\n",
      "2021-01-14 08:47:55,070 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.1755616968330262, 0.4596514093145251], [0.7367802262306213, 0.26321977], [2.321928094887362, 1.2877123795494492], [4.247927513443585, 6.422089779976135, 6.513417188326873, 4.156600105092847, 2.2654896748832876, 0.09132740835073783]]\n",
      "2021-01-14 08:47:55,075 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:47:55,077 : INFO : built Dictionary(235 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 1497 corpus positions)\n",
      "2021-01-14 08:47:55,189 : INFO : token count processed\n",
      "2021-01-14 08:47:55,220 : INFO : frequencies processed\n",
      "2021-01-14 08:48:04,379 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:48:04,380 : INFO : entropies processed\n",
      "2021-01-14 08:48:04,381 : INFO : extropies processed\n",
      "2021-01-14 08:48:04,394 : INFO : token count processed\n",
      "2021-01-14 08:48:04,398 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:48:04,401 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:48:04,402 : INFO : vocab #32006\n",
      "2021-01-14 08:48:04,408 : INFO : diff #set()\n",
      "2021-01-14 08:48:22,939 : INFO : alphabet #32006\n",
      "2021-01-14 08:48:32,106 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.1706096655561034, 0.4607000585449817], [0.7802162915468216, 0.21978371], [2.0, 1.2451124978365313], [4.247927513443585, 6.485445644653597, 6.717076222585369, 4.0162969355118125, 2.469148709141784, 0.23163057793177266]]\n",
      "2021-01-14 08:48:32,111 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:48:32,112 : INFO : built Dictionary(209 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 1383 corpus positions)\n",
      "2021-01-14 08:48:32,210 : INFO : token count processed\n",
      "2021-01-14 08:48:32,260 : INFO : frequencies processed\n",
      "2021-01-14 08:48:41,444 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:48:41,445 : INFO : entropies processed\n",
      "2021-01-14 08:48:41,446 : INFO : extropies processed\n",
      "2021-01-14 08:48:41,458 : INFO : token count processed\n",
      "2021-01-14 08:48:41,462 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:48:41,465 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:48:41,466 : INFO : vocab #32006\n",
      "2021-01-14 08:48:41,472 : INFO : diff #set()\n",
      "2021-01-14 08:48:59,982 : INFO : alphabet #32006\n",
      "2021-01-14 08:49:09,139 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.1640577143416733, 0.46209488470329885], [0.7441477477550507, 0.25585225], [2.321928094887362, 1.2877123795494492], [4.247927513443585, 6.2276600107346916, 6.346784844861679, 4.128802679316598, 2.0988573314180936, 0.11912483412698727]]\n",
      "2021-01-14 08:49:09,144 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:49:09,145 : INFO : built Dictionary(201 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 1581 corpus positions)\n",
      "2021-01-14 08:49:09,241 : INFO : token count processed\n",
      "2021-01-14 08:49:09,302 : INFO : frequencies processed\n",
      "2021-01-14 08:49:18,600 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:49:18,601 : INFO : entropies processed\n",
      "2021-01-14 08:49:18,602 : INFO : extropies processed\n",
      "2021-01-14 08:49:18,615 : INFO : token count processed\n",
      "2021-01-14 08:49:18,619 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:49:18,622 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:49:18,623 : INFO : vocab #32006\n",
      "2021-01-14 08:49:18,630 : INFO : diff #set()\n",
      "2021-01-14 08:49:37,089 : INFO : alphabet #32006\n",
      "2021-01-14 08:49:46,366 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.1888590085284505, 0.4568590284270025], [0.7587350606918335, 0.24126494], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.253918170574241, 6.428881348703129, 4.072964335314698, 2.180953835259544, 0.1749631781288885]]\n",
      "2021-01-14 08:49:46,369 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:49:46,371 : INFO : built Dictionary(173 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 678 corpus positions)\n",
      "2021-01-14 08:49:46,443 : INFO : token count processed\n",
      "2021-01-14 08:49:46,471 : INFO : frequencies processed\n",
      "2021-01-14 08:49:55,653 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:49:55,654 : INFO : entropies processed\n",
      "2021-01-14 08:49:55,655 : INFO : extropies processed\n",
      "2021-01-14 08:49:55,661 : INFO : token count processed\n",
      "2021-01-14 08:49:55,665 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:49:55,669 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:49:55,670 : INFO : vocab #32006\n",
      "2021-01-14 08:49:55,677 : INFO : diff #set()\n",
      "2021-01-14 08:50:14,164 : INFO : alphabet #32006\n",
      "2021-01-14 08:50:23,459 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.1592840409839553, 0.4631164687089125], [0.7613393217325211, 0.23866068], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.374522245625576, 6.570116317084961, 4.0523334419842, 2.3221888036413754, 0.1955940714593849]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 08:50:23,464 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:50:23,466 : INFO : built Dictionary(295 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 1960 corpus positions)\n",
      "2021-01-14 08:50:23,618 : INFO : token count processed\n",
      "2021-01-14 08:50:23,673 : INFO : frequencies processed\n",
      "2021-01-14 08:50:32,856 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:50:32,857 : INFO : entropies processed\n",
      "2021-01-14 08:50:32,858 : INFO : extropies processed\n",
      "2021-01-14 08:50:32,865 : INFO : token count processed\n",
      "2021-01-14 08:50:32,871 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:50:32,875 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:50:32,876 : INFO : vocab #32006\n",
      "2021-01-14 08:50:32,884 : INFO : diff #set()\n",
      "2021-01-14 08:50:51,315 : INFO : alphabet #32006\n",
      "2021-01-14 08:51:00,608 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.1730628763056623, 0.46017996575417097], [0.7899331599473953, 0.21006684], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.731238669067808, 6.990383859042441, 3.988782323468951, 2.742456345598856, 0.25914518997463354]]\n",
      "2021-01-14 08:51:00,612 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:51:00,614 : INFO : built Dictionary(220 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 1352 corpus positions)\n",
      "2021-01-14 08:51:00,718 : INFO : token count processed\n",
      "2021-01-14 08:51:00,772 : INFO : frequencies processed\n",
      "2021-01-14 08:51:09,966 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:51:09,968 : INFO : entropies processed\n",
      "2021-01-14 08:51:09,969 : INFO : extropies processed\n",
      "2021-01-14 08:51:09,982 : INFO : token count processed\n",
      "2021-01-14 08:51:09,988 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:51:09,992 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:51:09,993 : INFO : vocab #32006\n",
      "2021-01-14 08:51:09,999 : INFO : diff #set()\n",
      "2021-01-14 08:51:28,542 : INFO : alphabet #32006\n",
      "2021-01-14 08:51:37,727 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.1819405915709287, 0.4583076202271994], [0.7433938980102539, 0.2566061], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.503741451859337, 6.685992078624389, 4.065676886678533, 2.438064565180804, 0.1822506267650521]]\n",
      "2021-01-14 08:51:37,732 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:51:37,734 : INFO : built Dictionary(241 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 1674 corpus positions)\n",
      "2021-01-14 08:51:37,855 : INFO : token count processed\n",
      "2021-01-14 08:51:37,884 : INFO : frequencies processed\n",
      "2021-01-14 08:51:47,067 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:51:47,068 : INFO : entropies processed\n",
      "2021-01-14 08:51:47,069 : INFO : extropies processed\n",
      "2021-01-14 08:51:47,076 : INFO : token count processed\n",
      "2021-01-14 08:51:47,080 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:51:47,084 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:51:47,085 : INFO : vocab #32006\n",
      "2021-01-14 08:51:47,091 : INFO : diff #set()\n",
      "2021-01-14 08:52:05,694 : INFO : alphabet #32006\n",
      "2021-01-14 08:52:14,869 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.1848590560794994, 0.4576954276375132], [0.7668511420488358, 0.23314886], [2.0, 1.2451124978365313], [4.247927513443585, 6.334729224484471, 6.459079467201189, 4.123577270726868, 2.2111519537576036, 0.12435024271671757]]\n",
      "2021-01-14 08:52:14,875 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:52:14,877 : INFO : built Dictionary(238 unique tokens: [')', '1)', '29', '_', 'bug']...) from 2 documents (total 2023 corpus positions)\n",
      "2021-01-14 08:52:15,001 : INFO : token count processed\n",
      "2021-01-14 08:52:15,064 : INFO : frequencies processed\n",
      "2021-01-14 08:52:24,227 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:52:24,228 : INFO : entropies processed\n",
      "2021-01-14 08:52:24,229 : INFO : extropies processed\n",
      "2021-01-14 08:52:24,236 : INFO : token count processed\n",
      "2021-01-14 08:52:24,243 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:52:24,247 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:52:24,248 : INFO : vocab #32006\n",
      "2021-01-14 08:52:24,256 : INFO : diff #set()\n",
      "2021-01-14 08:52:42,912 : INFO : alphabet #32006\n",
      "2021-01-14 08:52:52,092 : INFO : Computed distances or similarities ('292', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.1870511431303108, 0.4572366783196058], [0.8238408118486404, 0.17615919], [2.0, 1.2451124978365313], [4.247927513443585, 6.21319712067992, 6.398891601558075, 4.06223303256543, 2.15096408811449, 0.18569448087815488]]\n",
      "2021-01-14 08:52:52,097 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 08:52:52,097 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:52:52,099 : INFO : built Dictionary(268 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1401 corpus positions)\n",
      "2021-01-14 08:52:52,177 : INFO : token count processed\n",
      "2021-01-14 08:52:52,208 : INFO : frequencies processed\n",
      "2021-01-14 08:53:01,388 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:53:01,389 : INFO : entropies processed\n",
      "2021-01-14 08:53:01,390 : INFO : extropies processed\n",
      "2021-01-14 08:53:01,397 : INFO : token count processed\n",
      "2021-01-14 08:53:01,403 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:53:01,408 : INFO : alphabet_target #32010\n",
      "2021-01-14 08:53:01,409 : INFO : vocab #32006\n",
      "2021-01-14 08:53:01,417 : INFO : diff #set()\n",
      "2021-01-14 08:53:20,107 : INFO : alphabet #32006\n",
      "2021-01-14 08:53:29,282 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.2631752095117204, 0.4418570845938834], [0.9145119860768318, 0.085488014], [1.0, 1.0], [3.321928094887362, 6.905617163738059, 7.016941134915424, 3.210604123709997, 3.6950130400280616, 0.11132397117736481]]\n",
      "2021-01-14 08:53:29,287 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:53:29,291 : INFO : built Dictionary(359 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 2291 corpus positions)\n",
      "2021-01-14 08:53:29,430 : INFO : token count processed\n",
      "2021-01-14 08:53:29,461 : INFO : frequencies processed\n",
      "2021-01-14 08:53:38,759 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:53:38,760 : INFO : entropies processed\n",
      "2021-01-14 08:53:38,761 : INFO : extropies processed\n",
      "2021-01-14 08:53:38,772 : INFO : token count processed\n",
      "2021-01-14 08:53:38,777 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:53:38,782 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:53:38,783 : INFO : vocab #32006\n",
      "2021-01-14 08:53:38,789 : INFO : diff #set()\n",
      "2021-01-14 08:53:57,179 : INFO : alphabet #32006\n",
      "2021-01-14 08:54:06,765 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.285907740940694, 0.437462974594277], [0.9483272358775139, 0.051672764], [1.0, 1.0], [3.321928094887362, 7.1219284286457345, 7.323309822863596, 3.1205467006695002, 4.001381727976234, 0.2013813942178615]]\n",
      "2021-01-14 08:54:06,770 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:54:06,772 : INFO : built Dictionary(276 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 2274 corpus positions)\n",
      "2021-01-14 08:54:06,849 : INFO : token count processed\n",
      "2021-01-14 08:54:06,878 : INFO : frequencies processed\n",
      "2021-01-14 08:54:16,077 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:54:16,078 : INFO : entropies processed\n",
      "2021-01-14 08:54:16,079 : INFO : extropies processed\n",
      "2021-01-14 08:54:16,086 : INFO : token count processed\n",
      "2021-01-14 08:54:16,092 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:54:16,097 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:54:16,098 : INFO : vocab #32006\n",
      "2021-01-14 08:54:16,105 : INFO : diff #set()\n",
      "2021-01-14 08:54:34,490 : INFO : alphabet #32006\n",
      "2021-01-14 08:54:43,795 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.280681723262388, 0.438465389449237], [0.9501677490770817, 0.04983225], [2.0, 1.2451124978365313], [3.321928094887362, 6.41099024988467, 6.4610088341072345, 3.2719095106647984, 3.1390807392198723, 0.05001858422256422]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 08:54:43,799 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:54:43,801 : INFO : built Dictionary(157 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 673 corpus positions)\n",
      "2021-01-14 08:54:43,861 : INFO : token count processed\n",
      "2021-01-14 08:54:43,894 : INFO : frequencies processed\n",
      "2021-01-14 08:54:53,216 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:54:53,217 : INFO : entropies processed\n",
      "2021-01-14 08:54:53,218 : INFO : extropies processed\n",
      "2021-01-14 08:54:53,225 : INFO : token count processed\n",
      "2021-01-14 08:54:53,232 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:54:53,237 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:54:53,238 : INFO : vocab #32006\n",
      "2021-01-14 08:54:53,246 : INFO : diff #set()\n",
      "2021-01-14 08:55:11,652 : INFO : alphabet #32006\n",
      "2021-01-14 08:55:20,921 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.2789210698842364, 0.43880413991292705], [0.9478878155350685, 0.052112184], [0.0, 0.0], [3.321928094887362, 6.077866832717642, 6.179182607520543, 3.220612320084461, 2.857254512633181, 0.10131577480290144]]\n",
      "2021-01-14 08:55:20,924 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:55:20,925 : INFO : built Dictionary(133 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 438 corpus positions)\n",
      "2021-01-14 08:55:20,960 : INFO : token count processed\n",
      "2021-01-14 08:55:20,988 : INFO : frequencies processed\n",
      "2021-01-14 08:55:30,198 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:55:30,199 : INFO : entropies processed\n",
      "2021-01-14 08:55:30,199 : INFO : extropies processed\n",
      "2021-01-14 08:55:30,206 : INFO : token count processed\n",
      "2021-01-14 08:55:30,212 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:55:30,216 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:55:30,218 : INFO : vocab #32006\n",
      "2021-01-14 08:55:30,225 : INFO : diff #set()\n",
      "2021-01-14 08:55:48,989 : INFO : alphabet #32006\n",
      "2021-01-14 08:55:58,140 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.2754378218724278, 0.4394758627933472], [0.9384037479758263, 0.061596252], [0.0, 0.0], [3.321928094887362, 5.977547459003844, 6.056280500637657, 3.243195053253549, 2.7343524057502946, 0.07873304163381256]]\n",
      "2021-01-14 08:55:58,146 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:55:58,148 : INFO : built Dictionary(236 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 2149 corpus positions)\n",
      "2021-01-14 08:55:58,213 : INFO : token count processed\n",
      "2021-01-14 08:55:58,241 : INFO : frequencies processed\n",
      "2021-01-14 08:56:07,460 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:56:07,461 : INFO : entropies processed\n",
      "2021-01-14 08:56:07,462 : INFO : extropies processed\n",
      "2021-01-14 08:56:07,469 : INFO : token count processed\n",
      "2021-01-14 08:56:07,473 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:56:07,477 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:56:07,478 : INFO : vocab #32006\n",
      "2021-01-14 08:56:07,484 : INFO : diff #set()\n",
      "2021-01-14 08:56:25,989 : INFO : alphabet #32006\n",
      "2021-01-14 08:56:35,158 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.2702817983019272, 0.44047395382721066], [0.9339622557163239, 0.066037744], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 6.4614394051846435, 6.542693299403306, 3.2406742006686997, 3.2207652045159434, 0.08125389421866203]]\n",
      "2021-01-14 08:56:35,162 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 08:56:35,163 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:56:35,165 : INFO : built Dictionary(198 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1373 corpus positions)\n",
      "2021-01-14 08:56:35,219 : INFO : token count processed\n",
      "2021-01-14 08:56:35,248 : INFO : frequencies processed\n",
      "2021-01-14 08:56:44,503 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:56:44,504 : INFO : entropies processed\n",
      "2021-01-14 08:56:44,504 : INFO : extropies processed\n",
      "2021-01-14 08:56:44,511 : INFO : token count processed\n",
      "2021-01-14 08:56:44,517 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:56:44,523 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:56:44,524 : INFO : vocab #32006\n",
      "2021-01-14 08:56:44,530 : INFO : diff #set()\n",
      "2021-01-14 08:57:03,041 : INFO : alphabet #32006\n",
      "2021-01-14 08:57:12,229 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.2693698688384136, 0.4406509550212077], [0.9270405620336533, 0.07295944], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 6.327195724598159, 6.407045348087271, 3.2420784713982496, 3.085117253199909, 0.07984962348911218]]\n",
      "2021-01-14 08:57:12,241 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 08:57:12,242 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:57:12,244 : INFO : built Dictionary(413 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 6265 corpus positions)\n",
      "2021-01-14 08:57:12,405 : INFO : token count processed\n",
      "2021-01-14 08:57:12,440 : INFO : frequencies processed\n",
      "2021-01-14 08:57:21,600 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:57:21,601 : INFO : entropies processed\n",
      "2021-01-14 08:57:21,602 : INFO : extropies processed\n",
      "2021-01-14 08:57:21,615 : INFO : token count processed\n",
      "2021-01-14 08:57:21,618 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:57:21,622 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:57:21,623 : INFO : vocab #32006\n",
      "2021-01-14 08:57:21,628 : INFO : diff #set()\n",
      "2021-01-14 08:57:40,165 : INFO : alphabet #32006\n",
      "2021-01-14 08:57:49,333 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.2687190138848388, 0.4407773699078102], [0.9301913529634476, 0.06980865], [2.0, 1.2451124978365313], [3.321928094887362, 6.9079058562486315, 7.005499540992986, 3.2243344101430083, 3.6835714461056237, 0.09759368474435437]]\n",
      "2021-01-14 08:57:49,339 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:57:49,341 : INFO : built Dictionary(317 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 2659 corpus positions)\n",
      "2021-01-14 08:57:49,443 : INFO : token count processed\n",
      "2021-01-14 08:57:49,474 : INFO : frequencies processed\n",
      "2021-01-14 08:57:59,021 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:57:59,022 : INFO : entropies processed\n",
      "2021-01-14 08:57:59,023 : INFO : extropies processed\n",
      "2021-01-14 08:57:59,030 : INFO : token count processed\n",
      "2021-01-14 08:57:59,033 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:57:59,037 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:57:59,038 : INFO : vocab #32006\n",
      "2021-01-14 08:57:59,043 : INFO : diff #set()\n",
      "2021-01-14 08:58:17,759 : INFO : alphabet #32006\n",
      "2021-01-14 08:58:27,307 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.2796721112846903, 0.43865957523007915], [0.9376035034656525, 0.062396497], [1.0, 1.0], [3.321928094887362, 6.61034830706307, 6.74912691234533, 3.1831494896051016, 3.427198817457968, 0.13877860528226016]]\n",
      "2021-01-14 08:58:27,311 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:58:27,312 : INFO : built Dictionary(204 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 706 corpus positions)\n",
      "2021-01-14 08:58:27,372 : INFO : token count processed\n",
      "2021-01-14 08:58:27,401 : INFO : frequencies processed\n",
      "2021-01-14 08:58:36,734 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:58:36,735 : INFO : entropies processed\n",
      "2021-01-14 08:58:36,736 : INFO : extropies processed\n",
      "2021-01-14 08:58:36,742 : INFO : token count processed\n",
      "2021-01-14 08:58:36,746 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:58:36,750 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:58:36,751 : INFO : vocab #32006\n",
      "2021-01-14 08:58:36,757 : INFO : diff #set()\n",
      "2021-01-14 08:58:55,349 : INFO : alphabet #32006\n",
      "2021-01-14 08:59:04,775 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.2766333571172475, 0.43924507952665465], [0.9477688372135162, 0.052231163], [0.0, 0.0], [3.321928094887362, 6.616715366949855, 6.720026305372992, 3.2186171564642247, 3.3980982104856294, 0.10331093842313699]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 08:59:04,781 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:59:04,783 : INFO : built Dictionary(423 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 2740 corpus positions)\n",
      "2021-01-14 08:59:04,926 : INFO : token count processed\n",
      "2021-01-14 08:59:04,984 : INFO : frequencies processed\n",
      "2021-01-14 08:59:14,146 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:59:14,147 : INFO : entropies processed\n",
      "2021-01-14 08:59:14,148 : INFO : extropies processed\n",
      "2021-01-14 08:59:14,155 : INFO : token count processed\n",
      "2021-01-14 08:59:14,162 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:59:14,167 : INFO : alphabet_target #32009\n",
      "2021-01-14 08:59:14,168 : INFO : vocab #32006\n",
      "2021-01-14 08:59:14,176 : INFO : diff #set()\n",
      "2021-01-14 08:59:32,557 : INFO : alphabet #32006\n",
      "2021-01-14 08:59:41,832 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.2804873333590179, 0.43850276446265607], [0.9421653710305691, 0.05783463], [1.0, 1.0], [3.321928094887362, 7.32185870753746, 7.508469905873657, 3.1353168965511653, 4.186541810986295, 0.18661119833619644]]\n",
      "2021-01-14 08:59:41,835 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 08:59:41,836 : INFO : built Dictionary(57 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 105 corpus positions)\n",
      "2021-01-14 08:59:41,851 : INFO : token count processed\n",
      "2021-01-14 08:59:41,879 : INFO : frequencies processed\n",
      "2021-01-14 08:59:51,030 : INFO : scalar_distribution processed\n",
      "2021-01-14 08:59:51,031 : INFO : entropies processed\n",
      "2021-01-14 08:59:51,032 : INFO : extropies processed\n",
      "2021-01-14 08:59:51,039 : INFO : token count processed\n",
      "2021-01-14 08:59:51,044 : INFO : alphabet_source #32006\n",
      "2021-01-14 08:59:51,047 : INFO : alphabet_target #32008\n",
      "2021-01-14 08:59:51,048 : INFO : vocab #32006\n",
      "2021-01-14 08:59:51,054 : INFO : diff #set()\n",
      "2021-01-14 09:00:09,750 : INFO : alphabet #32006\n",
      "2021-01-14 09:00:19,173 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/fireException.py')[[1.24840727082403, 0.44475928048102464], [0.9109912067651749, 0.08900879], [0.0, 0.0], [3.321928094887362, 5.176618657501385, 5.490246176466799, 3.0083005759219485, 2.168318081579437, 0.3136275189654141]]\n",
      "2021-01-14 09:00:19,176 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:00:19,177 : INFO : built Dictionary(153 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 498 corpus positions)\n",
      "2021-01-14 09:00:19,218 : INFO : token count processed\n",
      "2021-01-14 09:00:19,246 : INFO : frequencies processed\n",
      "2021-01-14 09:00:28,408 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:00:28,409 : INFO : entropies processed\n",
      "2021-01-14 09:00:28,410 : INFO : extropies processed\n",
      "2021-01-14 09:00:28,421 : INFO : token count processed\n",
      "2021-01-14 09:00:28,426 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:00:28,432 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:00:28,433 : INFO : vocab #32006\n",
      "2021-01-14 09:00:28,440 : INFO : diff #set()\n",
      "2021-01-14 09:00:47,069 : INFO : alphabet #32006\n",
      "2021-01-14 09:00:56,228 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.2834081276360116, 0.43794185888060644], [0.9431905448436737, 0.056809455], [0.0, 0.0], [3.321928094887362, 6.468846789852156, 6.610460982929704, 3.1803139018098143, 3.2885328880423415, 0.1416141930775474]]\n",
      "2021-01-14 09:00:56,234 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:00:56,236 : INFO : built Dictionary(368 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 2547 corpus positions)\n",
      "2021-01-14 09:00:56,363 : INFO : token count processed\n",
      "2021-01-14 09:00:56,390 : INFO : frequencies processed\n",
      "2021-01-14 09:01:05,764 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:01:05,765 : INFO : entropies processed\n",
      "2021-01-14 09:01:05,766 : INFO : extropies processed\n",
      "2021-01-14 09:01:05,773 : INFO : token count processed\n",
      "2021-01-14 09:01:05,777 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:01:05,783 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:01:05,784 : INFO : vocab #32006\n",
      "2021-01-14 09:01:05,792 : INFO : diff #set()\n",
      "2021-01-14 09:01:24,316 : INFO : alphabet #32006\n",
      "2021-01-14 09:01:33,493 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.2738664343112132, 0.4397795688043191], [0.9347721636295319, 0.06522784], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 6.957796704012729, 7.080517313710381, 3.19920748518971, 3.7585892188230186, 0.12272060969765164]]\n",
      "2021-01-14 09:01:33,500 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:01:33,502 : INFO : built Dictionary(277 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 3044 corpus positions)\n",
      "2021-01-14 09:01:33,585 : INFO : token count processed\n",
      "2021-01-14 09:01:33,616 : INFO : frequencies processed\n",
      "2021-01-14 09:01:42,773 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:01:42,774 : INFO : entropies processed\n",
      "2021-01-14 09:01:42,775 : INFO : extropies processed\n",
      "2021-01-14 09:01:42,782 : INFO : token count processed\n",
      "2021-01-14 09:01:42,789 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:01:42,794 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:01:42,795 : INFO : vocab #32006\n",
      "2021-01-14 09:01:42,802 : INFO : diff #set()\n",
      "2021-01-14 09:02:01,747 : INFO : alphabet #32006\n",
      "2021-01-14 09:02:10,931 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.275391327830228, 0.43948484279123184], [0.9499327875673771, 0.050067212], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 6.441859572014148, 6.571282245878164, 3.1925054210233466, 3.249354150990802, 0.12942267386401607]]\n",
      "2021-01-14 09:02:10,936 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:02:10,939 : INFO : built Dictionary(305 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1520 corpus positions)\n",
      "2021-01-14 09:02:11,040 : INFO : token count processed\n",
      "2021-01-14 09:02:11,100 : INFO : frequencies processed\n",
      "2021-01-14 09:02:20,390 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:02:20,391 : INFO : entropies processed\n",
      "2021-01-14 09:02:20,392 : INFO : extropies processed\n",
      "2021-01-14 09:02:20,399 : INFO : token count processed\n",
      "2021-01-14 09:02:20,403 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:02:20,407 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:02:20,408 : INFO : vocab #32006\n",
      "2021-01-14 09:02:20,414 : INFO : diff #set()\n",
      "2021-01-14 09:02:38,788 : INFO : alphabet #32006\n",
      "2021-01-14 09:02:48,288 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.281395187929644, 0.43832826740881115], [0.9401351250708103, 0.059864875], [0.0, 0.0], [3.321928094887362, 6.998955278238291, 7.141687541481892, 3.179195831643761, 3.81975944659453, 0.14273226324360166]]\n",
      "2021-01-14 09:02:48,292 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:02:48,294 : INFO : built Dictionary(219 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1339 corpus positions)\n",
      "2021-01-14 09:02:48,357 : INFO : token count processed\n",
      "2021-01-14 09:02:48,384 : INFO : frequencies processed\n",
      "2021-01-14 09:02:57,569 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:02:57,570 : INFO : entropies processed\n",
      "2021-01-14 09:02:57,571 : INFO : extropies processed\n",
      "2021-01-14 09:02:57,577 : INFO : token count processed\n",
      "2021-01-14 09:02:57,585 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:02:57,590 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:02:57,591 : INFO : vocab #32006\n",
      "2021-01-14 09:02:57,598 : INFO : diff #set()\n",
      "2021-01-14 09:03:15,959 : INFO : alphabet #32006\n",
      "2021-01-14 09:03:25,298 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.2798200874414605, 0.43863110317720516], [0.944091584533453, 0.055908415], [1.0, 1.0], [3.321928094887362, 6.492983191376071, 6.57932981189359, 3.235581474369843, 3.257401717006228, 0.08634662051751896]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 09:03:25,305 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:03:25,308 : INFO : built Dictionary(427 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 3278 corpus positions)\n",
      "2021-01-14 09:03:25,450 : INFO : token count processed\n",
      "2021-01-14 09:03:25,480 : INFO : frequencies processed\n",
      "2021-01-14 09:03:34,882 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:03:34,883 : INFO : entropies processed\n",
      "2021-01-14 09:03:34,884 : INFO : extropies processed\n",
      "2021-01-14 09:03:34,891 : INFO : token count processed\n",
      "2021-01-14 09:03:34,895 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:03:34,899 : INFO : alphabet_target #32008\n",
      "2021-01-14 09:03:34,900 : INFO : vocab #32006\n",
      "2021-01-14 09:03:34,906 : INFO : diff #set()\n",
      "2021-01-14 09:03:53,477 : INFO : alphabet #32006\n",
      "2021-01-14 09:04:02,767 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.2598792737224362, 0.44250151396486603], [0.9317065626382828, 0.06829344], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 6.560342487747443, 6.731965244178496, 3.1503053384563087, 3.4100371492911337, 0.17162275643105307]]\n",
      "2021-01-14 09:04:02,774 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 09:04:02,775 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:04:02,778 : INFO : built Dictionary(444 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 3483 corpus positions)\n",
      "2021-01-14 09:04:02,932 : INFO : token count processed\n",
      "2021-01-14 09:04:02,966 : INFO : frequencies processed\n",
      "2021-01-14 09:04:12,262 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:04:12,263 : INFO : entropies processed\n",
      "2021-01-14 09:04:12,264 : INFO : extropies processed\n",
      "2021-01-14 09:04:12,272 : INFO : token count processed\n",
      "2021-01-14 09:04:12,279 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:04:12,284 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:04:12,285 : INFO : vocab #32006\n",
      "2021-01-14 09:04:12,293 : INFO : diff #set()\n",
      "2021-01-14 09:04:31,397 : INFO : alphabet #32006\n",
      "2021-01-14 09:04:40,572 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.271162560627508, 0.44030313696422774], [0.9255590587854385, 0.07444094], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 7.046173750105238, 7.21928634075333, 3.14881550423927, 3.8973582458659677, 0.17311259064809192]]\n",
      "2021-01-14 09:04:40,583 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 09:04:40,583 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:04:40,586 : INFO : built Dictionary(498 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 5592 corpus positions)\n",
      "2021-01-14 09:04:40,791 : INFO : token count processed\n",
      "2021-01-14 09:04:40,825 : INFO : frequencies processed\n",
      "2021-01-14 09:04:49,989 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:04:49,990 : INFO : entropies processed\n",
      "2021-01-14 09:04:49,991 : INFO : extropies processed\n",
      "2021-01-14 09:04:50,000 : INFO : token count processed\n",
      "2021-01-14 09:04:50,006 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:04:50,013 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:04:50,014 : INFO : vocab #32006\n",
      "2021-01-14 09:04:50,020 : INFO : diff #set()\n",
      "2021-01-14 09:05:08,662 : INFO : alphabet #32006\n",
      "2021-01-14 09:05:17,834 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.2698588462707339, 0.44055602913060027], [0.9315818846225739, 0.068418115], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 7.009229588004272, 7.106896702387285, 3.224260980504349, 3.784968607499923, 0.09766711438301279]]\n",
      "2021-01-14 09:05:17,846 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:05:17,850 : INFO : built Dictionary(575 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 6533 corpus positions)\n",
      "2021-01-14 09:05:18,079 : INFO : token count processed\n",
      "2021-01-14 09:05:18,134 : INFO : frequencies processed\n",
      "2021-01-14 09:05:27,295 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:05:27,296 : INFO : entropies processed\n",
      "2021-01-14 09:05:27,297 : INFO : extropies processed\n",
      "2021-01-14 09:05:27,305 : INFO : token count processed\n",
      "2021-01-14 09:05:27,311 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:05:27,317 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:05:27,318 : INFO : vocab #32006\n",
      "2021-01-14 09:05:27,324 : INFO : diff #set()\n",
      "2021-01-14 09:05:45,953 : INFO : alphabet #32006\n",
      "2021-01-14 09:05:55,140 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.2052587120817009, 0.45346153470402967], [0.8301646858453751, 0.16983531], [2.321928094887362, 1.2877123795494492], [3.321928094887362, 7.376088004590871, 7.529906482162709, 3.1681096173155234, 4.207978387275347, 0.1538184775718383]]\n",
      "2021-01-14 09:05:55,143 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:05:55,144 : INFO : built Dictionary(128 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 319 corpus positions)\n",
      "2021-01-14 09:05:55,183 : INFO : token count processed\n",
      "2021-01-14 09:05:55,232 : INFO : frequencies processed\n",
      "2021-01-14 09:06:04,408 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:06:04,409 : INFO : entropies processed\n",
      "2021-01-14 09:06:04,410 : INFO : extropies processed\n",
      "2021-01-14 09:06:04,423 : INFO : token count processed\n",
      "2021-01-14 09:06:04,428 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:06:04,432 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:06:04,433 : INFO : vocab #32006\n",
      "2021-01-14 09:06:04,439 : INFO : diff #set()\n",
      "2021-01-14 09:06:22,971 : INFO : alphabet #32006\n",
      "2021-01-14 09:06:32,737 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.2419772052941225, 0.44603486495698386], [0.8880097344517708, 0.111990266], [1.0, 1.0], [3.321928094887362, 6.2993628166120885, 6.409936889544759, 3.2113540219546914, 3.0880087946573966, 0.1105740729326703]]\n",
      "2021-01-14 09:06:32,740 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:06:32,741 : INFO : built Dictionary(27 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 35 corpus positions)\n",
      "2021-01-14 09:06:32,750 : INFO : token count processed\n",
      "2021-01-14 09:06:32,777 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 09:06:32,778 : INFO : frequencies processed\n",
      "2021-01-14 09:06:32,779 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 09:06:32,784 : INFO : token count processed\n",
      "2021-01-14 09:06:32,788 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:06:32,792 : INFO : alphabet_target #32008\n",
      "2021-01-14 09:06:32,793 : INFO : vocab #32006\n",
      "2021-01-14 09:06:32,799 : INFO : diff #set()\n",
      "2021-01-14 09:06:51,381 : INFO : alphabet #32006\n",
      "2021-01-14 09:07:00,546 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2949014407930954, 0.43574856079850194], [0.9642045497894287, 0.03579545], [nan, nan], [3.321928094887362, 3.8936606896881862, 4.47606419505047, 2.739524589525078, 1.1541361001631079, 0.5824035053622838]]\n",
      "2021-01-14 09:07:00,568 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:07:00,573 : INFO : built Dictionary(736 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 12477 corpus positions)\n",
      "2021-01-14 09:07:00,941 : INFO : token count processed\n",
      "2021-01-14 09:07:00,969 : INFO : frequencies processed\n",
      "2021-01-14 09:07:10,253 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:07:10,254 : INFO : entropies processed\n",
      "2021-01-14 09:07:10,255 : INFO : extropies processed\n",
      "2021-01-14 09:07:10,265 : INFO : token count processed\n",
      "2021-01-14 09:07:10,273 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:07:10,279 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:07:10,280 : INFO : vocab #32006\n",
      "2021-01-14 09:07:10,286 : INFO : diff #set()\n",
      "2021-01-14 09:07:28,893 : INFO : alphabet #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 09:07:38,419 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.2827933624237884, 0.4380597983420784], [0.944061778485775, 0.05593822], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 7.434393313070278, 7.635662370812595, 3.120659037145046, 4.313734275925233, 0.20126905774231663]]\n",
      "2021-01-14 09:07:38,427 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:07:38,430 : INFO : built Dictionary(488 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 4104 corpus positions)\n",
      "2021-01-14 09:07:38,610 : INFO : token count processed\n",
      "2021-01-14 09:07:38,638 : INFO : frequencies processed\n",
      "2021-01-14 09:07:47,908 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:07:47,909 : INFO : entropies processed\n",
      "2021-01-14 09:07:47,909 : INFO : extropies processed\n",
      "2021-01-14 09:07:47,917 : INFO : token count processed\n",
      "2021-01-14 09:07:47,925 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:07:47,930 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:07:47,931 : INFO : vocab #32006\n",
      "2021-01-14 09:07:47,939 : INFO : diff #set()\n",
      "2021-01-14 09:08:06,444 : INFO : alphabet #32006\n",
      "2021-01-14 09:08:15,617 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.2830987068679225, 0.4380012116829823], [0.9423449151217937, 0.057655085], [1.0, 1.0], [3.321928094887362, 7.2991514951718255, 7.479428057524348, 3.141651532534839, 4.157499962636987, 0.18027656235252287]]\n",
      "2021-01-14 09:08:15,625 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:08:15,627 : INFO : built Dictionary(454 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 3521 corpus positions)\n",
      "2021-01-14 09:08:15,797 : INFO : token count processed\n",
      "2021-01-14 09:08:15,826 : INFO : frequencies processed\n",
      "2021-01-14 09:08:25,108 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:08:25,109 : INFO : entropies processed\n",
      "2021-01-14 09:08:25,110 : INFO : extropies processed\n",
      "2021-01-14 09:08:25,118 : INFO : token count processed\n",
      "2021-01-14 09:08:25,125 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:08:25,131 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:08:25,132 : INFO : vocab #32006\n",
      "2021-01-14 09:08:25,139 : INFO : diff #set()\n",
      "2021-01-14 09:08:43,577 : INFO : alphabet #32006\n",
      "2021-01-14 09:08:53,203 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.2856316498675788, 0.43751581758938995], [0.9463986307382584, 0.05360137], [1.0, 1.0], [3.321928094887362, 7.170319527000998, 7.342821164434198, 3.1494264574541626, 4.020893069546835, 0.17250163743320002]]\n",
      "2021-01-14 09:08:53,206 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:08:53,208 : INFO : built Dictionary(163 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 557 corpus positions)\n",
      "2021-01-14 09:08:53,258 : INFO : token count processed\n",
      "2021-01-14 09:08:53,285 : INFO : frequencies processed\n",
      "2021-01-14 09:09:02,457 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:09:02,458 : INFO : entropies processed\n",
      "2021-01-14 09:09:02,459 : INFO : extropies processed\n",
      "2021-01-14 09:09:02,472 : INFO : token count processed\n",
      "2021-01-14 09:09:02,477 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:09:02,480 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:09:02,481 : INFO : vocab #32006\n",
      "2021-01-14 09:09:02,488 : INFO : diff #set()\n",
      "2021-01-14 09:09:21,005 : INFO : alphabet #32006\n",
      "2021-01-14 09:09:30,168 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.2837199642073096, 0.43788205895336424], [0.9434823580086231, 0.056517642], [0.0, 0.0], [3.321928094887362, 6.353654804387375, 6.497027474363078, 3.178555424911659, 3.1750993794757156, 0.14337266997570275]]\n",
      "2021-01-14 09:09:30,171 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:09:30,173 : INFO : built Dictionary(158 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 685 corpus positions)\n",
      "2021-01-14 09:09:30,233 : INFO : token count processed\n",
      "2021-01-14 09:09:30,284 : INFO : frequencies processed\n",
      "2021-01-14 09:09:39,592 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:09:39,593 : INFO : entropies processed\n",
      "2021-01-14 09:09:39,594 : INFO : extropies processed\n",
      "2021-01-14 09:09:39,600 : INFO : token count processed\n",
      "2021-01-14 09:09:39,604 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:09:39,609 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:09:39,610 : INFO : vocab #32006\n",
      "2021-01-14 09:09:39,618 : INFO : diff #set()\n",
      "2021-01-14 09:09:58,501 : INFO : alphabet #32006\n",
      "2021-01-14 09:10:07,780 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.283470019569449, 0.4379299887583158], [0.9413205683231354, 0.05867943], [0.0, 0.0], [3.321928094887362, 6.245180322479091, 6.356293564413053, 3.2108148529534004, 3.034365469525691, 0.11111324193396221]]\n",
      "2021-01-14 09:10:07,785 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:10:07,787 : INFO : built Dictionary(386 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1757 corpus positions)\n",
      "2021-01-14 09:10:07,932 : INFO : token count processed\n",
      "2021-01-14 09:10:07,989 : INFO : frequencies processed\n",
      "2021-01-14 09:10:17,325 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:10:17,326 : INFO : entropies processed\n",
      "2021-01-14 09:10:17,327 : INFO : extropies processed\n",
      "2021-01-14 09:10:17,339 : INFO : token count processed\n",
      "2021-01-14 09:10:17,343 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:10:17,347 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:10:17,348 : INFO : vocab #32006\n",
      "2021-01-14 09:10:17,354 : INFO : diff #set()\n",
      "2021-01-14 09:10:35,892 : INFO : alphabet #32006\n",
      "2021-01-14 09:10:45,108 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.2767563103602717, 0.4392213586713464], [0.9324100986123085, 0.0675899], [1.0, 1.0], [3.321928094887362, 7.2691387000368, 7.454439891830579, 3.1366269030935827, 4.132511796943216, 0.18530119179377902]]\n",
      "2021-01-14 09:10:45,113 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:10:45,115 : INFO : built Dictionary(312 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1594 corpus positions)\n",
      "2021-01-14 09:10:45,218 : INFO : token count processed\n",
      "2021-01-14 09:10:45,281 : INFO : frequencies processed\n",
      "2021-01-14 09:10:54,634 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:10:54,635 : INFO : entropies processed\n",
      "2021-01-14 09:10:54,636 : INFO : extropies processed\n",
      "2021-01-14 09:10:54,643 : INFO : token count processed\n",
      "2021-01-14 09:10:54,647 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:10:54,651 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:10:54,652 : INFO : vocab #32006\n",
      "2021-01-14 09:10:54,658 : INFO : diff #set()\n",
      "2021-01-14 09:11:13,370 : INFO : alphabet #32006\n",
      "2021-01-14 09:11:22,551 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.2705938298584578, 0.4404134226253654], [0.9267141968011856, 0.0732858], [1.0, 1.0], [3.321928094887362, 7.08857858466988, 7.234982999093477, 3.1755236804637654, 3.9130549042061147, 0.1464044144235972]]\n",
      "2021-01-14 09:11:22,555 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:11:22,557 : INFO : built Dictionary(138 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 517 corpus positions)\n",
      "2021-01-14 09:11:22,604 : INFO : token count processed\n",
      "2021-01-14 09:11:22,628 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 09:11:22,628 : INFO : frequencies processed\n",
      "2021-01-14 09:11:22,629 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 09:11:22,634 : INFO : token count processed\n",
      "2021-01-14 09:11:22,637 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:11:22,641 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:11:22,642 : INFO : vocab #32006\n",
      "2021-01-14 09:11:22,648 : INFO : diff #set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 09:11:41,099 : INFO : alphabet #32006\n",
      "2021-01-14 09:11:50,484 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.2917209939697438, 0.43635329197198186], [0.9530741088092327, 0.04692589], [nan, nan], [3.321928094887362, 6.0479231618016716, 6.183520341719197, 3.1863309149698367, 2.8615922468318344, 0.13559717991752507]]\n",
      "2021-01-14 09:11:50,488 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:11:50,489 : INFO : built Dictionary(139 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 636 corpus positions)\n",
      "2021-01-14 09:11:50,525 : INFO : token count processed\n",
      "2021-01-14 09:11:50,553 : INFO : frequencies processed\n",
      "2021-01-14 09:11:59,706 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:11:59,707 : INFO : entropies processed\n",
      "2021-01-14 09:11:59,708 : INFO : extropies processed\n",
      "2021-01-14 09:11:59,719 : INFO : token count processed\n",
      "2021-01-14 09:11:59,725 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:11:59,729 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:11:59,730 : INFO : vocab #32006\n",
      "2021-01-14 09:11:59,736 : INFO : diff #set()\n",
      "2021-01-14 09:12:18,229 : INFO : alphabet #32006\n",
      "2021-01-14 09:12:27,438 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.2898795727315466, 0.43670418825000573], [0.9479167275130749, 0.052083272], [0.0, 0.0], [3.321928094887362, 6.036583168403119, 6.145848513629163, 3.212662749661318, 2.823920418741801, 0.10926534522604392]]\n",
      "2021-01-14 09:12:27,451 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 09:12:27,452 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:12:27,456 : INFO : built Dictionary(550 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 6994 corpus positions)\n",
      "2021-01-14 09:12:27,690 : INFO : token count processed\n",
      "2021-01-14 09:12:27,752 : INFO : frequencies processed\n",
      "2021-01-14 09:12:37,128 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:12:37,129 : INFO : entropies processed\n",
      "2021-01-14 09:12:37,131 : INFO : extropies processed\n",
      "2021-01-14 09:12:37,146 : INFO : token count processed\n",
      "2021-01-14 09:12:37,150 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:12:37,154 : INFO : alphabet_target #32010\n",
      "2021-01-14 09:12:37,155 : INFO : vocab #32006\n",
      "2021-01-14 09:12:37,161 : INFO : diff #set()\n",
      "2021-01-14 09:12:55,664 : INFO : alphabet #32006\n",
      "2021-01-14 09:13:04,997 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.2724130652549392, 0.44006083897771076], [0.9325070604681969, 0.06749294], [2.321928094887362, 1.2877123795494492], [3.321928094887362, 7.29352035514053, 7.441711250009547, 3.173737200018346, 4.119783155122185, 0.1481908948690167]]\n",
      "2021-01-14 09:13:05,004 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:13:05,006 : INFO : built Dictionary(363 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 3237 corpus positions)\n",
      "2021-01-14 09:13:05,118 : INFO : token count processed\n",
      "2021-01-14 09:13:05,145 : INFO : frequencies processed\n",
      "2021-01-14 09:13:14,412 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:13:14,413 : INFO : entropies processed\n",
      "2021-01-14 09:13:14,414 : INFO : extropies processed\n",
      "2021-01-14 09:13:14,421 : INFO : token count processed\n",
      "2021-01-14 09:13:14,424 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:13:14,428 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:13:14,429 : INFO : vocab #32006\n",
      "2021-01-14 09:13:14,435 : INFO : diff #set()\n",
      "2021-01-14 09:13:33,074 : INFO : alphabet #32006\n",
      "2021-01-14 09:13:42,379 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.2900658312193094, 0.43666866968080376], [0.9499698355793953, 0.050030164], [0.0, 0.0], [3.321928094887362, 6.8153433747477745, 6.943121670468772, 3.1941497991663645, 3.6211935755814095, 0.1277782957209972]]\n",
      "2021-01-14 09:13:42,382 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:13:42,383 : INFO : built Dictionary(114 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 258 corpus positions)\n",
      "2021-01-14 09:13:42,412 : INFO : token count processed\n",
      "2021-01-14 09:13:42,440 : INFO : frequencies processed\n",
      "2021-01-14 09:13:51,606 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:13:51,607 : INFO : entropies processed\n",
      "2021-01-14 09:13:51,608 : INFO : extropies processed\n",
      "2021-01-14 09:13:51,614 : INFO : token count processed\n",
      "2021-01-14 09:13:51,618 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:13:51,622 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:13:51,623 : INFO : vocab #32006\n",
      "2021-01-14 09:13:51,634 : INFO : diff #set()\n",
      "2021-01-14 09:14:10,095 : INFO : alphabet #32006\n",
      "2021-01-14 09:14:19,467 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.285360617522675, 0.4375677047782496], [0.9475970081984997, 0.05240299], [0.0, 0.0], [3.321928094887362, 6.150121915859574, 6.269862236502427, 3.2021877742445097, 2.947934141615065, 0.1197403206428529]]\n",
      "2021-01-14 09:14:19,471 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:14:19,473 : INFO : built Dictionary(277 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 969 corpus positions)\n",
      "2021-01-14 09:14:19,563 : INFO : token count processed\n",
      "2021-01-14 09:14:19,633 : INFO : frequencies processed\n",
      "2021-01-14 09:14:28,860 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:14:28,861 : INFO : entropies processed\n",
      "2021-01-14 09:14:28,862 : INFO : extropies processed\n",
      "2021-01-14 09:14:28,869 : INFO : token count processed\n",
      "2021-01-14 09:14:28,876 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:14:28,882 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:14:28,883 : INFO : vocab #32006\n",
      "2021-01-14 09:14:28,890 : INFO : diff #set()\n",
      "2021-01-14 09:14:47,265 : INFO : alphabet #32006\n",
      "2021-01-14 09:14:56,627 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.264349102627558, 0.4416280152382849], [0.9203478023409843, 0.0796522], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 7.0391145208191315, 7.247360761401498, 3.1136818543049953, 3.9254326665141357, 0.2082462405823664]]\n",
      "2021-01-14 09:14:56,636 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 09:14:56,637 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:14:56,640 : INFO : built Dictionary(583 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 4344 corpus positions)\n",
      "2021-01-14 09:14:56,887 : INFO : token count processed\n",
      "2021-01-14 09:14:56,917 : INFO : frequencies processed\n",
      "2021-01-14 09:15:06,415 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:15:06,416 : INFO : entropies processed\n",
      "2021-01-14 09:15:06,417 : INFO : extropies processed\n",
      "2021-01-14 09:15:06,425 : INFO : token count processed\n",
      "2021-01-14 09:15:06,431 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:15:06,437 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:15:06,438 : INFO : vocab #32006\n",
      "2021-01-14 09:15:06,445 : INFO : diff #set()\n",
      "2021-01-14 09:15:25,198 : INFO : alphabet #32006\n",
      "2021-01-14 09:15:34,357 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.2727294502069046, 0.43999957844034715], [0.9340020567178726, 0.06599794], [1.0, 1.0], [3.321928094887362, 7.482466367279176, 7.742083160911067, 3.0623113012554715, 4.420155066023705, 0.25961679363189116]]\n",
      "2021-01-14 09:15:34,361 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:15:34,362 : INFO : built Dictionary(163 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 566 corpus positions)\n",
      "2021-01-14 09:15:34,404 : INFO : token count processed\n",
      "2021-01-14 09:15:34,432 : INFO : frequencies processed\n",
      "2021-01-14 09:15:43,624 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:15:43,625 : INFO : entropies processed\n",
      "2021-01-14 09:15:43,626 : INFO : extropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 09:15:43,632 : INFO : token count processed\n",
      "2021-01-14 09:15:43,636 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:15:43,640 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:15:43,641 : INFO : vocab #32006\n",
      "2021-01-14 09:15:43,647 : INFO : diff #set()\n",
      "2021-01-14 09:16:02,236 : INFO : alphabet #32006\n",
      "2021-01-14 09:16:11,681 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.2838424691836978, 0.43785857102369447], [0.9426087699830532, 0.05739123], [0.0, 0.0], [3.321928094887362, 6.372162341197667, 6.512516349670017, 3.1815740864150115, 3.190588254782655, 0.14035400847235024]]\n",
      "2021-01-14 09:16:11,686 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:16:11,688 : INFO : built Dictionary(312 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1987 corpus positions)\n",
      "2021-01-14 09:16:11,779 : INFO : token count processed\n",
      "2021-01-14 09:16:11,807 : INFO : frequencies processed\n",
      "2021-01-14 09:16:20,981 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:16:20,983 : INFO : entropies processed\n",
      "2021-01-14 09:16:20,983 : INFO : extropies processed\n",
      "2021-01-14 09:16:20,990 : INFO : token count processed\n",
      "2021-01-14 09:16:20,994 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:16:20,998 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:16:20,999 : INFO : vocab #32006\n",
      "2021-01-14 09:16:21,009 : INFO : diff #set()\n",
      "2021-01-14 09:16:39,663 : INFO : alphabet #32006\n",
      "2021-01-14 09:16:48,829 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.286865791275096, 0.43727970561946555], [0.9488453827798367, 0.051154617], [0.0, 0.0], [3.321928094887362, 6.798155919669889, 6.964120727973356, 3.155963286583896, 3.6421926330859935, 0.1659648083034666]]\n",
      "2021-01-14 09:16:48,833 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:16:48,834 : INFO : built Dictionary(168 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 703 corpus positions)\n",
      "2021-01-14 09:16:48,886 : INFO : token count processed\n",
      "2021-01-14 09:16:48,913 : INFO : frequencies processed\n",
      "2021-01-14 09:16:58,378 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:16:58,380 : INFO : entropies processed\n",
      "2021-01-14 09:16:58,381 : INFO : extropies processed\n",
      "2021-01-14 09:16:58,387 : INFO : token count processed\n",
      "2021-01-14 09:16:58,394 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:16:58,401 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:16:58,402 : INFO : vocab #32006\n",
      "2021-01-14 09:16:58,408 : INFO : diff #set()\n",
      "2021-01-14 09:17:16,908 : INFO : alphabet #32006\n",
      "2021-01-14 09:17:26,089 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.2875170702818046, 0.43715520771034405], [0.9497584328055382, 0.050241567], [0.0, 0.0], [3.321928094887362, 6.271631856729336, 6.409881754046996, 3.1836781975697015, 3.0879536591596337, 0.13824989731766024]]\n",
      "2021-01-14 09:17:26,096 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:17:26,098 : INFO : built Dictionary(346 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 3218 corpus positions)\n",
      "2021-01-14 09:17:26,214 : INFO : token count processed\n",
      "2021-01-14 09:17:26,242 : INFO : frequencies processed\n",
      "2021-01-14 09:17:35,522 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:17:35,523 : INFO : entropies processed\n",
      "2021-01-14 09:17:35,524 : INFO : extropies processed\n",
      "2021-01-14 09:17:35,532 : INFO : token count processed\n",
      "2021-01-14 09:17:35,538 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:17:35,545 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:17:35,546 : INFO : vocab #32006\n",
      "2021-01-14 09:17:35,552 : INFO : diff #set()\n",
      "2021-01-14 09:17:54,134 : INFO : alphabet #32006\n",
      "2021-01-14 09:18:03,420 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.2858336591116093, 0.4374771523789053], [0.9453267529606819, 0.054673247], [0.0, 0.0], [3.321928094887362, 6.873598627629562, 7.0131930832702425, 3.1823336392466812, 3.6912649883828803, 0.13959445564068051]]\n",
      "2021-01-14 09:18:03,423 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:18:03,424 : INFO : built Dictionary(96 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 166 corpus positions)\n",
      "2021-01-14 09:18:03,453 : INFO : token count processed\n",
      "2021-01-14 09:18:03,479 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 09:18:03,480 : INFO : frequencies processed\n",
      "2021-01-14 09:18:03,481 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 09:18:03,486 : INFO : token count processed\n",
      "2021-01-14 09:18:03,490 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:18:03,493 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:18:03,494 : INFO : vocab #32006\n",
      "2021-01-14 09:18:03,500 : INFO : diff #set()\n",
      "2021-01-14 09:18:21,898 : INFO : alphabet #32006\n",
      "2021-01-14 09:18:31,088 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.291329582413024, 0.4364278311053311], [0.9605978094041348, 0.03940219], [nan, nan], [3.321928094887362, 6.049830202851529, 6.262863799700042, 3.1088944980388495, 2.9409357048126794, 0.2130335968485122]]\n",
      "2021-01-14 09:18:31,092 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:18:31,094 : INFO : built Dictionary(243 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1246 corpus positions)\n",
      "2021-01-14 09:18:31,165 : INFO : token count processed\n",
      "2021-01-14 09:18:31,191 : INFO : frequencies processed\n",
      "2021-01-14 09:18:40,486 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:18:40,487 : INFO : entropies processed\n",
      "2021-01-14 09:18:40,488 : INFO : extropies processed\n",
      "2021-01-14 09:18:40,494 : INFO : token count processed\n",
      "2021-01-14 09:18:40,498 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:18:40,504 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:18:40,505 : INFO : vocab #32006\n",
      "2021-01-14 09:18:40,513 : INFO : diff #set()\n",
      "2021-01-14 09:18:58,904 : INFO : alphabet #32006\n",
      "2021-01-14 09:19:08,331 : INFO : Computed distances or similarities ('290', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.2682930414517615, 0.4408601453716828], [0.931074969470501, 0.06892503], [0.0, 0.0], [3.321928094887362, 6.778844940588858, 6.943594861475399, 3.157178174000821, 3.6216667665880364, 0.16474992088654083]]\n",
      "2021-01-14 09:19:08,334 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:19:08,335 : INFO : built Dictionary(141 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 279 corpus positions)\n",
      "2021-01-14 09:19:08,382 : INFO : token count processed\n",
      "2021-01-14 09:19:08,412 : INFO : frequencies processed\n",
      "2021-01-14 09:19:17,716 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:19:17,717 : INFO : entropies processed\n",
      "2021-01-14 09:19:17,718 : INFO : extropies processed\n",
      "2021-01-14 09:19:17,724 : INFO : token count processed\n",
      "2021-01-14 09:19:17,729 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:19:17,735 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:19:17,736 : INFO : vocab #32006\n",
      "2021-01-14 09:19:17,744 : INFO : diff #set()\n",
      "2021-01-14 09:19:36,133 : INFO : alphabet #32006\n",
      "2021-01-14 09:19:45,407 : INFO : Computed distances or similarities ('290', 'sacp-python-common/setup.py')[[1.2695432686924717, 0.44061728797799904], [0.9331550374627113, 0.06684496], [1.0, 1.0], [3.321928094887362, 6.469677430851302, 6.6783575162178, 3.1132480095208637, 3.356429421330438, 0.20868008536649807]]\n",
      "2021-01-14 09:19:45,412 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:19:45,414 : INFO : built Dictionary(212 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1181 corpus positions)\n",
      "2021-01-14 09:19:45,476 : INFO : token count processed\n",
      "2021-01-14 09:19:45,506 : INFO : frequencies processed\n",
      "2021-01-14 09:19:54,713 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:19:54,714 : INFO : entropies processed\n",
      "2021-01-14 09:19:54,715 : INFO : extropies processed\n",
      "2021-01-14 09:19:54,722 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 09:19:54,725 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:19:54,729 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:19:54,730 : INFO : vocab #32006\n",
      "2021-01-14 09:19:54,735 : INFO : diff #set()\n",
      "2021-01-14 09:20:13,264 : INFO : alphabet #32006\n",
      "2021-01-14 09:20:22,429 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.2875044930567348, 0.4371576112900767], [0.952710434794426, 0.047289565], [0.0, 0.0], [3.321928094887362, 6.459180448028249, 6.599445823511447, 3.181662719404164, 3.2775177286240846, 0.14026537548319773]]\n",
      "2021-01-14 09:20:22,433 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:20:22,434 : INFO : built Dictionary(121 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 331 corpus positions)\n",
      "2021-01-14 09:20:22,464 : INFO : token count processed\n",
      "2021-01-14 09:20:22,490 : INFO : frequencies processed\n",
      "2021-01-14 09:20:31,731 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:20:31,732 : INFO : entropies processed\n",
      "2021-01-14 09:20:31,733 : INFO : extropies processed\n",
      "2021-01-14 09:20:31,739 : INFO : token count processed\n",
      "2021-01-14 09:20:31,746 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:20:31,751 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:20:31,752 : INFO : vocab #32006\n",
      "2021-01-14 09:20:31,759 : INFO : diff #set()\n",
      "2021-01-14 09:20:50,330 : INFO : alphabet #32006\n",
      "2021-01-14 09:20:59,516 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.2594846709029917, 0.4425787936858873], [0.9190080165863037, 0.08099198], [1.0, 1.0], [3.321928094887362, 6.097125733496388, 6.223415273228149, 3.195638555155602, 2.9014871783407865, 0.1262895397317605]]\n",
      "2021-01-14 09:20:59,519 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:20:59,520 : INFO : built Dictionary(114 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 327 corpus positions)\n",
      "2021-01-14 09:20:59,553 : INFO : token count processed\n",
      "2021-01-14 09:20:59,604 : INFO : frequencies processed\n",
      "2021-01-14 09:21:08,967 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:21:08,968 : INFO : entropies processed\n",
      "2021-01-14 09:21:08,969 : INFO : extropies processed\n",
      "2021-01-14 09:21:08,975 : INFO : token count processed\n",
      "2021-01-14 09:21:08,982 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:21:08,988 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:21:08,989 : INFO : vocab #32006\n",
      "2021-01-14 09:21:08,995 : INFO : diff #set()\n",
      "2021-01-14 09:21:27,505 : INFO : alphabet #32006\n",
      "2021-01-14 09:21:36,681 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.2810560104940636, 0.4383934438257856], [0.9423855058848858, 0.057614494], [0.0, 0.0], [3.321928094887362, 6.0695858597523715, 6.201288829571389, 3.190225125068345, 2.879360734684027, 0.13170296981901775]]\n",
      "2021-01-14 09:21:36,684 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 09:21:36,685 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:21:36,687 : INFO : built Dictionary(115 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 423 corpus positions)\n",
      "2021-01-14 09:21:36,723 : INFO : token count processed\n",
      "2021-01-14 09:21:36,750 : INFO : frequencies processed\n",
      "2021-01-14 09:21:45,896 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:21:45,897 : INFO : entropies processed\n",
      "2021-01-14 09:21:45,898 : INFO : extropies processed\n",
      "2021-01-14 09:21:45,904 : INFO : token count processed\n",
      "2021-01-14 09:21:45,911 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:21:45,916 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:21:45,918 : INFO : vocab #32006\n",
      "2021-01-14 09:21:45,926 : INFO : diff #set()\n",
      "2021-01-14 09:22:04,486 : INFO : alphabet #32006\n",
      "2021-01-14 09:22:13,641 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.2816318595575318, 0.4382828000104829], [0.9450901672244072, 0.054909833], [1.0, 1.0], [3.321928094887362, 6.104787343210121, 6.202415610934514, 3.2242998271629695, 2.8804875160471517, 0.09762826772439315]]\n",
      "2021-01-14 09:22:13,658 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 09:22:13,659 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:22:13,662 : INFO : built Dictionary(402 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 9108 corpus positions)\n",
      "2021-01-14 09:22:13,812 : INFO : token count processed\n",
      "2021-01-14 09:22:13,863 : INFO : frequencies processed\n",
      "2021-01-14 09:22:23,170 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:22:23,171 : INFO : entropies processed\n",
      "2021-01-14 09:22:23,172 : INFO : extropies processed\n",
      "2021-01-14 09:22:23,181 : INFO : token count processed\n",
      "2021-01-14 09:22:23,189 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:22:23,193 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:22:23,194 : INFO : vocab #32006\n",
      "2021-01-14 09:22:23,202 : INFO : diff #set()\n",
      "2021-01-14 09:22:41,671 : INFO : alphabet #32006\n",
      "2021-01-14 09:22:50,841 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.2661658108624794, 0.44127397704381144], [0.9271237403154373, 0.07287626], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 6.89087415148015, 7.040060578069858, 3.172741668297655, 3.718132483182496, 0.14918642658970782]]\n",
      "2021-01-14 09:22:50,847 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:22:50,849 : INFO : built Dictionary(261 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 2268 corpus positions)\n",
      "2021-01-14 09:22:50,920 : INFO : token count processed\n",
      "2021-01-14 09:22:50,948 : INFO : frequencies processed\n",
      "2021-01-14 09:23:00,225 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:23:00,226 : INFO : entropies processed\n",
      "2021-01-14 09:23:00,227 : INFO : extropies processed\n",
      "2021-01-14 09:23:00,234 : INFO : token count processed\n",
      "2021-01-14 09:23:00,240 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:23:00,245 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:23:00,246 : INFO : vocab #32006\n",
      "2021-01-14 09:23:00,253 : INFO : diff #set()\n",
      "2021-01-14 09:23:18,797 : INFO : alphabet #32006\n",
      "2021-01-14 09:23:27,980 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.264200527255653, 0.4416569945825691], [0.921662375330925, 0.078337625], [1.0, 1.0], [3.321928094887362, 6.655493573668506, 6.773029892711927, 3.2043917758439413, 3.4511017978245646, 0.11753631904342043]]\n",
      "2021-01-14 09:23:27,984 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:23:27,986 : INFO : built Dictionary(238 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1275 corpus positions)\n",
      "2021-01-14 09:23:28,058 : INFO : token count processed\n",
      "2021-01-14 09:23:28,088 : INFO : frequencies processed\n",
      "2021-01-14 09:23:37,361 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:23:37,362 : INFO : entropies processed\n",
      "2021-01-14 09:23:37,363 : INFO : extropies processed\n",
      "2021-01-14 09:23:37,369 : INFO : token count processed\n",
      "2021-01-14 09:23:37,376 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:23:37,381 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:23:37,382 : INFO : vocab #32006\n",
      "2021-01-14 09:23:37,389 : INFO : diff #set()\n",
      "2021-01-14 09:23:55,814 : INFO : alphabet #32006\n",
      "2021-01-14 09:24:05,123 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.270558942063397, 0.44042018970502406], [0.9287989661097527, 0.071201034], [1.0, 1.0], [3.321928094887362, 6.6236746347295465, 6.717852409803109, 3.227750319813799, 3.395924314915747, 0.09417777507356284]]\n",
      "2021-01-14 09:24:05,127 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:24:05,129 : INFO : built Dictionary(239 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1214 corpus positions)\n",
      "2021-01-14 09:24:05,195 : INFO : token count processed\n",
      "2021-01-14 09:24:05,224 : INFO : frequencies processed\n",
      "2021-01-14 09:24:14,382 : INFO : scalar_distribution processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 09:24:14,383 : INFO : entropies processed\n",
      "2021-01-14 09:24:14,384 : INFO : extropies processed\n",
      "2021-01-14 09:24:14,391 : INFO : token count processed\n",
      "2021-01-14 09:24:14,397 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:24:14,402 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:24:14,403 : INFO : vocab #32006\n",
      "2021-01-14 09:24:14,410 : INFO : diff #set()\n",
      "2021-01-14 09:24:32,983 : INFO : alphabet #32006\n",
      "2021-01-14 09:24:42,140 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.2747094468153164, 0.43961658549404614], [0.9347774758934975, 0.065222524], [1.0, 1.0], [3.321928094887362, 6.75472436518627, 6.846269273279303, 3.230383186794329, 3.5243411783919405, 0.09154490809303262]]\n",
      "2021-01-14 09:24:42,144 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:24:42,146 : INFO : built Dictionary(192 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1009 corpus positions)\n",
      "2021-01-14 09:24:42,200 : INFO : token count processed\n",
      "2021-01-14 09:24:42,234 : INFO : frequencies processed\n",
      "2021-01-14 09:24:51,407 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:24:51,408 : INFO : entropies processed\n",
      "2021-01-14 09:24:51,409 : INFO : extropies processed\n",
      "2021-01-14 09:24:51,416 : INFO : token count processed\n",
      "2021-01-14 09:24:51,422 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:24:51,427 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:24:51,428 : INFO : vocab #32006\n",
      "2021-01-14 09:24:51,436 : INFO : diff #set()\n",
      "2021-01-14 09:25:09,973 : INFO : alphabet #32006\n",
      "2021-01-14 09:25:19,302 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.2782197438730318, 0.4389392211569436], [0.9398585930466652, 0.060141407], [0.0, 0.0], [3.321928094887362, 6.597313085495733, 6.681329248340835, 3.23791193204226, 3.3594011534534727, 0.08401616284510194]]\n",
      "2021-01-14 09:25:19,306 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:25:19,307 : INFO : built Dictionary(216 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 975 corpus positions)\n",
      "2021-01-14 09:25:19,370 : INFO : token count processed\n",
      "2021-01-14 09:25:19,403 : INFO : frequencies processed\n",
      "2021-01-14 09:25:28,579 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:25:28,580 : INFO : entropies processed\n",
      "2021-01-14 09:25:28,581 : INFO : extropies processed\n",
      "2021-01-14 09:25:28,588 : INFO : token count processed\n",
      "2021-01-14 09:25:28,592 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:25:28,596 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:25:28,596 : INFO : vocab #32006\n",
      "2021-01-14 09:25:28,602 : INFO : diff #set()\n",
      "2021-01-14 09:25:47,081 : INFO : alphabet #32006\n",
      "2021-01-14 09:25:56,280 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.2798985325777568, 0.4386160110684201], [0.9362603053450584, 0.063739695], [1.0, 1.0], [3.321928094887362, 6.659481538516613, 6.7596147630082815, 3.2217948703956942, 3.4376866681209193, 0.1001332244916684]]\n",
      "2021-01-14 09:25:56,285 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:25:56,286 : INFO : built Dictionary(241 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1137 corpus positions)\n",
      "2021-01-14 09:25:56,363 : INFO : token count processed\n",
      "2021-01-14 09:25:56,397 : INFO : frequencies processed\n",
      "2021-01-14 09:26:05,828 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:26:05,829 : INFO : entropies processed\n",
      "2021-01-14 09:26:05,830 : INFO : extropies processed\n",
      "2021-01-14 09:26:05,836 : INFO : token count processed\n",
      "2021-01-14 09:26:05,843 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:26:05,850 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:26:05,851 : INFO : vocab #32006\n",
      "2021-01-14 09:26:05,857 : INFO : diff #set()\n",
      "2021-01-14 09:26:24,436 : INFO : alphabet #32006\n",
      "2021-01-14 09:26:33,617 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.239956264894836, 0.4464372879382755], [0.8871632292866707, 0.11283677], [1.0, 1.0], [3.321928094887362, 6.774682571479102, 6.846492273340624, 3.25011839302584, 3.524564178453262, 0.07180970186152269]]\n",
      "2021-01-14 09:26:33,631 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:26:33,634 : INFO : built Dictionary(418 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 7874 corpus positions)\n",
      "2021-01-14 09:26:33,795 : INFO : token count processed\n",
      "2021-01-14 09:26:33,852 : INFO : frequencies processed\n",
      "2021-01-14 09:26:43,146 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:26:43,147 : INFO : entropies processed\n",
      "2021-01-14 09:26:43,148 : INFO : extropies processed\n",
      "2021-01-14 09:26:43,157 : INFO : token count processed\n",
      "2021-01-14 09:26:43,161 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:26:43,165 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:26:43,166 : INFO : vocab #32006\n",
      "2021-01-14 09:26:43,176 : INFO : diff #set()\n",
      "2021-01-14 09:27:01,868 : INFO : alphabet #32006\n",
      "2021-01-14 09:27:11,039 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.262907021296739, 0.44190945124513287], [0.9261504635214806, 0.07384954], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 6.839453716525233, 6.978793962164342, 3.182587849248253, 3.6568658672769794, 0.13934024563910885]]\n",
      "2021-01-14 09:27:11,045 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:27:11,047 : INFO : built Dictionary(318 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 2316 corpus positions)\n",
      "2021-01-14 09:27:11,147 : INFO : token count processed\n",
      "2021-01-14 09:27:11,207 : INFO : frequencies processed\n",
      "2021-01-14 09:27:20,504 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:27:20,505 : INFO : entropies processed\n",
      "2021-01-14 09:27:20,506 : INFO : extropies processed\n",
      "2021-01-14 09:27:20,519 : INFO : token count processed\n",
      "2021-01-14 09:27:20,523 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:27:20,527 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:27:20,528 : INFO : vocab #32006\n",
      "2021-01-14 09:27:20,534 : INFO : diff #set()\n",
      "2021-01-14 09:27:38,967 : INFO : alphabet #32006\n",
      "2021-01-14 09:27:48,130 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.219631193948326, 0.4505252957006697], [0.8599920272827148, 0.14000797], [1.584962500721156, 1.1699250014423124], [3.321928094887362, 6.86432793886027, 6.9555184054203565, 3.230737628327276, 3.6335903105329943, 0.09119046656008667]]\n",
      "2021-01-14 09:27:48,134 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:27:48,135 : INFO : built Dictionary(152 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 518 corpus positions)\n",
      "2021-01-14 09:27:48,181 : INFO : token count processed\n",
      "2021-01-14 09:27:48,213 : INFO : frequencies processed\n",
      "2021-01-14 09:27:57,675 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:27:57,677 : INFO : entropies processed\n",
      "2021-01-14 09:27:57,677 : INFO : extropies processed\n",
      "2021-01-14 09:27:57,686 : INFO : token count processed\n",
      "2021-01-14 09:27:57,690 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:27:57,694 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:27:57,695 : INFO : vocab #32006\n",
      "2021-01-14 09:27:57,701 : INFO : diff #set()\n",
      "2021-01-14 09:28:16,170 : INFO : alphabet #32006\n",
      "2021-01-14 09:28:25,496 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.2132666560607344, 0.45182084014216445], [0.8512282222509384, 0.14877178], [1.0, 1.0], [3.321928094887362, 6.431978396403875, 6.51518870103158, 3.238717790259658, 3.1932606061442175, 0.08321030462770462]]\n",
      "2021-01-14 09:28:25,500 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:28:25,501 : INFO : built Dictionary(212 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 672 corpus positions)\n",
      "2021-01-14 09:28:25,566 : INFO : token count processed\n",
      "2021-01-14 09:28:25,593 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 09:28:34,765 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:28:34,766 : INFO : entropies processed\n",
      "2021-01-14 09:28:34,767 : INFO : extropies processed\n",
      "2021-01-14 09:28:34,773 : INFO : token count processed\n",
      "2021-01-14 09:28:34,777 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:28:34,780 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:28:34,781 : INFO : vocab #32006\n",
      "2021-01-14 09:28:34,786 : INFO : diff #set()\n",
      "2021-01-14 09:28:53,269 : INFO : alphabet #32006\n",
      "2021-01-14 09:29:02,743 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/test_auth_utility.py')[[1.277592396381655, 0.4390601240101921], [0.9326114356517792, 0.067388564], [0.0, 0.0], [3.321928094887362, 6.911818353685893, 7.0050088911775354, 3.2287375573957195, 3.6830807962901733, 0.09319053749164219]]\n",
      "2021-01-14 09:29:02,756 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:29:02,762 : INFO : built Dictionary(303 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 7202 corpus positions)\n",
      "2021-01-14 09:29:02,876 : INFO : token count processed\n",
      "2021-01-14 09:29:02,934 : INFO : frequencies processed\n",
      "2021-01-14 09:29:12,114 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:29:12,115 : INFO : entropies processed\n",
      "2021-01-14 09:29:12,116 : INFO : extropies processed\n",
      "2021-01-14 09:29:12,125 : INFO : token count processed\n",
      "2021-01-14 09:29:12,131 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:29:12,137 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:29:12,138 : INFO : vocab #32006\n",
      "2021-01-14 09:29:12,145 : INFO : diff #set()\n",
      "2021-01-14 09:29:30,631 : INFO : alphabet #32006\n",
      "2021-01-14 09:29:39,785 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.2751846986165494, 0.43952475621344533], [0.9341097697615623, 0.06589023], [0.0, 0.0], [3.321928094887362, 6.363791471162389, 6.389550014661922, 3.2961695513878295, 3.06762191977456, 0.025758543499533104]]\n",
      "2021-01-14 09:29:39,790 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:29:39,791 : INFO : built Dictionary(204 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1272 corpus positions)\n",
      "2021-01-14 09:29:39,849 : INFO : token count processed\n",
      "2021-01-14 09:29:39,880 : INFO : frequencies processed\n",
      "2021-01-14 09:29:49,142 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:29:49,144 : INFO : entropies processed\n",
      "2021-01-14 09:29:49,144 : INFO : extropies processed\n",
      "2021-01-14 09:29:49,151 : INFO : token count processed\n",
      "2021-01-14 09:29:49,155 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:29:49,159 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:29:49,160 : INFO : vocab #32006\n",
      "2021-01-14 09:29:49,165 : INFO : diff #set()\n",
      "2021-01-14 09:30:07,666 : INFO : alphabet #32006\n",
      "2021-01-14 09:30:16,863 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.2917496944938145, 0.4363478273402248], [0.9583279713988304, 0.04167203], [0.0, 0.0], [3.321928094887362, 6.29000629755059, 6.476022341360155, 3.1359120510777974, 3.1540942464727926, 0.18601604380956438]]\n",
      "2021-01-14 09:30:16,868 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:30:16,870 : INFO : built Dictionary(214 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1225 corpus positions)\n",
      "2021-01-14 09:30:16,936 : INFO : token count processed\n",
      "2021-01-14 09:30:16,966 : INFO : frequencies processed\n",
      "2021-01-14 09:30:26,517 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:30:26,518 : INFO : entropies processed\n",
      "2021-01-14 09:30:26,519 : INFO : extropies processed\n",
      "2021-01-14 09:30:26,526 : INFO : token count processed\n",
      "2021-01-14 09:30:26,532 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:30:26,537 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:30:26,539 : INFO : vocab #32006\n",
      "2021-01-14 09:30:26,545 : INFO : diff #set()\n",
      "2021-01-14 09:30:45,321 : INFO : alphabet #32006\n",
      "2021-01-14 09:30:54,485 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.28188541716766, 0.43823409908163924], [0.948302898555994, 0.0516971], [0.0, 0.0], [3.321928094887362, 6.361621244785958, 6.531551950733405, 3.1519973889399147, 3.209623855846043, 0.16993070594744708]]\n",
      "2021-01-14 09:30:54,490 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:30:54,491 : INFO : built Dictionary(223 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1248 corpus positions)\n",
      "2021-01-14 09:30:54,562 : INFO : token count processed\n",
      "2021-01-14 09:30:54,606 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 09:30:54,607 : INFO : frequencies processed\n",
      "2021-01-14 09:30:54,607 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 09:30:54,612 : INFO : token count processed\n",
      "2021-01-14 09:30:54,616 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:30:54,629 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:30:54,629 : INFO : vocab #32006\n",
      "2021-01-14 09:30:54,634 : INFO : diff #set()\n",
      "2021-01-14 09:31:13,132 : INFO : alphabet #32006\n",
      "2021-01-14 09:31:22,530 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.28332089398169, 0.43795859033032564], [0.9554951004683971, 0.0445049], [nan, nan], [3.321928094887362, 6.620594433343389, 6.802480866567864, 3.140041661662888, 3.4805527716805016, 0.1818864332244745]]\n",
      "2021-01-14 09:31:22,534 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:31:22,536 : INFO : built Dictionary(195 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1341 corpus positions)\n",
      "2021-01-14 09:31:22,590 : INFO : token count processed\n",
      "2021-01-14 09:31:22,613 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 09:31:22,614 : INFO : frequencies processed\n",
      "2021-01-14 09:31:22,615 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 09:31:22,621 : INFO : token count processed\n",
      "2021-01-14 09:31:22,624 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:31:22,628 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:31:22,629 : INFO : vocab #32006\n",
      "2021-01-14 09:31:22,635 : INFO : diff #set()\n",
      "2021-01-14 09:31:41,120 : INFO : alphabet #32006\n",
      "2021-01-14 09:31:50,292 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.2821777981494686, 0.4381779547635868], [0.952822357416153, 0.047177643], [nan, nan], [3.321928094887362, 6.207411496248084, 6.3068346812178175, 3.222504909917629, 2.9849065863304554, 0.09942318496973357]]\n",
      "2021-01-14 09:31:50,296 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:31:50,297 : INFO : built Dictionary(134 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 245 corpus positions)\n",
      "2021-01-14 09:31:50,331 : INFO : token count processed\n",
      "2021-01-14 09:31:50,359 : INFO : frequencies processed\n",
      "2021-01-14 09:31:59,639 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:31:59,640 : INFO : entropies processed\n",
      "2021-01-14 09:31:59,641 : INFO : extropies processed\n",
      "2021-01-14 09:31:59,648 : INFO : token count processed\n",
      "2021-01-14 09:31:59,652 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:31:59,656 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:31:59,657 : INFO : vocab #32006\n",
      "2021-01-14 09:31:59,669 : INFO : diff #set()\n",
      "2021-01-14 09:32:18,173 : INFO : alphabet #32006\n",
      "2021-01-14 09:32:27,341 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.2762922450433607, 0.439310902269911], [0.9390067718923092, 0.060993228], [0.0, 0.0], [3.321928094887362, 6.5805228788529595, 6.706310851141846, 3.196140122598476, 3.384382756254484, 0.1257879722888866]]\n",
      "2021-01-14 09:32:27,346 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 09:32:27,346 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:32:27,348 : INFO : built Dictionary(216 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1206 corpus positions)\n",
      "2021-01-14 09:32:27,413 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 09:32:27,441 : INFO : frequencies processed\n",
      "2021-01-14 09:32:36,591 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:32:36,592 : INFO : entropies processed\n",
      "2021-01-14 09:32:36,593 : INFO : extropies processed\n",
      "2021-01-14 09:32:36,599 : INFO : token count processed\n",
      "2021-01-14 09:32:36,606 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:32:36,613 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:32:36,614 : INFO : vocab #32006\n",
      "2021-01-14 09:32:36,621 : INFO : diff #set()\n",
      "2021-01-14 09:32:55,329 : INFO : alphabet #32006\n",
      "2021-01-14 09:33:04,712 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.277910426241991, 0.43899882474736357], [0.9389487765729427, 0.061051223], [1.0, 1.0], [3.321928094887362, 6.422089779976135, 6.487172451893047, 3.25684542297045, 3.1652443570056845, 0.06508267191691175]]\n",
      "2021-01-14 09:33:04,717 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:33:04,719 : INFO : built Dictionary(230 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1488 corpus positions)\n",
      "2021-01-14 09:33:04,783 : INFO : token count processed\n",
      "2021-01-14 09:33:04,809 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 09:33:04,809 : INFO : frequencies processed\n",
      "2021-01-14 09:33:04,810 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 09:33:04,815 : INFO : token count processed\n",
      "2021-01-14 09:33:04,819 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:33:04,822 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:33:04,823 : INFO : vocab #32006\n",
      "2021-01-14 09:33:04,831 : INFO : diff #set()\n",
      "2021-01-14 09:33:23,867 : INFO : alphabet #32006\n",
      "2021-01-14 09:33:33,374 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.2865186105944872, 0.4373461013466247], [0.9605365917086601, 0.03946341], [nan, nan], [3.321928094887362, 6.485445644653597, 6.697099889234049, 3.1102738503069096, 3.3751717943466866, 0.21165424458045212]]\n",
      "2021-01-14 09:33:33,378 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:33:33,380 : INFO : built Dictionary(205 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1374 corpus positions)\n",
      "2021-01-14 09:33:33,441 : INFO : token count processed\n",
      "2021-01-14 09:33:33,467 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 09:33:33,469 : INFO : frequencies processed\n",
      "2021-01-14 09:33:33,469 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 09:33:33,474 : INFO : token count processed\n",
      "2021-01-14 09:33:33,478 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:33:33,481 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:33:33,482 : INFO : vocab #32006\n",
      "2021-01-14 09:33:33,489 : INFO : diff #set()\n",
      "2021-01-14 09:33:52,334 : INFO : alphabet #32006\n",
      "2021-01-14 09:34:01,513 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.2826683349574421, 0.43808379197525593], [0.954652588814497, 0.04534741], [nan, nan], [3.321928094887362, 6.2276600107346916, 6.324902520295671, 3.224685585326382, 3.002974425408309, 0.09724250956097968]]\n",
      "2021-01-14 09:34:01,518 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:34:01,519 : INFO : built Dictionary(194 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1572 corpus positions)\n",
      "2021-01-14 09:34:01,576 : INFO : token count processed\n",
      "2021-01-14 09:34:01,603 : INFO : frequencies processed\n",
      "2021-01-14 09:34:10,748 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:34:10,749 : INFO : entropies processed\n",
      "2021-01-14 09:34:10,750 : INFO : extropies processed\n",
      "2021-01-14 09:34:10,759 : INFO : token count processed\n",
      "2021-01-14 09:34:10,763 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:34:10,767 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:34:10,768 : INFO : vocab #32006\n",
      "2021-01-14 09:34:10,774 : INFO : diff #set()\n",
      "2021-01-14 09:34:29,265 : INFO : alphabet #32006\n",
      "2021-01-14 09:34:38,600 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.290101923895411, 0.43666178765485814], [0.9562250673770905, 0.043774933], [0.0, 0.0], [3.321928094887362, 6.253918170574241, 6.402205715414488, 3.173640550047115, 3.080277620527126, 0.14828754484024742]]\n",
      "2021-01-14 09:34:38,603 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:34:38,605 : INFO : built Dictionary(167 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 669 corpus positions)\n",
      "2021-01-14 09:34:38,650 : INFO : token count processed\n",
      "2021-01-14 09:34:38,675 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 09:34:38,676 : INFO : frequencies processed\n",
      "2021-01-14 09:34:38,676 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 09:34:38,681 : INFO : token count processed\n",
      "2021-01-14 09:34:38,684 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:34:38,687 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:34:38,688 : INFO : vocab #32006\n",
      "2021-01-14 09:34:38,693 : INFO : diff #set()\n",
      "2021-01-14 09:34:57,630 : INFO : alphabet #32006\n",
      "2021-01-14 09:35:07,129 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.2823299481188928, 0.4381487439290734], [0.9538931846618652, 0.046106815], [nan, nan], [3.321928094887362, 6.374522245625576, 6.533615571279558, 3.1628347692333794, 3.211687476392196, 0.15909332565398238]]\n",
      "2021-01-14 09:35:07,135 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:35:07,137 : INFO : built Dictionary(288 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1951 corpus positions)\n",
      "2021-01-14 09:35:07,231 : INFO : token count processed\n",
      "2021-01-14 09:35:07,288 : INFO : frequencies processed\n",
      "2021-01-14 09:35:16,455 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:35:16,456 : INFO : entropies processed\n",
      "2021-01-14 09:35:16,457 : INFO : extropies processed\n",
      "2021-01-14 09:35:16,464 : INFO : token count processed\n",
      "2021-01-14 09:35:16,468 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:35:16,472 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:35:16,473 : INFO : vocab #32006\n",
      "2021-01-14 09:35:16,483 : INFO : diff #set()\n",
      "2021-01-14 09:35:34,967 : INFO : alphabet #32006\n",
      "2021-01-14 09:35:44,123 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.2662944896276889, 0.4412489217869836], [0.9279636740684509, 0.072036326], [0.0, 0.0], [3.321928094887362, 6.731238669067808, 6.971805883291341, 3.081360880663829, 3.649877788403979, 0.24056721422353355]]\n",
      "2021-01-14 09:35:44,128 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:35:44,129 : INFO : built Dictionary(213 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1343 corpus positions)\n",
      "2021-01-14 09:35:44,187 : INFO : token count processed\n",
      "2021-01-14 09:35:44,214 : INFO : frequencies processed\n",
      "2021-01-14 09:35:53,374 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:35:53,375 : INFO : entropies processed\n",
      "2021-01-14 09:35:53,376 : INFO : extropies processed\n",
      "2021-01-14 09:35:53,383 : INFO : token count processed\n",
      "2021-01-14 09:35:53,390 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:35:53,395 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:35:53,396 : INFO : vocab #32006\n",
      "2021-01-14 09:35:53,403 : INFO : diff #set()\n",
      "2021-01-14 09:36:12,825 : INFO : alphabet #32006\n",
      "2021-01-14 09:36:22,186 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.281468588036759, 0.43831416537736173], [0.9454357698559761, 0.05456423], [0.0, 0.0], [3.321928094887362, 6.503741451859337, 6.658864224873721, 3.166805321872978, 3.3369361299863587, 0.1551227730143836]]\n",
      "2021-01-14 09:36:22,191 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:36:22,193 : INFO : built Dictionary(235 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 1665 corpus positions)\n",
      "2021-01-14 09:36:22,272 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 09:36:22,304 : INFO : frequencies processed\n",
      "2021-01-14 09:36:31,756 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:36:31,757 : INFO : entropies processed\n",
      "2021-01-14 09:36:31,758 : INFO : extropies processed\n",
      "2021-01-14 09:36:31,764 : INFO : token count processed\n",
      "2021-01-14 09:36:31,772 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:36:31,777 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:36:31,779 : INFO : vocab #32006\n",
      "2021-01-14 09:36:31,786 : INFO : diff #set()\n",
      "2021-01-14 09:36:50,657 : INFO : alphabet #32006\n",
      "2021-01-14 09:36:59,912 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.2847783524349905, 0.4376792168633142], [0.9578464664518833, 0.042153534], [0.0, 0.0], [3.321928094887362, 6.334729224484471, 6.435458920554014, 3.221198398817819, 3.113530825666652, 0.10072969606954274]]\n",
      "2021-01-14 09:36:59,917 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:36:59,921 : INFO : built Dictionary(232 unique tokens: ['28', '9)', 'bug', 's', 'sec']...) from 2 documents (total 2014 corpus positions)\n",
      "2021-01-14 09:36:59,995 : INFO : token count processed\n",
      "2021-01-14 09:37:00,028 : INFO : frequencies processed\n",
      "2021-01-14 09:37:09,323 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:37:09,324 : INFO : entropies processed\n",
      "2021-01-14 09:37:09,325 : INFO : extropies processed\n",
      "2021-01-14 09:37:09,332 : INFO : token count processed\n",
      "2021-01-14 09:37:09,336 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:37:09,339 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:37:09,340 : INFO : vocab #32006\n",
      "2021-01-14 09:37:09,346 : INFO : diff #set()\n",
      "2021-01-14 09:37:27,866 : INFO : alphabet #32006\n",
      "2021-01-14 09:37:37,352 : INFO : Computed distances or similarities ('290', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.2799160211064768, 0.4386126465810286], [0.9616108946502209, 0.038389105], [0.0, 0.0], [3.321928094887362, 6.21319712067992, 6.37924204268443, 3.1558831728828523, 3.0573139477970677, 0.1660449220045095]]\n",
      "2021-01-14 09:37:37,357 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 09:37:37,358 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:37:37,361 : INFO : built Dictionary(301 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1458 corpus positions)\n",
      "2021-01-14 09:37:37,715 : INFO : token count processed\n",
      "2021-01-14 09:37:37,745 : INFO : frequencies processed\n",
      "2021-01-14 09:37:47,159 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:37:47,160 : INFO : entropies processed\n",
      "2021-01-14 09:37:47,161 : INFO : extropies processed\n",
      "2021-01-14 09:37:47,174 : INFO : token count processed\n",
      "2021-01-14 09:37:47,178 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:37:47,182 : INFO : alphabet_target #32010\n",
      "2021-01-14 09:37:47,182 : INFO : vocab #32006\n",
      "2021-01-14 09:37:47,188 : INFO : diff #set()\n",
      "2021-01-14 09:38:05,968 : INFO : alphabet #32006\n",
      "2021-01-14 09:38:15,256 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.1719615331853546, 0.4604133106047327], [0.791033536195755, 0.20896646], [3.084962500721156, 1.348005866045709], [5.494974824689258, 6.905617163738059, 7.161112589044388, 5.23947939938293, 1.6661377643551294, 0.2554954253063286]]\n",
      "2021-01-14 09:38:15,262 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:38:15,264 : INFO : built Dictionary(387 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 2348 corpus positions)\n",
      "2021-01-14 09:38:15,861 : INFO : token count processed\n",
      "2021-01-14 09:38:15,916 : INFO : frequencies processed\n",
      "2021-01-14 09:38:25,097 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:38:25,098 : INFO : entropies processed\n",
      "2021-01-14 09:38:25,098 : INFO : extropies processed\n",
      "2021-01-14 09:38:25,106 : INFO : token count processed\n",
      "2021-01-14 09:38:25,110 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:38:25,113 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:38:25,114 : INFO : vocab #32006\n",
      "2021-01-14 09:38:25,121 : INFO : diff #set()\n",
      "2021-01-14 09:38:43,537 : INFO : alphabet #32006\n",
      "2021-01-14 09:38:53,005 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.1597175173795349, 0.4630235167112674], [0.7832914590835571, 0.21670854], [3.606936732175321, 1.3711132522931742], [5.494974824689258, 7.1219284286457345, 7.409897238689013, 5.207006014645979, 1.9149224139997552, 0.2879688100432789]]\n",
      "2021-01-14 09:38:53,011 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:38:53,015 : INFO : built Dictionary(311 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 2331 corpus positions)\n",
      "2021-01-14 09:38:53,382 : INFO : token count processed\n",
      "2021-01-14 09:38:53,448 : INFO : frequencies processed\n",
      "2021-01-14 09:39:02,816 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:39:02,817 : INFO : entropies processed\n",
      "2021-01-14 09:39:02,818 : INFO : extropies processed\n",
      "2021-01-14 09:39:02,825 : INFO : token count processed\n",
      "2021-01-14 09:39:02,832 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:39:02,837 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:39:02,838 : INFO : vocab #32006\n",
      "2021-01-14 09:39:02,845 : INFO : diff #set()\n",
      "2021-01-14 09:39:21,821 : INFO : alphabet #32006\n",
      "2021-01-14 09:39:30,975 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2077472375897444, 0.45295040255229857], [0.8217768222093582, 0.17822318], [3.0957952550009336, 1.3487605247277434], [5.494974824689258, 6.41099024988467, 6.58577841177601, 5.3201866627979175, 1.0908035870867518, 0.17478816189133983]]\n",
      "2021-01-14 09:39:30,979 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:39:30,981 : INFO : built Dictionary(190 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 730 corpus positions)\n",
      "2021-01-14 09:39:31,166 : INFO : token count processed\n",
      "2021-01-14 09:39:31,198 : INFO : frequencies processed\n",
      "2021-01-14 09:39:40,359 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:39:40,360 : INFO : entropies processed\n",
      "2021-01-14 09:39:40,361 : INFO : extropies processed\n",
      "2021-01-14 09:39:40,368 : INFO : token count processed\n",
      "2021-01-14 09:39:40,375 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:39:40,379 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:39:40,380 : INFO : vocab #32006\n",
      "2021-01-14 09:39:40,388 : INFO : diff #set()\n",
      "2021-01-14 09:39:58,979 : INFO : alphabet #32006\n",
      "2021-01-14 09:40:08,234 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.1842357821577219, 0.4578260314974507], [0.8027672916650772, 0.19723271], [2.9219280948873623, 1.3359016564230495], [5.494974824689258, 6.077866832717642, 6.466850223780501, 5.1059914336264, 0.9718753990912425, 0.38898339106285906]]\n",
      "2021-01-14 09:40:08,238 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:40:08,239 : INFO : built Dictionary(165 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 495 corpus positions)\n",
      "2021-01-14 09:40:08,387 : INFO : token count processed\n",
      "2021-01-14 09:40:08,418 : INFO : frequencies processed\n",
      "2021-01-14 09:40:17,770 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:40:17,771 : INFO : entropies processed\n",
      "2021-01-14 09:40:17,772 : INFO : extropies processed\n",
      "2021-01-14 09:40:17,778 : INFO : token count processed\n",
      "2021-01-14 09:40:17,782 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:40:17,790 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:40:17,791 : INFO : vocab #32006\n",
      "2021-01-14 09:40:17,799 : INFO : diff #set()\n",
      "2021-01-14 09:40:36,802 : INFO : alphabet #32006\n",
      "2021-01-14 09:40:46,398 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.1839098526901128, 0.45789435803323675], [0.8018087446689606, 0.19819126], [3.095795255000934, 1.3487605247277434], [5.494974824689258, 5.977547459003844, 6.418124160433022, 5.05439812326008, 0.9231493357437639, 0.44057670142917793]]\n",
      "2021-01-14 09:40:46,403 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:40:46,405 : INFO : built Dictionary(270 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 2206 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 09:40:46,708 : INFO : token count processed\n",
      "2021-01-14 09:40:46,761 : INFO : frequencies processed\n",
      "2021-01-14 09:40:55,965 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:40:55,966 : INFO : entropies processed\n",
      "2021-01-14 09:40:55,967 : INFO : extropies processed\n",
      "2021-01-14 09:40:55,973 : INFO : token count processed\n",
      "2021-01-14 09:40:55,977 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:40:55,981 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:40:55,982 : INFO : vocab #32006\n",
      "2021-01-14 09:40:55,993 : INFO : diff #set()\n",
      "2021-01-14 09:41:14,497 : INFO : alphabet #32006\n",
      "2021-01-14 09:41:23,665 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.1873217138318115, 0.4571801183503875], [0.8051692545413971, 0.19483075], [3.0269868333592873, 1.3385887750658594], [5.494974824689258, 6.4614394051846435, 6.6650969606704695, 5.291317269203432, 1.1701221359812113, 0.20365755548582598]]\n",
      "2021-01-14 09:41:23,670 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 09:41:23,671 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:41:23,672 : INFO : built Dictionary(234 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1430 corpus positions)\n",
      "2021-01-14 09:41:23,925 : INFO : token count processed\n",
      "2021-01-14 09:41:23,953 : INFO : frequencies processed\n",
      "2021-01-14 09:41:33,339 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:41:33,340 : INFO : entropies processed\n",
      "2021-01-14 09:41:33,341 : INFO : extropies processed\n",
      "2021-01-14 09:41:33,348 : INFO : token count processed\n",
      "2021-01-14 09:41:33,354 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:41:33,359 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:41:33,361 : INFO : vocab #32006\n",
      "2021-01-14 09:41:33,368 : INFO : diff #set()\n",
      "2021-01-14 09:41:52,249 : INFO : alphabet #32006\n",
      "2021-01-14 09:42:01,766 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.1747258873770516, 0.45982806651835345], [0.7810569554567337, 0.21894304], [2.725480556997868, 1.3192201298976014], [5.494974824689258, 6.327195724598159, 6.586129137254355, 5.236041412033062, 1.0911543125650969, 0.2589334126561962]]\n",
      "2021-01-14 09:42:01,778 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 09:42:01,779 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:42:01,785 : INFO : built Dictionary(434 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 6322 corpus positions)\n",
      "2021-01-14 09:42:02,384 : INFO : token count processed\n",
      "2021-01-14 09:42:02,440 : INFO : frequencies processed\n",
      "2021-01-14 09:42:11,602 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:42:11,603 : INFO : entropies processed\n",
      "2021-01-14 09:42:11,604 : INFO : extropies processed\n",
      "2021-01-14 09:42:11,619 : INFO : token count processed\n",
      "2021-01-14 09:42:11,623 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:42:11,627 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:42:11,628 : INFO : vocab #32006\n",
      "2021-01-14 09:42:11,634 : INFO : diff #set()\n",
      "2021-01-14 09:42:30,002 : INFO : alphabet #32006\n",
      "2021-01-14 09:42:39,268 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.1491239199949494, 0.4653058814786027], [0.7634195685386658, 0.23658043], [4.329239931817578, 1.4004736949908603], [5.494974824689258, 6.9079058562486315, 7.044574274124765, 5.358306406813126, 1.5495994494355063, 0.13666841787613304]]\n",
      "2021-01-14 09:42:39,274 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:42:39,276 : INFO : built Dictionary(345 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 2716 corpus positions)\n",
      "2021-01-14 09:42:39,708 : INFO : token count processed\n",
      "2021-01-14 09:42:39,734 : INFO : frequencies processed\n",
      "2021-01-14 09:42:49,050 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:42:49,051 : INFO : entropies processed\n",
      "2021-01-14 09:42:49,052 : INFO : extropies processed\n",
      "2021-01-14 09:42:49,059 : INFO : token count processed\n",
      "2021-01-14 09:42:49,063 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:42:49,067 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:42:49,068 : INFO : vocab #32006\n",
      "2021-01-14 09:42:49,074 : INFO : diff #set()\n",
      "2021-01-14 09:43:07,976 : INFO : alphabet #32006\n",
      "2021-01-14 09:43:17,248 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.1645515971686085, 0.4619894491349031], [0.7594686597585678, 0.24053134], [3.605388542207534, 1.3722716594163982], [5.494974824689258, 6.61034830706307, 6.840089957123922, 5.265233174628406, 1.345115132434664, 0.229741650060852]]\n",
      "2021-01-14 09:43:17,251 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:43:17,253 : INFO : built Dictionary(236 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 763 corpus positions)\n",
      "2021-01-14 09:43:17,490 : INFO : token count processed\n",
      "2021-01-14 09:43:17,517 : INFO : frequencies processed\n",
      "2021-01-14 09:43:26,668 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:43:26,669 : INFO : entropies processed\n",
      "2021-01-14 09:43:26,670 : INFO : extropies processed\n",
      "2021-01-14 09:43:26,677 : INFO : token count processed\n",
      "2021-01-14 09:43:26,681 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:43:26,685 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:43:26,686 : INFO : vocab #32006\n",
      "2021-01-14 09:43:26,698 : INFO : diff #set()\n",
      "2021-01-14 09:43:45,196 : INFO : alphabet #32006\n",
      "2021-01-14 09:43:54,365 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.1738484099913684, 0.4600136768524585], [0.837761178612709, 0.16223882], [3.0269868333592873, 1.3385887750658594], [5.494974824689258, 6.616715366949855, 6.944679959874826, 5.167010231764287, 1.4497051351855674, 0.32796459292497104]]\n",
      "2021-01-14 09:43:54,372 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:43:54,374 : INFO : built Dictionary(445 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 2797 corpus positions)\n",
      "2021-01-14 09:43:54,996 : INFO : token count processed\n",
      "2021-01-14 09:43:55,037 : INFO : frequencies processed\n",
      "2021-01-14 09:44:04,555 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:44:04,556 : INFO : entropies processed\n",
      "2021-01-14 09:44:04,557 : INFO : extropies processed\n",
      "2021-01-14 09:44:04,572 : INFO : token count processed\n",
      "2021-01-14 09:44:04,576 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:44:04,581 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:44:04,582 : INFO : vocab #32006\n",
      "2021-01-14 09:44:04,588 : INFO : diff #set()\n",
      "2021-01-14 09:44:23,517 : INFO : alphabet #32006\n",
      "2021-01-14 09:44:32,988 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.093705412503616, 0.47762211151005124], [0.6887216866016388, 0.3112783], [4.12728313431329, 1.3943384207973184], [5.494974824689258, 7.32185870753746, 7.5596991115698735, 5.257134420656845, 2.0647242868806153, 0.23784040403241313]]\n",
      "2021-01-14 09:44:32,991 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:44:32,992 : INFO : built Dictionary(95 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 162 corpus positions)\n",
      "2021-01-14 09:44:33,048 : INFO : token count processed\n",
      "2021-01-14 09:44:33,079 : INFO : frequencies processed\n",
      "2021-01-14 09:44:42,517 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:44:42,518 : INFO : entropies processed\n",
      "2021-01-14 09:44:42,519 : INFO : extropies processed\n",
      "2021-01-14 09:44:42,526 : INFO : token count processed\n",
      "2021-01-14 09:44:42,531 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:44:42,535 : INFO : alphabet_target #32008\n",
      "2021-01-14 09:44:42,535 : INFO : vocab #32006\n",
      "2021-01-14 09:44:42,541 : INFO : diff #set()\n",
      "2021-01-14 09:45:01,171 : INFO : alphabet #32006\n",
      "2021-01-14 09:45:10,366 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/fireException.py')[[1.1866999943056726, 0.4573101031710219], [0.8279727250337601, 0.17202727], [1.5219280948873621, 1.1419011889093373], [5.494974824689258, 5.176618657501385, 6.251695856960624, 4.41989762523002, 0.7567210322713658, 1.0750771994592387]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 09:45:10,369 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:45:10,370 : INFO : built Dictionary(188 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 555 corpus positions)\n",
      "2021-01-14 09:45:10,548 : INFO : token count processed\n",
      "2021-01-14 09:45:10,576 : INFO : frequencies processed\n",
      "2021-01-14 09:45:19,728 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:45:19,729 : INFO : entropies processed\n",
      "2021-01-14 09:45:19,730 : INFO : extropies processed\n",
      "2021-01-14 09:45:19,742 : INFO : token count processed\n",
      "2021-01-14 09:45:19,746 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:45:19,750 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:45:19,751 : INFO : vocab #32006\n",
      "2021-01-14 09:45:19,757 : INFO : diff #set()\n",
      "2021-01-14 09:45:38,488 : INFO : alphabet #32006\n",
      "2021-01-14 09:45:47,862 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.1765972343091857, 0.45943272564957693], [0.829080119729042, 0.17091988], [2.5, 1.2968140217166513], [5.494974824689258, 6.468846789852156, 6.903813590257521, 5.060008024283894, 1.4088387655682624, 0.4349668004053644]]\n",
      "2021-01-14 09:45:47,868 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:45:47,870 : INFO : built Dictionary(395 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 2604 corpus positions)\n",
      "2021-01-14 09:45:48,483 : INFO : token count processed\n",
      "2021-01-14 09:45:48,515 : INFO : frequencies processed\n",
      "2021-01-14 09:45:58,021 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:45:58,022 : INFO : entropies processed\n",
      "2021-01-14 09:45:58,023 : INFO : extropies processed\n",
      "2021-01-14 09:45:58,030 : INFO : token count processed\n",
      "2021-01-14 09:45:58,038 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:45:58,043 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:45:58,045 : INFO : vocab #32006\n",
      "2021-01-14 09:45:58,051 : INFO : diff #set()\n",
      "2021-01-14 09:46:16,613 : INFO : alphabet #32006\n",
      "2021-01-14 09:46:25,878 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.18099707589056, 0.45850588753846583], [0.795480415225029, 0.20451958], [3.795088586397732, 1.378800483023463], [5.494974824689258, 6.957796704012729, 7.166623053868147, 5.2861484748338405, 1.6716482291788886, 0.2088263498554177]]\n",
      "2021-01-14 09:46:25,885 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:46:25,887 : INFO : built Dictionary(310 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 3101 corpus positions)\n",
      "2021-01-14 09:46:26,251 : INFO : token count processed\n",
      "2021-01-14 09:46:26,279 : INFO : frequencies processed\n",
      "2021-01-14 09:46:35,415 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:46:35,416 : INFO : entropies processed\n",
      "2021-01-14 09:46:35,416 : INFO : extropies processed\n",
      "2021-01-14 09:46:35,424 : INFO : token count processed\n",
      "2021-01-14 09:46:35,429 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:46:35,435 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:46:35,436 : INFO : vocab #32006\n",
      "2021-01-14 09:46:35,442 : INFO : diff #set()\n",
      "2021-01-14 09:46:54,424 : INFO : alphabet #32006\n",
      "2021-01-14 09:47:03,936 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.174030715839736, 0.4599751018760295], [0.814175471663475, 0.18582453], [3.238901256602631, 1.3579502728384498], [5.494974824689258, 6.441859572014148, 6.6662481635433615, 5.270586233160046, 1.1712733388541032, 0.22438859152921342]]\n",
      "2021-01-14 09:47:03,941 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:47:03,943 : INFO : built Dictionary(334 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1577 corpus positions)\n",
      "2021-01-14 09:47:04,362 : INFO : token count processed\n",
      "2021-01-14 09:47:04,391 : INFO : frequencies processed\n",
      "2021-01-14 09:47:13,803 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:47:13,804 : INFO : entropies processed\n",
      "2021-01-14 09:47:13,805 : INFO : extropies processed\n",
      "2021-01-14 09:47:13,812 : INFO : token count processed\n",
      "2021-01-14 09:47:13,816 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:47:13,823 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:47:13,824 : INFO : vocab #32006\n",
      "2021-01-14 09:47:13,833 : INFO : diff #set()\n",
      "2021-01-14 09:47:33,132 : INFO : alphabet #32006\n",
      "2021-01-14 09:47:42,429 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.1701501618912904, 0.4607976063409815], [0.7889624238014221, 0.21103758], [3.4613201402110083, 1.368539624592205], [5.494974824689258, 6.998955278238291, 7.262270300455537, 5.231659802472013, 1.7672954757662787, 0.26331502221724623]]\n",
      "2021-01-14 09:47:42,434 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:47:42,435 : INFO : built Dictionary(251 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1396 corpus positions)\n",
      "2021-01-14 09:47:42,748 : INFO : token count processed\n",
      "2021-01-14 09:47:42,808 : INFO : frequencies processed\n",
      "2021-01-14 09:47:52,284 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:47:52,285 : INFO : entropies processed\n",
      "2021-01-14 09:47:52,286 : INFO : extropies processed\n",
      "2021-01-14 09:47:52,292 : INFO : token count processed\n",
      "2021-01-14 09:47:52,299 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:47:52,304 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:47:52,305 : INFO : vocab #32006\n",
      "2021-01-14 09:47:52,312 : INFO : diff #set()\n",
      "2021-01-14 09:48:11,069 : INFO : alphabet #32006\n",
      "2021-01-14 09:48:20,248 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.1556014935605863, 0.46390763923076384], [0.7810811549425125, 0.21891885], [3.238901256602631, 1.3579502728384498], [5.494974824689258, 6.492983191376071, 6.738416249405937, 5.249541766659393, 1.2434414247166785, 0.2454330580298656]]\n",
      "2021-01-14 09:48:20,255 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:48:20,257 : INFO : built Dictionary(466 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 3335 corpus positions)\n",
      "2021-01-14 09:48:20,929 : INFO : token count processed\n",
      "2021-01-14 09:48:20,972 : INFO : frequencies processed\n",
      "2021-01-14 09:48:30,355 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:48:30,356 : INFO : entropies processed\n",
      "2021-01-14 09:48:30,357 : INFO : extropies processed\n",
      "2021-01-14 09:48:30,369 : INFO : token count processed\n",
      "2021-01-14 09:48:30,373 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:48:30,378 : INFO : alphabet_target #32008\n",
      "2021-01-14 09:48:30,379 : INFO : vocab #32006\n",
      "2021-01-14 09:48:30,385 : INFO : diff #set()\n",
      "2021-01-14 09:48:49,342 : INFO : alphabet #32006\n",
      "2021-01-14 09:48:58,806 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.2129342587855114, 0.4518887066029759], [0.8461247384548187, 0.15387526], [1.9219280948873623, 1.2148067842293933], [5.494974824689258, 6.560342487747443, 6.8310822945485175, 5.224235017888184, 1.3361074698592592, 0.2707398068010747]]\n",
      "2021-01-14 09:48:58,813 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 09:48:58,814 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:48:58,816 : INFO : built Dictionary(471 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 3540 corpus positions)\n",
      "2021-01-14 09:48:59,539 : INFO : token count processed\n",
      "2021-01-14 09:48:59,605 : INFO : frequencies processed\n",
      "2021-01-14 09:49:09,523 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:49:09,524 : INFO : entropies processed\n",
      "2021-01-14 09:49:09,525 : INFO : extropies processed\n",
      "2021-01-14 09:49:09,533 : INFO : token count processed\n",
      "2021-01-14 09:49:09,541 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:49:09,547 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:49:09,549 : INFO : vocab #32006\n",
      "2021-01-14 09:49:09,555 : INFO : diff #set()\n",
      "2021-01-14 09:49:28,539 : INFO : alphabet #32006\n",
      "2021-01-14 09:49:37,859 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.163314389467372, 0.4622536626524309], [0.7572277337312698, 0.24277227], [3.8561962982193796, 1.3866167862627314], [5.494974824689258, 7.046173750105238, 7.284784928709211, 5.256363646085285, 1.7898101040199528, 0.23861117860397307]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 09:49:37,869 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 09:49:37,871 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:49:37,873 : INFO : built Dictionary(518 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 5649 corpus positions)\n",
      "2021-01-14 09:49:38,775 : INFO : token count processed\n",
      "2021-01-14 09:49:38,831 : INFO : frequencies processed\n",
      "2021-01-14 09:49:48,013 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:49:48,013 : INFO : entropies processed\n",
      "2021-01-14 09:49:48,014 : INFO : extropies processed\n",
      "2021-01-14 09:49:48,022 : INFO : token count processed\n",
      "2021-01-14 09:49:48,025 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:49:48,029 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:49:48,029 : INFO : vocab #32006\n",
      "2021-01-14 09:49:48,035 : INFO : diff #set()\n",
      "2021-01-14 09:50:06,492 : INFO : alphabet #32006\n",
      "2021-01-14 09:50:15,824 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.1457762777183653, 0.4660318088068875], [0.7446616590023041, 0.25533834], [4.329239931817578, 1.4004736949908605], [5.494974824689258, 7.009229588004272, 7.147354021247948, 5.356850391445582, 1.65237919655869, 0.13812443324367596]]\n",
      "2021-01-14 09:50:15,837 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:50:15,840 : INFO : built Dictionary(599 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 6590 corpus positions)\n",
      "2021-01-14 09:50:16,861 : INFO : token count processed\n",
      "2021-01-14 09:50:16,928 : INFO : frequencies processed\n",
      "2021-01-14 09:50:26,424 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:50:26,425 : INFO : entropies processed\n",
      "2021-01-14 09:50:26,426 : INFO : extropies processed\n",
      "2021-01-14 09:50:26,435 : INFO : token count processed\n",
      "2021-01-14 09:50:26,442 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:50:26,449 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:50:26,450 : INFO : vocab #32006\n",
      "2021-01-14 09:50:26,457 : INFO : diff #set()\n",
      "2021-01-14 09:50:45,246 : INFO : alphabet #32006\n",
      "2021-01-14 09:50:54,782 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.1522582513498212, 0.46462825702855826], [0.7626336961984634, 0.2373663], [4.202819531114783, 1.3963466963769324], [5.494974824689258, 7.376088004590871, 7.566584937556964, 5.304477891723165, 2.071610112867706, 0.19049693296609327]]\n",
      "2021-01-14 09:50:54,786 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:50:54,787 : INFO : built Dictionary(167 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 376 corpus positions)\n",
      "2021-01-14 09:50:54,941 : INFO : token count processed\n",
      "2021-01-14 09:50:54,977 : INFO : frequencies processed\n",
      "2021-01-14 09:51:04,412 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:51:04,413 : INFO : entropies processed\n",
      "2021-01-14 09:51:04,414 : INFO : extropies processed\n",
      "2021-01-14 09:51:04,426 : INFO : token count processed\n",
      "2021-01-14 09:51:04,431 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:51:04,435 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:51:04,436 : INFO : vocab #32006\n",
      "2021-01-14 09:51:04,442 : INFO : diff #set()\n",
      "2021-01-14 09:51:23,352 : INFO : alphabet #32006\n",
      "2021-01-14 09:51:32,924 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.1886745918641235, 0.45689752314814713], [0.8243521004915237, 0.1756479], [1.5219280948873621, 1.1419011889093373], [5.494974824689258, 6.2993628166120885, 6.817171273812934, 4.977166367488413, 1.322196449123676, 0.5178084572008457]]\n",
      "2021-01-14 09:51:32,927 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:51:32,928 : INFO : built Dictionary(66 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 92 corpus positions)\n",
      "2021-01-14 09:51:32,952 : INFO : token count processed\n",
      "2021-01-14 09:51:32,983 : INFO : frequencies processed\n",
      "2021-01-14 09:51:42,433 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:51:42,434 : INFO : entropies processed\n",
      "2021-01-14 09:51:42,435 : INFO : extropies processed\n",
      "2021-01-14 09:51:42,441 : INFO : token count processed\n",
      "2021-01-14 09:51:42,447 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:51:42,453 : INFO : alphabet_target #32008\n",
      "2021-01-14 09:51:42,454 : INFO : vocab #32006\n",
      "2021-01-14 09:51:42,461 : INFO : diff #set()\n",
      "2021-01-14 09:52:01,314 : INFO : alphabet #32006\n",
      "2021-01-14 09:52:10,632 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2515355515356772, 0.44414133248659665], [0.8683616369962692, 0.13163836], [0.0, 0.0], [5.494974824689258, 3.8936606896881862, 5.797082818407667, 3.591552695969778, 0.30210799371840835, 1.9034221287194804]]\n",
      "2021-01-14 09:52:10,654 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:52:10,665 : INFO : built Dictionary(753 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 12534 corpus positions)\n",
      "2021-01-14 09:52:12,303 : INFO : token count processed\n",
      "2021-01-14 09:52:12,331 : INFO : frequencies processed\n",
      "2021-01-14 09:52:21,484 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:52:21,485 : INFO : entropies processed\n",
      "2021-01-14 09:52:21,486 : INFO : extropies processed\n",
      "2021-01-14 09:52:21,496 : INFO : token count processed\n",
      "2021-01-14 09:52:21,503 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:52:21,508 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:52:21,509 : INFO : vocab #32006\n",
      "2021-01-14 09:52:21,517 : INFO : diff #set()\n",
      "2021-01-14 09:52:40,040 : INFO : alphabet #32006\n",
      "2021-01-14 09:52:49,377 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.1376462370448501, 0.4678042524858705], [0.7414486706256866, 0.25855133], [4.523986065961298, 1.4059249530065845], [5.494974824689258, 7.434393313070278, 7.6527365872702, 5.276631550489336, 2.1577617625809413, 0.2183432741999214]]\n",
      "2021-01-14 09:52:49,386 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:52:49,388 : INFO : built Dictionary(500 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 4161 corpus positions)\n",
      "2021-01-14 09:52:50,172 : INFO : token count processed\n",
      "2021-01-14 09:52:50,229 : INFO : frequencies processed\n",
      "2021-01-14 09:52:59,617 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:52:59,618 : INFO : entropies processed\n",
      "2021-01-14 09:52:59,619 : INFO : extropies processed\n",
      "2021-01-14 09:52:59,627 : INFO : token count processed\n",
      "2021-01-14 09:52:59,634 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:52:59,639 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:52:59,640 : INFO : vocab #32006\n",
      "2021-01-14 09:52:59,647 : INFO : diff #set()\n",
      "2021-01-14 09:53:18,660 : INFO : alphabet #32006\n",
      "2021-01-14 09:53:28,154 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.1291388477210982, 0.469673455571176], [0.7295441627502441, 0.27045584], [4.737322779818596, 1.4110113147484409], [5.494974824689258, 7.2991514951718255, 7.515358080073918, 5.278768239787166, 2.02038325538466, 0.21620658490209266]]\n",
      "2021-01-14 09:53:28,162 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:53:28,165 : INFO : built Dictionary(472 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 3578 corpus positions)\n",
      "2021-01-14 09:53:28,887 : INFO : token count processed\n",
      "2021-01-14 09:53:28,918 : INFO : frequencies processed\n",
      "2021-01-14 09:53:38,443 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:53:38,444 : INFO : entropies processed\n",
      "2021-01-14 09:53:38,445 : INFO : extropies processed\n",
      "2021-01-14 09:53:38,461 : INFO : token count processed\n",
      "2021-01-14 09:53:38,466 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:53:38,470 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:53:38,471 : INFO : vocab #32006\n",
      "2021-01-14 09:53:38,477 : INFO : diff #set()\n",
      "2021-01-14 09:53:57,020 : INFO : alphabet #32006\n",
      "2021-01-14 09:54:06,193 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.1439772475253391, 0.466422860202569], [0.756398469209671, 0.24360153], [4.400432302535625, 1.4022627720197955], [5.494974824689258, 7.170319527000998, 7.392996403890551, 5.272297947799705, 1.898021579201293, 0.22267687688955373]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 09:54:06,197 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:54:06,199 : INFO : built Dictionary(199 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 614 corpus positions)\n",
      "2021-01-14 09:54:06,405 : INFO : token count processed\n",
      "2021-01-14 09:54:06,431 : INFO : frequencies processed\n",
      "2021-01-14 09:54:15,592 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:54:15,593 : INFO : entropies processed\n",
      "2021-01-14 09:54:15,594 : INFO : extropies processed\n",
      "2021-01-14 09:54:15,601 : INFO : token count processed\n",
      "2021-01-14 09:54:15,607 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:54:15,611 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:54:15,612 : INFO : vocab #32006\n",
      "2021-01-14 09:54:15,618 : INFO : diff #set()\n",
      "2021-01-14 09:54:34,349 : INFO : alphabet #32006\n",
      "2021-01-14 09:54:43,788 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.1698543595914574, 0.4608604239172444], [0.7872448563575745, 0.21275514], [2.25, 1.2709632597765914], [5.494974824689258, 6.353654804387375, 6.780287523836175, 5.0683421052404585, 1.2853126991469166, 0.42663271944879977]]\n",
      "2021-01-14 09:54:43,792 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:54:43,794 : INFO : built Dictionary(189 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 742 corpus positions)\n",
      "2021-01-14 09:54:44,001 : INFO : token count processed\n",
      "2021-01-14 09:54:44,072 : INFO : frequencies processed\n",
      "2021-01-14 09:54:53,648 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:54:53,649 : INFO : entropies processed\n",
      "2021-01-14 09:54:53,650 : INFO : extropies processed\n",
      "2021-01-14 09:54:53,657 : INFO : token count processed\n",
      "2021-01-14 09:54:53,661 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:54:53,665 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:54:53,666 : INFO : vocab #32006\n",
      "2021-01-14 09:54:53,672 : INFO : diff #set()\n",
      "2021-01-14 09:55:12,089 : INFO : alphabet #32006\n",
      "2021-01-14 09:55:21,378 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.1321553665656754, 0.46900897358653976], [0.7576245665550232, 0.24237543], [3.240223928941852, 1.3591202789297814], [5.494974824689258, 6.245180322479091, 6.591199469683984, 5.148955677484365, 1.0962246449947255, 0.3460191472048928]]\n",
      "2021-01-14 09:55:21,384 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:55:21,386 : INFO : built Dictionary(402 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1814 corpus positions)\n",
      "2021-01-14 09:55:21,975 : INFO : token count processed\n",
      "2021-01-14 09:55:22,003 : INFO : frequencies processed\n",
      "2021-01-14 09:55:31,352 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:55:31,353 : INFO : entropies processed\n",
      "2021-01-14 09:55:31,354 : INFO : extropies processed\n",
      "2021-01-14 09:55:31,366 : INFO : token count processed\n",
      "2021-01-14 09:55:31,370 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:55:31,374 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:55:31,375 : INFO : vocab #32006\n",
      "2021-01-14 09:55:31,380 : INFO : diff #set()\n",
      "2021-01-14 09:55:50,009 : INFO : alphabet #32006\n",
      "2021-01-14 09:55:59,511 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.1261101488098415, 0.4703425175594887], [0.7251729965209961, 0.274827], [4.536286231168867, 1.4059370971230252], [5.494974824689258, 7.2691387000368, 7.525929866585475, 5.238183658140583, 2.030955041896217, 0.25679116654867506]]\n",
      "2021-01-14 09:55:59,517 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:55:59,519 : INFO : built Dictionary(334 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1651 corpus positions)\n",
      "2021-01-14 09:55:59,923 : INFO : token count processed\n",
      "2021-01-14 09:55:59,961 : INFO : frequencies processed\n",
      "2021-01-14 09:56:09,402 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:56:09,403 : INFO : entropies processed\n",
      "2021-01-14 09:56:09,404 : INFO : extropies processed\n",
      "2021-01-14 09:56:09,412 : INFO : token count processed\n",
      "2021-01-14 09:56:09,420 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:56:09,425 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:56:09,426 : INFO : vocab #32006\n",
      "2021-01-14 09:56:09,434 : INFO : diff #set()\n",
      "2021-01-14 09:56:28,360 : INFO : alphabet #32006\n",
      "2021-01-14 09:56:37,654 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.1052490525882879, 0.47500318253109053], [0.7029582262039185, 0.29704177], [4.140319531114783, 1.3948921143685173], [5.494974824689258, 7.08857858466988, 7.317329052328455, 5.266224357030682, 1.822354227639197, 0.22875046765857565]]\n",
      "2021-01-14 09:56:37,657 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:56:37,659 : INFO : built Dictionary(174 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 574 corpus positions)\n",
      "2021-01-14 09:56:37,818 : INFO : token count processed\n",
      "2021-01-14 09:56:37,844 : INFO : frequencies processed\n",
      "2021-01-14 09:56:47,012 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:56:47,013 : INFO : entropies processed\n",
      "2021-01-14 09:56:47,014 : INFO : extropies processed\n",
      "2021-01-14 09:56:47,020 : INFO : token count processed\n",
      "2021-01-14 09:56:47,024 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:56:47,027 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:56:47,028 : INFO : vocab #32006\n",
      "2021-01-14 09:56:47,039 : INFO : diff #set()\n",
      "2021-01-14 09:57:05,514 : INFO : alphabet #32006\n",
      "2021-01-14 09:57:14,867 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.191401490101825, 0.45632897692039737], [0.8027239441871643, 0.19727606], [1.9182958340544896, 1.2183406773511978], [5.494974824689258, 6.0479231618016716, 6.517373842208313, 5.025524144282617, 1.0223990175190547, 0.4694506804066414]]\n",
      "2021-01-14 09:57:14,871 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:57:14,872 : INFO : built Dictionary(171 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 693 corpus positions)\n",
      "2021-01-14 09:57:15,038 : INFO : token count processed\n",
      "2021-01-14 09:57:15,069 : INFO : frequencies processed\n",
      "2021-01-14 09:57:24,453 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:57:24,454 : INFO : entropies processed\n",
      "2021-01-14 09:57:24,455 : INFO : extropies processed\n",
      "2021-01-14 09:57:24,462 : INFO : token count processed\n",
      "2021-01-14 09:57:24,469 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:57:24,475 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:57:24,476 : INFO : vocab #32006\n",
      "2021-01-14 09:57:24,483 : INFO : diff #set()\n",
      "2021-01-14 09:57:43,475 : INFO : alphabet #32006\n",
      "2021-01-14 09:57:52,954 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.1550585891176863, 0.46402450729166256], [0.7743405550718307, 0.22565944], [3.085055102756477, 1.3486914941048562], [5.494974824689258, 6.036583168403119, 6.4195219168329345, 5.112036076259443, 0.9245470921436763, 0.38293874842981523]]\n",
      "2021-01-14 09:57:52,968 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 09:57:52,969 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:57:52,972 : INFO : built Dictionary(576 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 7051 corpus positions)\n",
      "2021-01-14 09:57:54,054 : INFO : token count processed\n",
      "2021-01-14 09:57:54,086 : INFO : frequencies processed\n",
      "2021-01-14 09:58:03,453 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:58:03,454 : INFO : entropies processed\n",
      "2021-01-14 09:58:03,455 : INFO : extropies processed\n",
      "2021-01-14 09:58:03,464 : INFO : token count processed\n",
      "2021-01-14 09:58:03,468 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:58:03,472 : INFO : alphabet_target #32010\n",
      "2021-01-14 09:58:03,473 : INFO : vocab #32006\n",
      "2021-01-14 09:58:03,479 : INFO : diff #set()\n",
      "2021-01-14 09:58:21,982 : INFO : alphabet #32006\n",
      "2021-01-14 09:58:31,146 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.1536297442717045, 0.4643323684861955], [0.8017149716615677, 0.19828503], [4.132944044980959, 1.3967346668229057], [5.494974824689258, 7.29352035514053, 7.480069748702605, 5.308425431127183, 1.9850949240133469, 0.18654939356207478]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 09:58:31,154 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:58:31,156 : INFO : built Dictionary(386 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 3294 corpus positions)\n",
      "2021-01-14 09:58:31,646 : INFO : token count processed\n",
      "2021-01-14 09:58:31,674 : INFO : frequencies processed\n",
      "2021-01-14 09:58:40,826 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:58:40,827 : INFO : entropies processed\n",
      "2021-01-14 09:58:40,828 : INFO : extropies processed\n",
      "2021-01-14 09:58:40,835 : INFO : token count processed\n",
      "2021-01-14 09:58:40,843 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:58:40,850 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:58:40,851 : INFO : vocab #32006\n",
      "2021-01-14 09:58:40,856 : INFO : diff #set()\n",
      "2021-01-14 09:58:59,675 : INFO : alphabet #32006\n",
      "2021-01-14 09:59:09,087 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.1641432578157134, 0.4620766191833843], [0.761014997959137, 0.238985], [4.001629167387822, 1.3881959048360293], [5.494974824689258, 6.8153433747477745, 7.014445333152681, 5.295872866284352, 1.5194705084634226, 0.1991019584049063]]\n",
      "2021-01-14 09:59:09,090 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:59:09,091 : INFO : built Dictionary(152 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 315 corpus positions)\n",
      "2021-01-14 09:59:09,215 : INFO : token count processed\n",
      "2021-01-14 09:59:09,248 : INFO : frequencies processed\n",
      "2021-01-14 09:59:18,802 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:59:18,803 : INFO : entropies processed\n",
      "2021-01-14 09:59:18,804 : INFO : extropies processed\n",
      "2021-01-14 09:59:18,810 : INFO : token count processed\n",
      "2021-01-14 09:59:18,814 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:59:18,819 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:59:18,820 : INFO : vocab #32006\n",
      "2021-01-14 09:59:18,830 : INFO : diff #set()\n",
      "2021-01-14 09:59:37,506 : INFO : alphabet #32006\n",
      "2021-01-14 09:59:46,773 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.186807411483711, 0.45728763984822846], [0.8152352422475815, 0.18476476], [1.5219280948873621, 1.1419011889093373], [5.494974824689258, 6.150121915859574, 6.707720582600022, 4.937376157948812, 1.2127457579107634, 0.5575986667404473]]\n",
      "2021-01-14 09:59:46,777 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 09:59:46,780 : INFO : built Dictionary(306 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1026 corpus positions)\n",
      "2021-01-14 09:59:47,139 : INFO : token count processed\n",
      "2021-01-14 09:59:47,167 : INFO : frequencies processed\n",
      "2021-01-14 09:59:56,314 : INFO : scalar_distribution processed\n",
      "2021-01-14 09:59:56,315 : INFO : entropies processed\n",
      "2021-01-14 09:59:56,316 : INFO : extropies processed\n",
      "2021-01-14 09:59:56,323 : INFO : token count processed\n",
      "2021-01-14 09:59:56,327 : INFO : alphabet_source #32006\n",
      "2021-01-14 09:59:56,331 : INFO : alphabet_target #32009\n",
      "2021-01-14 09:59:56,332 : INFO : vocab #32006\n",
      "2021-01-14 09:59:56,341 : INFO : diff #set()\n",
      "2021-01-14 10:00:14,672 : INFO : alphabet #32006\n",
      "2021-01-14 10:00:24,134 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.1235352391293134, 0.4709128351503211], [0.7286111414432526, 0.27138886], [3.6211755429194716, 1.3730961658387513], [5.494974824689258, 7.0391145208191315, 7.382112474302904, 5.151976871205486, 1.8871376496136456, 0.3429979534837724]]\n",
      "2021-01-14 10:00:24,143 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 10:00:24,144 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:00:24,147 : INFO : built Dictionary(604 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 4401 corpus positions)\n",
      "2021-01-14 10:00:25,188 : INFO : token count processed\n",
      "2021-01-14 10:00:25,219 : INFO : frequencies processed\n",
      "2021-01-14 10:00:34,590 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:00:34,591 : INFO : entropies processed\n",
      "2021-01-14 10:00:34,592 : INFO : extropies processed\n",
      "2021-01-14 10:00:34,600 : INFO : token count processed\n",
      "2021-01-14 10:00:34,607 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:00:34,612 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:00:34,613 : INFO : vocab #32006\n",
      "2021-01-14 10:00:34,621 : INFO : diff #set()\n",
      "2021-01-14 10:00:53,497 : INFO : alphabet #32006\n",
      "2021-01-14 10:01:03,063 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.1304777196788294, 0.46937829518853197], [0.7053292691707611, 0.29467073], [4.21506101220307, 1.3965159328027845], [5.494974824689258, 7.482466367279176, 7.7824781823798785, 5.194963009588556, 2.2875033576906203, 0.30001181510070296]]\n",
      "2021-01-14 10:01:03,067 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:01:03,068 : INFO : built Dictionary(200 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 623 corpus positions)\n",
      "2021-01-14 10:01:03,272 : INFO : token count processed\n",
      "2021-01-14 10:01:03,335 : INFO : frequencies processed\n",
      "2021-01-14 10:01:12,816 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:01:12,817 : INFO : entropies processed\n",
      "2021-01-14 10:01:12,818 : INFO : extropies processed\n",
      "2021-01-14 10:01:12,832 : INFO : token count processed\n",
      "2021-01-14 10:01:12,836 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:01:12,840 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:01:12,841 : INFO : vocab #32006\n",
      "2021-01-14 10:01:12,847 : INFO : diff #set()\n",
      "2021-01-14 10:01:31,942 : INFO : alphabet #32006\n",
      "2021-01-14 10:01:41,196 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.187676053626323, 0.4571060684886987], [0.7969416826963425, 0.20305832], [1.9182958340544896, 1.2183406773511978], [5.494974824689258, 6.372162341197667, 6.804199993760523, 5.062937172126402, 1.309225169071265, 0.4320376525628564]]\n",
      "2021-01-14 10:01:41,201 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:01:41,203 : INFO : built Dictionary(336 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 2044 corpus positions)\n",
      "2021-01-14 10:01:41,612 : INFO : token count processed\n",
      "2021-01-14 10:01:41,642 : INFO : frequencies processed\n",
      "2021-01-14 10:01:50,806 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:01:50,808 : INFO : entropies processed\n",
      "2021-01-14 10:01:50,808 : INFO : extropies processed\n",
      "2021-01-14 10:01:50,815 : INFO : token count processed\n",
      "2021-01-14 10:01:50,819 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:01:50,826 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:01:50,827 : INFO : vocab #32006\n",
      "2021-01-14 10:01:50,836 : INFO : diff #set()\n",
      "2021-01-14 10:02:09,325 : INFO : alphabet #32006\n",
      "2021-01-14 10:02:18,485 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.1600027928112775, 0.46296236436735544], [0.7815368175506592, 0.21846318], [3.893660689688186, 1.3840535698355405], [5.494974824689258, 6.798155919669889, 7.061685538813231, 5.231445205545915, 1.566710714123973, 0.26352961914334205]]\n",
      "2021-01-14 10:02:18,489 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:02:18,491 : INFO : built Dictionary(203 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 760 corpus positions)\n",
      "2021-01-14 10:02:18,684 : INFO : token count processed\n",
      "2021-01-14 10:02:18,712 : INFO : frequencies processed\n",
      "2021-01-14 10:02:28,022 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:02:28,023 : INFO : entropies processed\n",
      "2021-01-14 10:02:28,023 : INFO : extropies processed\n",
      "2021-01-14 10:02:28,030 : INFO : token count processed\n",
      "2021-01-14 10:02:28,036 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:02:28,042 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:02:28,043 : INFO : vocab #32006\n",
      "2021-01-14 10:02:28,049 : INFO : diff #set()\n",
      "2021-01-14 10:02:46,918 : INFO : alphabet #32006\n",
      "2021-01-14 10:02:56,361 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.1822490870028226, 0.458242831194598], [0.792911171913147, 0.20708883], [2.5, 1.2968140217166513], [5.494974824689258, 6.271631856729336, 6.672262287541098, 5.0943443938774955, 1.1772874628518402, 0.40063043081176275]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 10:02:56,368 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:02:56,373 : INFO : built Dictionary(365 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 3275 corpus positions)\n",
      "2021-01-14 10:02:56,903 : INFO : token count processed\n",
      "2021-01-14 10:02:56,973 : INFO : frequencies processed\n",
      "2021-01-14 10:03:06,454 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:03:06,455 : INFO : entropies processed\n",
      "2021-01-14 10:03:06,456 : INFO : extropies processed\n",
      "2021-01-14 10:03:06,470 : INFO : token count processed\n",
      "2021-01-14 10:03:06,475 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:03:06,479 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:03:06,480 : INFO : vocab #32006\n",
      "2021-01-14 10:03:06,486 : INFO : diff #set()\n",
      "2021-01-14 10:03:25,032 : INFO : alphabet #32006\n",
      "2021-01-14 10:03:34,201 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.1167081841018305, 0.4724316783535864], [0.7168090045452118, 0.283191], [4.264578373902383, 1.3985884469165661], [5.494974824689258, 6.873598627629562, 7.0685071977397405, 5.30006625457908, 1.5735323730504822, 0.19490857011017848]]\n",
      "2021-01-14 10:03:34,204 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:03:34,206 : INFO : built Dictionary(133 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 223 corpus positions)\n",
      "2021-01-14 10:03:34,314 : INFO : token count processed\n",
      "2021-01-14 10:03:34,341 : INFO : frequencies processed\n",
      "2021-01-14 10:03:43,627 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:03:43,628 : INFO : entropies processed\n",
      "2021-01-14 10:03:43,629 : INFO : extropies processed\n",
      "2021-01-14 10:03:43,635 : INFO : token count processed\n",
      "2021-01-14 10:03:43,641 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:03:43,646 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:03:43,647 : INFO : vocab #32006\n",
      "2021-01-14 10:03:43,653 : INFO : diff #set()\n",
      "2021-01-14 10:04:02,493 : INFO : alphabet #32006\n",
      "2021-01-14 10:04:12,388 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.1722181476732039, 0.46035891978490345], [0.8123177736997604, 0.18768223], [1.584962500721156, 1.1699250014423124], [5.494974824689258, 6.049830202851529, 6.72536527786707, 4.819439749673718, 1.2303904531778116, 0.6755350750155404]]\n",
      "2021-01-14 10:04:12,392 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:04:12,394 : INFO : built Dictionary(272 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1303 corpus positions)\n",
      "2021-01-14 10:04:12,709 : INFO : token count processed\n",
      "2021-01-14 10:04:12,757 : INFO : frequencies processed\n",
      "2021-01-14 10:04:22,159 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:04:22,160 : INFO : entropies processed\n",
      "2021-01-14 10:04:22,161 : INFO : extropies processed\n",
      "2021-01-14 10:04:22,168 : INFO : token count processed\n",
      "2021-01-14 10:04:22,175 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:04:22,180 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:04:22,181 : INFO : vocab #32006\n",
      "2021-01-14 10:04:22,188 : INFO : diff #set()\n",
      "2021-01-14 10:04:40,571 : INFO : alphabet #32006\n",
      "2021-01-14 10:04:49,856 : INFO : Computed distances or similarities ('288', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.1403158356137055, 0.46722076403890367], [0.7734394520521164, 0.22656055], [3.384183719779189, 1.359350160404111], [5.494974824689258, 6.778844940588858, 7.080277818754498, 5.193541946523618, 1.5853029940652394, 0.3014328781656399]]\n",
      "2021-01-14 10:04:49,859 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:04:49,861 : INFO : built Dictionary(177 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 336 corpus positions)\n",
      "2021-01-14 10:04:50,032 : INFO : token count processed\n",
      "2021-01-14 10:04:50,088 : INFO : frequencies processed\n",
      "2021-01-14 10:04:59,269 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:04:59,270 : INFO : entropies processed\n",
      "2021-01-14 10:04:59,271 : INFO : extropies processed\n",
      "2021-01-14 10:04:59,278 : INFO : token count processed\n",
      "2021-01-14 10:04:59,284 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:04:59,290 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:04:59,291 : INFO : vocab #32006\n",
      "2021-01-14 10:04:59,299 : INFO : diff #set()\n",
      "2021-01-14 10:05:17,832 : INFO : alphabet #32006\n",
      "2021-01-14 10:05:27,118 : INFO : Computed distances or similarities ('288', 'sacp-python-common/setup.py')[[1.1646408356547953, 0.46197040337063766], [0.7847203016281128, 0.2152797], [2.503258334775646, 1.2991301890771525], [5.494974824689258, 6.469677430851302, 7.01220638794149, 4.95244586759907, 1.5172315632522322, 0.5425289570901883]]\n",
      "2021-01-14 10:05:27,123 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:05:27,124 : INFO : built Dictionary(241 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1238 corpus positions)\n",
      "2021-01-14 10:05:27,381 : INFO : token count processed\n",
      "2021-01-14 10:05:27,409 : INFO : frequencies processed\n",
      "2021-01-14 10:05:36,561 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:05:36,562 : INFO : entropies processed\n",
      "2021-01-14 10:05:36,563 : INFO : extropies processed\n",
      "2021-01-14 10:05:36,570 : INFO : token count processed\n",
      "2021-01-14 10:05:36,576 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:05:36,580 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:05:36,581 : INFO : vocab #32006\n",
      "2021-01-14 10:05:36,590 : INFO : diff #set()\n",
      "2021-01-14 10:05:55,241 : INFO : alphabet #32006\n",
      "2021-01-14 10:06:04,401 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.1678732916124044, 0.4612815720683691], [0.7908356189727783, 0.20916438], [3.3921472236645345, 1.3581797203736243], [5.494974824689258, 6.459180448028249, 6.763165145142494, 5.190990127575013, 1.268190320453236, 0.30398469711424525]]\n",
      "2021-01-14 10:06:04,404 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:06:04,406 : INFO : built Dictionary(157 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 388 corpus positions)\n",
      "2021-01-14 10:06:04,537 : INFO : token count processed\n",
      "2021-01-14 10:06:04,565 : INFO : frequencies processed\n",
      "2021-01-14 10:06:14,075 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:06:14,076 : INFO : entropies processed\n",
      "2021-01-14 10:06:14,077 : INFO : extropies processed\n",
      "2021-01-14 10:06:14,090 : INFO : token count processed\n",
      "2021-01-14 10:06:14,093 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:06:14,097 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:06:14,098 : INFO : vocab #32006\n",
      "2021-01-14 10:06:14,103 : INFO : diff #set()\n",
      "2021-01-14 10:06:32,672 : INFO : alphabet #32006\n",
      "2021-01-14 10:06:41,859 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.1799907013991693, 0.45871755295019223], [0.7904561758041382, 0.20954382], [2.5, 1.2968140217166515], [5.494974824689258, 6.097125733496388, 6.621388500589502, 4.970712057596144, 1.1264136759002437, 0.5242627670931137]]\n",
      "2021-01-14 10:06:41,863 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:06:41,864 : INFO : built Dictionary(149 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 384 corpus positions)\n",
      "2021-01-14 10:06:41,992 : INFO : token count processed\n",
      "2021-01-14 10:06:42,021 : INFO : frequencies processed\n",
      "2021-01-14 10:06:51,178 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:06:51,179 : INFO : entropies processed\n",
      "2021-01-14 10:06:51,179 : INFO : extropies processed\n",
      "2021-01-14 10:06:51,186 : INFO : token count processed\n",
      "2021-01-14 10:06:51,193 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:06:51,199 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:06:51,200 : INFO : vocab #32006\n",
      "2021-01-14 10:06:51,206 : INFO : diff #set()\n",
      "2021-01-14 10:07:10,097 : INFO : alphabet #32006\n",
      "2021-01-14 10:07:19,288 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.1789488105390649, 0.4589368943241045], [0.7896361351013184, 0.21036386], [2.5, 1.2968140217166515], [5.494974824689258, 6.0695858597523715, 6.594266647332686, 4.970294037108944, 1.099291822643428, 0.5246807875803148]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 10:07:19,292 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 10:07:19,293 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:07:19,294 : INFO : built Dictionary(151 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 480 corpus positions)\n",
      "2021-01-14 10:07:19,432 : INFO : token count processed\n",
      "2021-01-14 10:07:19,463 : INFO : frequencies processed\n",
      "2021-01-14 10:07:28,985 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:07:28,986 : INFO : entropies processed\n",
      "2021-01-14 10:07:28,987 : INFO : extropies processed\n",
      "2021-01-14 10:07:28,999 : INFO : token count processed\n",
      "2021-01-14 10:07:29,003 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:07:29,007 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:07:29,008 : INFO : vocab #32006\n",
      "2021-01-14 10:07:29,014 : INFO : diff #set()\n",
      "2021-01-14 10:07:47,522 : INFO : alphabet #32006\n",
      "2021-01-14 10:07:56,680 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.16932223973442, 0.4609734698162802], [0.7869136482477188, 0.21308635], [2.5219280948873624, 1.3037754718406493], [5.494974824689258, 6.104787343210121, 6.55177398194725, 5.0479881859521285, 1.0567991572579913, 0.44698663873712885]]\n",
      "2021-01-14 10:07:56,697 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 10:07:56,697 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:07:56,701 : INFO : built Dictionary(428 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 9165 corpus positions)\n",
      "2021-01-14 10:07:57,309 : INFO : token count processed\n",
      "2021-01-14 10:07:57,358 : INFO : frequencies processed\n",
      "2021-01-14 10:08:06,654 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:08:06,655 : INFO : entropies processed\n",
      "2021-01-14 10:08:06,656 : INFO : extropies processed\n",
      "2021-01-14 10:08:06,665 : INFO : token count processed\n",
      "2021-01-14 10:08:06,673 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:08:06,680 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:08:06,681 : INFO : vocab #32006\n",
      "2021-01-14 10:08:06,687 : INFO : diff #set()\n",
      "2021-01-14 10:08:25,073 : INFO : alphabet #32006\n",
      "2021-01-14 10:08:34,351 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.1542521533669199, 0.46419821302584374], [0.7849173694849014, 0.21508263], [3.918295834054489, 1.3855815028066252], [5.494974824689258, 6.89087415148015, 7.073645502341728, 5.312203473827681, 1.57867067765247, 0.1827713508615778]]\n",
      "2021-01-14 10:08:34,357 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:08:34,359 : INFO : built Dictionary(292 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 2325 corpus positions)\n",
      "2021-01-14 10:08:34,690 : INFO : token count processed\n",
      "2021-01-14 10:08:34,717 : INFO : frequencies processed\n",
      "2021-01-14 10:08:43,871 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:08:43,872 : INFO : entropies processed\n",
      "2021-01-14 10:08:43,873 : INFO : extropies processed\n",
      "2021-01-14 10:08:43,880 : INFO : token count processed\n",
      "2021-01-14 10:08:43,887 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:08:43,892 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:08:43,893 : INFO : vocab #32006\n",
      "2021-01-14 10:08:43,900 : INFO : diff #set()\n",
      "2021-01-14 10:09:02,253 : INFO : alphabet #32006\n",
      "2021-01-14 10:09:11,541 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.1531647675915921, 0.46443264122259587], [0.7414257526397705, 0.25857425], [3.2810361125534233, 1.353462307959731], [5.494974824689258, 6.655493573668506, 6.876796860235837, 5.273671538121928, 1.3818220355465787, 0.22130328656733056]]\n",
      "2021-01-14 10:09:11,546 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:09:11,547 : INFO : built Dictionary(267 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1332 corpus positions)\n",
      "2021-01-14 10:09:11,846 : INFO : token count processed\n",
      "2021-01-14 10:09:11,874 : INFO : frequencies processed\n",
      "2021-01-14 10:09:21,033 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:09:21,034 : INFO : entropies processed\n",
      "2021-01-14 10:09:21,035 : INFO : extropies processed\n",
      "2021-01-14 10:09:21,042 : INFO : token count processed\n",
      "2021-01-14 10:09:21,046 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:09:21,049 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:09:21,051 : INFO : vocab #32006\n",
      "2021-01-14 10:09:21,061 : INFO : diff #set()\n",
      "2021-01-14 10:09:39,417 : INFO : alphabet #32006\n",
      "2021-01-14 10:09:48,705 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.1326589252079893, 0.4688982322395852], [0.7483973205089569, 0.25160268], [3.5841837197791886, 1.3754132526953111], [5.494974824689258, 6.6236746347295465, 6.862823139625492, 5.255826319793313, 1.367848314936234, 0.23914850489594563]]\n",
      "2021-01-14 10:09:48,709 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:09:48,711 : INFO : built Dictionary(266 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1271 corpus positions)\n",
      "2021-01-14 10:09:49,005 : INFO : token count processed\n",
      "2021-01-14 10:09:49,038 : INFO : frequencies processed\n",
      "2021-01-14 10:09:58,199 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:09:58,200 : INFO : entropies processed\n",
      "2021-01-14 10:09:58,201 : INFO : extropies processed\n",
      "2021-01-14 10:09:58,207 : INFO : token count processed\n",
      "2021-01-14 10:09:58,211 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:09:58,215 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:09:58,216 : INFO : vocab #32006\n",
      "2021-01-14 10:09:58,222 : INFO : diff #set()\n",
      "2021-01-14 10:10:16,785 : INFO : alphabet #32006\n",
      "2021-01-14 10:10:25,981 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.1352795253657046, 0.4683227596765029], [0.7020958662033081, 0.29790413], [3.8268748818646374, 1.3871628992312002], [5.494974824689258, 6.75472436518627, 6.988298946700399, 5.261400243175129, 1.493324122011141, 0.23357458151412924]]\n",
      "2021-01-14 10:10:25,985 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:10:25,986 : INFO : built Dictionary(223 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1066 corpus positions)\n",
      "2021-01-14 10:10:26,212 : INFO : token count processed\n",
      "2021-01-14 10:10:26,240 : INFO : frequencies processed\n",
      "2021-01-14 10:10:35,422 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:10:35,423 : INFO : entropies processed\n",
      "2021-01-14 10:10:35,424 : INFO : extropies processed\n",
      "2021-01-14 10:10:35,438 : INFO : token count processed\n",
      "2021-01-14 10:10:35,443 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:10:35,447 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:10:35,448 : INFO : vocab #32006\n",
      "2021-01-14 10:10:35,454 : INFO : diff #set()\n",
      "2021-01-14 10:10:53,944 : INFO : alphabet #32006\n",
      "2021-01-14 10:11:03,107 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.1468642297239058, 0.46579564098871945], [0.7022457718849182, 0.29775423], [3.238901256602631, 1.3579502728384498], [5.494974824689258, 6.597313085495733, 6.859033309617881, 5.23325460056711, 1.3640584849286226, 0.2617202241221479]]\n",
      "2021-01-14 10:11:03,112 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:11:03,113 : INFO : built Dictionary(247 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1032 corpus positions)\n",
      "2021-01-14 10:11:03,386 : INFO : token count processed\n",
      "2021-01-14 10:11:03,441 : INFO : frequencies processed\n",
      "2021-01-14 10:11:12,597 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:11:12,598 : INFO : entropies processed\n",
      "2021-01-14 10:11:12,598 : INFO : extropies processed\n",
      "2021-01-14 10:11:12,606 : INFO : token count processed\n",
      "2021-01-14 10:11:12,610 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:11:12,614 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:11:12,615 : INFO : vocab #32006\n",
      "2021-01-14 10:11:12,621 : INFO : diff #set()\n",
      "2021-01-14 10:11:31,098 : INFO : alphabet #32006\n",
      "2021-01-14 10:11:40,265 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.1661813624643165, 0.46164186310899125], [0.7522458881139755, 0.24775411], [3.2810361125534233, 1.3534623079597308], [5.494974824689258, 6.659481538516613, 6.9381442611510735, 5.216312102054797, 1.4431694364618153, 0.27866272263446046]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 10:11:40,270 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:11:40,272 : INFO : built Dictionary(273 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1194 corpus positions)\n",
      "2021-01-14 10:11:40,588 : INFO : token count processed\n",
      "2021-01-14 10:11:40,616 : INFO : frequencies processed\n",
      "2021-01-14 10:11:49,775 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:11:49,776 : INFO : entropies processed\n",
      "2021-01-14 10:11:49,777 : INFO : extropies processed\n",
      "2021-01-14 10:11:49,783 : INFO : token count processed\n",
      "2021-01-14 10:11:49,790 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:11:49,794 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:11:49,795 : INFO : vocab #32006\n",
      "2021-01-14 10:11:49,802 : INFO : diff #set()\n",
      "2021-01-14 10:12:08,294 : INFO : alphabet #32006\n",
      "2021-01-14 10:12:17,454 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.1633674155781637, 0.46224233239306145], [0.7456803917884827, 0.2543196], [3.238901256602631, 1.3579502728384498], [5.494974824689258, 6.774682571479102, 7.013605099072034, 5.256052297096325, 1.518630274382776, 0.23892252759293253]]\n",
      "2021-01-14 10:12:17,468 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:12:17,471 : INFO : built Dictionary(446 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 7931 corpus positions)\n",
      "2021-01-14 10:12:18,199 : INFO : token count processed\n",
      "2021-01-14 10:12:18,227 : INFO : frequencies processed\n",
      "2021-01-14 10:12:27,516 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:12:27,517 : INFO : entropies processed\n",
      "2021-01-14 10:12:27,517 : INFO : extropies processed\n",
      "2021-01-14 10:12:27,526 : INFO : token count processed\n",
      "2021-01-14 10:12:27,533 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:12:27,539 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:12:27,540 : INFO : vocab #32006\n",
      "2021-01-14 10:12:27,546 : INFO : diff #set()\n",
      "2021-01-14 10:12:46,032 : INFO : alphabet #32006\n",
      "2021-01-14 10:12:55,341 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.1622611709382629, 0.46247882237374377], [0.7991655021905899, 0.2008345], [3.708132064658602, 1.375948324431107], [5.494974824689258, 6.839453716525233, 7.017755256966196, 5.316673284248295, 1.5227804322769378, 0.17830154044096336]]\n",
      "2021-01-14 10:12:55,347 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:12:55,349 : INFO : built Dictionary(346 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 2373 corpus positions)\n",
      "2021-01-14 10:12:55,777 : INFO : token count processed\n",
      "2021-01-14 10:12:55,833 : INFO : frequencies processed\n",
      "2021-01-14 10:13:05,002 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:13:05,003 : INFO : entropies processed\n",
      "2021-01-14 10:13:05,005 : INFO : extropies processed\n",
      "2021-01-14 10:13:05,016 : INFO : token count processed\n",
      "2021-01-14 10:13:05,020 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:13:05,025 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:13:05,026 : INFO : vocab #32006\n",
      "2021-01-14 10:13:05,032 : INFO : diff #set()\n",
      "2021-01-14 10:13:23,525 : INFO : alphabet #32006\n",
      "2021-01-14 10:13:32,812 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.1306448671819251, 0.46934147281083], [0.7008753418922424, 0.29912466], [3.736006946447656, 1.3801962359387017], [5.494974824689258, 6.86432793886027, 7.04286881767359, 5.316433945875939, 1.5478939929843314, 0.17854087881331981]]\n",
      "2021-01-14 10:13:32,816 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:13:32,817 : INFO : built Dictionary(186 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 575 corpus positions)\n",
      "2021-01-14 10:13:33,001 : INFO : token count processed\n",
      "2021-01-14 10:13:33,047 : INFO : frequencies processed\n",
      "2021-01-14 10:13:42,247 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:13:42,248 : INFO : entropies processed\n",
      "2021-01-14 10:13:42,249 : INFO : extropies processed\n",
      "2021-01-14 10:13:42,262 : INFO : token count processed\n",
      "2021-01-14 10:13:42,269 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:13:42,275 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:13:42,276 : INFO : vocab #32006\n",
      "2021-01-14 10:13:42,281 : INFO : diff #set()\n",
      "2021-01-14 10:14:00,644 : INFO : alphabet #32006\n",
      "2021-01-14 10:14:09,930 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.1404804141733835, 0.46718484008468847], [0.6946816444396973, 0.30531836], [2.918295834054489, 1.3370612537534623], [5.494974824689258, 6.431978396403875, 6.800325815580757, 5.126627405512376, 1.3053509908914984, 0.36834741917688163]]\n",
      "2021-01-14 10:14:09,933 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:14:09,934 : INFO : built Dictionary(243 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 729 corpus positions)\n",
      "2021-01-14 10:14:10,197 : INFO : token count processed\n",
      "2021-01-14 10:14:10,230 : INFO : frequencies processed\n",
      "2021-01-14 10:14:19,412 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:14:19,413 : INFO : entropies processed\n",
      "2021-01-14 10:14:19,414 : INFO : extropies processed\n",
      "2021-01-14 10:14:19,421 : INFO : token count processed\n",
      "2021-01-14 10:14:19,429 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:14:19,434 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:14:19,435 : INFO : vocab #32006\n",
      "2021-01-14 10:14:19,443 : INFO : diff #set()\n",
      "2021-01-14 10:14:37,940 : INFO : alphabet #32006\n",
      "2021-01-14 10:14:47,115 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/test_auth_utility.py')[[1.1712141014245654, 0.46057180604339537], [0.7369321584701538, 0.26306784], [3.2359263506290326, 1.3581587235455306], [5.494974824689258, 6.911818353685893, 7.203118765511737, 5.203674412863415, 1.7081439408224783, 0.2913004118258433]]\n",
      "2021-01-14 10:14:47,128 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:14:47,131 : INFO : built Dictionary(332 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 7259 corpus positions)\n",
      "2021-01-14 10:14:47,573 : INFO : token count processed\n",
      "2021-01-14 10:14:47,602 : INFO : frequencies processed\n",
      "2021-01-14 10:14:56,761 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:14:56,762 : INFO : entropies processed\n",
      "2021-01-14 10:14:56,762 : INFO : extropies processed\n",
      "2021-01-14 10:14:56,771 : INFO : token count processed\n",
      "2021-01-14 10:14:56,778 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:14:56,786 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:14:56,786 : INFO : vocab #32006\n",
      "2021-01-14 10:14:56,793 : INFO : diff #set()\n",
      "2021-01-14 10:15:15,301 : INFO : alphabet #32006\n",
      "2021-01-14 10:15:24,471 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.1400232049102605, 0.4672846526642845], [0.7435014247894287, 0.25649858], [3.521640636343319, 1.3740281872300928], [5.494974824689258, 6.363791471162389, 6.43655359983924, 5.422212696012409, 0.9415787751499813, 0.0727621286768505]]\n",
      "2021-01-14 10:15:24,476 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:15:24,477 : INFO : built Dictionary(230 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1329 corpus positions)\n",
      "2021-01-14 10:15:24,724 : INFO : token count processed\n",
      "2021-01-14 10:15:24,752 : INFO : frequencies processed\n",
      "2021-01-14 10:15:33,911 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:15:33,912 : INFO : entropies processed\n",
      "2021-01-14 10:15:33,913 : INFO : extropies processed\n",
      "2021-01-14 10:15:33,927 : INFO : token count processed\n",
      "2021-01-14 10:15:33,932 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:15:33,936 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:15:33,936 : INFO : vocab #32006\n",
      "2021-01-14 10:15:33,943 : INFO : diff #set()\n",
      "2021-01-14 10:15:52,515 : INFO : alphabet #32006\n",
      "2021-01-14 10:16:01,693 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.1168530225560596, 0.47239935382595394], [0.7556180059909821, 0.244382], [3.7256507561120937, 1.376114516866461], [5.494974824689258, 6.29000629755059, 6.6251997157896305, 5.159781406450218, 1.1302248911003723, 0.33519341823904014]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 10:16:01,698 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:16:01,699 : INFO : built Dictionary(243 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1282 corpus positions)\n",
      "2021-01-14 10:16:01,969 : INFO : token count processed\n",
      "2021-01-14 10:16:02,025 : INFO : frequencies processed\n",
      "2021-01-14 10:16:11,211 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:16:11,212 : INFO : entropies processed\n",
      "2021-01-14 10:16:11,213 : INFO : extropies processed\n",
      "2021-01-14 10:16:11,220 : INFO : token count processed\n",
      "2021-01-14 10:16:11,227 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:16:11,232 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:16:11,233 : INFO : vocab #32006\n",
      "2021-01-14 10:16:11,240 : INFO : diff #set()\n",
      "2021-01-14 10:16:29,752 : INFO : alphabet #32006\n",
      "2021-01-14 10:16:38,999 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.145555966200917, 0.46607966221952035], [0.767037034034729, 0.23296297], [3.3921472236645345, 1.3581797203736243], [5.494974824689258, 6.361621244785958, 6.690962332920256, 5.165633736554961, 1.1959875082309974, 0.3293410881342975]]\n",
      "2021-01-14 10:16:39,003 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:16:39,005 : INFO : built Dictionary(254 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1305 corpus positions)\n",
      "2021-01-14 10:16:39,270 : INFO : token count processed\n",
      "2021-01-14 10:16:39,323 : INFO : frequencies processed\n",
      "2021-01-14 10:16:48,622 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:16:48,623 : INFO : entropies processed\n",
      "2021-01-14 10:16:48,624 : INFO : extropies processed\n",
      "2021-01-14 10:16:48,631 : INFO : token count processed\n",
      "2021-01-14 10:16:48,635 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:16:48,643 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:16:48,644 : INFO : vocab #32006\n",
      "2021-01-14 10:16:48,652 : INFO : diff #set()\n",
      "2021-01-14 10:17:07,024 : INFO : alphabet #32006\n",
      "2021-01-14 10:17:16,389 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.1088041849524706, 0.47420239732810404], [0.7016574740409851, 0.29834253], [3.0, 1.334696163725574], [5.494974824689258, 6.620594433343389, 6.94246151658017, 5.173107741452476, 1.4474866918909122, 0.32186708323678115]]\n",
      "2021-01-14 10:17:16,394 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:17:16,396 : INFO : built Dictionary(224 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1398 corpus positions)\n",
      "2021-01-14 10:17:16,624 : INFO : token count processed\n",
      "2021-01-14 10:17:16,677 : INFO : frequencies processed\n",
      "2021-01-14 10:17:25,855 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:17:25,856 : INFO : entropies processed\n",
      "2021-01-14 10:17:25,857 : INFO : extropies processed\n",
      "2021-01-14 10:17:25,868 : INFO : token count processed\n",
      "2021-01-14 10:17:25,873 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:17:25,877 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:17:25,878 : INFO : vocab #32006\n",
      "2021-01-14 10:17:25,884 : INFO : diff #set()\n",
      "2021-01-14 10:17:44,264 : INFO : alphabet #32006\n",
      "2021-01-14 10:17:53,551 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.1213709252259527, 0.47139328068875436], [0.7084679305553436, 0.29153207], [3.2810361125534233, 1.353462307959731], [5.494974824689258, 6.207411496248084, 6.456648988969942, 5.245737331967401, 0.9616741642806836, 0.2492374927218579]]\n",
      "2021-01-14 10:17:53,554 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:17:53,555 : INFO : built Dictionary(165 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 302 corpus positions)\n",
      "2021-01-14 10:17:53,705 : INFO : token count processed\n",
      "2021-01-14 10:17:53,764 : INFO : frequencies processed\n",
      "2021-01-14 10:18:02,959 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:18:02,960 : INFO : entropies processed\n",
      "2021-01-14 10:18:02,960 : INFO : extropies processed\n",
      "2021-01-14 10:18:02,967 : INFO : token count processed\n",
      "2021-01-14 10:18:02,971 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:18:02,975 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:18:02,976 : INFO : vocab #32006\n",
      "2021-01-14 10:18:02,987 : INFO : diff #set()\n",
      "2021-01-14 10:18:21,385 : INFO : alphabet #32006\n",
      "2021-01-14 10:18:30,767 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.1211212994981063, 0.4714487569553977], [0.6925037503242493, 0.30749625], [3.2359263506290326, 1.3581587235455306], [5.494974824689258, 6.5805228788529595, 6.999231307761677, 5.076266395780542, 1.5042564830724183, 0.41870842890871707]]\n",
      "2021-01-14 10:18:30,772 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 10:18:30,773 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:18:30,774 : INFO : built Dictionary(248 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1263 corpus positions)\n",
      "2021-01-14 10:18:31,055 : INFO : token count processed\n",
      "2021-01-14 10:18:31,087 : INFO : frequencies processed\n",
      "2021-01-14 10:18:40,547 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:18:40,548 : INFO : entropies processed\n",
      "2021-01-14 10:18:40,549 : INFO : extropies processed\n",
      "2021-01-14 10:18:40,555 : INFO : token count processed\n",
      "2021-01-14 10:18:40,559 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:18:40,566 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:18:40,567 : INFO : vocab #32006\n",
      "2021-01-14 10:18:40,574 : INFO : diff #set()\n",
      "2021-01-14 10:18:59,267 : INFO : alphabet #32006\n",
      "2021-01-14 10:19:08,436 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.1651577201009502, 0.4618601179563839], [0.7711517214775085, 0.22884828], [3.1820058147602133, 1.3495612861500506], [5.494974824689258, 6.422089779976135, 6.656657776561579, 5.2604068281038145, 1.1616829518723204, 0.2345679965854437]]\n",
      "2021-01-14 10:19:08,441 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:19:08,443 : INFO : built Dictionary(262 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1545 corpus positions)\n",
      "2021-01-14 10:19:08,736 : INFO : token count processed\n",
      "2021-01-14 10:19:08,792 : INFO : frequencies processed\n",
      "2021-01-14 10:19:18,079 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:19:18,080 : INFO : entropies processed\n",
      "2021-01-14 10:19:18,081 : INFO : extropies processed\n",
      "2021-01-14 10:19:18,088 : INFO : token count processed\n",
      "2021-01-14 10:19:18,095 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:19:18,100 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:19:18,100 : INFO : vocab #32006\n",
      "2021-01-14 10:19:18,107 : INFO : diff #set()\n",
      "2021-01-14 10:19:36,603 : INFO : alphabet #32006\n",
      "2021-01-14 10:19:45,758 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.1407283193005504, 0.4671307381623906], [0.7473669648170471, 0.25263304], [2.8073549220576037, 1.3157132888195113], [5.494974824689258, 6.485445644653597, 6.836604879794102, 5.143815589548753, 1.3416300551048437, 0.3511592351405053]]\n",
      "2021-01-14 10:19:45,762 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:19:45,764 : INFO : built Dictionary(235 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1431 corpus positions)\n",
      "2021-01-14 10:19:46,023 : INFO : token count processed\n",
      "2021-01-14 10:19:46,063 : INFO : frequencies processed\n",
      "2021-01-14 10:19:55,400 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:19:55,402 : INFO : entropies processed\n",
      "2021-01-14 10:19:55,403 : INFO : extropies processed\n",
      "2021-01-14 10:19:55,413 : INFO : token count processed\n",
      "2021-01-14 10:19:55,417 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:19:55,421 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:19:55,422 : INFO : vocab #32006\n",
      "2021-01-14 10:19:55,428 : INFO : diff #set()\n",
      "2021-01-14 10:20:13,958 : INFO : alphabet #32006\n",
      "2021-01-14 10:20:23,121 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.139449542744033, 0.46740994822313586], [0.7230226993560791, 0.2769773], [3.125, 1.3407118537600056], [5.494974824689258, 6.2276600107346916, 6.479147448644013, 5.243487386779937, 0.9841726239547546, 0.25148743790932127]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 10:20:23,126 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:20:23,128 : INFO : built Dictionary(221 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1629 corpus positions)\n",
      "2021-01-14 10:20:23,355 : INFO : token count processed\n",
      "2021-01-14 10:20:23,383 : INFO : frequencies processed\n",
      "2021-01-14 10:20:32,549 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:20:32,550 : INFO : entropies processed\n",
      "2021-01-14 10:20:32,551 : INFO : extropies processed\n",
      "2021-01-14 10:20:32,558 : INFO : token count processed\n",
      "2021-01-14 10:20:32,562 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:20:32,565 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:20:32,566 : INFO : vocab #32006\n",
      "2021-01-14 10:20:32,578 : INFO : diff #set()\n",
      "2021-01-14 10:20:51,145 : INFO : alphabet #32006\n",
      "2021-01-14 10:21:00,325 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.1248062325507795, 0.47063114964583086], [0.748896449804306, 0.25110355], [3.6211755429194716, 1.3730961658387513], [5.494974824689258, 6.253918170574241, 6.534656004251416, 5.214236991012084, 1.0396811795621579, 0.28073783367717553]]\n",
      "2021-01-14 10:21:00,329 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:21:00,330 : INFO : built Dictionary(199 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 726 corpus positions)\n",
      "2021-01-14 10:21:00,524 : INFO : token count processed\n",
      "2021-01-14 10:21:00,553 : INFO : frequencies processed\n",
      "2021-01-14 10:21:09,817 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:21:09,818 : INFO : entropies processed\n",
      "2021-01-14 10:21:09,819 : INFO : extropies processed\n",
      "2021-01-14 10:21:09,826 : INFO : token count processed\n",
      "2021-01-14 10:21:09,830 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:21:09,834 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:21:09,835 : INFO : vocab #32006\n",
      "2021-01-14 10:21:09,841 : INFO : diff #set()\n",
      "2021-01-14 10:21:28,373 : INFO : alphabet #32006\n",
      "2021-01-14 10:21:37,828 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.1229805264669748, 0.4710358797610742], [0.696522980928421, 0.30347702], [2.8073549220576037, 1.3157132888195113], [5.494974824689258, 6.374522245625576, 6.760860364377902, 5.108636705936932, 1.265885539688644, 0.3863381187523265]]\n",
      "2021-01-14 10:21:37,833 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:21:37,835 : INFO : built Dictionary(321 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 2008 corpus positions)\n",
      "2021-01-14 10:21:38,266 : INFO : token count processed\n",
      "2021-01-14 10:21:38,320 : INFO : frequencies processed\n",
      "2021-01-14 10:21:47,532 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:21:47,533 : INFO : entropies processed\n",
      "2021-01-14 10:21:47,534 : INFO : extropies processed\n",
      "2021-01-14 10:21:47,541 : INFO : token count processed\n",
      "2021-01-14 10:21:47,545 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:21:47,549 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:21:47,550 : INFO : vocab #32006\n",
      "2021-01-14 10:21:47,560 : INFO : diff #set()\n",
      "2021-01-14 10:22:06,101 : INFO : alphabet #32006\n",
      "2021-01-14 10:22:15,390 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.1637595538568182, 0.4621585601863842], [0.7976562529802322, 0.20234375], [2.8073549220576037, 1.3157132888195113], [5.494974824689258, 6.731238669067808, 7.084555647153152, 5.141657846603913, 1.589580822463894, 0.3533169780853447]]\n",
      "2021-01-14 10:22:15,395 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:22:15,396 : INFO : built Dictionary(238 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1400 corpus positions)\n",
      "2021-01-14 10:22:15,656 : INFO : token count processed\n",
      "2021-01-14 10:22:15,687 : INFO : frequencies processed\n",
      "2021-01-14 10:22:24,850 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:22:24,851 : INFO : entropies processed\n",
      "2021-01-14 10:22:24,852 : INFO : extropies processed\n",
      "2021-01-14 10:22:24,859 : INFO : token count processed\n",
      "2021-01-14 10:22:24,866 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:22:24,871 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:22:24,872 : INFO : vocab #32006\n",
      "2021-01-14 10:22:24,880 : INFO : diff #set()\n",
      "2021-01-14 10:22:43,301 : INFO : alphabet #32006\n",
      "2021-01-14 10:22:52,596 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.109218198655461, 0.47410931720457294], [0.7311074435710907, 0.26889256], [3.8796640049025943, 1.387374506454425], [5.494974824689258, 6.503741451859337, 6.7884733688935555, 5.21024290765504, 1.2934985442042972, 0.2847319170342182]]\n",
      "2021-01-14 10:22:52,601 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:22:52,602 : INFO : built Dictionary(263 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 1722 corpus positions)\n",
      "2021-01-14 10:22:52,893 : INFO : token count processed\n",
      "2021-01-14 10:22:52,946 : INFO : frequencies processed\n",
      "2021-01-14 10:23:02,124 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:23:02,125 : INFO : entropies processed\n",
      "2021-01-14 10:23:02,126 : INFO : extropies processed\n",
      "2021-01-14 10:23:02,132 : INFO : token count processed\n",
      "2021-01-14 10:23:02,139 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:23:02,142 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:23:02,143 : INFO : vocab #32006\n",
      "2021-01-14 10:23:02,150 : INFO : diff #set()\n",
      "2021-01-14 10:23:20,682 : INFO : alphabet #32006\n",
      "2021-01-14 10:23:29,847 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.1328838562638708, 0.4688487828641919], [0.7509298622608185, 0.24907014], [3.5219280948873624, 1.3671580312847744], [5.494974824689258, 6.334729224484471, 6.5635423714710015, 5.266161677702728, 1.0685675467817433, 0.2288131469865302]]\n",
      "2021-01-14 10:23:29,852 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:23:29,854 : INFO : built Dictionary(265 unique tokens: ['--', '.', '_', '`', 'an']...) from 2 documents (total 2071 corpus positions)\n",
      "2021-01-14 10:23:30,130 : INFO : token count processed\n",
      "2021-01-14 10:23:30,169 : INFO : frequencies processed\n",
      "2021-01-14 10:23:39,350 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:23:39,351 : INFO : entropies processed\n",
      "2021-01-14 10:23:39,352 : INFO : extropies processed\n",
      "2021-01-14 10:23:39,365 : INFO : token count processed\n",
      "2021-01-14 10:23:39,371 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:23:39,375 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:23:39,376 : INFO : vocab #32006\n",
      "2021-01-14 10:23:39,382 : INFO : diff #set()\n",
      "2021-01-14 10:23:58,128 : INFO : alphabet #32006\n",
      "2021-01-14 10:24:07,298 : INFO : Computed distances or similarities ('288', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.1470934376940038, 0.46574591605757426], [0.8035018146038055, 0.19649819], [2.8073549220576037, 1.3157132888195113], [5.494974824689258, 6.21319712067992, 6.5063714480582036, 5.201800497310975, 1.0113966233689453, 0.2931743273782832]]\n",
      "2021-01-14 10:24:07,303 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 10:24:07,304 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:24:07,307 : INFO : built Dictionary(279 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 1428 corpus positions)\n",
      "2021-01-14 10:24:07,498 : INFO : token count processed\n",
      "2021-01-14 10:24:07,526 : INFO : frequencies processed\n",
      "2021-01-14 10:24:16,691 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:24:16,692 : INFO : entropies processed\n",
      "2021-01-14 10:24:16,693 : INFO : extropies processed\n",
      "2021-01-14 10:24:16,706 : INFO : token count processed\n",
      "2021-01-14 10:24:16,711 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:24:16,715 : INFO : alphabet_target #32010\n",
      "2021-01-14 10:24:16,716 : INFO : vocab #32006\n",
      "2021-01-14 10:24:16,722 : INFO : diff #set()\n",
      "2021-01-14 10:24:35,213 : INFO : alphabet #32006\n",
      "2021-01-14 10:24:44,400 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.1351062634891786, 0.4683607636304742], [0.7550995945930481, 0.2449004], [3.0220552088742, 1.3359632893587228], [4.682162149295791, 6.905617163738059, 7.06818810470142, 4.519591208332429, 2.3860259554056293, 0.1625709409633611]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 10:24:44,406 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:24:44,410 : INFO : built Dictionary(364 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 2318 corpus positions)\n",
      "2021-01-14 10:24:44,716 : INFO : token count processed\n",
      "2021-01-14 10:24:44,744 : INFO : frequencies processed\n",
      "2021-01-14 10:24:53,925 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:24:53,926 : INFO : entropies processed\n",
      "2021-01-14 10:24:53,927 : INFO : extropies processed\n",
      "2021-01-14 10:24:53,938 : INFO : token count processed\n",
      "2021-01-14 10:24:53,943 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:24:53,947 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:24:53,948 : INFO : vocab #32006\n",
      "2021-01-14 10:24:53,954 : INFO : diff #set()\n",
      "2021-01-14 10:25:12,418 : INFO : alphabet #32006\n",
      "2021-01-14 10:25:21,644 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.1611367547963523, 0.46271944511638813], [0.7945028990507126, 0.2054971], [3.7841837197791888, 1.3830098939805733], [4.682162149295791, 7.1219284286457345, 7.348985859254564, 4.455104718686962, 2.6668237099587735, 0.22705743060882977]]\n",
      "2021-01-14 10:25:21,650 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:25:21,652 : INFO : built Dictionary(290 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 2301 corpus positions)\n",
      "2021-01-14 10:25:21,867 : INFO : token count processed\n",
      "2021-01-14 10:25:21,895 : INFO : frequencies processed\n",
      "2021-01-14 10:25:31,183 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:25:31,184 : INFO : entropies processed\n",
      "2021-01-14 10:25:31,185 : INFO : extropies processed\n",
      "2021-01-14 10:25:31,192 : INFO : token count processed\n",
      "2021-01-14 10:25:31,196 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:25:31,200 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:25:31,201 : INFO : vocab #32006\n",
      "2021-01-14 10:25:31,207 : INFO : diff #set()\n",
      "2021-01-14 10:25:49,585 : INFO : alphabet #32006\n",
      "2021-01-14 10:25:59,059 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2278781538402994, 0.44885758149576205], [0.8760684207081795, 0.12393158], [2.9219280948873623, 1.3359016564230495], [4.682162149295791, 6.41099024988467, 6.515435435017267, 4.577716964163194, 1.833273285721476, 0.10444518513259649]]\n",
      "2021-01-14 10:25:59,062 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:25:59,064 : INFO : built Dictionary(166 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 700 corpus positions)\n",
      "2021-01-14 10:25:59,160 : INFO : token count processed\n",
      "2021-01-14 10:25:59,188 : INFO : frequencies processed\n",
      "2021-01-14 10:26:08,354 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:26:08,355 : INFO : entropies processed\n",
      "2021-01-14 10:26:08,356 : INFO : extropies processed\n",
      "2021-01-14 10:26:08,362 : INFO : token count processed\n",
      "2021-01-14 10:26:08,366 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:26:08,373 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:26:08,374 : INFO : vocab #32006\n",
      "2021-01-14 10:26:08,383 : INFO : diff #set()\n",
      "2021-01-14 10:26:26,750 : INFO : alphabet #32006\n",
      "2021-01-14 10:26:36,017 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.1908269470469686, 0.4564486489213159], [0.8410441726446152, 0.15895583], [3.2776134368191157, 1.3618978811135465], [4.682162149295791, 6.077866832717642, 6.2912301182942425, 4.46879886371919, 1.6090679689984517, 0.21336328557660078]]\n",
      "2021-01-14 10:26:36,020 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:26:36,021 : INFO : built Dictionary(143 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 465 corpus positions)\n",
      "2021-01-14 10:26:36,098 : INFO : token count processed\n",
      "2021-01-14 10:26:36,125 : INFO : frequencies processed\n",
      "2021-01-14 10:26:45,472 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:26:45,473 : INFO : entropies processed\n",
      "2021-01-14 10:26:45,474 : INFO : extropies processed\n",
      "2021-01-14 10:26:45,480 : INFO : token count processed\n",
      "2021-01-14 10:26:45,488 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:26:45,493 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:26:45,494 : INFO : vocab #32006\n",
      "2021-01-14 10:26:45,502 : INFO : diff #set()\n",
      "2021-01-14 10:27:04,037 : INFO : alphabet #32006\n",
      "2021-01-14 10:27:13,313 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.1865992387332178, 0.45733117540978335], [0.8326163291931152, 0.16738367], [3.121928094887362, 1.3519647487142497], [4.682162149295791, 5.977547459003844, 6.20410651122636, 4.455603097073276, 1.5219443619305695, 0.22655905222251604]]\n",
      "2021-01-14 10:27:13,319 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:27:13,320 : INFO : built Dictionary(247 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 2176 corpus positions)\n",
      "2021-01-14 10:27:13,495 : INFO : token count processed\n",
      "2021-01-14 10:27:13,524 : INFO : frequencies processed\n",
      "2021-01-14 10:27:22,681 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:27:22,682 : INFO : entropies processed\n",
      "2021-01-14 10:27:22,683 : INFO : extropies processed\n",
      "2021-01-14 10:27:22,690 : INFO : token count processed\n",
      "2021-01-14 10:27:22,696 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:27:22,700 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:27:22,701 : INFO : vocab #32006\n",
      "2021-01-14 10:27:22,709 : INFO : diff #set()\n",
      "2021-01-14 10:27:41,506 : INFO : alphabet #32006\n",
      "2021-01-14 10:27:50,691 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.186873790910449, 0.45727375953583294], [0.8295597732067108, 0.17044023], [3.1820058147602133, 1.3495612861500508], [4.682162149295791, 6.4614394051846435, 6.586916680952223, 4.55668487352821, 1.9047545316564323, 0.1254772757675795]]\n",
      "2021-01-14 10:27:50,695 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 10:27:50,696 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:27:50,698 : INFO : built Dictionary(213 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 1400 corpus positions)\n",
      "2021-01-14 10:27:50,833 : INFO : token count processed\n",
      "2021-01-14 10:27:50,888 : INFO : frequencies processed\n",
      "2021-01-14 10:28:00,073 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:28:00,075 : INFO : entropies processed\n",
      "2021-01-14 10:28:00,076 : INFO : extropies processed\n",
      "2021-01-14 10:28:00,089 : INFO : token count processed\n",
      "2021-01-14 10:28:00,093 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:28:00,097 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:28:00,098 : INFO : vocab #32006\n",
      "2021-01-14 10:28:00,104 : INFO : diff #set()\n",
      "2021-01-14 10:28:18,631 : INFO : alphabet #32006\n",
      "2021-01-14 10:28:27,788 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.1565889154272648, 0.4636952331742275], [0.8008056282997131, 0.19919437], [2.5216406363433186, 1.2998438251349493], [4.682162149295791, 6.327195724598159, 6.487549778059866, 4.521808095834085, 1.805387628764075, 0.16035405346170695]]\n",
      "2021-01-14 10:28:27,799 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 10:28:27,800 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:28:27,803 : INFO : built Dictionary(422 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 6292 corpus positions)\n",
      "2021-01-14 10:28:28,152 : INFO : token count processed\n",
      "2021-01-14 10:28:28,209 : INFO : frequencies processed\n",
      "2021-01-14 10:28:37,393 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:28:37,394 : INFO : entropies processed\n",
      "2021-01-14 10:28:37,395 : INFO : extropies processed\n",
      "2021-01-14 10:28:37,403 : INFO : token count processed\n",
      "2021-01-14 10:28:37,411 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:28:37,418 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:28:37,419 : INFO : vocab #32006\n",
      "2021-01-14 10:28:37,425 : INFO : diff #set()\n",
      "2021-01-14 10:28:55,997 : INFO : alphabet #32006\n",
      "2021-01-14 10:29:05,227 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.169273717005118, 0.4609837809589986], [0.8151556998491287, 0.1848443], [3.572431251322119, 1.3732570370060984], [4.682162149295791, 6.9079058562486315, 7.02206147081755, 4.568006534726872, 2.3398993215217594, 0.11415561456891865]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 10:29:05,234 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:29:05,236 : INFO : built Dictionary(324 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 2686 corpus positions)\n",
      "2021-01-14 10:29:05,470 : INFO : token count processed\n",
      "2021-01-14 10:29:05,496 : INFO : frequencies processed\n",
      "2021-01-14 10:29:14,674 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:29:14,675 : INFO : entropies processed\n",
      "2021-01-14 10:29:14,676 : INFO : extropies processed\n",
      "2021-01-14 10:29:14,683 : INFO : token count processed\n",
      "2021-01-14 10:29:14,690 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:29:14,695 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:29:14,696 : INFO : vocab #32006\n",
      "2021-01-14 10:29:14,701 : INFO : diff #set()\n",
      "2021-01-14 10:29:33,183 : INFO : alphabet #32006\n",
      "2021-01-14 10:29:42,548 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.1718527798274074, 0.46043636534124005], [0.8027541786432266, 0.19724582], [3.577819531114783, 1.3721479049971124], [4.682162149295791, 6.61034830706307, 6.786825685730539, 4.5056847706283225, 2.1046635364347486, 0.17647737866746915]]\n",
      "2021-01-14 10:29:42,552 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:29:42,553 : INFO : built Dictionary(214 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 733 corpus positions)\n",
      "2021-01-14 10:29:42,701 : INFO : token count processed\n",
      "2021-01-14 10:29:42,730 : INFO : frequencies processed\n",
      "2021-01-14 10:29:52,022 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:29:52,023 : INFO : entropies processed\n",
      "2021-01-14 10:29:52,024 : INFO : extropies processed\n",
      "2021-01-14 10:29:52,031 : INFO : token count processed\n",
      "2021-01-14 10:29:52,035 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:29:52,042 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:29:52,043 : INFO : vocab #32006\n",
      "2021-01-14 10:29:52,053 : INFO : diff #set()\n",
      "2021-01-14 10:30:10,468 : INFO : alphabet #32006\n",
      "2021-01-14 10:30:19,846 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.1848790041918218, 0.4576912488432723], [0.8452634364366531, 0.15473656], [3.121928094887362, 1.3519647487142497], [4.682162149295791, 6.616715366949855, 6.812178170629247, 4.486699345616399, 2.1300160213334562, 0.1954628036793924]]\n",
      "2021-01-14 10:30:19,852 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:30:19,855 : INFO : built Dictionary(426 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 2767 corpus positions)\n",
      "2021-01-14 10:30:20,224 : INFO : token count processed\n",
      "2021-01-14 10:30:20,256 : INFO : frequencies processed\n",
      "2021-01-14 10:30:29,444 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:30:29,445 : INFO : entropies processed\n",
      "2021-01-14 10:30:29,446 : INFO : extropies processed\n",
      "2021-01-14 10:30:29,453 : INFO : token count processed\n",
      "2021-01-14 10:30:29,461 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:30:29,466 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:30:29,467 : INFO : vocab #32006\n",
      "2021-01-14 10:30:29,474 : INFO : diff #set()\n",
      "2021-01-14 10:30:47,879 : INFO : alphabet #32006\n",
      "2021-01-14 10:30:57,179 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.1352373849509463, 0.46833200235625017], [0.7382132709026337, 0.26178673], [3.9705730958116847, 1.3904984042298727], [4.682162149295791, 7.32185870753746, 7.522742215626143, 4.4812786412071075, 2.840580066330352, 0.20088350808868238]]\n",
      "2021-01-14 10:30:57,182 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:30:57,183 : INFO : built Dictionary(73 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 132 corpus positions)\n",
      "2021-01-14 10:30:57,215 : INFO : token count processed\n",
      "2021-01-14 10:30:57,241 : INFO : frequencies processed\n",
      "2021-01-14 10:31:06,632 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:31:06,633 : INFO : entropies processed\n",
      "2021-01-14 10:31:06,634 : INFO : extropies processed\n",
      "2021-01-14 10:31:06,640 : INFO : token count processed\n",
      "2021-01-14 10:31:06,644 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:31:06,649 : INFO : alphabet_target #32008\n",
      "2021-01-14 10:31:06,650 : INFO : vocab #32006\n",
      "2021-01-14 10:31:06,660 : INFO : diff #set()\n",
      "2021-01-14 10:31:25,146 : INFO : alphabet #32006\n",
      "2021-01-14 10:31:34,439 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/fireException.py')[[1.1404926677422755, 0.4671821656155302], [0.8171460926532745, 0.1828539], [1.5, 1.1225562489182657], [4.682162149295791, 5.176618657501385, 5.850274032946261, 4.0085067738509155, 1.1681118836504698, 0.6736553754448753]]\n",
      "2021-01-14 10:31:34,442 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:31:34,444 : INFO : built Dictionary(165 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 525 corpus positions)\n",
      "2021-01-14 10:31:34,546 : INFO : token count processed\n",
      "2021-01-14 10:31:34,579 : INFO : frequencies processed\n",
      "2021-01-14 10:31:43,907 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:31:43,908 : INFO : entropies processed\n",
      "2021-01-14 10:31:43,909 : INFO : extropies processed\n",
      "2021-01-14 10:31:43,915 : INFO : token count processed\n",
      "2021-01-14 10:31:43,922 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:31:43,926 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:31:43,927 : INFO : vocab #32006\n",
      "2021-01-14 10:31:43,933 : INFO : diff #set()\n",
      "2021-01-14 10:32:02,642 : INFO : alphabet #32006\n",
      "2021-01-14 10:32:11,815 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.1928980696241682, 0.4560175476698677], [0.8312280476093292, 0.16877195], [2.75, 1.3226647836567114], [4.682162149295791, 6.468846789852156, 6.729744809109815, 4.421264130038131, 2.047582659814024, 0.26089801925765865]]\n",
      "2021-01-14 10:32:11,822 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:32:11,824 : INFO : built Dictionary(375 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 2574 corpus positions)\n",
      "2021-01-14 10:32:12,125 : INFO : token count processed\n",
      "2021-01-14 10:32:12,180 : INFO : frequencies processed\n",
      "2021-01-14 10:32:21,370 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:32:21,372 : INFO : entropies processed\n",
      "2021-01-14 10:32:21,372 : INFO : extropies processed\n",
      "2021-01-14 10:32:21,380 : INFO : token count processed\n",
      "2021-01-14 10:32:21,387 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:32:21,392 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:32:21,392 : INFO : vocab #32006\n",
      "2021-01-14 10:32:21,400 : INFO : diff #set()\n",
      "2021-01-14 10:32:39,987 : INFO : alphabet #32006\n",
      "2021-01-14 10:32:49,180 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.2003907670777252, 0.45446473188399655], [0.8358474671840668, 0.16415253], [3.6835423624332306, 1.377974449419992], [4.682162149295791, 6.957796704012729, 7.112564991215656, 4.527393862092865, 2.4304028419198653, 0.154768287202927]]\n",
      "2021-01-14 10:32:49,187 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:32:49,189 : INFO : built Dictionary(288 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 3071 corpus positions)\n",
      "2021-01-14 10:32:49,400 : INFO : token count processed\n",
      "2021-01-14 10:32:49,431 : INFO : frequencies processed\n",
      "2021-01-14 10:32:58,616 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:32:58,617 : INFO : entropies processed\n",
      "2021-01-14 10:32:58,618 : INFO : extropies processed\n",
      "2021-01-14 10:32:58,626 : INFO : token count processed\n",
      "2021-01-14 10:32:58,633 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:32:58,639 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:32:58,640 : INFO : vocab #32006\n",
      "2021-01-14 10:32:58,646 : INFO : diff #set()\n",
      "2021-01-14 10:33:17,340 : INFO : alphabet #32006\n",
      "2021-01-14 10:33:26,979 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.1670502872270503, 0.46145675801533725], [0.8206948637962341, 0.17930514], [3.1820058147602133, 1.3495612861500508], [4.682162149295791, 6.441859572014148, 6.603313577821874, 4.520708143488065, 1.9211514285260831, 0.16145400580772584]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 10:33:26,984 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:33:26,986 : INFO : built Dictionary(310 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 1547 corpus positions)\n",
      "2021-01-14 10:33:27,216 : INFO : token count processed\n",
      "2021-01-14 10:33:27,244 : INFO : frequencies processed\n",
      "2021-01-14 10:33:36,438 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:33:36,439 : INFO : entropies processed\n",
      "2021-01-14 10:33:36,440 : INFO : extropies processed\n",
      "2021-01-14 10:33:36,447 : INFO : token count processed\n",
      "2021-01-14 10:33:36,454 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:33:36,459 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:33:36,460 : INFO : vocab #32006\n",
      "2021-01-14 10:33:36,467 : INFO : diff #set()\n",
      "2021-01-14 10:33:54,956 : INFO : alphabet #32006\n",
      "2021-01-14 10:34:04,361 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.1725393335931038, 0.4602908608085485], [0.8075703531503677, 0.19242965], [3.7345216647797517, 1.3834830067024517], [4.682162149295791, 6.998955278238291, 7.181718658274259, 4.499398769259822, 2.4995565089784684, 0.18276338003596848]]\n",
      "2021-01-14 10:34:04,366 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:34:04,368 : INFO : built Dictionary(228 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 1366 corpus positions)\n",
      "2021-01-14 10:34:04,518 : INFO : token count processed\n",
      "2021-01-14 10:34:04,546 : INFO : frequencies processed\n",
      "2021-01-14 10:34:13,821 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:34:13,822 : INFO : entropies processed\n",
      "2021-01-14 10:34:13,823 : INFO : extropies processed\n",
      "2021-01-14 10:34:13,830 : INFO : token count processed\n",
      "2021-01-14 10:34:13,834 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:34:13,838 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:34:13,839 : INFO : vocab #32006\n",
      "2021-01-14 10:34:13,849 : INFO : diff #set()\n",
      "2021-01-14 10:34:32,409 : INFO : alphabet #32006\n",
      "2021-01-14 10:34:41,738 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.160196149244109, 0.4629209251900193], [0.8077725172042847, 0.19222748], [3.3927474104487847, 1.3672090515720436], [4.682162149295791, 6.492983191376071, 6.635641664167058, 4.539503676504803, 1.9534795148712671, 0.14265847279098676]]\n",
      "2021-01-14 10:34:41,745 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:34:41,747 : INFO : built Dictionary(444 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 3305 corpus positions)\n",
      "2021-01-14 10:34:42,139 : INFO : token count processed\n",
      "2021-01-14 10:34:42,169 : INFO : frequencies processed\n",
      "2021-01-14 10:34:51,525 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:34:51,526 : INFO : entropies processed\n",
      "2021-01-14 10:34:51,527 : INFO : extropies processed\n",
      "2021-01-14 10:34:51,535 : INFO : token count processed\n",
      "2021-01-14 10:34:51,539 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:34:51,543 : INFO : alphabet_target #32008\n",
      "2021-01-14 10:34:51,543 : INFO : vocab #32006\n",
      "2021-01-14 10:34:51,550 : INFO : diff #set()\n",
      "2021-01-14 10:35:09,940 : INFO : alphabet #32006\n",
      "2021-01-14 10:35:19,250 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.2408567516803422, 0.446257887412988], [0.9016909897327423, 0.09830901], [2.0, 1.2451124978365313], [4.682162149295791, 6.560342487747443, 6.7804554251166005, 4.462049211926632, 2.0982932758208097, 0.2201129373691577]]\n",
      "2021-01-14 10:35:19,257 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 10:35:19,258 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:35:19,261 : INFO : built Dictionary(451 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 3510 corpus positions)\n",
      "2021-01-14 10:35:19,642 : INFO : token count processed\n",
      "2021-01-14 10:35:19,670 : INFO : frequencies processed\n",
      "2021-01-14 10:35:28,931 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:35:28,932 : INFO : entropies processed\n",
      "2021-01-14 10:35:28,933 : INFO : extropies processed\n",
      "2021-01-14 10:35:28,949 : INFO : token count processed\n",
      "2021-01-14 10:35:28,953 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:35:28,958 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:35:28,959 : INFO : vocab #32006\n",
      "2021-01-14 10:35:28,965 : INFO : diff #set()\n",
      "2021-01-14 10:35:47,343 : INFO : alphabet #32006\n",
      "2021-01-14 10:35:56,632 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.163493491309142, 0.4622153956168801], [0.7954030930995941, 0.2045969], [3.681880802803402, 1.3784731924832672], [4.682162149295791, 7.046173750105238, 7.245496638710738, 4.48283926069029, 2.563334489414947, 0.19932288860549985]]\n",
      "2021-01-14 10:35:56,643 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 10:35:56,643 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:35:56,646 : INFO : built Dictionary(504 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 5619 corpus positions)\n",
      "2021-01-14 10:35:57,123 : INFO : token count processed\n",
      "2021-01-14 10:35:57,151 : INFO : frequencies processed\n",
      "2021-01-14 10:36:06,310 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:36:06,311 : INFO : entropies processed\n",
      "2021-01-14 10:36:06,312 : INFO : extropies processed\n",
      "2021-01-14 10:36:06,320 : INFO : token count processed\n",
      "2021-01-14 10:36:06,327 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:36:06,332 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:36:06,334 : INFO : vocab #32006\n",
      "2021-01-14 10:36:06,340 : INFO : diff #set()\n",
      "2021-01-14 10:36:25,003 : INFO : alphabet #32006\n",
      "2021-01-14 10:36:34,187 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.1656515424685774, 0.4617548023723718], [0.8032795041799545, 0.1967205], [3.7841837197791888, 1.3830098939805733], [4.682162149295791, 7.009229588004272, 7.121199122095609, 4.570192615204453, 2.4390369727998182, 0.1119695340913367]]\n",
      "2021-01-14 10:36:34,200 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:36:34,203 : INFO : built Dictionary(581 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 6560 corpus positions)\n",
      "2021-01-14 10:36:34,771 : INFO : token count processed\n",
      "2021-01-14 10:36:34,799 : INFO : frequencies processed\n",
      "2021-01-14 10:36:43,962 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:36:43,963 : INFO : entropies processed\n",
      "2021-01-14 10:36:43,964 : INFO : extropies processed\n",
      "2021-01-14 10:36:43,972 : INFO : token count processed\n",
      "2021-01-14 10:36:43,979 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:36:43,985 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:36:43,987 : INFO : vocab #32006\n",
      "2021-01-14 10:36:43,994 : INFO : diff #set()\n",
      "2021-01-14 10:37:02,752 : INFO : alphabet #32006\n",
      "2021-01-14 10:37:11,941 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.135474217696045, 0.4682800624391973], [0.7683942317962646, 0.23160577], [3.970573095811685, 1.3904984042298727], [4.682162149295791, 7.376088004590871, 7.542672893245688, 4.515577260640973, 2.860510743949897, 0.1665848886548167]]\n",
      "2021-01-14 10:37:11,944 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:37:11,946 : INFO : built Dictionary(145 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 346 corpus positions)\n",
      "2021-01-14 10:37:12,022 : INFO : token count processed\n",
      "2021-01-14 10:37:12,051 : INFO : frequencies processed\n",
      "2021-01-14 10:37:21,232 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:37:21,233 : INFO : entropies processed\n",
      "2021-01-14 10:37:21,234 : INFO : extropies processed\n",
      "2021-01-14 10:37:21,241 : INFO : token count processed\n",
      "2021-01-14 10:37:21,248 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:37:21,253 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:37:21,254 : INFO : vocab #32006\n",
      "2021-01-14 10:37:21,262 : INFO : diff #set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 10:37:39,923 : INFO : alphabet #32006\n",
      "2021-01-14 10:37:49,098 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.1852156938455634, 0.45762072953090976], [0.8275785893201828, 0.17242141], [1.5, 1.1225562489182657], [4.682162149295791, 6.2993628166120885, 6.603889864085137, 4.377635101822743, 1.9217277147893466, 0.3045270474730488]]\n",
      "2021-01-14 10:37:49,101 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:37:49,102 : INFO : built Dictionary(44 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 62 corpus positions)\n",
      "2021-01-14 10:37:49,117 : INFO : token count processed\n",
      "2021-01-14 10:37:49,145 : INFO : frequencies processed\n",
      "2021-01-14 10:37:58,314 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:37:58,315 : INFO : entropies processed\n",
      "2021-01-14 10:37:58,316 : INFO : extropies processed\n",
      "2021-01-14 10:37:58,322 : INFO : token count processed\n",
      "2021-01-14 10:37:58,327 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:37:58,331 : INFO : alphabet_target #32008\n",
      "2021-01-14 10:37:58,332 : INFO : vocab #32006\n",
      "2021-01-14 10:37:58,341 : INFO : diff #set()\n",
      "2021-01-14 10:38:16,845 : INFO : alphabet #32006\n",
      "2021-01-14 10:38:26,112 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2254675922227696, 0.4493437709426325], [0.830003172159195, 0.16999683], [0.0, 0.0], [4.682162149295791, 3.8936606896881862, 5.177316637094896, 3.3985062018890817, 0.4951544877991054, 1.28365594740671]]\n",
      "2021-01-14 10:38:26,134 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:38:26,139 : INFO : built Dictionary(739 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 12504 corpus positions)\n",
      "2021-01-14 10:38:27,027 : INFO : token count processed\n",
      "2021-01-14 10:38:27,057 : INFO : frequencies processed\n",
      "2021-01-14 10:38:36,345 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:38:36,346 : INFO : entropies processed\n",
      "2021-01-14 10:38:36,346 : INFO : extropies processed\n",
      "2021-01-14 10:38:36,357 : INFO : token count processed\n",
      "2021-01-14 10:38:36,364 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:38:36,370 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:38:36,371 : INFO : vocab #32006\n",
      "2021-01-14 10:38:36,376 : INFO : diff #set()\n",
      "2021-01-14 10:38:54,796 : INFO : alphabet #32006\n",
      "2021-01-14 10:39:04,243 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.155610398954494, 0.46390572270620717], [0.7782624363899231, 0.22173756], [4.023465189601647, 1.3915925696512925], [4.682162149295791, 7.434393313070278, 7.6412658352553, 4.475289627110769, 2.9591036859595095, 0.20687252218502206]]\n",
      "2021-01-14 10:39:04,252 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:39:04,255 : INFO : built Dictionary(490 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 4131 corpus positions)\n",
      "2021-01-14 10:39:04,728 : INFO : token count processed\n",
      "2021-01-14 10:39:04,789 : INFO : frequencies processed\n",
      "2021-01-14 10:39:14,051 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:39:14,052 : INFO : entropies processed\n",
      "2021-01-14 10:39:14,053 : INFO : extropies processed\n",
      "2021-01-14 10:39:14,061 : INFO : token count processed\n",
      "2021-01-14 10:39:14,067 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:39:14,073 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:39:14,074 : INFO : vocab #32006\n",
      "2021-01-14 10:39:14,081 : INFO : diff #set()\n",
      "2021-01-14 10:39:32,499 : INFO : alphabet #32006\n",
      "2021-01-14 10:39:41,795 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.1524441001207817, 0.4645881395683568], [0.7687685936689377, 0.2312314], [4.026986833359287, 1.3922018630035278], [4.682162149295791, 7.2991514951718255, 7.490696510349638, 4.4906171341179775, 2.808534361053847, 0.19154501517781242]]\n",
      "2021-01-14 10:39:41,803 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:39:41,805 : INFO : built Dictionary(459 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 3548 corpus positions)\n",
      "2021-01-14 10:39:42,221 : INFO : token count processed\n",
      "2021-01-14 10:39:42,249 : INFO : frequencies processed\n",
      "2021-01-14 10:39:51,555 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:39:51,556 : INFO : entropies processed\n",
      "2021-01-14 10:39:51,557 : INFO : extropies processed\n",
      "2021-01-14 10:39:51,564 : INFO : token count processed\n",
      "2021-01-14 10:39:51,571 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:39:51,576 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:39:51,577 : INFO : vocab #32006\n",
      "2021-01-14 10:39:51,585 : INFO : diff #set()\n",
      "2021-01-14 10:40:10,129 : INFO : alphabet #32006\n",
      "2021-01-14 10:40:19,405 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.1602482618060248, 0.4629097579571588], [0.7864744067192078, 0.2135256], [3.7841837197791888, 1.3830098939805733], [4.682162149295791, 7.170319527000998, 7.360848509480805, 4.4916331668159835, 2.678686360185014, 0.19052898247980732]]\n",
      "2021-01-14 10:40:19,408 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:40:19,410 : INFO : built Dictionary(179 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 584 corpus positions)\n",
      "2021-01-14 10:40:19,512 : INFO : token count processed\n",
      "2021-01-14 10:40:19,539 : INFO : frequencies processed\n",
      "2021-01-14 10:40:28,751 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:40:28,752 : INFO : entropies processed\n",
      "2021-01-14 10:40:28,753 : INFO : extropies processed\n",
      "2021-01-14 10:40:28,759 : INFO : token count processed\n",
      "2021-01-14 10:40:28,763 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:40:28,771 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:40:28,772 : INFO : vocab #32006\n",
      "2021-01-14 10:40:28,781 : INFO : diff #set()\n",
      "2021-01-14 10:40:47,302 : INFO : alphabet #32006\n",
      "2021-01-14 10:40:56,456 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.1935452415238168, 0.45588300668251447], [0.8255151808261871, 0.17448482], [1.5, 1.1225562489182657], [4.682162149295791, 6.353654804387375, 6.635250905796857, 4.40056604788631, 1.9530887565010664, 0.2815961014094821]]\n",
      "2021-01-14 10:40:56,460 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:40:56,461 : INFO : built Dictionary(171 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 712 corpus positions)\n",
      "2021-01-14 10:40:56,559 : INFO : token count processed\n",
      "2021-01-14 10:40:56,585 : INFO : frequencies processed\n",
      "2021-01-14 10:41:05,724 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:41:05,725 : INFO : entropies processed\n",
      "2021-01-14 10:41:05,726 : INFO : extropies processed\n",
      "2021-01-14 10:41:05,732 : INFO : token count processed\n",
      "2021-01-14 10:41:05,736 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:41:05,740 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:41:05,741 : INFO : vocab #32006\n",
      "2021-01-14 10:41:05,751 : INFO : diff #set()\n",
      "2021-01-14 10:41:24,617 : INFO : alphabet #32006\n",
      "2021-01-14 10:41:33,948 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.160953136706539, 0.4627587627948646], [0.7969370931386948, 0.2030629], [2.5, 1.2968140217166515], [4.682162149295791, 6.245180322479091, 6.464001138755225, 4.463341333019657, 1.781838989459434, 0.2188208162761338]]\n",
      "2021-01-14 10:41:33,953 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:41:33,955 : INFO : built Dictionary(388 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 1784 corpus positions)\n",
      "2021-01-14 10:41:34,289 : INFO : token count processed\n",
      "2021-01-14 10:41:34,346 : INFO : frequencies processed\n",
      "2021-01-14 10:41:43,532 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:41:43,533 : INFO : entropies processed\n",
      "2021-01-14 10:41:43,534 : INFO : extropies processed\n",
      "2021-01-14 10:41:43,540 : INFO : token count processed\n",
      "2021-01-14 10:41:43,544 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:41:43,549 : INFO : alphabet_target #32009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 10:41:43,550 : INFO : vocab #32006\n",
      "2021-01-14 10:41:43,560 : INFO : diff #set()\n",
      "2021-01-14 10:42:02,105 : INFO : alphabet #32006\n",
      "2021-01-14 10:42:11,450 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.1453632804923117, 0.46612152314386723], [0.7683402895927429, 0.23165971], [4.095795255000933, 1.3969025219274842], [4.682162149295791, 7.2691387000368, 7.475988497306473, 4.475312352026116, 2.7938263480106826, 0.20684979726967345]]\n",
      "2021-01-14 10:42:11,455 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:42:11,457 : INFO : built Dictionary(319 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 1621 corpus positions)\n",
      "2021-01-14 10:42:11,703 : INFO : token count processed\n",
      "2021-01-14 10:42:11,737 : INFO : frequencies processed\n",
      "2021-01-14 10:42:21,112 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:42:21,113 : INFO : entropies processed\n",
      "2021-01-14 10:42:21,114 : INFO : extropies processed\n",
      "2021-01-14 10:42:21,128 : INFO : token count processed\n",
      "2021-01-14 10:42:21,136 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:42:21,141 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:42:21,142 : INFO : vocab #32006\n",
      "2021-01-14 10:42:21,149 : INFO : diff #set()\n",
      "2021-01-14 10:42:39,710 : INFO : alphabet #32006\n",
      "2021-01-14 10:42:48,882 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.109377637630836, 0.47407348127723486], [0.7091677784919739, 0.29083222], [3.546439344671015, 1.3714037024572785], [4.682162149295791, 7.08857858466988, 7.257248748105404, 4.5134919858602665, 2.575086598809613, 0.16867016343552432]]\n",
      "2021-01-14 10:42:48,886 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:42:48,887 : INFO : built Dictionary(154 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 544 corpus positions)\n",
      "2021-01-14 10:42:48,982 : INFO : token count processed\n",
      "2021-01-14 10:42:49,010 : INFO : frequencies processed\n",
      "2021-01-14 10:42:58,290 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:42:58,291 : INFO : entropies processed\n",
      "2021-01-14 10:42:58,291 : INFO : extropies processed\n",
      "2021-01-14 10:42:58,298 : INFO : token count processed\n",
      "2021-01-14 10:42:58,305 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:42:58,309 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:42:58,310 : INFO : vocab #32006\n",
      "2021-01-14 10:42:58,316 : INFO : diff #set()\n",
      "2021-01-14 10:43:17,022 : INFO : alphabet #32006\n",
      "2021-01-14 10:43:26,336 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.204724407837982, 0.4535714289028213], [0.8391956835985184, 0.16080432], [0.9182958340544896, 0.9182958340544896], [4.682162149295791, 6.0479231618016716, 6.344643090003439, 4.3854422210940225, 1.6624809407076482, 0.2967199282017674]]\n",
      "2021-01-14 10:43:26,339 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:43:26,341 : INFO : built Dictionary(152 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 663 corpus positions)\n",
      "2021-01-14 10:43:26,424 : INFO : token count processed\n",
      "2021-01-14 10:43:26,450 : INFO : frequencies processed\n",
      "2021-01-14 10:43:35,613 : INFO : scalar_distribution processed\n",
      "2021-01-14 10:43:35,614 : INFO : entropies processed\n",
      "2021-01-14 10:43:35,615 : INFO : extropies processed\n",
      "2021-01-14 10:43:35,621 : INFO : token count processed\n",
      "2021-01-14 10:43:35,625 : INFO : alphabet_source #32006\n",
      "2021-01-14 10:43:35,631 : INFO : alphabet_target #32009\n",
      "2021-01-14 10:43:35,632 : INFO : vocab #32006\n",
      "2021-01-14 10:43:35,641 : INFO : diff #set()\n",
      "2021-01-14 10:43:54,171 : INFO : alphabet #32006\n",
      "2021-01-14 10:44:03,568 : INFO : Computed distances or similarities ('273', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.1656786280447418, 0.4617490273258312], [0.8047495186328888, 0.19525048], [2.5, 1.2968140217166515], [4.682162149295791, 6.036583168403119, 6.266922634201844, 4.451822683497067, 1.5847604849060533, 0.23033946579872477]]\n",
      "2021-01-14 10:44:03,581 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 10:44:03,582 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 10:44:03,585 : INFO : built Dictionary(560 unique tokens: ['.', ':', 'an', 'atus', 's']...) from 2 documents (total 7021 corpus positions)\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "2021-01-14 12:15:46,246 : INFO : alphabet #32006\n",
      "2021-01-14 12:15:55,516 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.205527488351072, 0.4534062736835959], [0.8874243050813675, 0.112575695], [1.7924812503605778, 1.1575860145844845], [4.898378365512006, 6.0479231618016716, 6.348805582623612, 4.597495944690065, 1.4504272171116064, 0.3008824208219405]]\n",
      "2021-01-14 12:15:55,520 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:15:55,522 : INFO : built Dictionary(157 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 663 corpus positions)\n",
      "2021-01-14 12:15:55,629 : INFO : token count processed\n",
      "2021-01-14 12:15:55,661 : INFO : frequencies processed\n",
      "2021-01-14 12:16:04,822 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:16:04,823 : INFO : entropies processed\n",
      "2021-01-14 12:16:04,824 : INFO : extropies processed\n",
      "2021-01-14 12:16:04,838 : INFO : token count processed\n",
      "2021-01-14 12:16:04,842 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:16:04,849 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:16:04,850 : INFO : vocab #32006\n",
      "2021-01-14 12:16:04,857 : INFO : diff #set()\n",
      "2021-01-14 12:16:23,365 : INFO : alphabet #32006\n",
      "2021-01-14 12:16:32,824 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.2134908002390363, 0.45177508751877776], [0.8878070712089539, 0.11219293], [2.1280852788913944, 1.2238339714721664], [4.898378365512006, 6.036583168403119, 6.295476966578189, 4.639484567336936, 1.3970986010661832, 0.25889379817506963]]\n",
      "2021-01-14 12:16:32,838 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 12:16:32,839 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:16:32,842 : INFO : built Dictionary(567 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 7021 corpus positions)\n",
      "2021-01-14 12:16:33,448 : INFO : token count processed\n",
      "2021-01-14 12:16:33,479 : INFO : frequencies processed\n",
      "2021-01-14 12:16:42,931 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:16:42,932 : INFO : entropies processed\n",
      "2021-01-14 12:16:42,933 : INFO : extropies processed\n",
      "2021-01-14 12:16:42,945 : INFO : token count processed\n",
      "2021-01-14 12:16:42,950 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:16:42,954 : INFO : alphabet_target #32010\n",
      "2021-01-14 12:16:42,955 : INFO : vocab #32006\n",
      "2021-01-14 12:16:42,961 : INFO : diff #set()\n",
      "2021-01-14 12:17:01,540 : INFO : alphabet #32006\n",
      "2021-01-14 12:17:10,893 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.2177111813813848, 0.4509153438894204], [0.8946033269166946, 0.10539667], [3.180832987205441, 1.3478475537994532], [4.898378365512006, 7.29352035514053, 7.461636025097501, 4.730262695555034, 2.5632576595854957, 0.16811566995697103]]\n",
      "2021-01-14 12:17:10,900 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:17:10,903 : INFO : built Dictionary(376 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 3264 corpus positions)\n",
      "2021-01-14 12:17:11,223 : INFO : token count processed\n",
      "2021-01-14 12:17:11,275 : INFO : frequencies processed\n",
      "2021-01-14 12:17:20,542 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:17:20,543 : INFO : entropies processed\n",
      "2021-01-14 12:17:20,544 : INFO : extropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 12:17:20,551 : INFO : token count processed\n",
      "2021-01-14 12:17:20,555 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:17:20,558 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:17:20,559 : INFO : vocab #32006\n",
      "2021-01-14 12:17:20,565 : INFO : diff #set()\n",
      "2021-01-14 12:17:39,365 : INFO : alphabet #32006\n",
      "2021-01-14 12:17:48,565 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.213556713462826, 0.45176163498229416], [0.8831385672092438, 0.11686143], [3.180832987205441, 1.3478475537994532], [4.898378365512006, 6.8153433747477745, 6.9782206555299915, 4.735501084729789, 2.079842290017986, 0.16287728078221697]]\n",
      "2021-01-14 12:17:48,568 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:17:48,569 : INFO : built Dictionary(134 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 285 corpus positions)\n",
      "2021-01-14 12:17:48,649 : INFO : token count processed\n",
      "2021-01-14 12:17:48,677 : INFO : frequencies processed\n",
      "2021-01-14 12:17:57,882 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:17:57,883 : INFO : entropies processed\n",
      "2021-01-14 12:17:57,884 : INFO : extropies processed\n",
      "2021-01-14 12:17:57,891 : INFO : token count processed\n",
      "2021-01-14 12:17:57,895 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:17:57,899 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:17:57,900 : INFO : vocab #32006\n",
      "2021-01-14 12:17:57,907 : INFO : diff #set()\n",
      "2021-01-14 12:18:16,458 : INFO : alphabet #32006\n",
      "2021-01-14 12:18:25,913 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.218532886711819, 0.4507483328237437], [0.8986656963825226, 0.1013343], [1.584962500721156, 1.1699250014423124], [4.898378365512006, 6.150121915859574, 6.5142070506367595, 4.534293230734821, 1.6158286851247539, 0.3640851347771852]]\n",
      "2021-01-14 12:18:25,918 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:18:25,919 : INFO : built Dictionary(298 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 996 corpus positions)\n",
      "2021-01-14 12:18:26,160 : INFO : token count processed\n",
      "2021-01-14 12:18:26,207 : INFO : frequencies processed\n",
      "2021-01-14 12:18:35,449 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:18:35,451 : INFO : entropies processed\n",
      "2021-01-14 12:18:35,451 : INFO : extropies processed\n",
      "2021-01-14 12:18:35,458 : INFO : token count processed\n",
      "2021-01-14 12:18:35,463 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:18:35,470 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:18:35,470 : INFO : vocab #32006\n",
      "2021-01-14 12:18:35,478 : INFO : diff #set()\n",
      "2021-01-14 12:18:53,952 : INFO : alphabet #32006\n",
      "2021-01-14 12:19:03,109 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.231912814855493, 0.4480461751660071], [0.9107072353363037, 0.089292765], [1.9219280948873623, 1.2148067842293933], [4.898378365512006, 7.0391145208191315, 7.345191167709207, 4.59230171862193, 2.4468128021972015, 0.30607664689007574]]\n",
      "2021-01-14 12:19:03,118 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 12:19:03,119 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:19:03,126 : INFO : built Dictionary(599 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 4371 corpus positions)\n",
      "2021-01-14 12:19:03,805 : INFO : token count processed\n",
      "2021-01-14 12:19:03,832 : INFO : frequencies processed\n",
      "2021-01-14 12:19:13,239 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:19:13,240 : INFO : entropies processed\n",
      "2021-01-14 12:19:13,241 : INFO : extropies processed\n",
      "2021-01-14 12:19:13,256 : INFO : token count processed\n",
      "2021-01-14 12:19:13,261 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:19:13,266 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:19:13,268 : INFO : vocab #32006\n",
      "2021-01-14 12:19:13,275 : INFO : diff #set()\n",
      "2021-01-14 12:19:31,730 : INFO : alphabet #32006\n",
      "2021-01-14 12:19:41,047 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.2364794457753687, 0.44713131698525777], [0.9173539653420448, 0.082646035], [2.9219280948873623, 1.3359016564230495], [4.898378365512006, 7.482466367279176, 7.770914049201717, 4.609930683589465, 2.872535683689711, 0.2884476819225412]]\n",
      "2021-01-14 12:19:41,051 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:19:41,053 : INFO : built Dictionary(182 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 593 corpus positions)\n",
      "2021-01-14 12:19:41,171 : INFO : token count processed\n",
      "2021-01-14 12:19:41,200 : INFO : frequencies processed\n",
      "2021-01-14 12:19:50,743 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:19:50,744 : INFO : entropies processed\n",
      "2021-01-14 12:19:50,744 : INFO : extropies processed\n",
      "2021-01-14 12:19:50,751 : INFO : token count processed\n",
      "2021-01-14 12:19:50,754 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:19:50,758 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:19:50,759 : INFO : vocab #32006\n",
      "2021-01-14 12:19:50,764 : INFO : diff #set()\n",
      "2021-01-14 12:20:09,324 : INFO : alphabet #32006\n",
      "2021-01-14 12:20:18,627 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.2315010268175206, 0.44812885496457106], [0.8988140150904655, 0.101185985], [1.7924812503605778, 1.1575860145844845], [4.898378365512006, 6.372162341197667, 6.665319684994911, 4.605221021714762, 1.7669413194829051, 0.2931573437972439]]\n",
      "2021-01-14 12:20:18,632 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:20:18,634 : INFO : built Dictionary(326 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 2014 corpus positions)\n",
      "2021-01-14 12:20:18,893 : INFO : token count processed\n",
      "2021-01-14 12:20:18,922 : INFO : frequencies processed\n",
      "2021-01-14 12:20:28,111 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:20:28,112 : INFO : entropies processed\n",
      "2021-01-14 12:20:28,113 : INFO : extropies processed\n",
      "2021-01-14 12:20:28,120 : INFO : token count processed\n",
      "2021-01-14 12:20:28,124 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:20:28,129 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:20:28,130 : INFO : vocab #32006\n",
      "2021-01-14 12:20:28,136 : INFO : diff #set()\n",
      "2021-01-14 12:20:46,778 : INFO : alphabet #32006\n",
      "2021-01-14 12:20:56,284 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.2015491691861735, 0.45422560349613306], [0.8754840791225433, 0.12451592], [3.121928094887362, 1.3519647487142497], [4.898378365512006, 6.798155919669889, 7.01612676951059, 4.680407515671304, 2.1177484039985845, 0.21797084984070114]]\n",
      "2021-01-14 12:20:56,288 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:20:56,289 : INFO : built Dictionary(188 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 730 corpus positions)\n",
      "2021-01-14 12:20:56,418 : INFO : token count processed\n",
      "2021-01-14 12:20:56,449 : INFO : frequencies processed\n",
      "2021-01-14 12:21:05,621 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:21:05,622 : INFO : entropies processed\n",
      "2021-01-14 12:21:05,623 : INFO : extropies processed\n",
      "2021-01-14 12:21:05,630 : INFO : token count processed\n",
      "2021-01-14 12:21:05,634 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:21:05,639 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:21:05,640 : INFO : vocab #32006\n",
      "2021-01-14 12:21:05,646 : INFO : diff #set()\n",
      "2021-01-14 12:21:24,159 : INFO : alphabet #32006\n",
      "2021-01-14 12:21:33,332 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.2059619940026527, 0.45331696680119565], [0.8844143226742744, 0.11558568], [1.584962500721156, 1.1699250014423124], [4.898378365512006, 6.271631856729336, 6.55241045245088, 4.617599769790461, 1.6540320869388747, 0.28077859572154473]]\n",
      "2021-01-14 12:21:33,339 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:21:33,342 : INFO : built Dictionary(359 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 3245 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 12:21:33,656 : INFO : token count processed\n",
      "2021-01-14 12:21:33,683 : INFO : frequencies processed\n",
      "2021-01-14 12:21:43,036 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:21:43,038 : INFO : entropies processed\n",
      "2021-01-14 12:21:43,038 : INFO : extropies processed\n",
      "2021-01-14 12:21:43,046 : INFO : token count processed\n",
      "2021-01-14 12:21:43,050 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:21:43,054 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:21:43,055 : INFO : vocab #32006\n",
      "2021-01-14 12:21:43,060 : INFO : diff #set()\n",
      "2021-01-14 12:22:01,719 : INFO : alphabet #32006\n",
      "2021-01-14 12:22:10,890 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.1992067925683052, 0.45470939948860717], [0.8761282563209534, 0.12387174], [3.2516291673878226, 1.3589504783379556], [4.898378365512006, 6.873598627629562, 7.047314416945401, 4.724662576196167, 2.1489360514333953, 0.17371578931583898]]\n",
      "2021-01-14 12:22:10,893 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:22:10,894 : INFO : built Dictionary(115 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 193 corpus positions)\n",
      "2021-01-14 12:22:10,971 : INFO : token count processed\n",
      "2021-01-14 12:22:11,001 : INFO : frequencies processed\n",
      "2021-01-14 12:22:20,160 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:22:20,161 : INFO : entropies processed\n",
      "2021-01-14 12:22:20,162 : INFO : extropies processed\n",
      "2021-01-14 12:22:20,174 : INFO : token count processed\n",
      "2021-01-14 12:22:20,179 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:22:20,183 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:22:20,184 : INFO : vocab #32006\n",
      "2021-01-14 12:22:20,190 : INFO : diff #set()\n",
      "2021-01-14 12:22:38,876 : INFO : alphabet #32006\n",
      "2021-01-14 12:22:48,067 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.232528193912463, 0.44792267471772396], [0.9116065055131912, 0.088393494], [1.584962500721156, 1.1699250014423124], [4.898378365512006, 6.049830202851529, 6.529440986555995, 4.41876758180754, 1.6310626210439896, 0.47961078370446586]]\n",
      "2021-01-14 12:22:48,071 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:22:48,073 : INFO : built Dictionary(262 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 1273 corpus positions)\n",
      "2021-01-14 12:22:48,280 : INFO : token count processed\n",
      "2021-01-14 12:22:48,311 : INFO : frequencies processed\n",
      "2021-01-14 12:22:57,496 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:22:57,497 : INFO : entropies processed\n",
      "2021-01-14 12:22:57,497 : INFO : extropies processed\n",
      "2021-01-14 12:22:57,505 : INFO : token count processed\n",
      "2021-01-14 12:22:57,509 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:22:57,513 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:22:57,514 : INFO : vocab #32006\n",
      "2021-01-14 12:22:57,521 : INFO : diff #set()\n",
      "2021-01-14 12:23:16,269 : INFO : alphabet #32006\n",
      "2021-01-14 12:23:25,670 : INFO : Computed distances or similarities ('285', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.2414792560699215, 0.44613395251907945], [0.9166955128312111, 0.08330449], [2.0, 1.2451124978365313], [4.898378365512006, 6.778844940588858, 7.034042483182821, 4.643180822918042, 2.1356641176708155, 0.25519754259396343]]\n",
      "2021-01-14 12:23:25,673 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:23:25,674 : INFO : built Dictionary(162 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 306 corpus positions)\n",
      "2021-01-14 12:23:25,773 : INFO : token count processed\n",
      "2021-01-14 12:23:25,805 : INFO : frequencies processed\n",
      "2021-01-14 12:23:35,131 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:23:35,132 : INFO : entropies processed\n",
      "2021-01-14 12:23:35,133 : INFO : extropies processed\n",
      "2021-01-14 12:23:35,146 : INFO : token count processed\n",
      "2021-01-14 12:23:35,151 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:23:35,155 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:23:35,156 : INFO : vocab #32006\n",
      "2021-01-14 12:23:35,162 : INFO : diff #set()\n",
      "2021-01-14 12:23:53,579 : INFO : alphabet #32006\n",
      "2021-01-14 12:24:02,863 : INFO : Computed distances or similarities ('285', 'sacp-python-common/setup.py')[[1.2330962060499102, 0.44780874074784477], [0.9107149168848991, 0.08928508], [1.584962500721156, 1.1699250014423124], [4.898378365512006, 6.469677430851302, 6.892319560949054, 4.475736235414254, 1.9939411954370483, 0.42264213009775187]]\n",
      "2021-01-14 12:24:02,868 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:24:02,869 : INFO : built Dictionary(229 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 1208 corpus positions)\n",
      "2021-01-14 12:24:03,039 : INFO : token count processed\n",
      "2021-01-14 12:24:03,068 : INFO : frequencies processed\n",
      "2021-01-14 12:24:12,471 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:24:12,472 : INFO : entropies processed\n",
      "2021-01-14 12:24:12,473 : INFO : extropies processed\n",
      "2021-01-14 12:24:12,487 : INFO : token count processed\n",
      "2021-01-14 12:24:12,492 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:24:12,497 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:24:12,498 : INFO : vocab #32006\n",
      "2021-01-14 12:24:12,507 : INFO : diff #set()\n",
      "2021-01-14 12:24:30,887 : INFO : alphabet #32006\n",
      "2021-01-14 12:24:40,200 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.2293364174434163, 0.44856397274790466], [0.8905425667762756, 0.10945743], [2.5216406363433186, 1.2998438251349493], [4.898378365512006, 6.459180448028249, 6.689596500243426, 4.667962313296829, 1.7912181347314204, 0.23041605221517703]]\n",
      "2021-01-14 12:24:40,203 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:24:40,205 : INFO : built Dictionary(142 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 358 corpus positions)\n",
      "2021-01-14 12:24:40,288 : INFO : token count processed\n",
      "2021-01-14 12:24:40,316 : INFO : frequencies processed\n",
      "2021-01-14 12:24:49,485 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:24:49,486 : INFO : entropies processed\n",
      "2021-01-14 12:24:49,487 : INFO : extropies processed\n",
      "2021-01-14 12:24:49,501 : INFO : token count processed\n",
      "2021-01-14 12:24:49,507 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:24:49,512 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:24:49,513 : INFO : vocab #32006\n",
      "2021-01-14 12:24:49,520 : INFO : diff #set()\n",
      "2021-01-14 12:25:08,274 : INFO : alphabet #32006\n",
      "2021-01-14 12:25:17,819 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.1996073214957732, 0.454626600951656], [0.8815593123435974, 0.11844069], [1.584962500721156, 1.1699250014423124], [4.898378365512006, 6.097125733496388, 6.4604620599558285, 4.5350420390525645, 1.5620836944438228, 0.36333632645944025]]\n",
      "2021-01-14 12:25:17,822 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:25:17,823 : INFO : built Dictionary(134 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 354 corpus positions)\n",
      "2021-01-14 12:25:17,897 : INFO : token count processed\n",
      "2021-01-14 12:25:17,925 : INFO : frequencies processed\n",
      "2021-01-14 12:25:27,093 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:25:27,094 : INFO : entropies processed\n",
      "2021-01-14 12:25:27,095 : INFO : extropies processed\n",
      "2021-01-14 12:25:27,102 : INFO : token count processed\n",
      "2021-01-14 12:25:27,107 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:25:27,111 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:25:27,112 : INFO : vocab #32006\n",
      "2021-01-14 12:25:27,118 : INFO : diff #set()\n",
      "2021-01-14 12:25:45,679 : INFO : alphabet #32006\n",
      "2021-01-14 12:25:55,090 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.1927580265049122, 0.45604667177706015], [0.8802036717534065, 0.11979633], [1.584962500721156, 1.1699250014423124], [4.898378365512006, 6.0695858597523715, 6.426056839584407, 4.541907385679971, 1.5276784740724016, 0.3564709798320358]]\n",
      "2021-01-14 12:25:55,093 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 12:25:55,094 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 12:25:55,095 : INFO : built Dictionary(136 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 450 corpus positions)\n",
      "2021-01-14 12:25:55,178 : INFO : token count processed\n",
      "2021-01-14 12:25:55,217 : INFO : frequencies processed\n",
      "2021-01-14 12:26:04,489 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:26:04,490 : INFO : entropies processed\n",
      "2021-01-14 12:26:04,491 : INFO : extropies processed\n",
      "2021-01-14 12:26:04,498 : INFO : token count processed\n",
      "2021-01-14 12:26:04,502 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:26:04,507 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:26:04,508 : INFO : vocab #32006\n",
      "2021-01-14 12:26:04,515 : INFO : diff #set()\n",
      "2021-01-14 12:26:23,022 : INFO : alphabet #32006\n",
      "2021-01-14 12:26:32,196 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.1828916260643871, 0.45810794638620495], [0.8723434507846832, 0.12765655], [1.584962500721156, 1.1699250014423124], [4.898378365512006, 6.104787343210121, 6.401418607525539, 4.601747101196587, 1.5030402420135331, 0.2966312643154181]]\n",
      "2021-01-14 12:26:32,212 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 12:26:32,213 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:26:32,217 : INFO : built Dictionary(417 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 9135 corpus positions)\n",
      "2021-01-14 12:26:32,601 : INFO : token count processed\n",
      "2021-01-14 12:26:32,629 : INFO : frequencies processed\n",
      "2021-01-14 12:26:41,836 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:26:41,837 : INFO : entropies processed\n",
      "2021-01-14 12:26:41,838 : INFO : extropies processed\n",
      "2021-01-14 12:26:41,847 : INFO : token count processed\n",
      "2021-01-14 12:26:41,851 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:26:41,856 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:26:41,857 : INFO : vocab #32006\n",
      "2021-01-14 12:26:41,863 : INFO : diff #set()\n",
      "2021-01-14 12:27:00,717 : INFO : alphabet #32006\n",
      "2021-01-14 12:27:10,098 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.2176819681847668, 0.4509212837305645], [0.9102506563067436, 0.08974934], [3.180832987205441, 1.3478475537994532], [4.898378365512006, 6.89087415148015, 7.058127656447468, 4.731124860544689, 2.159749290935462, 0.1672535049673174]]\n",
      "2021-01-14 12:27:10,104 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:27:10,106 : INFO : built Dictionary(281 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 2295 corpus positions)\n",
      "2021-01-14 12:27:10,311 : INFO : token count processed\n",
      "2021-01-14 12:27:10,360 : INFO : frequencies processed\n",
      "2021-01-14 12:27:19,783 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:27:19,784 : INFO : entropies processed\n",
      "2021-01-14 12:27:19,785 : INFO : extropies processed\n",
      "2021-01-14 12:27:19,792 : INFO : token count processed\n",
      "2021-01-14 12:27:19,796 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:27:19,800 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:27:19,801 : INFO : vocab #32006\n",
      "2021-01-14 12:27:19,808 : INFO : diff #set()\n",
      "2021-01-14 12:27:38,354 : INFO : alphabet #32006\n",
      "2021-01-14 12:27:47,553 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.232065830440884, 0.4480154601006894], [0.9140652939677238, 0.085934706], [1.9219280948873623, 1.2148067842293933], [4.898378365512006, 6.655493573668506, 6.835247440952351, 4.718624498228161, 1.9368690754403453, 0.17975386728384457]]\n",
      "2021-01-14 12:27:47,558 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:27:47,559 : INFO : built Dictionary(256 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 1302 corpus positions)\n",
      "2021-01-14 12:27:47,741 : INFO : token count processed\n",
      "2021-01-14 12:27:47,780 : INFO : frequencies processed\n",
      "2021-01-14 12:27:57,048 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:27:57,049 : INFO : entropies processed\n",
      "2021-01-14 12:27:57,050 : INFO : extropies processed\n",
      "2021-01-14 12:27:57,064 : INFO : token count processed\n",
      "2021-01-14 12:27:57,068 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:27:57,071 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:27:57,072 : INFO : vocab #32006\n",
      "2021-01-14 12:27:57,078 : INFO : diff #set()\n",
      "2021-01-14 12:28:15,667 : INFO : alphabet #32006\n",
      "2021-01-14 12:28:25,189 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.2117808593561854, 0.4521243575148237], [0.8889808803796768, 0.11101912], [2.5, 1.2968140217166515], [4.898378365512006, 6.6236746347295465, 6.802248362307629, 4.719804637933923, 1.9038699967956232, 0.17857372757808232]]\n",
      "2021-01-14 12:28:25,193 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:28:25,195 : INFO : built Dictionary(262 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 1241 corpus positions)\n",
      "2021-01-14 12:28:25,401 : INFO : token count processed\n",
      "2021-01-14 12:28:25,434 : INFO : frequencies processed\n",
      "2021-01-14 12:28:34,717 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:28:34,718 : INFO : entropies processed\n",
      "2021-01-14 12:28:34,719 : INFO : extropies processed\n",
      "2021-01-14 12:28:34,726 : INFO : token count processed\n",
      "2021-01-14 12:28:34,730 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:28:34,734 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:28:34,735 : INFO : vocab #32006\n",
      "2021-01-14 12:28:34,742 : INFO : diff #set()\n",
      "2021-01-14 12:28:53,206 : INFO : alphabet #32006\n",
      "2021-01-14 12:29:02,499 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.241764635570467, 0.44607715909726964], [0.9096774309873581, 0.09032257], [0.0, 0.0], [4.898378365512006, 6.75472436518627, 6.946942754273371, 4.706159976424905, 2.048564388761365, 0.19221838908710076]]\n",
      "2021-01-14 12:29:02,503 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:29:02,505 : INFO : built Dictionary(213 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 1036 corpus positions)\n",
      "2021-01-14 12:29:02,646 : INFO : token count processed\n",
      "2021-01-14 12:29:02,675 : INFO : frequencies processed\n",
      "2021-01-14 12:29:11,918 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:29:11,920 : INFO : entropies processed\n",
      "2021-01-14 12:29:11,921 : INFO : extropies processed\n",
      "2021-01-14 12:29:11,927 : INFO : token count processed\n",
      "2021-01-14 12:29:11,932 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:29:11,936 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:29:11,937 : INFO : vocab #32006\n",
      "2021-01-14 12:29:11,944 : INFO : diff #set()\n",
      "2021-01-14 12:29:30,463 : INFO : alphabet #32006\n",
      "2021-01-14 12:29:39,907 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.235939267713731, 0.4472393389389817], [0.9040127992630005, 0.0959872], [0.9182958340544896, 0.9182958340544896], [4.898378365512006, 6.597313085495733, 6.79097808241381, 4.704713368593929, 1.8925997169018043, 0.19366499691807704]]\n",
      "2021-01-14 12:29:39,911 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:29:39,914 : INFO : built Dictionary(235 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 1002 corpus positions)\n",
      "2021-01-14 12:29:40,081 : INFO : token count processed\n",
      "2021-01-14 12:29:40,109 : INFO : frequencies processed\n",
      "2021-01-14 12:29:49,295 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:29:49,296 : INFO : entropies processed\n",
      "2021-01-14 12:29:49,297 : INFO : extropies processed\n",
      "2021-01-14 12:29:49,308 : INFO : token count processed\n",
      "2021-01-14 12:29:49,313 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:29:49,318 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:29:49,319 : INFO : vocab #32006\n",
      "2021-01-14 12:29:49,325 : INFO : diff #set()\n",
      "2021-01-14 12:30:08,118 : INFO : alphabet #32006\n",
      "2021-01-14 12:30:17,327 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.2172021597623317, 0.4510188642911988], [0.8869195505976677, 0.11308045], [2.2516291673878226, 1.2667563532600834], [4.898378365512006, 6.659481538516613, 6.858913262158634, 4.698946641869984, 1.960534896646628, 0.19943172364202066]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 12:30:17,332 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:30:17,333 : INFO : built Dictionary(261 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 1164 corpus positions)\n",
      "2021-01-14 12:30:17,541 : INFO : token count processed\n",
      "2021-01-14 12:30:17,569 : INFO : frequencies processed\n",
      "2021-01-14 12:30:26,747 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:30:26,748 : INFO : entropies processed\n",
      "2021-01-14 12:30:26,749 : INFO : extropies processed\n",
      "2021-01-14 12:30:26,756 : INFO : token count processed\n",
      "2021-01-14 12:30:26,759 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:30:26,763 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:30:26,764 : INFO : vocab #32006\n",
      "2021-01-14 12:30:26,770 : INFO : diff #set()\n",
      "2021-01-14 12:30:45,274 : INFO : alphabet #32006\n",
      "2021-01-14 12:30:54,528 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.2200177325200385, 0.4504468524514246], [0.886537529528141, 0.11346247], [1.9219280948873623, 1.2148067842293933], [4.898378365512006, 6.774682571479102, 6.94337353578803, 4.729687401203076, 2.0449951702760245, 0.1686909643089285]]\n",
      "2021-01-14 12:30:54,542 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:30:54,545 : INFO : built Dictionary(431 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 7901 corpus positions)\n",
      "2021-01-14 12:30:54,988 : INFO : token count processed\n",
      "2021-01-14 12:30:55,050 : INFO : frequencies processed\n",
      "2021-01-14 12:31:04,404 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:31:04,405 : INFO : entropies processed\n",
      "2021-01-14 12:31:04,406 : INFO : extropies processed\n",
      "2021-01-14 12:31:04,415 : INFO : token count processed\n",
      "2021-01-14 12:31:04,420 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:31:04,424 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:31:04,425 : INFO : vocab #32006\n",
      "2021-01-14 12:31:04,432 : INFO : diff #set()\n",
      "2021-01-14 12:31:23,119 : INFO : alphabet #32006\n",
      "2021-01-14 12:31:32,437 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.222657526233326, 0.4499118682016079], [0.9126849099993706, 0.08731509], [3.4565647621309536, 1.3654661895235272], [4.898378365512006, 6.839453716525233, 6.9984175020053545, 4.739414580031884, 2.100039136493349, 0.1589637854801218]]\n",
      "2021-01-14 12:31:32,443 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:31:32,445 : INFO : built Dictionary(340 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 2343 corpus positions)\n",
      "2021-01-14 12:31:32,727 : INFO : token count processed\n",
      "2021-01-14 12:31:32,777 : INFO : frequencies processed\n",
      "2021-01-14 12:31:41,935 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:31:41,936 : INFO : entropies processed\n",
      "2021-01-14 12:31:41,937 : INFO : extropies processed\n",
      "2021-01-14 12:31:41,952 : INFO : token count processed\n",
      "2021-01-14 12:31:41,956 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:31:41,961 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:31:41,962 : INFO : vocab #32006\n",
      "2021-01-14 12:31:41,968 : INFO : diff #set()\n",
      "2021-01-14 12:32:00,876 : INFO : alphabet #32006\n",
      "2021-01-14 12:32:10,056 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.239137799851982, 0.4466004727650549], [0.9015084654092789, 0.098491535], [1.584962500721156, 1.1699250014423124], [4.898378365512006, 6.86432793886027, 7.02023409383874, 4.742472210533537, 2.121855728326734, 0.15590615497846994]]\n",
      "2021-01-14 12:32:10,059 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:32:10,061 : INFO : built Dictionary(173 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 545 corpus positions)\n",
      "2021-01-14 12:32:10,172 : INFO : token count processed\n",
      "2021-01-14 12:32:10,232 : INFO : frequencies processed\n",
      "2021-01-14 12:32:19,525 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:32:19,527 : INFO : entropies processed\n",
      "2021-01-14 12:32:19,527 : INFO : extropies processed\n",
      "2021-01-14 12:32:19,534 : INFO : token count processed\n",
      "2021-01-14 12:32:19,538 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:32:19,542 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:32:19,543 : INFO : vocab #32006\n",
      "2021-01-14 12:32:19,550 : INFO : diff #set()\n",
      "2021-01-14 12:32:37,932 : INFO : alphabet #32006\n",
      "2021-01-14 12:32:47,734 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.2250452933313696, 0.44942905342065453], [0.8965423554182053, 0.103457645], [1.5, 1.1225562489182657], [4.898378365512006, 6.431978396403875, 6.688954477758615, 4.641402284157265, 1.790576112246609, 0.2569760813547397]]\n",
      "2021-01-14 12:32:47,738 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:32:47,739 : INFO : built Dictionary(229 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 699 corpus positions)\n",
      "2021-01-14 12:32:47,912 : INFO : token count processed\n",
      "2021-01-14 12:32:47,945 : INFO : frequencies processed\n",
      "2021-01-14 12:32:57,316 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:32:57,317 : INFO : entropies processed\n",
      "2021-01-14 12:32:57,318 : INFO : extropies processed\n",
      "2021-01-14 12:32:57,325 : INFO : token count processed\n",
      "2021-01-14 12:32:57,329 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:32:57,333 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:32:57,335 : INFO : vocab #32006\n",
      "2021-01-14 12:32:57,341 : INFO : diff #set()\n",
      "2021-01-14 12:33:16,084 : INFO : alphabet #32006\n",
      "2021-01-14 12:33:25,384 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/test_auth_utility.py')[[1.2200149390618877, 0.4504474192514084], [0.8746225833892822, 0.12537742], [2.5216406363433186, 1.2998438251349493], [4.898378365512006, 6.911818353685893, 7.115900857711633, 4.694295861486266, 2.2175224921996275, 0.2040825040257399]]\n",
      "2021-01-14 12:33:25,397 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:33:25,400 : INFO : built Dictionary(317 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 7229 corpus positions)\n",
      "2021-01-14 12:33:25,678 : INFO : token count processed\n",
      "2021-01-14 12:33:25,716 : INFO : frequencies processed\n",
      "2021-01-14 12:33:34,866 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:33:34,868 : INFO : entropies processed\n",
      "2021-01-14 12:33:34,868 : INFO : extropies processed\n",
      "2021-01-14 12:33:34,883 : INFO : token count processed\n",
      "2021-01-14 12:33:34,887 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:33:34,891 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:33:34,892 : INFO : vocab #32006\n",
      "2021-01-14 12:33:34,898 : INFO : diff #set()\n",
      "2021-01-14 12:33:53,311 : INFO : alphabet #32006\n",
      "2021-01-14 12:34:02,858 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.1963003144014708, 0.4553111400307371], [0.8223254084587097, 0.17767459], [2.9852281360342516, 1.3330291169122896], [4.898378365512006, 6.363791471162389, 6.409791419277951, 4.852378417396444, 1.5114130537659456, 0.045999948115562184]]\n",
      "2021-01-14 12:34:02,862 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:34:02,864 : INFO : built Dictionary(222 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 1299 corpus positions)\n",
      "2021-01-14 12:34:03,034 : INFO : token count processed\n",
      "2021-01-14 12:34:03,068 : INFO : frequencies processed\n",
      "2021-01-14 12:34:12,336 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:34:12,337 : INFO : entropies processed\n",
      "2021-01-14 12:34:12,338 : INFO : extropies processed\n",
      "2021-01-14 12:34:12,345 : INFO : token count processed\n",
      "2021-01-14 12:34:12,349 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:34:12,353 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:34:12,355 : INFO : vocab #32006\n",
      "2021-01-14 12:34:12,361 : INFO : diff #set()\n",
      "2021-01-14 12:34:30,866 : INFO : alphabet #32006\n",
      "2021-01-14 12:34:40,039 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.2086219435345664, 0.45277101539598524], [0.8965624049305916, 0.103437595], [2.2516291673878226, 1.2667563532600834], [4.898378365512006, 6.29000629755059, 6.565174027419871, 4.623210635642725, 1.666795661907865, 0.2751677298692803]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 12:34:40,043 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:34:40,044 : INFO : built Dictionary(231 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 1252 corpus positions)\n",
      "2021-01-14 12:34:40,218 : INFO : token count processed\n",
      "2021-01-14 12:34:40,251 : INFO : frequencies processed\n",
      "2021-01-14 12:34:49,500 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:34:49,501 : INFO : entropies processed\n",
      "2021-01-14 12:34:49,501 : INFO : extropies processed\n",
      "2021-01-14 12:34:49,509 : INFO : token count processed\n",
      "2021-01-14 12:34:49,513 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:34:49,517 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:34:49,518 : INFO : vocab #32006\n",
      "2021-01-14 12:34:49,524 : INFO : diff #set()\n",
      "2021-01-14 12:35:08,236 : INFO : alphabet #32006\n",
      "2021-01-14 12:35:17,413 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.1813892695426527, 0.4584234524128098], [0.8665198981761932, 0.1334801], [2.4193819456463714, 1.2761517340193214], [4.898378365512006, 6.361621244785958, 6.612048536329555, 4.647951073968409, 1.713670170817549, 0.25042729154359655]]\n",
      "2021-01-14 12:35:17,418 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:35:17,420 : INFO : built Dictionary(240 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 1275 corpus positions)\n",
      "2021-01-14 12:35:17,592 : INFO : token count processed\n",
      "2021-01-14 12:35:17,655 : INFO : frequencies processed\n",
      "2021-01-14 12:35:26,982 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:35:26,983 : INFO : entropies processed\n",
      "2021-01-14 12:35:26,984 : INFO : extropies processed\n",
      "2021-01-14 12:35:26,991 : INFO : token count processed\n",
      "2021-01-14 12:35:26,996 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:35:27,000 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:35:27,001 : INFO : vocab #32006\n",
      "2021-01-14 12:35:27,008 : INFO : diff #set()\n",
      "2021-01-14 12:35:45,755 : INFO : alphabet #32006\n",
      "2021-01-14 12:35:55,139 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.207609611231909, 0.45297864029590423], [0.902411438524723, 0.09758856], [2.321928094887362, 1.2877123795494492], [4.898378365512006, 6.620594433343389, 6.885253590269333, 4.6337192085860615, 1.986875224757327, 0.26465915692594333]]\n",
      "2021-01-14 12:35:55,144 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:35:55,145 : INFO : built Dictionary(212 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 1368 corpus positions)\n",
      "2021-01-14 12:35:55,300 : INFO : token count processed\n",
      "2021-01-14 12:35:55,333 : INFO : frequencies processed\n",
      "2021-01-14 12:36:04,759 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:36:04,760 : INFO : entropies processed\n",
      "2021-01-14 12:36:04,761 : INFO : extropies processed\n",
      "2021-01-14 12:36:04,769 : INFO : token count processed\n",
      "2021-01-14 12:36:04,773 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:36:04,777 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:36:04,778 : INFO : vocab #32006\n",
      "2021-01-14 12:36:04,785 : INFO : diff #set()\n",
      "2021-01-14 12:36:23,456 : INFO : alphabet #32006\n",
      "2021-01-14 12:36:32,632 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.1875019411908154, 0.4571424514739529], [0.891071543097496, 0.10892846], [2.321928094887362, 1.2877123795494492], [4.898378365512006, 6.207411496248084, 6.39539955453356, 4.710390307226531, 1.4970211890215541, 0.18798805828547582]]\n",
      "2021-01-14 12:36:32,635 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:36:32,636 : INFO : built Dictionary(156 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 272 corpus positions)\n",
      "2021-01-14 12:36:32,740 : INFO : token count processed\n",
      "2021-01-14 12:36:32,769 : INFO : frequencies processed\n",
      "2021-01-14 12:36:42,039 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:36:42,040 : INFO : entropies processed\n",
      "2021-01-14 12:36:42,041 : INFO : extropies processed\n",
      "2021-01-14 12:36:42,048 : INFO : token count processed\n",
      "2021-01-14 12:36:42,053 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:36:42,059 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:36:42,060 : INFO : vocab #32006\n",
      "2021-01-14 12:36:42,066 : INFO : diff #set()\n",
      "2021-01-14 12:37:00,720 : INFO : alphabet #32006\n",
      "2021-01-14 12:37:10,176 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.2479110878306732, 0.4448574525094056], [0.9215857088565826, 0.07841429], [0.0, 0.0], [4.898378365512006, 6.5805228788529595, 6.934026034157445, 4.544875210207521, 2.0356476686454394, 0.3535031553044856]]\n",
      "2021-01-14 12:37:10,181 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 12:37:10,182 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:37:10,183 : INFO : built Dictionary(237 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 1233 corpus positions)\n",
      "2021-01-14 12:37:10,365 : INFO : token count processed\n",
      "2021-01-14 12:37:10,396 : INFO : frequencies processed\n",
      "2021-01-14 12:37:19,567 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:37:19,568 : INFO : entropies processed\n",
      "2021-01-14 12:37:19,569 : INFO : extropies processed\n",
      "2021-01-14 12:37:19,576 : INFO : token count processed\n",
      "2021-01-14 12:37:19,582 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:37:19,586 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:37:19,587 : INFO : vocab #32006\n",
      "2021-01-14 12:37:19,594 : INFO : diff #set()\n",
      "2021-01-14 12:37:37,975 : INFO : alphabet #32006\n",
      "2021-01-14 12:37:47,311 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.215738990897671, 0.45131669574260913], [0.8693517297506332, 0.13064827], [1.3709505944546687, 1.0438561897747245], [4.898378365512006, 6.422089779976135, 6.5824435765590525, 4.738024568929088, 1.6840652110470469, 0.1603537965829176]]\n",
      "2021-01-14 12:37:47,316 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:37:47,318 : INFO : built Dictionary(248 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 1515 corpus positions)\n",
      "2021-01-14 12:37:47,500 : INFO : token count processed\n",
      "2021-01-14 12:37:47,564 : INFO : frequencies processed\n",
      "2021-01-14 12:37:56,975 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:37:56,976 : INFO : entropies processed\n",
      "2021-01-14 12:37:56,977 : INFO : extropies processed\n",
      "2021-01-14 12:37:56,991 : INFO : token count processed\n",
      "2021-01-14 12:37:56,996 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:37:57,001 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:37:57,002 : INFO : vocab #32006\n",
      "2021-01-14 12:37:57,009 : INFO : diff #set()\n",
      "2021-01-14 12:38:15,929 : INFO : alphabet #32006\n",
      "2021-01-14 12:38:25,216 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.203005291225125, 0.4539253736625774], [0.910020150244236, 0.08997985], [2.0, 1.2451124978365313], [4.898378365512006, 6.485445644653597, 6.776125915570366, 4.607698094595237, 1.87774755005836, 0.29068027091676907]]\n",
      "2021-01-14 12:38:25,220 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:38:25,222 : INFO : built Dictionary(222 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 1401 corpus positions)\n",
      "2021-01-14 12:38:25,394 : INFO : token count processed\n",
      "2021-01-14 12:38:25,424 : INFO : frequencies processed\n",
      "2021-01-14 12:38:34,581 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:38:34,583 : INFO : entropies processed\n",
      "2021-01-14 12:38:34,583 : INFO : extropies processed\n",
      "2021-01-14 12:38:34,591 : INFO : token count processed\n",
      "2021-01-14 12:38:34,598 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:38:34,602 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:38:34,603 : INFO : vocab #32006\n",
      "2021-01-14 12:38:34,610 : INFO : diff #set()\n",
      "2021-01-14 12:38:53,232 : INFO : alphabet #32006\n",
      "2021-01-14 12:39:02,611 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.1879904508095398, 0.4570403859075379], [0.8935823366045952, 0.10641766], [2.321928094887362, 1.2877123795494492], [4.898378365512006, 6.2276600107346916, 6.411568900770014, 4.714469475476683, 1.5131905352580084, 0.18390889003532251]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 12:39:02,616 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:39:02,618 : INFO : built Dictionary(210 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 1599 corpus positions)\n",
      "2021-01-14 12:39:02,766 : INFO : token count processed\n",
      "2021-01-14 12:39:02,794 : INFO : frequencies processed\n",
      "2021-01-14 12:39:11,991 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:39:11,992 : INFO : entropies processed\n",
      "2021-01-14 12:39:11,993 : INFO : extropies processed\n",
      "2021-01-14 12:39:12,000 : INFO : token count processed\n",
      "2021-01-14 12:39:12,006 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:39:12,010 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:39:12,011 : INFO : vocab #32006\n",
      "2021-01-14 12:39:12,018 : INFO : diff #set()\n",
      "2021-01-14 12:39:30,479 : INFO : alphabet #32006\n",
      "2021-01-14 12:39:39,665 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.2228525151049257, 0.4498724018821361], [0.9043093025684357, 0.0956907], [2.6464393446710153, 1.3017576173934458], [4.898378365512006, 6.253918170574241, 6.4769919550898, 4.675304580996447, 1.5786135895777944, 0.22307378451555948]]\n",
      "2021-01-14 12:39:39,669 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:39:39,670 : INFO : built Dictionary(186 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 696 corpus positions)\n",
      "2021-01-14 12:39:39,788 : INFO : token count processed\n",
      "2021-01-14 12:39:39,820 : INFO : frequencies processed\n",
      "2021-01-14 12:39:49,223 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:39:49,224 : INFO : entropies processed\n",
      "2021-01-14 12:39:49,225 : INFO : extropies processed\n",
      "2021-01-14 12:39:49,231 : INFO : token count processed\n",
      "2021-01-14 12:39:49,236 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:39:49,240 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:39:49,241 : INFO : vocab #32006\n",
      "2021-01-14 12:39:49,249 : INFO : diff #set()\n",
      "2021-01-14 12:40:07,825 : INFO : alphabet #32006\n",
      "2021-01-14 12:40:17,254 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.1996908900766012, 0.4546093292067852], [0.8903517425060272, 0.10964826], [1.584962500721156, 1.1699250014423124], [4.898378365512006, 6.374522245625576, 6.669094902914818, 4.6038057082227635, 1.7707165374028122, 0.2945726572892422]]\n",
      "2021-01-14 12:40:17,259 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:40:17,261 : INFO : built Dictionary(300 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 1978 corpus positions)\n",
      "2021-01-14 12:40:17,516 : INFO : token count processed\n",
      "2021-01-14 12:40:17,553 : INFO : frequencies processed\n",
      "2021-01-14 12:40:26,727 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:40:26,728 : INFO : entropies processed\n",
      "2021-01-14 12:40:26,729 : INFO : extropies processed\n",
      "2021-01-14 12:40:26,742 : INFO : token count processed\n",
      "2021-01-14 12:40:26,746 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:40:26,749 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:40:26,750 : INFO : vocab #32006\n",
      "2021-01-14 12:40:26,756 : INFO : diff #set()\n",
      "2021-01-14 12:40:45,475 : INFO : alphabet #32006\n",
      "2021-01-14 12:40:55,058 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.1676302247367825, 0.46133329780517845], [0.7918177843093872, 0.20818222], [3.2806390622295662, 1.3536704341564332], [4.898378365512006, 6.731238669067808, 7.0105082039402085, 4.619108830639604, 2.112129838428203, 0.2792695348724008]]\n",
      "2021-01-14 12:40:55,062 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:40:55,065 : INFO : built Dictionary(229 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 1370 corpus positions)\n",
      "2021-01-14 12:40:55,237 : INFO : token count processed\n",
      "2021-01-14 12:40:55,276 : INFO : frequencies processed\n",
      "2021-01-14 12:41:04,610 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:41:04,612 : INFO : entropies processed\n",
      "2021-01-14 12:41:04,612 : INFO : extropies processed\n",
      "2021-01-14 12:41:04,620 : INFO : token count processed\n",
      "2021-01-14 12:41:04,626 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:41:04,630 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:41:04,631 : INFO : vocab #32006\n",
      "2021-01-14 12:41:04,638 : INFO : diff #set()\n",
      "2021-01-14 12:41:23,056 : INFO : alphabet #32006\n",
      "2021-01-14 12:41:32,322 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.1904913482253252, 0.45651858009399215], [0.8478531837463379, 0.15214682], [2.6464393446710153, 1.3017576173934458], [4.898378365512006, 6.503741451859337, 6.730992240685046, 4.671127576686297, 1.83261387517304, 0.22725078882570848]]\n",
      "2021-01-14 12:41:32,328 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:41:32,329 : INFO : built Dictionary(252 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 1692 corpus positions)\n",
      "2021-01-14 12:41:32,532 : INFO : token count processed\n",
      "2021-01-14 12:41:32,588 : INFO : frequencies processed\n",
      "2021-01-14 12:41:41,935 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:41:41,936 : INFO : entropies processed\n",
      "2021-01-14 12:41:41,937 : INFO : extropies processed\n",
      "2021-01-14 12:41:41,944 : INFO : token count processed\n",
      "2021-01-14 12:41:41,948 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:41:41,954 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:41:41,955 : INFO : vocab #32006\n",
      "2021-01-14 12:41:41,963 : INFO : diff #set()\n",
      "2021-01-14 12:42:00,415 : INFO : alphabet #32006\n",
      "2021-01-14 12:42:09,712 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.1899879577791876, 0.4566235154160734], [0.8883238509297371, 0.11167615], [2.4193819456463714, 1.2761517340193214], [4.898378365512006, 6.334729224484471, 6.503177136960814, 4.729930453035663, 1.6047987714488086, 0.16844791247634294]]\n",
      "2021-01-14 12:42:09,717 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:42:09,719 : INFO : built Dictionary(250 unique tokens: ['.', '03', '06', '2', '20']...) from 2 documents (total 2041 corpus positions)\n",
      "2021-01-14 12:42:09,901 : INFO : token count processed\n",
      "2021-01-14 12:42:09,963 : INFO : frequencies processed\n",
      "2021-01-14 12:42:19,135 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:42:19,136 : INFO : entropies processed\n",
      "2021-01-14 12:42:19,137 : INFO : extropies processed\n",
      "2021-01-14 12:42:19,151 : INFO : token count processed\n",
      "2021-01-14 12:42:19,156 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:42:19,161 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:42:19,162 : INFO : vocab #32006\n",
      "2021-01-14 12:42:19,169 : INFO : diff #set()\n",
      "2021-01-14 12:42:37,744 : INFO : alphabet #32006\n",
      "2021-01-14 12:42:47,044 : INFO : Computed distances or similarities ('285', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.1921025993541758, 0.45618302733394595], [0.8986587524414062, 0.10134125], [2.321928094887362, 1.2877123795494492], [4.898378365512006, 6.21319712067992, 6.448184455893398, 4.663391030298528, 1.5498060903813924, 0.2349873352134777]]\n",
      "2021-01-14 12:42:47,049 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 12:42:47,050 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:42:47,051 : INFO : built Dictionary(264 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1396 corpus positions)\n",
      "2021-01-14 12:42:47,098 : INFO : token count processed\n",
      "2021-01-14 12:42:47,126 : INFO : frequencies processed\n",
      "2021-01-14 12:42:56,303 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:42:56,304 : INFO : entropies processed\n",
      "2021-01-14 12:42:56,305 : INFO : extropies processed\n",
      "2021-01-14 12:42:56,312 : INFO : token count processed\n",
      "2021-01-14 12:42:56,316 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:42:56,320 : INFO : alphabet_target #32010\n",
      "2021-01-14 12:42:56,321 : INFO : vocab #32006\n",
      "2021-01-14 12:42:56,327 : INFO : diff #set()\n",
      "2021-01-14 12:43:14,828 : INFO : alphabet #32006\n",
      "2021-01-14 12:43:24,178 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.3130600309983116, 0.4323277332185807], [0.9817009679973125, 0.018299032], [0.0, 0.0], [2.321928094887362, 6.905617163738059, 7.002744581321759, 2.224800677303662, 4.680816486434397, 0.09712741758369958]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 12:43:24,184 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:43:24,186 : INFO : built Dictionary(355 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 2286 corpus positions)\n",
      "2021-01-14 12:43:24,267 : INFO : token count processed\n",
      "2021-01-14 12:43:24,299 : INFO : frequencies processed\n",
      "2021-01-14 12:43:33,735 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:43:33,736 : INFO : entropies processed\n",
      "2021-01-14 12:43:33,737 : INFO : extropies processed\n",
      "2021-01-14 12:43:33,744 : INFO : token count processed\n",
      "2021-01-14 12:43:33,751 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:43:33,754 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:43:33,755 : INFO : vocab #32006\n",
      "2021-01-14 12:43:33,762 : INFO : diff #set()\n",
      "2021-01-14 12:43:52,436 : INFO : alphabet #32006\n",
      "2021-01-14 12:44:01,619 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.3125343686450435, 0.4324260056666394], [0.9785551242530346, 0.021444876], [0.0, 0.0], [2.321928094887362, 7.1219284286457345, 7.312129717957387, 2.131726805575709, 4.990201623070025, 0.19020128931165292]]\n",
      "2021-01-14 12:44:01,624 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:44:01,626 : INFO : built Dictionary(274 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 2269 corpus positions)\n",
      "2021-01-14 12:44:01,682 : INFO : token count processed\n",
      "2021-01-14 12:44:01,725 : INFO : frequencies processed\n",
      "2021-01-14 12:44:10,875 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:44:10,876 : INFO : entropies processed\n",
      "2021-01-14 12:44:10,877 : INFO : extropies processed\n",
      "2021-01-14 12:44:10,892 : INFO : token count processed\n",
      "2021-01-14 12:44:10,896 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:44:10,900 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:44:10,901 : INFO : vocab #32006\n",
      "2021-01-14 12:44:10,907 : INFO : diff #set()\n",
      "2021-01-14 12:44:29,635 : INFO : alphabet #32006\n",
      "2021-01-14 12:44:38,841 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.3159035201069855, 0.43179691697770034], [0.9822703804820776, 0.01772962], [0.0, 0.0], [2.321928094887362, 6.41099024988467, 6.450684613887018, 2.2822337308850145, 4.128756518999657, 0.03969436400234816]]\n",
      "2021-01-14 12:44:38,844 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:44:38,846 : INFO : built Dictionary(152 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 668 corpus positions)\n",
      "2021-01-14 12:44:38,879 : INFO : token count processed\n",
      "2021-01-14 12:44:38,907 : INFO : frequencies processed\n",
      "2021-01-14 12:44:48,062 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:44:48,063 : INFO : entropies processed\n",
      "2021-01-14 12:44:48,063 : INFO : extropies processed\n",
      "2021-01-14 12:44:48,070 : INFO : token count processed\n",
      "2021-01-14 12:44:48,078 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:44:48,082 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:44:48,083 : INFO : vocab #32006\n",
      "2021-01-14 12:44:48,089 : INFO : diff #set()\n",
      "2021-01-14 12:45:06,628 : INFO : alphabet #32006\n",
      "2021-01-14 12:45:17,587 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.3094316565939739, 0.4330069682489903], [0.9789578150957823, 0.021042185], [0.0, 0.0], [2.321928094887362, 6.077866832717642, 6.143740628684805, 2.2560542989201995, 3.8218125337974427, 0.06587379596716314]]\n",
      "2021-01-14 12:45:17,590 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:45:17,595 : INFO : built Dictionary(128 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 433 corpus positions)\n",
      "2021-01-14 12:45:17,634 : INFO : token count processed\n",
      "2021-01-14 12:45:17,697 : INFO : frequencies processed\n",
      "2021-01-14 12:45:34,568 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:45:34,571 : INFO : entropies processed\n",
      "2021-01-14 12:45:34,573 : INFO : extropies processed\n",
      "2021-01-14 12:45:34,583 : INFO : token count processed\n",
      "2021-01-14 12:45:34,589 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:45:34,596 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:45:34,599 : INFO : vocab #32006\n",
      "2021-01-14 12:45:34,608 : INFO : diff #set()\n",
      "2021-01-14 12:46:03,609 : INFO : alphabet #32006\n",
      "2021-01-14 12:46:13,517 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.3082593034407675, 0.43322689028453915], [0.97875458560884, 0.021245414], [0.0, 0.0], [2.321928094887362, 5.977547459003844, 6.009545938780356, 2.28992961511085, 3.6876178438929936, 0.0319984797765116]]\n",
      "2021-01-14 12:46:13,524 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:46:13,527 : INFO : built Dictionary(233 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 2144 corpus positions)\n",
      "2021-01-14 12:46:13,567 : INFO : token count processed\n",
      "2021-01-14 12:46:13,599 : INFO : frequencies processed\n",
      "2021-01-14 12:46:23,489 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:46:23,490 : INFO : entropies processed\n",
      "2021-01-14 12:46:23,491 : INFO : extropies processed\n",
      "2021-01-14 12:46:23,501 : INFO : token count processed\n",
      "2021-01-14 12:46:23,506 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:46:23,510 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:46:23,511 : INFO : vocab #32006\n",
      "2021-01-14 12:46:23,517 : INFO : diff #set()\n",
      "2021-01-14 12:46:43,568 : INFO : alphabet #32006\n",
      "2021-01-14 12:46:53,764 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.3097808489152447, 0.4329415063206692], [0.975366372615099, 0.024633627], [0.0, 0.0], [2.321928094887362, 6.4614394051846435, 6.531163744228806, 2.2522037558431993, 4.209235649341444, 0.06972433904416242]]\n",
      "2021-01-14 12:46:53,768 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 12:46:53,769 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:46:53,773 : INFO : built Dictionary(196 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1368 corpus positions)\n",
      "2021-01-14 12:46:53,839 : INFO : token count processed\n",
      "2021-01-14 12:46:53,914 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 12:46:53,915 : INFO : frequencies processed\n",
      "2021-01-14 12:46:53,918 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 12:46:53,924 : INFO : token count processed\n",
      "2021-01-14 12:46:53,930 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:46:53,935 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:46:53,936 : INFO : vocab #32006\n",
      "2021-01-14 12:46:53,945 : INFO : diff #set()\n",
      "2021-01-14 12:47:13,970 : INFO : alphabet #32006\n",
      "2021-01-14 12:47:23,959 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.3175227199001236, 0.4314952304083975], [0.98545085452497, 0.0145491455], [nan, nan], [2.321928094887362, 6.327195724598159, 6.396420989123202, 2.2527028303623187, 4.07449289423584, 0.06922526452504307]]\n",
      "2021-01-14 12:47:23,971 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 12:47:23,972 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:47:23,977 : INFO : built Dictionary(411 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 6260 corpus positions)\n",
      "2021-01-14 12:47:24,080 : INFO : token count processed\n",
      "2021-01-14 12:47:24,153 : INFO : frequencies processed\n",
      "2021-01-14 12:47:34,103 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:47:34,104 : INFO : entropies processed\n",
      "2021-01-14 12:47:34,105 : INFO : extropies processed\n",
      "2021-01-14 12:47:34,114 : INFO : token count processed\n",
      "2021-01-14 12:47:34,118 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:47:34,122 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:47:34,123 : INFO : vocab #32006\n",
      "2021-01-14 12:47:34,129 : INFO : diff #set()\n",
      "2021-01-14 12:47:53,766 : INFO : alphabet #32006\n",
      "2021-01-14 12:48:03,829 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.3148104132985146, 0.43200082143013996], [0.9836121182888746, 0.016387882], [0.0, 0.0], [2.321928094887362, 6.9079058562486315, 7.002360750083977, 2.2274732010520175, 4.680432655196615, 0.09445489383534511]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 12:48:03,835 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:48:03,837 : INFO : built Dictionary(313 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 2654 corpus positions)\n",
      "2021-01-14 12:48:03,896 : INFO : token count processed\n",
      "2021-01-14 12:48:03,934 : INFO : frequencies processed\n",
      "2021-01-14 12:48:13,881 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:48:13,882 : INFO : entropies processed\n",
      "2021-01-14 12:48:13,883 : INFO : extropies processed\n",
      "2021-01-14 12:48:13,893 : INFO : token count processed\n",
      "2021-01-14 12:48:13,898 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:48:13,904 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:48:13,905 : INFO : vocab #32006\n",
      "2021-01-14 12:48:13,914 : INFO : diff #set()\n",
      "2021-01-14 12:48:34,342 : INFO : alphabet #32006\n",
      "2021-01-14 12:48:44,372 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.318224445499102, 0.4313646169772423], [0.9860633853822947, 0.013936615], [0.0, 0.0], [2.321928094887362, 6.61034830706307, 6.7404922799697005, 2.1917841219807315, 4.418564185082339, 0.13014397290663027]]\n",
      "2021-01-14 12:48:44,376 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:48:44,378 : INFO : built Dictionary(199 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 701 corpus positions)\n",
      "2021-01-14 12:48:44,411 : INFO : token count processed\n",
      "2021-01-14 12:48:44,444 : INFO : frequencies processed\n",
      "2021-01-14 12:48:54,449 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:48:54,450 : INFO : entropies processed\n",
      "2021-01-14 12:48:54,451 : INFO : extropies processed\n",
      "2021-01-14 12:48:54,462 : INFO : token count processed\n",
      "2021-01-14 12:48:54,466 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:48:54,471 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:48:54,472 : INFO : vocab #32006\n",
      "2021-01-14 12:48:54,478 : INFO : diff #set()\n",
      "2021-01-14 12:49:14,637 : INFO : alphabet #32006\n",
      "2021-01-14 12:49:24,278 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.3183501718050734, 0.43134122366915667], [0.9862783085554838, 0.013721691], [0.0, 0.0], [2.321928094887362, 6.616715366949855, 6.6918630523218425, 2.246780409515374, 4.369934957434481, 0.0751476853719879]]\n",
      "2021-01-14 12:49:24,284 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:49:24,289 : INFO : built Dictionary(419 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 2735 corpus positions)\n",
      "2021-01-14 12:49:24,378 : INFO : token count processed\n",
      "2021-01-14 12:49:24,412 : INFO : frequencies processed\n",
      "2021-01-14 12:49:34,301 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:49:34,302 : INFO : entropies processed\n",
      "2021-01-14 12:49:34,303 : INFO : extropies processed\n",
      "2021-01-14 12:49:34,313 : INFO : token count processed\n",
      "2021-01-14 12:49:34,318 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:49:34,322 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:49:34,323 : INFO : vocab #32006\n",
      "2021-01-14 12:49:34,329 : INFO : diff #set()\n",
      "2021-01-14 12:49:54,592 : INFO : alphabet #32006\n",
      "2021-01-14 12:50:04,642 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.2988351056983751, 0.4350029271439218], [0.9490516372025013, 0.050948363], [0.0, 0.0], [2.321928094887362, 7.32185870753746, 7.49886531821422, 2.1449214842106024, 5.176937223326858, 0.17700661067675938]]\n",
      "2021-01-14 12:50:04,645 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:50:04,648 : INFO : built Dictionary(53 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 100 corpus positions)\n",
      "2021-01-14 12:50:04,660 : INFO : token count processed\n",
      "2021-01-14 12:50:04,688 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 12:50:04,691 : INFO : frequencies processed\n",
      "2021-01-14 12:50:04,691 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 12:50:04,699 : INFO : token count processed\n",
      "2021-01-14 12:50:04,704 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:50:04,713 : INFO : alphabet_target #32008\n",
      "2021-01-14 12:50:04,714 : INFO : vocab #32006\n",
      "2021-01-14 12:50:04,722 : INFO : diff #set()\n",
      "2021-01-14 12:50:24,831 : INFO : alphabet #32006\n",
      "2021-01-14 12:50:34,761 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/fireException.py')[[1.31055175433856, 0.4327970572926073], [0.9899663608521223, 0.010033639], [nan, nan], [2.321928094887362, 5.176618657501385, 5.387797953594136, 2.110748798794612, 3.065869858706774, 0.21117929609275077]]\n",
      "2021-01-14 12:50:34,764 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:50:34,766 : INFO : built Dictionary(148 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 493 corpus positions)\n",
      "2021-01-14 12:50:34,793 : INFO : token count processed\n",
      "2021-01-14 12:50:34,826 : INFO : frequencies processed\n",
      "2021-01-14 12:50:44,502 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:50:44,503 : INFO : entropies processed\n",
      "2021-01-14 12:50:44,504 : INFO : extropies processed\n",
      "2021-01-14 12:50:44,513 : INFO : token count processed\n",
      "2021-01-14 12:50:44,517 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:50:44,522 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:50:44,522 : INFO : vocab #32006\n",
      "2021-01-14 12:50:44,529 : INFO : diff #set()\n",
      "2021-01-14 12:51:04,627 : INFO : alphabet #32006\n",
      "2021-01-14 12:51:14,594 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.3114541692241473, 0.4326280889816023], [0.9714191928505898, 0.028580807], [0.0, 0.0], [2.321928094887362, 6.468846789852156, 6.571487359734853, 2.219287525004665, 4.249559264847491, 0.10264056988269665]]\n",
      "2021-01-14 12:51:14,600 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:51:14,602 : INFO : built Dictionary(365 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 2542 corpus positions)\n",
      "2021-01-14 12:51:14,681 : INFO : token count processed\n",
      "2021-01-14 12:51:14,720 : INFO : frequencies processed\n",
      "2021-01-14 12:51:25,024 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:51:25,025 : INFO : entropies processed\n",
      "2021-01-14 12:51:25,026 : INFO : extropies processed\n",
      "2021-01-14 12:51:25,034 : INFO : token count processed\n",
      "2021-01-14 12:51:25,038 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:51:25,043 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:51:25,044 : INFO : vocab #32006\n",
      "2021-01-14 12:51:25,050 : INFO : diff #set()\n",
      "2021-01-14 12:51:44,654 : INFO : alphabet #32006\n",
      "2021-01-14 12:51:54,703 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.3161790006673868, 0.43174556012806375], [0.9805730953812599, 0.019426905], [0.0, 0.0], [2.321928094887362, 6.957796704012729, 7.071598500278961, 2.2081262986211296, 4.7496704053915995, 0.1138017962662321]]\n",
      "2021-01-14 12:51:54,709 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:51:54,711 : INFO : built Dictionary(274 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 3039 corpus positions)\n",
      "2021-01-14 12:51:54,762 : INFO : token count processed\n",
      "2021-01-14 12:51:54,798 : INFO : frequencies processed\n",
      "2021-01-14 12:52:06,749 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:52:06,750 : INFO : entropies processed\n",
      "2021-01-14 12:52:06,751 : INFO : extropies processed\n",
      "2021-01-14 12:52:06,759 : INFO : token count processed\n",
      "2021-01-14 12:52:06,763 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:52:06,768 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:52:06,769 : INFO : vocab #32006\n",
      "2021-01-14 12:52:06,775 : INFO : diff #set()\n",
      "2021-01-14 12:52:31,345 : INFO : alphabet #32006\n",
      "2021-01-14 12:52:43,042 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.3080728727989834, 0.43326188344621336], [0.9745293315500021, 0.025470668], [0.0, 0.0], [2.321928094887362, 6.441859572014148, 6.561524301290657, 2.202263365610854, 4.239596206403295, 0.11966472927650873]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 12:52:43,047 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:52:43,049 : INFO : built Dictionary(300 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1515 corpus positions)\n",
      "2021-01-14 12:52:43,106 : INFO : token count processed\n",
      "2021-01-14 12:52:43,139 : INFO : frequencies processed\n",
      "2021-01-14 12:52:56,007 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:52:56,008 : INFO : entropies processed\n",
      "2021-01-14 12:52:56,010 : INFO : extropies processed\n",
      "2021-01-14 12:52:56,019 : INFO : token count processed\n",
      "2021-01-14 12:52:56,023 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:52:56,028 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:52:56,029 : INFO : vocab #32006\n",
      "2021-01-14 12:52:56,036 : INFO : diff #set()\n",
      "2021-01-14 12:53:19,635 : INFO : alphabet #32006\n",
      "2021-01-14 12:53:30,820 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.3177173601712169, 0.4314589937429329], [0.9852229384705424, 0.0147770615], [0.0, 0.0], [2.321928094887362, 6.998955278238291, 7.12678976780116, 2.1940936053244933, 4.804861672913798, 0.12783448956286936]]\n",
      "2021-01-14 12:53:30,825 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:53:30,827 : INFO : built Dictionary(215 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1334 corpus positions)\n",
      "2021-01-14 12:53:30,886 : INFO : token count processed\n",
      "2021-01-14 12:53:30,920 : INFO : frequencies processed\n",
      "2021-01-14 12:53:40,883 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:53:40,884 : INFO : entropies processed\n",
      "2021-01-14 12:53:40,884 : INFO : extropies processed\n",
      "2021-01-14 12:53:40,894 : INFO : token count processed\n",
      "2021-01-14 12:53:40,899 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:53:40,903 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:53:40,904 : INFO : vocab #32006\n",
      "2021-01-14 12:53:40,910 : INFO : diff #set()\n",
      "2021-01-14 12:54:00,867 : INFO : alphabet #32006\n",
      "2021-01-14 12:54:10,698 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.3055004450830905, 0.43374530771949593], [0.9687572624534369, 0.031242738], [0.0, 0.0], [2.321928094887362, 6.492983191376071, 6.561250630301812, 2.253660655961621, 4.23932253541445, 0.06826743892574072]]\n",
      "2021-01-14 12:54:10,705 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:54:10,708 : INFO : built Dictionary(425 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 3273 corpus positions)\n",
      "2021-01-14 12:54:10,807 : INFO : token count processed\n",
      "2021-01-14 12:54:10,837 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 12:54:10,838 : INFO : frequencies processed\n",
      "2021-01-14 12:54:10,838 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 12:54:10,845 : INFO : token count processed\n",
      "2021-01-14 12:54:10,850 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:54:10,856 : INFO : alphabet_target #32008\n",
      "2021-01-14 12:54:10,858 : INFO : vocab #32006\n",
      "2021-01-14 12:54:10,866 : INFO : diff #set()\n",
      "2021-01-14 12:54:30,889 : INFO : alphabet #32006\n",
      "2021-01-14 12:54:41,050 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.327479783238495, 0.42964927437890915], [0.9969808175228536, 0.0030191825], [nan, nan], [2.321928094887362, 6.560342487747443, 6.7261474393837, 2.1561231432511043, 4.4042193444963385, 0.1658049516362574]]\n",
      "2021-01-14 12:54:41,058 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 12:54:41,059 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:54:41,064 : INFO : built Dictionary(441 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 3478 corpus positions)\n",
      "2021-01-14 12:54:41,160 : INFO : token count processed\n",
      "2021-01-14 12:54:41,194 : INFO : frequencies processed\n",
      "2021-01-14 12:54:51,283 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:54:51,284 : INFO : entropies processed\n",
      "2021-01-14 12:54:51,285 : INFO : extropies processed\n",
      "2021-01-14 12:54:51,294 : INFO : token count processed\n",
      "2021-01-14 12:54:51,299 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:54:51,303 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:54:51,304 : INFO : vocab #32006\n",
      "2021-01-14 12:54:51,310 : INFO : diff #set()\n",
      "2021-01-14 12:55:10,909 : INFO : alphabet #32006\n",
      "2021-01-14 12:55:20,933 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.3179606799191723, 0.4314137028566292], [0.9872317919507623, 0.012768208], [0.0, 0.0], [2.321928094887362, 7.046173750105238, 7.21421427088239, 2.1538875741102093, 4.892286175995029, 0.16804052077715248]]\n",
      "2021-01-14 12:55:20,944 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 12:55:20,945 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:55:20,950 : INFO : built Dictionary(495 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 5587 corpus positions)\n",
      "2021-01-14 12:55:21,079 : INFO : token count processed\n",
      "2021-01-14 12:55:21,115 : INFO : frequencies processed\n",
      "2021-01-14 12:55:31,043 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:55:31,044 : INFO : entropies processed\n",
      "2021-01-14 12:55:31,045 : INFO : extropies processed\n",
      "2021-01-14 12:55:31,056 : INFO : token count processed\n",
      "2021-01-14 12:55:31,061 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:55:31,065 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:55:31,066 : INFO : vocab #32006\n",
      "2021-01-14 12:55:31,072 : INFO : diff #set()\n",
      "2021-01-14 12:55:51,207 : INFO : alphabet #32006\n",
      "2021-01-14 12:56:01,192 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.3087484363633342, 0.43313510655806553], [0.9772187750786543, 0.022781225], [0.0, 0.0], [2.321928094887362, 7.009229588004272, 7.102600072712842, 2.2285576101787923, 4.780671977825479, 0.09337048470856946]]\n",
      "2021-01-14 12:56:01,205 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:56:01,209 : INFO : built Dictionary(573 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 6528 corpus positions)\n",
      "2021-01-14 12:56:01,349 : INFO : token count processed\n",
      "2021-01-14 12:56:01,383 : INFO : frequencies processed\n",
      "2021-01-14 12:56:11,292 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:56:11,293 : INFO : entropies processed\n",
      "2021-01-14 12:56:11,294 : INFO : extropies processed\n",
      "2021-01-14 12:56:11,307 : INFO : token count processed\n",
      "2021-01-14 12:56:11,311 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:56:11,316 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:56:11,317 : INFO : vocab #32006\n",
      "2021-01-14 12:56:11,323 : INFO : diff #set()\n",
      "2021-01-14 12:56:31,294 : INFO : alphabet #32006\n",
      "2021-01-14 12:56:40,876 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.3070777643470877, 0.4334487616558535], [0.9675108902156353, 0.03248911], [1.0, 1.0], [2.321928094887362, 7.376088004590871, 7.528272742550051, 2.1697433569281817, 5.206344647662689, 0.15218473795918008]]\n",
      "2021-01-14 12:56:40,879 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:56:40,880 : INFO : built Dictionary(125 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 314 corpus positions)\n",
      "2021-01-14 12:56:40,902 : INFO : token count processed\n",
      "2021-01-14 12:56:40,930 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 12:56:40,931 : INFO : frequencies processed\n",
      "2021-01-14 12:56:40,932 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 12:56:40,938 : INFO : token count processed\n",
      "2021-01-14 12:56:40,943 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:56:40,949 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:56:40,952 : INFO : vocab #32006\n",
      "2021-01-14 12:56:40,963 : INFO : diff #set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 12:57:01,212 : INFO : alphabet #32006\n",
      "2021-01-14 12:57:11,345 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.3108171547919911, 0.4327473499693728], [0.9803659003227949, 0.0196341], [nan, nan], [2.321928094887362, 6.2993628166120885, 6.382880450022341, 2.238410461477109, 4.0609523551349795, 0.08351763341025276]]\n",
      "2021-01-14 12:57:11,348 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:57:11,349 : INFO : built Dictionary(22 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 30 corpus positions)\n",
      "2021-01-14 12:57:11,360 : INFO : token count processed\n",
      "2021-01-14 12:57:11,389 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 12:57:11,390 : INFO : frequencies processed\n",
      "2021-01-14 12:57:11,391 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 12:57:11,397 : INFO : token count processed\n",
      "2021-01-14 12:57:11,401 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:57:11,406 : INFO : alphabet_target #32008\n",
      "2021-01-14 12:57:11,406 : INFO : vocab #32006\n",
      "2021-01-14 12:57:11,413 : INFO : diff #set()\n",
      "2021-01-14 12:57:31,618 : INFO : alphabet #32006\n",
      "2021-01-14 12:57:42,020 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.325617401467424, 0.4299933425717478], [0.9916656464338303, 0.008334354], [nan, nan], [2.321928094887362, 3.8936606896881862, 4.137537511266051, 2.078051273309497, 1.8156094163786887, 0.2438768215778646]]\n",
      "2021-01-14 12:57:42,043 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:57:42,048 : INFO : built Dictionary(732 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 12472 corpus positions)\n",
      "2021-01-14 12:57:42,337 : INFO : token count processed\n",
      "2021-01-14 12:57:42,411 : INFO : frequencies processed\n",
      "2021-01-14 12:57:52,390 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:57:52,392 : INFO : entropies processed\n",
      "2021-01-14 12:57:52,392 : INFO : extropies processed\n",
      "2021-01-14 12:57:52,404 : INFO : token count processed\n",
      "2021-01-14 12:57:52,409 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:57:52,413 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:57:52,414 : INFO : vocab #32006\n",
      "2021-01-14 12:57:52,421 : INFO : diff #set()\n",
      "2021-01-14 12:58:12,824 : INFO : alphabet #32006\n",
      "2021-01-14 12:58:22,811 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.2971400977785625, 0.435323906002531], [0.9559065103530884, 0.04409349], [1.0, 1.0], [2.321928094887362, 7.434393313070278, 7.632345032317157, 2.123976375640484, 5.310416937429794, 0.1979517192468787]]\n",
      "2021-01-14 12:58:22,820 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:58:22,822 : INFO : built Dictionary(484 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 4099 corpus positions)\n",
      "2021-01-14 12:58:22,930 : INFO : token count processed\n",
      "2021-01-14 12:58:22,960 : INFO : frequencies processed\n",
      "2021-01-14 12:58:32,908 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:58:32,910 : INFO : entropies processed\n",
      "2021-01-14 12:58:32,910 : INFO : extropies processed\n",
      "2021-01-14 12:58:32,921 : INFO : token count processed\n",
      "2021-01-14 12:58:32,926 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:58:32,930 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:58:32,931 : INFO : vocab #32006\n",
      "2021-01-14 12:58:32,938 : INFO : diff #set()\n",
      "2021-01-14 12:58:53,120 : INFO : alphabet #32006\n",
      "2021-01-14 12:59:03,016 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.3113423577856698, 0.4326490174125601], [0.9752170871943235, 0.024782913], [0.0, 0.0], [2.321928094887362, 7.2991514951718255, 7.472592326814857, 2.1484872632443306, 5.150664231927495, 0.17344083164303115]]\n",
      "2021-01-14 12:59:03,024 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:59:03,026 : INFO : built Dictionary(450 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 3516 corpus positions)\n",
      "2021-01-14 12:59:03,124 : INFO : token count processed\n",
      "2021-01-14 12:59:03,156 : INFO : frequencies processed\n",
      "2021-01-14 12:59:13,125 : INFO : scalar_distribution processed\n",
      "2021-01-14 12:59:13,126 : INFO : entropies processed\n",
      "2021-01-14 12:59:13,127 : INFO : extropies processed\n",
      "2021-01-14 12:59:13,136 : INFO : token count processed\n",
      "2021-01-14 12:59:13,141 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:59:13,145 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:59:13,146 : INFO : vocab #32006\n",
      "2021-01-14 12:59:13,152 : INFO : diff #set()\n",
      "2021-01-14 12:59:33,397 : INFO : alphabet #32006\n",
      "2021-01-14 12:59:43,409 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.3160923955796742, 0.4317617042862916], [0.9825155716389418, 0.017484428], [0.0, 0.0], [2.321928094887362, 7.170319527000998, 7.335262309514246, 2.156985312374114, 5.013334214626884, 0.1649427825132488]]\n",
      "2021-01-14 12:59:43,413 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 12:59:43,414 : INFO : built Dictionary(159 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 552 corpus positions)\n",
      "2021-01-14 12:59:43,443 : INFO : token count processed\n",
      "2021-01-14 12:59:43,471 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 12:59:43,472 : INFO : frequencies processed\n",
      "2021-01-14 12:59:43,473 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 12:59:43,479 : INFO : token count processed\n",
      "2021-01-14 12:59:43,484 : INFO : alphabet_source #32006\n",
      "2021-01-14 12:59:43,488 : INFO : alphabet_target #32009\n",
      "2021-01-14 12:59:43,489 : INFO : vocab #32006\n",
      "2021-01-14 12:59:43,496 : INFO : diff #set()\n",
      "2021-01-14 13:00:03,442 : INFO : alphabet #32006\n",
      "2021-01-14 13:00:13,343 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.3151431563796492, 0.43193873227423646], [0.9850262049585581, 0.014973795], [nan, nan], [2.321928094887362, 6.353654804387375, 6.464294558709746, 2.211288340564991, 4.142366463822384, 0.11063975432237072]]\n",
      "2021-01-14 13:00:13,347 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:00:13,348 : INFO : built Dictionary(154 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 680 corpus positions)\n",
      "2021-01-14 13:00:13,375 : INFO : token count processed\n",
      "2021-01-14 13:00:13,404 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 13:00:13,405 : INFO : frequencies processed\n",
      "2021-01-14 13:00:13,406 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 13:00:13,412 : INFO : token count processed\n",
      "2021-01-14 13:00:13,417 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:00:13,422 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:00:13,423 : INFO : vocab #32006\n",
      "2021-01-14 13:00:13,429 : INFO : diff #set()\n",
      "2021-01-14 13:00:33,864 : INFO : alphabet #32006\n",
      "2021-01-14 13:00:44,177 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.3162231698681985, 0.43173732695926], [0.984834142960608, 0.015165857], [nan, nan], [2.321928094887362, 6.245180322479091, 6.327772618595407, 2.2393357987710463, 4.005844523708046, 0.08259229611631635]]\n",
      "2021-01-14 13:00:44,183 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:00:44,185 : INFO : built Dictionary(382 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1752 corpus positions)\n",
      "2021-01-14 13:00:44,266 : INFO : token count processed\n",
      "2021-01-14 13:00:44,299 : INFO : frequencies processed\n",
      "2021-01-14 13:00:54,966 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:00:54,967 : INFO : entropies processed\n",
      "2021-01-14 13:00:54,968 : INFO : extropies processed\n",
      "2021-01-14 13:00:54,976 : INFO : token count processed\n",
      "2021-01-14 13:00:54,980 : INFO : alphabet_source #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 13:00:54,984 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:00:54,985 : INFO : vocab #32006\n",
      "2021-01-14 13:00:54,992 : INFO : diff #set()\n",
      "2021-01-14 13:01:15,677 : INFO : alphabet #32006\n",
      "2021-01-14 13:01:26,583 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.30773914613496, 0.43332453829316747], [0.9736163951456547, 0.026383605], [0.0, 0.0], [2.321928094887362, 7.2691387000368, 7.442130586864413, 2.148936208059749, 5.12020249197705, 0.1729918868276128]]\n",
      "2021-01-14 13:01:26,589 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:01:26,590 : INFO : built Dictionary(308 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1589 corpus positions)\n",
      "2021-01-14 13:01:26,653 : INFO : token count processed\n",
      "2021-01-14 13:01:26,687 : INFO : frequencies processed\n",
      "2021-01-14 13:01:38,280 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:01:38,281 : INFO : entropies processed\n",
      "2021-01-14 13:01:38,282 : INFO : extropies processed\n",
      "2021-01-14 13:01:38,290 : INFO : token count processed\n",
      "2021-01-14 13:01:38,294 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:01:38,299 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:01:38,299 : INFO : vocab #32006\n",
      "2021-01-14 13:01:38,317 : INFO : diff #set()\n",
      "2021-01-14 13:02:00,816 : INFO : alphabet #32006\n",
      "2021-01-14 13:02:12,156 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.3034218537304294, 0.4341367163728535], [0.9726151898503304, 0.02738481], [0.0, 0.0], [2.321928094887362, 7.08857858466988, 7.220890461601528, 2.189616217955714, 4.898962366714166, 0.1323118769316487]]\n",
      "2021-01-14 13:02:12,160 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:02:12,161 : INFO : built Dictionary(133 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 512 corpus positions)\n",
      "2021-01-14 13:02:12,186 : INFO : token count processed\n",
      "2021-01-14 13:02:12,217 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 13:02:12,221 : INFO : frequencies processed\n",
      "2021-01-14 13:02:12,223 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 13:02:12,235 : INFO : token count processed\n",
      "2021-01-14 13:02:12,243 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:02:12,250 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:02:12,254 : INFO : vocab #32006\n",
      "2021-01-14 13:02:12,263 : INFO : diff #set()\n",
      "2021-01-14 13:02:37,060 : INFO : alphabet #32006\n",
      "2021-01-14 13:02:47,069 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.314322657508218, 0.4320918678973997], [0.9828786831349134, 0.017121317], [nan, nan], [2.321928094887362, 6.0479231618016716, 6.1390709768012615, 2.230780279887772, 3.8171428819138993, 0.0911478149995899]]\n",
      "2021-01-14 13:02:47,073 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:02:47,074 : INFO : built Dictionary(135 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 631 corpus positions)\n",
      "2021-01-14 13:02:47,104 : INFO : token count processed\n",
      "2021-01-14 13:02:47,134 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 13:02:47,137 : INFO : frequencies processed\n",
      "2021-01-14 13:02:47,139 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 13:02:47,146 : INFO : token count processed\n",
      "2021-01-14 13:02:47,153 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:02:47,159 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:02:47,161 : INFO : vocab #32006\n",
      "2021-01-14 13:02:47,170 : INFO : diff #set()\n",
      "2021-01-14 13:03:08,132 : INFO : alphabet #32006\n",
      "2021-01-14 13:03:18,347 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.3160917367314828, 0.4317618271076002], [0.9839752092957497, 0.01602479], [nan, nan], [2.321928094887362, 6.036583168403119, 6.112654132672085, 2.245857130618396, 3.7907260377847227, 0.07607096426896565]]\n",
      "2021-01-14 13:03:18,362 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 13:03:18,362 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:03:18,366 : INFO : built Dictionary(549 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 6989 corpus positions)\n",
      "2021-01-14 13:03:18,561 : INFO : token count processed\n",
      "2021-01-14 13:03:18,643 : INFO : frequencies processed\n",
      "2021-01-14 13:03:28,810 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:03:28,811 : INFO : entropies processed\n",
      "2021-01-14 13:03:28,812 : INFO : extropies processed\n",
      "2021-01-14 13:03:28,826 : INFO : token count processed\n",
      "2021-01-14 13:03:28,831 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:03:28,838 : INFO : alphabet_target #32010\n",
      "2021-01-14 13:03:28,839 : INFO : vocab #32006\n",
      "2021-01-14 13:03:28,846 : INFO : diff #set()\n",
      "2021-01-14 13:03:49,152 : INFO : alphabet #32006\n",
      "2021-01-14 13:03:59,099 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.3112413227379882, 0.432667930502108], [0.9805585145950317, 0.019441485], [0.0, 0.0], [2.321928094887362, 7.29352035514053, 7.438975104948532, 2.176473345079361, 5.117047010061169, 0.14545474980800144]]\n",
      "2021-01-14 13:03:59,106 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:03:59,109 : INFO : built Dictionary(358 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 3232 corpus positions)\n",
      "2021-01-14 13:03:59,178 : INFO : token count processed\n",
      "2021-01-14 13:03:59,211 : INFO : frequencies processed\n",
      "2021-01-14 13:04:09,168 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:04:09,169 : INFO : entropies processed\n",
      "2021-01-14 13:04:09,169 : INFO : extropies processed\n",
      "2021-01-14 13:04:09,178 : INFO : token count processed\n",
      "2021-01-14 13:04:09,182 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:04:09,186 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:04:09,187 : INFO : vocab #32006\n",
      "2021-01-14 13:04:09,193 : INFO : diff #set()\n",
      "2021-01-14 13:04:28,958 : INFO : alphabet #32006\n",
      "2021-01-14 13:04:39,019 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.3164119964801857, 0.43170213309182964], [0.9842729512602091, 0.015727049], [0.0, 0.0], [2.321928094887362, 6.8153433747477745, 6.933772381014175, 2.203499088620961, 4.6118442861268125, 0.11842900626640063]]\n",
      "2021-01-14 13:04:39,022 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:04:39,024 : INFO : built Dictionary(110 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 253 corpus positions)\n",
      "2021-01-14 13:04:39,043 : INFO : token count processed\n",
      "2021-01-14 13:04:39,070 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 13:04:39,071 : INFO : frequencies processed\n",
      "2021-01-14 13:04:39,072 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 13:04:39,078 : INFO : token count processed\n",
      "2021-01-14 13:04:39,082 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:04:39,087 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:04:39,088 : INFO : vocab #32006\n",
      "2021-01-14 13:04:39,094 : INFO : diff #set()\n",
      "2021-01-14 13:04:59,546 : INFO : alphabet #32006\n",
      "2021-01-14 13:05:09,448 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.3091732469650295, 0.43305542419318716], [0.9784261807799339, 0.02157382], [nan, nan], [2.321928094887362, 6.150121915859574, 6.212944018561091, 2.2591059921858463, 3.8910159236737285, 0.06282210270151634]]\n",
      "2021-01-14 13:05:09,452 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:05:09,457 : INFO : built Dictionary(274 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 964 corpus positions)\n",
      "2021-01-14 13:05:09,524 : INFO : token count processed\n",
      "2021-01-14 13:05:09,559 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 13:05:19,232 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:05:19,233 : INFO : entropies processed\n",
      "2021-01-14 13:05:19,234 : INFO : extropies processed\n",
      "2021-01-14 13:05:19,245 : INFO : token count processed\n",
      "2021-01-14 13:05:19,252 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:05:19,256 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:05:19,257 : INFO : vocab #32006\n",
      "2021-01-14 13:05:19,264 : INFO : diff #set()\n",
      "2021-01-14 13:05:39,374 : INFO : alphabet #32006\n",
      "2021-01-14 13:05:49,302 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.295040738401491, 0.43572211301857133], [0.9566172696650028, 0.04338273], [0.0, 0.0], [2.321928094887362, 7.0391145208191315, 7.230592501203669, 2.130450114502824, 4.908664406316307, 0.1914779803845379]]\n",
      "2021-01-14 13:05:49,311 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 13:05:49,312 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:05:49,318 : INFO : built Dictionary(577 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 4339 corpus positions)\n",
      "2021-01-14 13:05:49,474 : INFO : token count processed\n",
      "2021-01-14 13:05:49,504 : INFO : frequencies processed\n",
      "2021-01-14 13:05:59,434 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:05:59,435 : INFO : entropies processed\n",
      "2021-01-14 13:05:59,436 : INFO : extropies processed\n",
      "2021-01-14 13:05:59,447 : INFO : token count processed\n",
      "2021-01-14 13:05:59,452 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:05:59,456 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:05:59,457 : INFO : vocab #32006\n",
      "2021-01-14 13:05:59,464 : INFO : diff #set()\n",
      "2021-01-14 13:06:19,581 : INFO : alphabet #32006\n",
      "2021-01-14 13:06:29,541 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.2962976383504936, 0.4354836164524095], [0.9574420340359211, 0.042557966], [1.584962500721156, 1.1699250014423124], [2.321928094887362, 7.482466367279176, 7.73414015813615, 2.070254304030388, 5.412212063248788, 0.25167379085697483]]\n",
      "2021-01-14 13:06:29,544 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:06:29,546 : INFO : built Dictionary(159 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 561 corpus positions)\n",
      "2021-01-14 13:06:29,588 : INFO : token count processed\n",
      "2021-01-14 13:06:29,656 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 13:06:29,659 : INFO : frequencies processed\n",
      "2021-01-14 13:06:29,660 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 13:06:29,666 : INFO : token count processed\n",
      "2021-01-14 13:06:29,673 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:06:29,680 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:06:29,680 : INFO : vocab #32006\n",
      "2021-01-14 13:06:29,689 : INFO : diff #set()\n",
      "2021-01-14 13:06:49,718 : INFO : alphabet #32006\n",
      "2021-01-14 13:06:59,027 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.316052689205401, 0.43176910640279226], [0.9854711256921291, 0.014528874], [nan, nan], [2.321928094887362, 6.372162341197667, 6.480263223922043, 2.213827212162986, 4.158335129034681, 0.10810088272437568]]\n",
      "2021-01-14 13:06:59,032 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:06:59,034 : INFO : built Dictionary(307 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1982 corpus positions)\n",
      "2021-01-14 13:06:59,092 : INFO : token count processed\n",
      "2021-01-14 13:06:59,126 : INFO : frequencies processed\n",
      "2021-01-14 13:07:08,963 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:07:08,964 : INFO : entropies processed\n",
      "2021-01-14 13:07:08,965 : INFO : extropies processed\n",
      "2021-01-14 13:07:08,973 : INFO : token count processed\n",
      "2021-01-14 13:07:08,978 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:07:08,982 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:07:08,982 : INFO : vocab #32006\n",
      "2021-01-14 13:07:08,989 : INFO : diff #set()\n",
      "2021-01-14 13:07:29,201 : INFO : alphabet #32006\n",
      "2021-01-14 13:07:39,227 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.3188919396940528, 0.43124044845829973], [0.9872281374409795, 0.012771863], [0.0, 0.0], [2.321928094887362, 6.798155919669889, 6.950934127980041, 2.1691498865772108, 4.629006033092679, 0.15277820831015188]]\n",
      "2021-01-14 13:07:39,231 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:07:39,233 : INFO : built Dictionary(163 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 698 corpus positions)\n",
      "2021-01-14 13:07:39,261 : INFO : token count processed\n",
      "2021-01-14 13:07:39,295 : INFO : frequencies processed\n",
      "2021-01-14 13:07:49,350 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:07:49,351 : INFO : entropies processed\n",
      "2021-01-14 13:07:49,352 : INFO : extropies processed\n",
      "2021-01-14 13:07:49,363 : INFO : token count processed\n",
      "2021-01-14 13:07:49,368 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:07:49,375 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:07:49,375 : INFO : vocab #32006\n",
      "2021-01-14 13:07:49,384 : INFO : diff #set()\n",
      "2021-01-14 13:08:09,359 : INFO : alphabet #32006\n",
      "2021-01-14 13:08:19,407 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.3108484971979952, 0.4327414805481812], [0.9780197665095329, 0.021980233], [0.0, 0.0], [2.321928094887362, 6.271631856729336, 6.375600789683412, 2.217959161933285, 4.053672694796051, 0.10396893295407672]]\n",
      "2021-01-14 13:08:19,414 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:08:19,418 : INFO : built Dictionary(340 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 3213 corpus positions)\n",
      "2021-01-14 13:08:19,507 : INFO : token count processed\n",
      "2021-01-14 13:08:19,542 : INFO : frequencies processed\n",
      "2021-01-14 13:08:29,629 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:08:29,630 : INFO : entropies processed\n",
      "2021-01-14 13:08:29,631 : INFO : extropies processed\n",
      "2021-01-14 13:08:29,643 : INFO : token count processed\n",
      "2021-01-14 13:08:29,648 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:08:29,652 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:08:29,652 : INFO : vocab #32006\n",
      "2021-01-14 13:08:29,659 : INFO : diff #set()\n",
      "2021-01-14 13:08:49,839 : INFO : alphabet #32006\n",
      "2021-01-14 13:08:59,589 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.309013456737333, 0.43308539284695785], [0.9719064403325319, 0.02809356], [1.0, 1.0], [2.321928094887362, 6.873598627629562, 7.001643138857407, 2.1938835836595167, 4.679715043970045, 0.128044511227845]]\n",
      "2021-01-14 13:08:59,592 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:08:59,593 : INFO : built Dictionary(91 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 161 corpus positions)\n",
      "2021-01-14 13:08:59,609 : INFO : token count processed\n",
      "2021-01-14 13:08:59,638 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 13:08:59,639 : INFO : frequencies processed\n",
      "2021-01-14 13:08:59,639 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 13:08:59,645 : INFO : token count processed\n",
      "2021-01-14 13:08:59,649 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:08:59,654 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:08:59,655 : INFO : vocab #32006\n",
      "2021-01-14 13:08:59,661 : INFO : diff #set()\n",
      "2021-01-14 13:09:20,131 : INFO : alphabet #32006\n",
      "2021-01-14 13:09:30,116 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.309208543243823, 0.433048804936113], [0.9788468144834042, 0.021153186], [nan, nan], [2.321928094887362, 6.049830202851529, 6.181103062598141, 2.19065523514075, 3.859174967710779, 0.13127285974661174]]\n",
      "2021-01-14 13:09:30,120 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 13:09:30,122 : INFO : built Dictionary(238 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1241 corpus positions)\n",
      "2021-01-14 13:09:30,164 : INFO : token count processed\n",
      "2021-01-14 13:09:30,194 : INFO : frequencies processed\n",
      "2021-01-14 13:09:40,268 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:09:40,269 : INFO : entropies processed\n",
      "2021-01-14 13:09:40,270 : INFO : extropies processed\n",
      "2021-01-14 13:09:40,277 : INFO : token count processed\n",
      "2021-01-14 13:09:40,282 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:09:40,286 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:09:40,287 : INFO : vocab #32006\n",
      "2021-01-14 13:09:40,294 : INFO : diff #set()\n",
      "2021-01-14 13:10:00,093 : INFO : alphabet #32006\n",
      "2021-01-14 13:10:10,155 : INFO : Computed distances or similarities ('286', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.3067596466246836, 0.43350853716520854], [0.9705179538577795, 0.029482046], [0.0, 0.0], [2.321928094887362, 6.778844940588858, 6.9244028376561015, 2.176370197820118, 4.60247474276874, 0.14555789706724376]]\n",
      "2021-01-14 13:10:10,159 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:10:10,160 : INFO : built Dictionary(138 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 274 corpus positions)\n",
      "2021-01-14 13:10:10,186 : INFO : token count processed\n",
      "2021-01-14 13:10:10,215 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 13:10:10,216 : INFO : frequencies processed\n",
      "2021-01-14 13:10:10,217 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 13:10:10,223 : INFO : token count processed\n",
      "2021-01-14 13:10:10,227 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:10:10,232 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:10:10,233 : INFO : vocab #32006\n",
      "2021-01-14 13:10:10,240 : INFO : diff #set()\n",
      "2021-01-14 13:10:31,074 : INFO : alphabet #32006\n",
      "2021-01-14 13:10:41,246 : INFO : Computed distances or similarities ('286', 'sacp-python-common/setup.py')[[1.3171407696588926, 0.43156635673335053], [0.9863215312361717, 0.013678469], [nan, nan], [2.321928094887362, 6.469677430851302, 6.6443861217172415, 2.1472194040214223, 4.32245802682988, 0.17470869086593943]]\n",
      "2021-01-14 13:10:41,251 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:10:41,252 : INFO : built Dictionary(207 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1176 corpus positions)\n",
      "2021-01-14 13:10:41,292 : INFO : token count processed\n",
      "2021-01-14 13:10:41,334 : INFO : frequencies processed\n",
      "2021-01-14 13:10:51,566 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:10:51,568 : INFO : entropies processed\n",
      "2021-01-14 13:10:51,569 : INFO : extropies processed\n",
      "2021-01-14 13:10:51,576 : INFO : token count processed\n",
      "2021-01-14 13:10:51,581 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:10:51,585 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:10:51,586 : INFO : vocab #32006\n",
      "2021-01-14 13:10:51,592 : INFO : diff #set()\n",
      "2021-01-14 13:11:11,187 : INFO : alphabet #32006\n",
      "2021-01-14 13:11:21,077 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.3151080042453192, 0.43194529074507726], [0.9834392946213484, 0.016560705], [0.0, 0.0], [2.321928094887362, 6.459180448028249, 6.577665508821273, 2.203443034094338, 4.255737413933911, 0.11848506079302368]]\n",
      "2021-01-14 13:11:21,081 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:11:21,082 : INFO : built Dictionary(118 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 326 corpus positions)\n",
      "2021-01-14 13:11:21,119 : INFO : token count processed\n",
      "2021-01-14 13:11:21,151 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 13:11:21,153 : INFO : frequencies processed\n",
      "2021-01-14 13:11:21,155 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 13:11:21,162 : INFO : token count processed\n",
      "2021-01-14 13:11:21,168 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:11:21,175 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:11:21,177 : INFO : vocab #32006\n",
      "2021-01-14 13:11:21,186 : INFO : diff #set()\n",
      "2021-01-14 13:11:41,382 : INFO : alphabet #32006\n",
      "2021-01-14 13:11:51,367 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.3187279169231239, 0.4312709536559025], [0.9880310958251357, 0.011968904], [nan, nan], [2.321928094887362, 6.097125733496388, 6.188772469770005, 2.2302813586137455, 3.866844374882643, 0.09164673627361708]]\n",
      "2021-01-14 13:11:51,370 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:11:51,380 : INFO : built Dictionary(110 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 322 corpus positions)\n",
      "2021-01-14 13:11:51,402 : INFO : token count processed\n",
      "2021-01-14 13:11:51,433 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 13:11:51,434 : INFO : frequencies processed\n",
      "2021-01-14 13:11:51,434 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 13:11:51,440 : INFO : token count processed\n",
      "2021-01-14 13:11:51,445 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:11:51,450 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:11:51,451 : INFO : vocab #32006\n",
      "2021-01-14 13:11:51,457 : INFO : diff #set()\n",
      "2021-01-14 13:12:11,598 : INFO : alphabet #32006\n",
      "2021-01-14 13:12:21,477 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.3156154901968364, 0.43185062642459526], [0.9854797180742025, 0.014520282], [nan, nan], [2.321928094887362, 6.0695858597523715, 6.153913187796603, 2.237600766843131, 3.831985092909241, 0.08432732804423182]]\n",
      "2021-01-14 13:12:21,480 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 13:12:21,481 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:12:21,486 : INFO : built Dictionary(112 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 418 corpus positions)\n",
      "2021-01-14 13:12:21,530 : INFO : token count processed\n",
      "2021-01-14 13:12:21,610 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 13:12:21,611 : INFO : frequencies processed\n",
      "2021-01-14 13:12:21,613 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 13:12:21,619 : INFO : token count processed\n",
      "2021-01-14 13:12:21,623 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:12:21,627 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:12:21,628 : INFO : vocab #32006\n",
      "2021-01-14 13:12:21,639 : INFO : diff #set()\n",
      "2021-01-14 13:12:41,821 : INFO : alphabet #32006\n",
      "2021-01-14 13:12:52,276 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.3126867173611956, 0.43239751951401895], [0.9840783048421144, 0.015921695], [nan, nan], [2.321928094887362, 6.104787343210121, 6.166508356291444, 2.2602070818060396, 3.8445802614040816, 0.06172101308132305]]\n",
      "2021-01-14 13:12:52,294 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 13:12:52,295 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:12:52,301 : INFO : built Dictionary(399 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 9103 corpus positions)\n",
      "2021-01-14 13:12:52,386 : INFO : token count processed\n",
      "2021-01-14 13:12:52,421 : INFO : frequencies processed\n",
      "2021-01-14 13:13:03,054 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:13:03,055 : INFO : entropies processed\n",
      "2021-01-14 13:13:03,056 : INFO : extropies processed\n",
      "2021-01-14 13:13:03,069 : INFO : token count processed\n",
      "2021-01-14 13:13:03,074 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:13:03,081 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:13:03,081 : INFO : vocab #32006\n",
      "2021-01-14 13:13:03,091 : INFO : diff #set()\n",
      "2021-01-14 13:13:23,207 : INFO : alphabet #32006\n",
      "2021-01-14 13:13:33,118 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.3159594218969772, 0.43178649442005806], [0.9802510403096676, 0.01974896], [0.0, 0.0], [2.321928094887362, 6.89087415148015, 7.037481598414119, 2.175320647953394, 4.715553503526756, 0.14660744693396843]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 13:13:33,124 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:13:33,126 : INFO : built Dictionary(257 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 2263 corpus positions)\n",
      "2021-01-14 13:13:33,172 : INFO : token count processed\n",
      "2021-01-14 13:13:33,204 : INFO : frequencies processed\n",
      "2021-01-14 13:13:43,039 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:13:43,040 : INFO : entropies processed\n",
      "2021-01-14 13:13:43,041 : INFO : extropies processed\n",
      "2021-01-14 13:13:43,052 : INFO : token count processed\n",
      "2021-01-14 13:13:43,057 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:13:43,062 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:13:43,063 : INFO : vocab #32006\n",
      "2021-01-14 13:13:43,069 : INFO : diff #set()\n",
      "2021-01-14 13:14:03,343 : INFO : alphabet #32006\n",
      "2021-01-14 13:14:17,969 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.3044610840006623, 0.43394093610118545], [0.9723245371133089, 0.027675463], [0.0, 0.0], [2.321928094887362, 6.655493573668506, 6.763630463831927, 2.2137912047239414, 4.441702368944565, 0.10813689016342032]]\n",
      "2021-01-14 13:14:17,973 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:14:17,975 : INFO : built Dictionary(234 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1270 corpus positions)\n",
      "2021-01-14 13:14:18,015 : INFO : token count processed\n",
      "2021-01-14 13:14:18,051 : INFO : frequencies processed\n",
      "2021-01-14 13:14:33,424 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:14:33,426 : INFO : entropies processed\n",
      "2021-01-14 13:14:33,426 : INFO : extropies processed\n",
      "2021-01-14 13:14:33,434 : INFO : token count processed\n",
      "2021-01-14 13:14:33,438 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:14:33,443 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:14:33,444 : INFO : vocab #32006\n",
      "2021-01-14 13:14:33,450 : INFO : diff #set()\n",
      "2021-01-14 13:14:53,902 : INFO : alphabet #32006\n",
      "2021-01-14 13:15:04,114 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.317112039547658, 0.43157170776913223], [0.9863669201731682, 0.01363308], [0.0, 0.0], [2.321928094887362, 6.6236746347295465, 6.701841949123581, 2.243760780493327, 4.37991385423622, 0.0781673143940349]]\n",
      "2021-01-14 13:15:04,118 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:15:04,122 : INFO : built Dictionary(235 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1209 corpus positions)\n",
      "2021-01-14 13:15:04,165 : INFO : token count processed\n",
      "2021-01-14 13:15:04,228 : INFO : frequencies processed\n",
      "2021-01-14 13:15:14,219 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:15:14,220 : INFO : entropies processed\n",
      "2021-01-14 13:15:14,221 : INFO : extropies processed\n",
      "2021-01-14 13:15:14,232 : INFO : token count processed\n",
      "2021-01-14 13:15:14,237 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:15:14,241 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:15:14,242 : INFO : vocab #32006\n",
      "2021-01-14 13:15:14,249 : INFO : diff #set()\n",
      "2021-01-14 13:15:34,177 : INFO : alphabet #32006\n",
      "2021-01-14 13:15:43,963 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.298173308087707, 0.43512819354432963], [0.9640347696840763, 0.03596523], [0.0, 0.0], [2.321928094887362, 6.75472436518627, 6.828418258039761, 2.2482342020338706, 4.5064901631523995, 0.07369389285349115]]\n",
      "2021-01-14 13:15:43,967 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:15:43,969 : INFO : built Dictionary(188 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1004 corpus positions)\n",
      "2021-01-14 13:15:44,017 : INFO : token count processed\n",
      "2021-01-14 13:15:44,048 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 13:15:44,051 : INFO : frequencies processed\n",
      "2021-01-14 13:15:44,053 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 13:15:44,060 : INFO : token count processed\n",
      "2021-01-14 13:15:44,067 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:15:44,073 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:15:44,076 : INFO : vocab #32006\n",
      "2021-01-14 13:15:44,084 : INFO : diff #set()\n",
      "2021-01-14 13:16:04,053 : INFO : alphabet #32006\n",
      "2021-01-14 13:16:14,240 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.3060842962741792, 0.4336354926902057], [0.9721459001302719, 0.0278541], [nan, nan], [2.321928094887362, 6.597313085495733, 6.660380535459172, 2.2588606449239226, 4.33845244057181, 0.06306744996343916]]\n",
      "2021-01-14 13:16:14,244 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:16:14,245 : INFO : built Dictionary(212 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 970 corpus positions)\n",
      "2021-01-14 13:16:14,293 : INFO : token count processed\n",
      "2021-01-14 13:16:14,329 : INFO : frequencies processed\n",
      "2021-01-14 13:16:24,391 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:16:24,392 : INFO : entropies processed\n",
      "2021-01-14 13:16:24,393 : INFO : extropies processed\n",
      "2021-01-14 13:16:24,403 : INFO : token count processed\n",
      "2021-01-14 13:16:24,408 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:16:24,413 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:16:24,414 : INFO : vocab #32006\n",
      "2021-01-14 13:16:24,420 : INFO : diff #set()\n",
      "2021-01-14 13:16:44,773 : INFO : alphabet #32006\n",
      "2021-01-14 13:16:54,844 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.3065658578855686, 0.4335449588752263], [0.9736899416893721, 0.026310058], [0.0, 0.0], [2.321928094887362, 6.659481538516613, 6.739428116728857, 2.2419815166751187, 4.417500021841494, 0.07994657821224394]]\n",
      "2021-01-14 13:16:54,848 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:16:54,850 : INFO : built Dictionary(237 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1132 corpus positions)\n",
      "2021-01-14 13:16:54,902 : INFO : token count processed\n",
      "2021-01-14 13:16:54,973 : INFO : frequencies processed\n",
      "2021-01-14 13:17:05,290 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:17:05,290 : INFO : entropies processed\n",
      "2021-01-14 13:17:05,291 : INFO : extropies processed\n",
      "2021-01-14 13:17:05,299 : INFO : token count processed\n",
      "2021-01-14 13:17:05,304 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:17:05,308 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:17:05,308 : INFO : vocab #32006\n",
      "2021-01-14 13:17:05,315 : INFO : diff #set()\n",
      "2021-01-14 13:17:25,159 : INFO : alphabet #32006\n",
      "2021-01-14 13:17:35,171 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.3125980166189992, 0.4324141043163189], [0.9826779440045357, 0.017322056], [0.0, 0.0], [2.321928094887362, 6.774682571479102, 6.833261316413029, 2.263349349953435, 4.511333221525668, 0.05857874493392767]]\n",
      "2021-01-14 13:17:35,185 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:17:35,188 : INFO : built Dictionary(415 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 7869 corpus positions)\n",
      "2021-01-14 13:17:35,286 : INFO : token count processed\n",
      "2021-01-14 13:17:35,318 : INFO : frequencies processed\n",
      "2021-01-14 13:17:44,985 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:17:44,986 : INFO : entropies processed\n",
      "2021-01-14 13:17:44,987 : INFO : extropies processed\n",
      "2021-01-14 13:17:44,997 : INFO : token count processed\n",
      "2021-01-14 13:17:45,001 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:17:45,006 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:17:45,007 : INFO : vocab #32006\n",
      "2021-01-14 13:17:45,014 : INFO : diff #set()\n",
      "2021-01-14 13:18:04,995 : INFO : alphabet #32006\n",
      "2021-01-14 13:18:15,014 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.31741230396305, 0.4315157895251877], [0.9811553210020065, 0.018844679], [0.0, 0.0], [2.321928094887362, 6.839453716525233, 6.975876915055979, 2.1855048963566155, 4.653948820168617, 0.13642319853074625]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 13:18:15,020 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:18:15,022 : INFO : built Dictionary(315 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 2311 corpus positions)\n",
      "2021-01-14 13:18:15,109 : INFO : token count processed\n",
      "2021-01-14 13:18:15,188 : INFO : frequencies processed\n",
      "2021-01-14 13:18:25,231 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:18:25,232 : INFO : entropies processed\n",
      "2021-01-14 13:18:25,233 : INFO : extropies processed\n",
      "2021-01-14 13:18:25,243 : INFO : token count processed\n",
      "2021-01-14 13:18:25,247 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:18:25,251 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:18:25,252 : INFO : vocab #32006\n",
      "2021-01-14 13:18:25,258 : INFO : diff #set()\n",
      "2021-01-14 13:18:45,464 : INFO : alphabet #32006\n",
      "2021-01-14 13:18:55,424 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.3053839461798595, 0.4337672263472866], [0.9728103876113892, 0.027189612], [0.0, 0.0], [2.321928094887362, 6.86432793886027, 6.949245826257308, 2.237010207490324, 4.627317731369946, 0.08491788739703843]]\n",
      "2021-01-14 13:18:55,427 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:18:55,428 : INFO : built Dictionary(149 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 513 corpus positions)\n",
      "2021-01-14 13:18:55,458 : INFO : token count processed\n",
      "2021-01-14 13:18:55,488 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 13:18:55,490 : INFO : frequencies processed\n",
      "2021-01-14 13:18:55,492 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 13:18:55,498 : INFO : token count processed\n",
      "2021-01-14 13:18:55,503 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:18:55,509 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:18:55,511 : INFO : vocab #32006\n",
      "2021-01-14 13:18:55,518 : INFO : diff #set()\n",
      "2021-01-14 13:19:15,317 : INFO : alphabet #32006\n",
      "2021-01-14 13:19:25,340 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.3016777251042246, 0.4344656895676905], [0.9649535492062569, 0.03504645], [nan, nan], [2.321928094887362, 6.431978396403875, 6.49763713338745, 2.256269357903788, 4.175709038500088, 0.0656587369835746]]\n",
      "2021-01-14 13:19:25,344 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:19:25,345 : INFO : built Dictionary(207 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 667 corpus positions)\n",
      "2021-01-14 13:19:25,383 : INFO : token count processed\n",
      "2021-01-14 13:19:25,413 : INFO : frequencies processed\n",
      "2021-01-14 13:19:35,374 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:19:35,375 : INFO : entropies processed\n",
      "2021-01-14 13:19:35,378 : INFO : extropies processed\n",
      "2021-01-14 13:19:35,389 : INFO : token count processed\n",
      "2021-01-14 13:19:35,393 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:19:35,398 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:19:35,398 : INFO : vocab #32006\n",
      "2021-01-14 13:19:35,407 : INFO : diff #set()\n",
      "2021-01-14 13:19:55,810 : INFO : alphabet #32006\n",
      "2021-01-14 13:20:05,671 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/test_auth_utility.py')[[1.3081459771655919, 0.4332481610318262], [0.9683064892888069, 0.03169351], [0.0, 0.0], [2.321928094887362, 6.911818353685893, 6.976621256279703, 2.257125192293552, 4.654693161392341, 0.06480290259380972]]\n",
      "2021-01-14 13:20:05,684 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:20:05,687 : INFO : built Dictionary(299 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 7197 corpus positions)\n",
      "2021-01-14 13:20:05,764 : INFO : token count processed\n",
      "2021-01-14 13:20:05,821 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 13:20:05,827 : INFO : frequencies processed\n",
      "2021-01-14 13:20:05,828 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 13:20:05,840 : INFO : token count processed\n",
      "2021-01-14 13:20:05,846 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:20:05,851 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:20:05,851 : INFO : vocab #32006\n",
      "2021-01-14 13:20:05,858 : INFO : diff #set()\n",
      "2021-01-14 13:20:25,919 : INFO : alphabet #32006\n",
      "2021-01-14 13:20:35,896 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.320181086741878, 0.43100084114738363], [0.9879082832485437, 0.012091717], [nan, nan], [2.321928094887362, 6.363791471162389, 6.384829593958456, 2.300889972091296, 4.062901499071094, 0.02103812279606654]]\n",
      "2021-01-14 13:20:35,901 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:20:35,902 : INFO : built Dictionary(199 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1267 corpus positions)\n",
      "2021-01-14 13:20:35,941 : INFO : token count processed\n",
      "2021-01-14 13:20:36,000 : INFO : frequencies processed\n",
      "2021-01-14 13:20:45,918 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:20:45,919 : INFO : entropies processed\n",
      "2021-01-14 13:20:45,920 : INFO : extropies processed\n",
      "2021-01-14 13:20:45,930 : INFO : token count processed\n",
      "2021-01-14 13:20:45,935 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:20:45,940 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:20:45,941 : INFO : vocab #32006\n",
      "2021-01-14 13:20:45,947 : INFO : diff #set()\n",
      "2021-01-14 13:21:06,215 : INFO : alphabet #32006\n",
      "2021-01-14 13:21:16,396 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.3173075973240287, 0.4315352873976575], [0.9865568447858095, 0.013443155], [0.0, 0.0], [2.321928094887362, 6.29000629755059, 6.454167111584503, 2.1577672808534487, 4.132239016697142, 0.16416081403391303]]\n",
      "2021-01-14 13:21:16,401 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:21:16,402 : INFO : built Dictionary(209 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1220 corpus positions)\n",
      "2021-01-14 13:21:16,441 : INFO : token count processed\n",
      "2021-01-14 13:21:16,475 : INFO : frequencies processed\n",
      "2021-01-14 13:21:26,404 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:21:26,405 : INFO : entropies processed\n",
      "2021-01-14 13:21:26,406 : INFO : extropies processed\n",
      "2021-01-14 13:21:26,417 : INFO : token count processed\n",
      "2021-01-14 13:21:26,422 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:21:26,426 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:21:26,427 : INFO : vocab #32006\n",
      "2021-01-14 13:21:26,434 : INFO : diff #set()\n",
      "2021-01-14 13:21:46,507 : INFO : alphabet #32006\n",
      "2021-01-14 13:21:56,477 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.3131787942417954, 0.4323055366447694], [0.9831005036830902, 0.016899496], [0.0, 0.0], [2.321928094887362, 6.361621244785958, 6.511043588044139, 2.1725057516291812, 4.189115493156777, 0.1494223432581805]]\n",
      "2021-01-14 13:21:56,482 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:21:56,483 : INFO : built Dictionary(218 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1243 corpus positions)\n",
      "2021-01-14 13:21:56,524 : INFO : token count processed\n",
      "2021-01-14 13:21:56,553 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 13:21:56,554 : INFO : frequencies processed\n",
      "2021-01-14 13:21:56,554 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 13:21:56,560 : INFO : token count processed\n",
      "2021-01-14 13:21:56,565 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:21:56,569 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:21:56,570 : INFO : vocab #32006\n",
      "2021-01-14 13:21:56,577 : INFO : diff #set()\n",
      "2021-01-14 13:22:16,741 : INFO : alphabet #32006\n",
      "2021-01-14 13:22:26,758 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.301959498364689, 0.4344125084348354], [0.9700864013284445, 0.029913599], [nan, nan], [2.321928094887362, 6.620594433343389, 6.781160817932568, 2.161361710298184, 4.459232723045206, 0.1605663845891785]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 13:22:26,763 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:22:26,765 : INFO : built Dictionary(190 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1336 corpus positions)\n",
      "2021-01-14 13:22:26,803 : INFO : token count processed\n",
      "2021-01-14 13:22:26,831 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 13:22:26,832 : INFO : frequencies processed\n",
      "2021-01-14 13:22:26,833 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 13:22:26,839 : INFO : token count processed\n",
      "2021-01-14 13:22:26,843 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:22:26,848 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:22:26,849 : INFO : vocab #32006\n",
      "2021-01-14 13:22:26,856 : INFO : diff #set()\n",
      "2021-01-14 13:22:46,939 : INFO : alphabet #32006\n",
      "2021-01-14 13:22:56,874 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.3022300883661182, 0.43436145025352146], [0.9718115385621786, 0.028188461], [nan, nan], [2.321928094887362, 6.207411496248084, 6.284738459480296, 2.24460113165515, 3.9628103645929342, 0.07732696323221244]]\n",
      "2021-01-14 13:22:56,878 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:22:56,879 : INFO : built Dictionary(129 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 240 corpus positions)\n",
      "2021-01-14 13:22:56,902 : INFO : token count processed\n",
      "2021-01-14 13:22:56,936 : INFO : frequencies processed\n",
      "2021-01-14 13:23:06,880 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:23:06,881 : INFO : entropies processed\n",
      "2021-01-14 13:23:06,882 : INFO : extropies processed\n",
      "2021-01-14 13:23:06,893 : INFO : token count processed\n",
      "2021-01-14 13:23:06,898 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:23:06,902 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:23:06,903 : INFO : vocab #32006\n",
      "2021-01-14 13:23:06,910 : INFO : diff #set()\n",
      "2021-01-14 13:23:27,030 : INFO : alphabet #32006\n",
      "2021-01-14 13:23:36,959 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.3004663777658063, 0.43469446442038057], [0.9599896259605885, 0.040010374], [0.0, 0.0], [2.321928094887362, 6.5805228788529595, 6.652348046964182, 2.2501029267761403, 4.33041995207682, 0.0718251681112223]]\n",
      "2021-01-14 13:23:36,963 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 13:23:36,964 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:23:36,969 : INFO : built Dictionary(213 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1201 corpus positions)\n",
      "2021-01-14 13:23:37,011 : INFO : token count processed\n",
      "2021-01-14 13:23:37,040 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 13:23:37,041 : INFO : frequencies processed\n",
      "2021-01-14 13:23:37,041 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 13:23:37,047 : INFO : token count processed\n",
      "2021-01-14 13:23:37,052 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:23:37,057 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:23:37,058 : INFO : vocab #32006\n",
      "2021-01-14 13:23:37,064 : INFO : diff #set()\n",
      "2021-01-14 13:23:57,150 : INFO : alphabet #32006\n",
      "2021-01-14 13:24:07,135 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.3220748440902785, 0.4306493404142497], [0.9890582375228405, 0.0109417625], [nan, nan], [2.321928094887362, 6.422089779976135, 6.470192474708897, 2.2738254001545997, 4.148264379821535, 0.048102694732762075]]\n",
      "2021-01-14 13:24:07,140 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:24:07,142 : INFO : built Dictionary(225 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1483 corpus positions)\n",
      "2021-01-14 13:24:07,182 : INFO : token count processed\n",
      "2021-01-14 13:24:07,210 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 13:24:07,211 : INFO : frequencies processed\n",
      "2021-01-14 13:24:07,212 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 13:24:07,218 : INFO : token count processed\n",
      "2021-01-14 13:24:07,223 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:24:07,227 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:24:07,228 : INFO : vocab #32006\n",
      "2021-01-14 13:24:07,235 : INFO : diff #set()\n",
      "2021-01-14 13:24:27,224 : INFO : alphabet #32006\n",
      "2021-01-14 13:24:37,367 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.308673637801824, 0.4331491396731753], [0.9750628583133221, 0.024937142], [nan, nan], [2.321928094887362, 6.485445644653597, 6.677715677707289, 2.12965806183367, 4.355787582819927, 0.19227003305369195]]\n",
      "2021-01-14 13:24:37,372 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:24:37,374 : INFO : built Dictionary(200 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1369 corpus positions)\n",
      "2021-01-14 13:24:37,411 : INFO : token count processed\n",
      "2021-01-14 13:24:37,440 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 13:24:37,441 : INFO : frequencies processed\n",
      "2021-01-14 13:24:37,442 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 13:24:37,448 : INFO : token count processed\n",
      "2021-01-14 13:24:37,453 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:24:37,457 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:24:37,458 : INFO : vocab #32006\n",
      "2021-01-14 13:24:37,465 : INFO : diff #set()\n",
      "2021-01-14 13:24:58,715 : INFO : alphabet #32006\n",
      "2021-01-14 13:25:13,337 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.304318504242907, 0.43396778620608], [0.9724051188677549, 0.027594881], [nan, nan], [2.321928094887362, 6.2276600107346916, 6.303306998027213, 2.2462811075948403, 3.9813789031398508, 0.0756469872925214]]\n",
      "2021-01-14 13:25:13,342 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:25:13,344 : INFO : built Dictionary(189 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1567 corpus positions)\n",
      "2021-01-14 13:25:13,378 : INFO : token count processed\n",
      "2021-01-14 13:25:13,412 : INFO : frequencies processed\n",
      "2021-01-14 13:25:28,734 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:25:28,736 : INFO : entropies processed\n",
      "2021-01-14 13:25:28,736 : INFO : extropies processed\n",
      "2021-01-14 13:25:28,744 : INFO : token count processed\n",
      "2021-01-14 13:25:28,749 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:25:28,754 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:25:28,755 : INFO : vocab #32006\n",
      "2021-01-14 13:25:28,762 : INFO : diff #set()\n",
      "2021-01-14 13:25:55,765 : INFO : alphabet #32006\n",
      "2021-01-14 13:26:07,111 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.3133009298935487, 0.4322827121528097], [0.9839867576956749, 0.016013242], [0.0, 0.0], [2.321928094887362, 6.253918170574241, 6.383299882563772, 2.192546382897831, 4.061371787676411, 0.12938171198953174]]\n",
      "2021-01-14 13:26:07,115 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:26:07,116 : INFO : built Dictionary(162 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 664 corpus positions)\n",
      "2021-01-14 13:26:07,144 : INFO : token count processed\n",
      "2021-01-14 13:26:07,170 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 13:26:07,171 : INFO : frequencies processed\n",
      "2021-01-14 13:26:07,172 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 13:26:07,178 : INFO : token count processed\n",
      "2021-01-14 13:26:07,182 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:26:07,186 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:26:07,187 : INFO : vocab #32006\n",
      "2021-01-14 13:26:07,194 : INFO : diff #set()\n",
      "2021-01-14 13:26:39,041 : INFO : alphabet #32006\n",
      "2021-01-14 13:26:50,599 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.295613741464742, 0.4356133533866803], [0.9619933404028416, 0.03800666], [nan, nan], [2.321928094887362, 6.374522245625576, 6.4990831316061275, 2.19736720890681, 4.177155036718766, 0.12456088598055182]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 13:26:50,605 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:26:50,607 : INFO : built Dictionary(284 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1946 corpus positions)\n",
      "2021-01-14 13:26:50,666 : INFO : token count processed\n",
      "2021-01-14 13:26:50,694 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 13:26:50,695 : INFO : frequencies processed\n",
      "2021-01-14 13:26:50,696 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 13:26:50,703 : INFO : token count processed\n",
      "2021-01-14 13:26:50,707 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:26:50,712 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:26:50,713 : INFO : vocab #32006\n",
      "2021-01-14 13:26:50,720 : INFO : diff #set()\n",
      "2021-01-14 13:27:11,314 : INFO : alphabet #32006\n",
      "2021-01-14 13:27:21,175 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.3162255367218756, 0.43173688578500313], [0.983954569324851, 0.01604543], [nan, nan], [2.321928094887362, 6.731238669067808, 6.9583164290526724, 2.094850334902498, 4.636388334165311, 0.2270777599848648]]\n",
      "2021-01-14 13:27:21,179 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:27:21,181 : INFO : built Dictionary(208 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1338 corpus positions)\n",
      "2021-01-14 13:27:21,218 : INFO : token count processed\n",
      "2021-01-14 13:27:21,249 : INFO : frequencies processed\n",
      "2021-01-14 13:27:31,172 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:27:31,172 : INFO : entropies processed\n",
      "2021-01-14 13:27:31,175 : INFO : extropies processed\n",
      "2021-01-14 13:27:31,184 : INFO : token count processed\n",
      "2021-01-14 13:27:31,190 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:27:31,196 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:27:31,199 : INFO : vocab #32006\n",
      "2021-01-14 13:27:31,207 : INFO : diff #set()\n",
      "2021-01-14 13:27:54,038 : INFO : alphabet #32006\n",
      "2021-01-14 13:28:04,144 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.313308205376318, 0.4322813525996744], [0.9840006567537785, 0.015999343], [0.0, 0.0], [2.321928094887362, 6.503741451859337, 6.638748039892516, 2.1869215068541834, 4.316819945005154, 0.13500658803317833]]\n",
      "2021-01-14 13:28:04,149 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:28:04,151 : INFO : built Dictionary(230 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 1660 corpus positions)\n",
      "2021-01-14 13:28:04,214 : INFO : token count processed\n",
      "2021-01-14 13:28:04,250 : INFO : frequencies processed\n",
      "2021-01-14 13:28:14,197 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:28:14,199 : INFO : entropies processed\n",
      "2021-01-14 13:28:14,199 : INFO : extropies processed\n",
      "2021-01-14 13:28:14,210 : INFO : token count processed\n",
      "2021-01-14 13:28:14,215 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:28:14,220 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:28:14,221 : INFO : vocab #32006\n",
      "2021-01-14 13:28:14,228 : INFO : diff #set()\n",
      "2021-01-14 13:28:34,245 : INFO : alphabet #32006\n",
      "2021-01-14 13:28:44,359 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.3139794627983743, 0.4321559530138032], [0.9827858880162239, 0.017214112], [0.0, 0.0], [2.321928094887362, 6.334729224484471, 6.41856247748728, 2.2380948418845534, 4.096634382599918, 0.0838332530028083]]\n",
      "2021-01-14 13:28:44,365 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:28:44,367 : INFO : built Dictionary(228 unique tokens: ['vert', '▁changes', '▁in', '▁production', '▁re']...) from 2 documents (total 2009 corpus positions)\n",
      "2021-01-14 13:28:44,424 : INFO : token count processed\n",
      "2021-01-14 13:28:44,456 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 13:28:44,459 : INFO : frequencies processed\n",
      "2021-01-14 13:28:44,461 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 13:28:44,469 : INFO : token count processed\n",
      "2021-01-14 13:28:44,475 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:28:44,481 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:28:44,484 : INFO : vocab #32006\n",
      "2021-01-14 13:28:44,492 : INFO : diff #set()\n",
      "2021-01-14 13:29:04,457 : INFO : alphabet #32006\n",
      "2021-01-14 13:29:14,543 : INFO : Computed distances or similarities ('286', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.3158612621787582, 0.4318047960520751], [0.9837134070694447, 0.016286593], [nan, nan], [2.321928094887362, 6.21319712067992, 6.365189456137323, 2.1699357594299595, 4.043261361249961, 0.1519923354574022]]\n",
      "2021-01-14 13:29:14,548 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 13:29:14,549 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:29:14,550 : INFO : built Dictionary(274 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1410 corpus positions)\n",
      "2021-01-14 13:29:14,683 : INFO : token count processed\n",
      "2021-01-14 13:29:14,715 : INFO : frequencies processed\n",
      "2021-01-14 13:29:24,663 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:29:24,664 : INFO : entropies processed\n",
      "2021-01-14 13:29:24,665 : INFO : extropies processed\n",
      "2021-01-14 13:29:24,673 : INFO : token count processed\n",
      "2021-01-14 13:29:24,678 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:29:24,682 : INFO : alphabet_target #32010\n",
      "2021-01-14 13:29:24,683 : INFO : vocab #32006\n",
      "2021-01-14 13:29:24,689 : INFO : diff #set()\n",
      "2021-01-14 13:29:44,751 : INFO : alphabet #32006\n",
      "2021-01-14 13:29:54,923 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.2007419612707817, 0.4543922084452675], [0.8157414942979813, 0.1842585], [2.321928094887362, 1.2877123795494492], [4.247927513443585, 6.905617163738059, 7.042732384869209, 4.110812292312435, 2.794804871425624, 0.13711522113115038]]\n",
      "2021-01-14 13:29:54,929 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:29:54,931 : INFO : built Dictionary(363 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 2300 corpus positions)\n",
      "2021-01-14 13:29:55,154 : INFO : token count processed\n",
      "2021-01-14 13:29:55,237 : INFO : frequencies processed\n",
      "2021-01-14 13:30:05,238 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:30:05,239 : INFO : entropies processed\n",
      "2021-01-14 13:30:05,240 : INFO : extropies processed\n",
      "2021-01-14 13:30:05,251 : INFO : token count processed\n",
      "2021-01-14 13:30:05,255 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:30:05,260 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:30:05,260 : INFO : vocab #32006\n",
      "2021-01-14 13:30:05,267 : INFO : diff #set()\n",
      "2021-01-14 13:30:25,398 : INFO : alphabet #32006\n",
      "2021-01-14 13:30:35,524 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.2091183840739574, 0.45266926716523204], [0.8356326818466187, 0.16436732], [2.807354922057604, 1.3343545280186873], [4.247927513443585, 7.1219284286457345, 7.336630949620185, 4.033224992469135, 3.0887034361765995, 0.2147025209744502]]\n",
      "2021-01-14 13:30:35,530 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:30:35,532 : INFO : built Dictionary(283 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 2283 corpus positions)\n",
      "2021-01-14 13:30:35,683 : INFO : token count processed\n",
      "2021-01-14 13:30:35,754 : INFO : frequencies processed\n",
      "2021-01-14 13:30:45,927 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:30:45,928 : INFO : entropies processed\n",
      "2021-01-14 13:30:45,929 : INFO : extropies processed\n",
      "2021-01-14 13:30:45,937 : INFO : token count processed\n",
      "2021-01-14 13:30:45,942 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:30:45,946 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:30:45,947 : INFO : vocab #32006\n",
      "2021-01-14 13:30:45,953 : INFO : diff #set()\n",
      "2021-01-14 13:31:06,000 : INFO : alphabet #32006\n",
      "2021-01-14 13:31:16,025 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2467973661288798, 0.4450779652296594], [0.8953702673316002, 0.10462973], [2.584962500721156, 1.315172029168969], [4.247927513443585, 6.41099024988467, 6.482872595561519, 4.176045167766736, 2.2349450821179335, 0.07188234567684848]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 13:31:16,029 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:31:16,031 : INFO : built Dictionary(164 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 682 corpus positions)\n",
      "2021-01-14 13:31:16,108 : INFO : token count processed\n",
      "2021-01-14 13:31:16,179 : INFO : frequencies processed\n",
      "2021-01-14 13:31:26,336 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:31:26,337 : INFO : entropies processed\n",
      "2021-01-14 13:31:26,338 : INFO : extropies processed\n",
      "2021-01-14 13:31:26,346 : INFO : token count processed\n",
      "2021-01-14 13:31:26,351 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:31:26,355 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:31:26,356 : INFO : vocab #32006\n",
      "2021-01-14 13:31:26,363 : INFO : diff #set()\n",
      "2021-01-14 13:31:46,446 : INFO : alphabet #32006\n",
      "2021-01-14 13:31:56,402 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.2421717633833964, 0.4459961615478638], [0.8806230872869492, 0.11937691], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.077866832717642, 6.237160323214468, 4.08863402294676, 1.9892328097708827, 0.15929349049682617]]\n",
      "2021-01-14 13:31:56,406 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:31:56,407 : INFO : built Dictionary(140 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 447 corpus positions)\n",
      "2021-01-14 13:31:56,474 : INFO : token count processed\n",
      "2021-01-14 13:31:56,542 : INFO : frequencies processed\n",
      "2021-01-14 13:32:06,510 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:32:06,511 : INFO : entropies processed\n",
      "2021-01-14 13:32:06,512 : INFO : extropies processed\n",
      "2021-01-14 13:32:06,520 : INFO : token count processed\n",
      "2021-01-14 13:32:06,524 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:32:06,529 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:32:06,529 : INFO : vocab #32006\n",
      "2021-01-14 13:32:06,536 : INFO : diff #set()\n",
      "2021-01-14 13:32:26,567 : INFO : alphabet #32006\n",
      "2021-01-14 13:32:36,546 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.2426094862901105, 0.44590910995131555], [0.8857115656137466, 0.114288434], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 5.977547459003844, 6.134517740424487, 4.090957232022943, 1.8865902269809016, 0.15697028142064262]]\n",
      "2021-01-14 13:32:36,551 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:32:36,553 : INFO : built Dictionary(243 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 2158 corpus positions)\n",
      "2021-01-14 13:32:36,676 : INFO : token count processed\n",
      "2021-01-14 13:32:36,743 : INFO : frequencies processed\n",
      "2021-01-14 13:32:46,589 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:32:46,590 : INFO : entropies processed\n",
      "2021-01-14 13:32:46,591 : INFO : extropies processed\n",
      "2021-01-14 13:32:46,598 : INFO : token count processed\n",
      "2021-01-14 13:32:46,603 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:32:46,607 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:32:46,607 : INFO : vocab #32006\n",
      "2021-01-14 13:32:46,613 : INFO : diff #set()\n",
      "2021-01-14 13:33:06,632 : INFO : alphabet #32006\n",
      "2021-01-14 13:33:16,699 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.2449994534805562, 0.4454344068768661], [0.8819058462977409, 0.11809415], [2.321928094887362, 1.2877123795494492], [4.247927513443585, 6.4614394051846435, 6.56612426212466, 4.143242656503569, 2.318196748681075, 0.1046848569400165]]\n",
      "2021-01-14 13:33:16,704 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 13:33:16,705 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:33:16,707 : INFO : built Dictionary(207 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1382 corpus positions)\n",
      "2021-01-14 13:33:16,798 : INFO : token count processed\n",
      "2021-01-14 13:33:16,834 : INFO : frequencies processed\n",
      "2021-01-14 13:33:26,778 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:33:26,779 : INFO : entropies processed\n",
      "2021-01-14 13:33:26,780 : INFO : extropies processed\n",
      "2021-01-14 13:33:26,792 : INFO : token count processed\n",
      "2021-01-14 13:33:26,797 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:33:26,802 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:33:26,803 : INFO : vocab #32006\n",
      "2021-01-14 13:33:26,810 : INFO : diff #set()\n",
      "2021-01-14 13:33:46,758 : INFO : alphabet #32006\n",
      "2021-01-14 13:33:56,994 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.2001782252780753, 0.45450863412377074], [0.8362167775630951, 0.16378322], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.327195724598159, 6.443938168948004, 4.13118506909374, 2.196010655504419, 0.11674244434984526]]\n",
      "2021-01-14 13:33:57,006 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 13:33:57,007 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:33:57,013 : INFO : built Dictionary(418 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 6274 corpus positions)\n",
      "2021-01-14 13:33:57,262 : INFO : token count processed\n",
      "2021-01-14 13:33:57,298 : INFO : frequencies processed\n",
      "2021-01-14 13:34:07,244 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:34:07,245 : INFO : entropies processed\n",
      "2021-01-14 13:34:07,246 : INFO : extropies processed\n",
      "2021-01-14 13:34:07,255 : INFO : token count processed\n",
      "2021-01-14 13:34:07,260 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:34:07,264 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:34:07,265 : INFO : vocab #32006\n",
      "2021-01-14 13:34:07,271 : INFO : diff #set()\n",
      "2021-01-14 13:34:27,401 : INFO : alphabet #32006\n",
      "2021-01-14 13:34:37,533 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.2336728052517125, 0.4476931436192643], [0.8668633550405502, 0.13313664], [3.0, 1.3485155455967714], [4.247927513443585, 6.9079058562486315, 7.014287847331671, 4.141545522360547, 2.7663603338880858, 0.10638199108303947]]\n",
      "2021-01-14 13:34:37,540 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:34:37,542 : INFO : built Dictionary(323 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 2668 corpus positions)\n",
      "2021-01-14 13:34:37,727 : INFO : token count processed\n",
      "2021-01-14 13:34:37,762 : INFO : frequencies processed\n",
      "2021-01-14 13:34:47,745 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:34:47,747 : INFO : entropies processed\n",
      "2021-01-14 13:34:47,748 : INFO : extropies processed\n",
      "2021-01-14 13:34:47,756 : INFO : token count processed\n",
      "2021-01-14 13:34:47,760 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:34:47,765 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:34:47,765 : INFO : vocab #32006\n",
      "2021-01-14 13:34:47,772 : INFO : diff #set()\n",
      "2021-01-14 13:35:07,897 : INFO : alphabet #32006\n",
      "2021-01-14 13:35:17,881 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.204233635882318, 0.453672416444964], [0.8360564559698105, 0.16394354], [2.321928094887362, 1.2877123795494492], [4.247927513443585, 6.61034830706307, 6.768577781605602, 4.089698038901053, 2.520650268162017, 0.1582294745425319]]\n",
      "2021-01-14 13:35:17,885 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:35:17,886 : INFO : built Dictionary(209 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 715 corpus positions)\n",
      "2021-01-14 13:35:18,006 : INFO : token count processed\n",
      "2021-01-14 13:35:18,041 : INFO : frequencies processed\n",
      "2021-01-14 13:35:28,328 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:35:28,329 : INFO : entropies processed\n",
      "2021-01-14 13:35:28,330 : INFO : extropies processed\n",
      "2021-01-14 13:35:28,338 : INFO : token count processed\n",
      "2021-01-14 13:35:28,343 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:35:28,347 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:35:28,347 : INFO : vocab #32006\n",
      "2021-01-14 13:35:28,354 : INFO : diff #set()\n",
      "2021-01-14 13:35:48,544 : INFO : alphabet #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 13:35:58,577 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.2186536650643338, 0.45072379513140604], [0.872469961643219, 0.12753004], [2.321928094887362, 1.2877123795494492], [4.247927513443585, 6.616715366949855, 6.760749666797497, 4.103893213595943, 2.5128221533539117, 0.14403429984764227]]\n",
      "2021-01-14 13:35:58,584 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:35:58,586 : INFO : built Dictionary(427 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 2749 corpus positions)\n",
      "2021-01-14 13:35:58,860 : INFO : token count processed\n",
      "2021-01-14 13:35:58,904 : INFO : frequencies processed\n",
      "2021-01-14 13:36:08,656 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:36:08,657 : INFO : entropies processed\n",
      "2021-01-14 13:36:08,658 : INFO : extropies processed\n",
      "2021-01-14 13:36:08,667 : INFO : token count processed\n",
      "2021-01-14 13:36:08,671 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:36:08,676 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:36:08,676 : INFO : vocab #32006\n",
      "2021-01-14 13:36:08,683 : INFO : diff #set()\n",
      "2021-01-14 13:36:28,815 : INFO : alphabet #32006\n",
      "2021-01-14 13:36:38,915 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.2086780080348851, 0.45275952237588696], [0.8232767283916473, 0.17672327], [2.807354922057604, 1.3343545280186873], [4.247927513443585, 7.32185870753746, 7.519969671264652, 4.049816549716393, 3.272042157821067, 0.19811096372719206]]\n",
      "2021-01-14 13:36:38,918 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:36:38,919 : INFO : built Dictionary(66 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 114 corpus positions)\n",
      "2021-01-14 13:36:38,949 : INFO : token count processed\n",
      "2021-01-14 13:36:38,981 : INFO : frequencies processed\n",
      "2021-01-14 13:36:49,158 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:36:49,160 : INFO : entropies processed\n",
      "2021-01-14 13:36:49,161 : INFO : extropies processed\n",
      "2021-01-14 13:36:49,171 : INFO : token count processed\n",
      "2021-01-14 13:36:49,176 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:36:49,180 : INFO : alphabet_target #32008\n",
      "2021-01-14 13:36:49,181 : INFO : vocab #32006\n",
      "2021-01-14 13:36:49,187 : INFO : diff #set()\n",
      "2021-01-14 13:37:09,259 : INFO : alphabet #32006\n",
      "2021-01-14 13:37:19,762 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2114126176743585, 0.4521996447011568], [0.8566295951604843, 0.1433704], [0.0, 0.0], [4.247927513443585, 5.176618657501385, 5.707273655953118, 3.717272514991853, 1.459346142509533, 0.530654998451733]]\n",
      "2021-01-14 13:37:19,766 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:37:19,767 : INFO : built Dictionary(160 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 507 corpus positions)\n",
      "2021-01-14 13:37:19,856 : INFO : token count processed\n",
      "2021-01-14 13:37:19,893 : INFO : frequencies processed\n",
      "2021-01-14 13:37:31,519 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:37:31,521 : INFO : entropies processed\n",
      "2021-01-14 13:37:31,521 : INFO : extropies processed\n",
      "2021-01-14 13:37:31,529 : INFO : token count processed\n",
      "2021-01-14 13:37:31,534 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:37:31,539 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:37:31,540 : INFO : vocab #32006\n",
      "2021-01-14 13:37:31,548 : INFO : diff #set()\n",
      "2021-01-14 13:37:54,527 : INFO : alphabet #32006\n",
      "2021-01-14 13:38:04,234 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.2116416713155547, 0.45215281162846255], [0.8420791029930115, 0.1579209], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.468846789852156, 6.664108359247316, 4.052665944048425, 2.416180845803731, 0.19526156939515982]]\n",
      "2021-01-14 13:38:04,241 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:38:04,243 : INFO : built Dictionary(373 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 2556 corpus positions)\n",
      "2021-01-14 13:38:04,475 : INFO : token count processed\n",
      "2021-01-14 13:38:04,512 : INFO : frequencies processed\n",
      "2021-01-14 13:38:14,515 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:38:14,517 : INFO : entropies processed\n",
      "2021-01-14 13:38:14,517 : INFO : extropies processed\n",
      "2021-01-14 13:38:14,525 : INFO : token count processed\n",
      "2021-01-14 13:38:14,530 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:38:14,534 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:38:14,535 : INFO : vocab #32006\n",
      "2021-01-14 13:38:14,541 : INFO : diff #set()\n",
      "2021-01-14 13:38:34,830 : INFO : alphabet #32006\n",
      "2021-01-14 13:38:44,933 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.23718469742602, 0.4469903629997757], [0.8680159449577332, 0.13198406], [2.807354922057604, 1.3343545280186873], [4.247927513443585, 6.957796704012729, 7.097150871377434, 4.10857334607888, 2.849223357933849, 0.13935416736470518]]\n",
      "2021-01-14 13:38:44,940 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:38:44,942 : INFO : built Dictionary(283 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 3053 corpus positions)\n",
      "2021-01-14 13:38:45,083 : INFO : token count processed\n",
      "2021-01-14 13:38:45,119 : INFO : frequencies processed\n",
      "2021-01-14 13:38:55,098 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:38:55,099 : INFO : entropies processed\n",
      "2021-01-14 13:38:55,100 : INFO : extropies processed\n",
      "2021-01-14 13:38:55,111 : INFO : token count processed\n",
      "2021-01-14 13:38:55,116 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:38:55,120 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:38:55,121 : INFO : vocab #32006\n",
      "2021-01-14 13:38:55,128 : INFO : diff #set()\n",
      "2021-01-14 13:39:14,935 : INFO : alphabet #32006\n",
      "2021-01-14 13:39:25,020 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.2261460531654815, 0.44920682476248314], [0.8677471876144409, 0.13225281], [2.584962500721156, 1.315172029168969], [4.247927513443585, 6.441859572014148, 6.587005885175406, 4.102781200282328, 2.339078371731821, 0.14514631316125826]]\n",
      "2021-01-14 13:39:25,024 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:39:25,026 : INFO : built Dictionary(308 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1529 corpus positions)\n",
      "2021-01-14 13:39:25,207 : INFO : token count processed\n",
      "2021-01-14 13:39:25,244 : INFO : frequencies processed\n",
      "2021-01-14 13:39:35,173 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:39:35,174 : INFO : entropies processed\n",
      "2021-01-14 13:39:35,175 : INFO : extropies processed\n",
      "2021-01-14 13:39:35,186 : INFO : token count processed\n",
      "2021-01-14 13:39:35,191 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:39:35,195 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:39:35,196 : INFO : vocab #32006\n",
      "2021-01-14 13:39:35,202 : INFO : diff #set()\n",
      "2021-01-14 13:39:55,075 : INFO : alphabet #32006\n",
      "2021-01-14 13:40:05,048 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.2107546743984865, 0.45233422395548484], [0.8408436626195908, 0.15915634], [2.807354922057604, 1.3343545280186873], [4.247927513443585, 6.998955278238291, 7.160750122549664, 4.086132669132213, 2.9128226091060787, 0.16179484431137325]]\n",
      "2021-01-14 13:40:05,053 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:40:05,054 : INFO : built Dictionary(226 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1348 corpus positions)\n",
      "2021-01-14 13:40:05,154 : INFO : token count processed\n",
      "2021-01-14 13:40:05,184 : INFO : frequencies processed\n",
      "2021-01-14 13:40:15,170 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:40:15,171 : INFO : entropies processed\n",
      "2021-01-14 13:40:15,172 : INFO : extropies processed\n",
      "2021-01-14 13:40:15,180 : INFO : token count processed\n",
      "2021-01-14 13:40:15,184 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:40:15,189 : INFO : alphabet_target #32009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 13:40:15,189 : INFO : vocab #32006\n",
      "2021-01-14 13:40:15,196 : INFO : diff #set()\n",
      "2021-01-14 13:40:35,265 : INFO : alphabet #32006\n",
      "2021-01-14 13:40:45,015 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.234769984416508, 0.44747334489598356], [0.8714463114738464, 0.12855369], [2.0, 1.2451124978365313], [4.247927513443585, 6.492983191376071, 6.611248858382594, 4.129661846437062, 2.363321344939009, 0.11826566700652297]]\n",
      "2021-01-14 13:40:45,022 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:40:45,024 : INFO : built Dictionary(437 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 3287 corpus positions)\n",
      "2021-01-14 13:40:45,270 : INFO : token count processed\n",
      "2021-01-14 13:40:45,302 : INFO : frequencies processed\n",
      "2021-01-14 13:40:55,228 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:40:55,229 : INFO : entropies processed\n",
      "2021-01-14 13:40:55,230 : INFO : extropies processed\n",
      "2021-01-14 13:40:55,239 : INFO : token count processed\n",
      "2021-01-14 13:40:55,243 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:40:55,247 : INFO : alphabet_target #32008\n",
      "2021-01-14 13:40:55,248 : INFO : vocab #32006\n",
      "2021-01-14 13:40:55,254 : INFO : diff #set()\n",
      "2021-01-14 13:41:15,503 : INFO : alphabet #32006\n",
      "2021-01-14 13:41:25,492 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.2560244865710721, 0.44325760023992394], [0.9027798175811768, 0.09722018], [1.0, 1.0], [4.247927513443585, 6.560342487747443, 6.752416952834592, 4.055853048356436, 2.5044894393910067, 0.1920744650871491]]\n",
      "2021-01-14 13:41:25,500 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 13:41:25,501 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:41:25,506 : INFO : built Dictionary(450 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 3492 corpus positions)\n",
      "2021-01-14 13:41:25,783 : INFO : token count processed\n",
      "2021-01-14 13:41:25,818 : INFO : frequencies processed\n",
      "2021-01-14 13:41:35,849 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:41:35,850 : INFO : entropies processed\n",
      "2021-01-14 13:41:35,851 : INFO : extropies processed\n",
      "2021-01-14 13:41:35,862 : INFO : token count processed\n",
      "2021-01-14 13:41:35,867 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:41:35,872 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:41:35,873 : INFO : vocab #32006\n",
      "2021-01-14 13:41:35,879 : INFO : diff #set()\n",
      "2021-01-14 13:41:55,967 : INFO : alphabet #32006\n",
      "2021-01-14 13:42:06,103 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.2015607885275668, 0.454223206195825], [0.828031986951828, 0.17196801], [2.584962500721156, 1.315172029168969], [4.247927513443585, 7.046173750105238, 7.233719900388756, 4.060381363160067, 2.985792386945171, 0.1875461502835183]]\n",
      "2021-01-14 13:42:06,114 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 13:42:06,115 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:42:06,121 : INFO : built Dictionary(505 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 5601 corpus positions)\n",
      "2021-01-14 13:42:06,443 : INFO : token count processed\n",
      "2021-01-14 13:42:06,478 : INFO : frequencies processed\n",
      "2021-01-14 13:42:16,501 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:42:16,502 : INFO : entropies processed\n",
      "2021-01-14 13:42:16,503 : INFO : extropies processed\n",
      "2021-01-14 13:42:16,511 : INFO : token count processed\n",
      "2021-01-14 13:42:16,516 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:42:16,520 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:42:16,520 : INFO : vocab #32006\n",
      "2021-01-14 13:42:16,527 : INFO : diff #set()\n",
      "2021-01-14 13:42:36,380 : INFO : alphabet #32006\n",
      "2021-01-14 13:42:46,059 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.2273987995245237, 0.44895417929356296], [0.860991820693016, 0.13900818], [2.321928094887362, 1.2877123795494492], [4.247927513443585, 7.009229588004272, 7.117123662161038, 4.140033439286819, 2.869196148717453, 0.107894074156766]]\n",
      "2021-01-14 13:42:46,072 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:42:46,075 : INFO : built Dictionary(580 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 6542 corpus positions)\n",
      "2021-01-14 13:42:46,485 : INFO : token count processed\n",
      "2021-01-14 13:42:46,519 : INFO : frequencies processed\n",
      "2021-01-14 13:42:56,584 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:42:56,585 : INFO : entropies processed\n",
      "2021-01-14 13:42:56,586 : INFO : extropies processed\n",
      "2021-01-14 13:42:56,596 : INFO : token count processed\n",
      "2021-01-14 13:42:56,600 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:42:56,604 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:42:56,605 : INFO : vocab #32006\n",
      "2021-01-14 13:42:56,612 : INFO : diff #set()\n",
      "2021-01-14 13:43:16,433 : INFO : alphabet #32006\n",
      "2021-01-14 13:43:26,551 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.2006832049649647, 0.4544043403175425], [0.8181023895740509, 0.18189761], [3.169925001442312, 1.3594000115384994], [4.247927513443585, 7.376088004590871, 7.537355898406154, 4.0866596196283025, 3.2894283849625685, 0.16126789381528273]]\n",
      "2021-01-14 13:43:26,554 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:43:26,556 : INFO : built Dictionary(137 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 328 corpus positions)\n",
      "2021-01-14 13:43:26,614 : INFO : token count processed\n",
      "2021-01-14 13:43:26,647 : INFO : frequencies processed\n",
      "2021-01-14 13:43:36,648 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:43:36,650 : INFO : entropies processed\n",
      "2021-01-14 13:43:36,650 : INFO : extropies processed\n",
      "2021-01-14 13:43:36,658 : INFO : token count processed\n",
      "2021-01-14 13:43:36,663 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:43:36,667 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:43:36,668 : INFO : vocab #32006\n",
      "2021-01-14 13:43:36,674 : INFO : diff #set()\n",
      "2021-01-14 13:43:56,729 : INFO : alphabet #32006\n",
      "2021-01-14 13:44:06,698 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.2008370403689956, 0.45437257809525894], [0.8324417024850845, 0.1675583], [1.0, 1.0], [4.247927513443585, 6.2993628166120885, 6.505906946834651, 4.041383383221023, 2.257979433391066, 0.20654413022256257]]\n",
      "2021-01-14 13:44:06,701 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:44:06,702 : INFO : built Dictionary(35 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 44 corpus positions)\n",
      "2021-01-14 13:44:06,717 : INFO : token count processed\n",
      "2021-01-14 13:44:06,750 : INFO : frequencies processed\n",
      "2021-01-14 13:44:16,505 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:44:16,506 : INFO : entropies processed\n",
      "2021-01-14 13:44:16,507 : INFO : extropies processed\n",
      "2021-01-14 13:44:16,514 : INFO : token count processed\n",
      "2021-01-14 13:44:16,519 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:44:16,523 : INFO : alphabet_target #32008\n",
      "2021-01-14 13:44:16,523 : INFO : vocab #32006\n",
      "2021-01-14 13:44:16,530 : INFO : diff #set()\n",
      "2021-01-14 13:44:36,654 : INFO : alphabet #32006\n",
      "2021-01-14 13:44:46,612 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2194909014497608, 0.4505537730958054], [0.8279535323381424, 0.17204647], [0.0, 0.0], [4.247927513443585, 3.8936606896881862, 4.859828661431134, 3.2817595417006373, 0.6119011479875489, 0.9661679717429479]]\n",
      "2021-01-14 13:44:46,636 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:44:46,640 : INFO : built Dictionary(740 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 12486 corpus positions)\n",
      "2021-01-14 13:44:47,198 : INFO : token count processed\n",
      "2021-01-14 13:44:47,231 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 13:44:57,109 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:44:57,110 : INFO : entropies processed\n",
      "2021-01-14 13:44:57,111 : INFO : extropies processed\n",
      "2021-01-14 13:44:57,125 : INFO : token count processed\n",
      "2021-01-14 13:44:57,130 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:44:57,137 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:44:57,137 : INFO : vocab #32006\n",
      "2021-01-14 13:44:57,144 : INFO : diff #set()\n",
      "2021-01-14 13:45:17,182 : INFO : alphabet #32006\n",
      "2021-01-14 13:45:27,141 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.198736229072533, 0.45480671431962455], [0.8158667385578156, 0.18413326], [3.0, 1.3485155455967714], [4.247927513443585, 7.434393313070278, 7.638464967887668, 4.043855858626195, 3.3905374544440825, 0.20407165481738954]]\n",
      "2021-01-14 13:45:27,150 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:45:27,152 : INFO : built Dictionary(491 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 4113 corpus positions)\n",
      "2021-01-14 13:45:27,454 : INFO : token count processed\n",
      "2021-01-14 13:45:27,484 : INFO : frequencies processed\n",
      "2021-01-14 13:45:37,409 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:45:37,410 : INFO : entropies processed\n",
      "2021-01-14 13:45:37,411 : INFO : extropies processed\n",
      "2021-01-14 13:45:37,422 : INFO : token count processed\n",
      "2021-01-14 13:45:37,427 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:45:37,432 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:45:37,432 : INFO : vocab #32006\n",
      "2021-01-14 13:45:37,439 : INFO : diff #set()\n",
      "2021-01-14 13:45:57,208 : INFO : alphabet #32006\n",
      "2021-01-14 13:46:07,046 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.193359874546307, 0.4559215346304484], [0.812107264995575, 0.18789274], [3.0, 1.3485155455967714], [4.247927513443585, 7.2991514951718255, 7.48679182154531, 4.0602871870701005, 3.238864308101725, 0.1876403263734847]]\n",
      "2021-01-14 13:46:07,054 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:46:07,057 : INFO : built Dictionary(458 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 3530 corpus positions)\n",
      "2021-01-14 13:46:07,338 : INFO : token count processed\n",
      "2021-01-14 13:46:07,410 : INFO : frequencies processed\n",
      "2021-01-14 13:46:17,362 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:46:17,363 : INFO : entropies processed\n",
      "2021-01-14 13:46:17,364 : INFO : extropies processed\n",
      "2021-01-14 13:46:17,375 : INFO : token count processed\n",
      "2021-01-14 13:46:17,381 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:46:17,386 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:46:17,386 : INFO : vocab #32006\n",
      "2021-01-14 13:46:17,393 : INFO : diff #set()\n",
      "2021-01-14 13:46:37,481 : INFO : alphabet #32006\n",
      "2021-01-14 13:46:47,594 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.1979088164395857, 0.4549779283473233], [0.8226174712181091, 0.17738253], [2.807354922057604, 1.3343545280186873], [4.247927513443585, 7.170319527000998, 7.352584980383457, 4.065662060061126, 3.104657466939872, 0.1822654533824597]]\n",
      "2021-01-14 13:46:47,598 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:46:47,599 : INFO : built Dictionary(171 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 566 corpus positions)\n",
      "2021-01-14 13:46:47,679 : INFO : token count processed\n",
      "2021-01-14 13:46:47,749 : INFO : frequencies processed\n",
      "2021-01-14 13:46:57,954 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:46:57,955 : INFO : entropies processed\n",
      "2021-01-14 13:46:57,956 : INFO : extropies processed\n",
      "2021-01-14 13:46:57,966 : INFO : token count processed\n",
      "2021-01-14 13:46:57,970 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:46:57,975 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:46:57,975 : INFO : vocab #32006\n",
      "2021-01-14 13:46:57,982 : INFO : diff #set()\n",
      "2021-01-14 13:47:17,715 : INFO : alphabet #32006\n",
      "2021-01-14 13:47:27,730 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.1986407395568721, 0.45482646710237273], [0.8275219351053238, 0.17247806], [1.0, 1.0], [4.247927513443585, 6.353654804387375, 6.55294146766299, 4.0486408501679705, 2.3050139542194046, 0.19928666327561473]]\n",
      "2021-01-14 13:47:27,734 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:47:27,735 : INFO : built Dictionary(166 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 694 corpus positions)\n",
      "2021-01-14 13:47:27,816 : INFO : token count processed\n",
      "2021-01-14 13:47:27,886 : INFO : frequencies processed\n",
      "2021-01-14 13:47:37,863 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:47:37,864 : INFO : entropies processed\n",
      "2021-01-14 13:47:37,865 : INFO : extropies processed\n",
      "2021-01-14 13:47:37,875 : INFO : token count processed\n",
      "2021-01-14 13:47:37,880 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:47:37,884 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:47:37,885 : INFO : vocab #32006\n",
      "2021-01-14 13:47:37,891 : INFO : diff #set()\n",
      "2021-01-14 13:47:57,251 : INFO : alphabet #32006\n",
      "2021-01-14 13:48:07,306 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.2015030501798818, 0.45423511901030134], [0.8214157223701477, 0.17858428], [1.0, 1.0], [4.247927513443585, 6.245180322479091, 6.408410646232122, 4.084697189690553, 2.1604831327885368, 0.16323032375303104]]\n",
      "2021-01-14 13:48:07,312 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:48:07,314 : INFO : built Dictionary(392 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1766 corpus positions)\n",
      "2021-01-14 13:48:07,543 : INFO : token count processed\n",
      "2021-01-14 13:48:07,613 : INFO : frequencies processed\n",
      "2021-01-14 13:48:17,274 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:48:17,275 : INFO : entropies processed\n",
      "2021-01-14 13:48:17,276 : INFO : extropies processed\n",
      "2021-01-14 13:48:17,287 : INFO : token count processed\n",
      "2021-01-14 13:48:17,292 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:48:17,296 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:48:17,297 : INFO : vocab #32006\n",
      "2021-01-14 13:48:17,303 : INFO : diff #set()\n",
      "2021-01-14 13:48:37,106 : INFO : alphabet #32006\n",
      "2021-01-14 13:48:47,088 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.1991801323632305, 0.4547149118364414], [0.8139843195676804, 0.18601568], [2.321928094887362, 1.2877123795494492], [4.247927513443585, 7.2691387000368, 7.472508875598354, 4.044557337882031, 3.224581362154769, 0.20337017556155423]]\n",
      "2021-01-14 13:48:47,093 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:48:47,095 : INFO : built Dictionary(318 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1603 corpus positions)\n",
      "2021-01-14 13:48:47,264 : INFO : token count processed\n",
      "2021-01-14 13:48:47,335 : INFO : frequencies processed\n",
      "2021-01-14 13:48:57,272 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:48:57,274 : INFO : entropies processed\n",
      "2021-01-14 13:48:57,276 : INFO : extropies processed\n",
      "2021-01-14 13:48:57,284 : INFO : token count processed\n",
      "2021-01-14 13:48:57,288 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:48:57,292 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:48:57,293 : INFO : vocab #32006\n",
      "2021-01-14 13:48:57,300 : INFO : diff #set()\n",
      "2021-01-14 13:49:17,333 : INFO : alphabet #32006\n",
      "2021-01-14 13:49:27,191 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.1933783817068924, 0.4559176876822309], [0.8090130537748337, 0.19098695], [2.321928094887362, 1.2877123795494492], [4.247927513443585, 7.08857858466988, 7.253871841048959, 4.082634257064505, 3.0059443276053734, 0.165293256379079]]\n",
      "2021-01-14 13:49:27,195 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:49:27,196 : INFO : built Dictionary(145 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 526 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 13:49:27,263 : INFO : token count processed\n",
      "2021-01-14 13:49:27,333 : INFO : frequencies processed\n",
      "2021-01-14 13:49:37,329 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:49:37,330 : INFO : entropies processed\n",
      "2021-01-14 13:49:37,331 : INFO : extropies processed\n",
      "2021-01-14 13:49:37,338 : INFO : token count processed\n",
      "2021-01-14 13:49:37,343 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:49:37,346 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:49:37,347 : INFO : vocab #32006\n",
      "2021-01-14 13:49:37,353 : INFO : diff #set()\n",
      "2021-01-14 13:50:00,526 : INFO : alphabet #32006\n",
      "2021-01-14 13:50:10,513 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.2031097841675757, 0.4539038440963761], [0.841006264090538, 0.15899374], [1.0, 1.0], [4.247927513443585, 6.0479231618016716, 6.242805905753194, 4.053044769492063, 1.9948783923096087, 0.19488274395152239]]\n",
      "2021-01-14 13:50:10,519 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:50:10,520 : INFO : built Dictionary(147 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 645 corpus positions)\n",
      "2021-01-14 13:50:10,584 : INFO : token count processed\n",
      "2021-01-14 13:50:10,643 : INFO : frequencies processed\n",
      "2021-01-14 13:50:20,363 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:50:20,365 : INFO : entropies processed\n",
      "2021-01-14 13:50:20,367 : INFO : extropies processed\n",
      "2021-01-14 13:50:20,381 : INFO : token count processed\n",
      "2021-01-14 13:50:20,386 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:50:20,390 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:50:20,391 : INFO : vocab #32006\n",
      "2021-01-14 13:50:20,398 : INFO : diff #set()\n",
      "2021-01-14 13:50:40,483 : INFO : alphabet #32006\n",
      "2021-01-14 13:50:50,463 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.2054283537821269, 0.4534266544116398], [0.830060139298439, 0.16993986], [1.0, 1.0], [4.247927513443585, 6.036583168403119, 6.203620962077711, 4.080889719768994, 1.9556934486341255, 0.16703779367459148]]\n",
      "2021-01-14 13:50:50,476 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 13:50:50,477 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:50:50,484 : INFO : built Dictionary(555 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 7003 corpus positions)\n",
      "2021-01-14 13:50:50,846 : INFO : token count processed\n",
      "2021-01-14 13:50:50,912 : INFO : frequencies processed\n",
      "2021-01-14 13:51:01,142 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:51:01,143 : INFO : entropies processed\n",
      "2021-01-14 13:51:01,144 : INFO : extropies processed\n",
      "2021-01-14 13:51:01,154 : INFO : token count processed\n",
      "2021-01-14 13:51:01,158 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:51:01,162 : INFO : alphabet_target #32010\n",
      "2021-01-14 13:51:01,163 : INFO : vocab #32006\n",
      "2021-01-14 13:51:01,169 : INFO : diff #set()\n",
      "2021-01-14 13:51:20,824 : INFO : alphabet #32006\n",
      "2021-01-14 13:51:30,869 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.2150708031413953, 0.45145283779724255], [0.8428112715482712, 0.15718873], [3.169925001442312, 1.3594000115384994], [4.247927513443585, 7.29352035514053, 7.448393309109074, 4.093054559475041, 3.2004657956654885, 0.1548729539685434]]\n",
      "2021-01-14 13:51:30,876 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:51:30,879 : INFO : built Dictionary(368 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 3246 corpus positions)\n",
      "2021-01-14 13:51:31,075 : INFO : token count processed\n",
      "2021-01-14 13:51:31,108 : INFO : frequencies processed\n",
      "2021-01-14 13:51:41,193 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:51:41,194 : INFO : entropies processed\n",
      "2021-01-14 13:51:41,195 : INFO : extropies processed\n",
      "2021-01-14 13:51:41,203 : INFO : token count processed\n",
      "2021-01-14 13:51:41,208 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:51:41,212 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:51:41,213 : INFO : vocab #32006\n",
      "2021-01-14 13:51:41,220 : INFO : diff #set()\n",
      "2021-01-14 13:52:01,196 : INFO : alphabet #32006\n",
      "2021-01-14 13:52:11,063 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.204857217021027, 0.45354410810832263], [0.8222357928752899, 0.1777642], [2.321928094887362, 1.2877123795494492], [4.247927513443585, 6.8153433747477745, 6.956576388667884, 4.106694499523476, 2.7086488752242985, 0.14123301392010923]]\n",
      "2021-01-14 13:52:11,067 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:52:11,068 : INFO : built Dictionary(122 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 267 corpus positions)\n",
      "2021-01-14 13:52:11,133 : INFO : token count processed\n",
      "2021-01-14 13:52:11,168 : INFO : frequencies processed\n",
      "2021-01-14 13:52:21,240 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:52:21,241 : INFO : entropies processed\n",
      "2021-01-14 13:52:21,242 : INFO : extropies processed\n",
      "2021-01-14 13:52:21,252 : INFO : token count processed\n",
      "2021-01-14 13:52:21,257 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:52:21,262 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:52:21,262 : INFO : vocab #32006\n",
      "2021-01-14 13:52:21,269 : INFO : diff #set()\n",
      "2021-01-14 13:52:41,240 : INFO : alphabet #32006\n",
      "2021-01-14 13:52:52,005 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.1979439533142726, 0.4549706549578315], [0.8324978053569794, 0.1675022], [1.0, 1.0], [4.247927513443585, 6.150121915859574, 6.3589726194022616, 4.039076809900899, 2.1110451059586763, 0.2088507035426872]]\n",
      "2021-01-14 13:52:52,009 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:52:52,011 : INFO : built Dictionary(285 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 978 corpus positions)\n",
      "2021-01-14 13:52:52,154 : INFO : token count processed\n",
      "2021-01-14 13:52:52,188 : INFO : frequencies processed\n",
      "2021-01-14 13:53:03,009 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:53:03,010 : INFO : entropies processed\n",
      "2021-01-14 13:53:03,011 : INFO : extropies processed\n",
      "2021-01-14 13:53:03,018 : INFO : token count processed\n",
      "2021-01-14 13:53:03,022 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:53:03,027 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:53:03,027 : INFO : vocab #32006\n",
      "2021-01-14 13:53:03,034 : INFO : diff #set()\n",
      "2021-01-14 13:53:22,959 : INFO : alphabet #32006\n",
      "2021-01-14 13:53:33,011 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.196632766254837, 0.45524223045482304], [0.8146033734083176, 0.18539663], [2.0, 1.2451124978365313], [4.247927513443585, 7.0391145208191315, 7.280616394843417, 4.0064256394193, 3.0326888813998316, 0.24150187402428536]]\n",
      "2021-01-14 13:53:33,020 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 13:53:33,021 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:53:33,027 : INFO : built Dictionary(589 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 4353 corpus positions)\n",
      "2021-01-14 13:53:33,464 : INFO : token count processed\n",
      "2021-01-14 13:53:33,545 : INFO : frequencies processed\n",
      "2021-01-14 13:53:43,900 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:53:43,901 : INFO : entropies processed\n",
      "2021-01-14 13:53:43,902 : INFO : extropies processed\n",
      "2021-01-14 13:53:43,910 : INFO : token count processed\n",
      "2021-01-14 13:53:43,914 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:53:43,918 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:53:43,919 : INFO : vocab #32006\n",
      "2021-01-14 13:53:43,925 : INFO : diff #set()\n",
      "2021-01-14 13:54:04,017 : INFO : alphabet #32006\n",
      "2021-01-14 13:54:14,379 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.2064054874433385, 0.45322584887093675], [0.8233656883239746, 0.17663431], [2.321928094887362, 1.2877123795494492], [4.247927513443585, 7.482466367279176, 7.749977359838632, 3.9804165208841296, 3.502049846395047, 0.26751099255945654]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 13:54:14,383 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:54:14,384 : INFO : built Dictionary(171 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 575 corpus positions)\n",
      "2021-01-14 13:54:14,478 : INFO : token count processed\n",
      "2021-01-14 13:54:14,559 : INFO : frequencies processed\n",
      "2021-01-14 13:54:24,454 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:54:24,455 : INFO : entropies processed\n",
      "2021-01-14 13:54:24,456 : INFO : extropies processed\n",
      "2021-01-14 13:54:24,467 : INFO : token count processed\n",
      "2021-01-14 13:54:24,472 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:54:24,479 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:54:24,479 : INFO : vocab #32006\n",
      "2021-01-14 13:54:24,488 : INFO : diff #set()\n",
      "2021-01-14 13:54:44,658 : INFO : alphabet #32006\n",
      "2021-01-14 13:54:54,603 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.1990915053899904, 0.45473323758879164], [0.8251815885305405, 0.17481841], [1.0, 1.0], [4.247927513443585, 6.372162341197667, 6.5675464985233205, 4.052543356117932, 2.3196189850797353, 0.19538415732565362]]\n",
      "2021-01-14 13:54:54,609 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:54:54,614 : INFO : built Dictionary(318 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1996 corpus positions)\n",
      "2021-01-14 13:54:54,795 : INFO : token count processed\n",
      "2021-01-14 13:54:54,858 : INFO : frequencies processed\n",
      "2021-01-14 13:55:04,769 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:55:04,770 : INFO : entropies processed\n",
      "2021-01-14 13:55:04,771 : INFO : extropies processed\n",
      "2021-01-14 13:55:04,779 : INFO : token count processed\n",
      "2021-01-14 13:55:04,784 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:55:04,788 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:55:04,789 : INFO : vocab #32006\n",
      "2021-01-14 13:55:04,795 : INFO : diff #set()\n",
      "2021-01-14 13:55:24,700 : INFO : alphabet #32006\n",
      "2021-01-14 13:55:34,683 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.2054068472895145, 0.45343107609782674], [0.8389954715967178, 0.16100453], [2.0, 1.2451124978365313], [4.247927513443585, 6.798155919669889, 6.984340281092833, 4.06174315202064, 2.736412767649248, 0.18618436142294392]]\n",
      "2021-01-14 13:55:34,687 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:55:34,688 : INFO : built Dictionary(174 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 712 corpus positions)\n",
      "2021-01-14 13:55:34,773 : INFO : token count processed\n",
      "2021-01-14 13:55:34,844 : INFO : frequencies processed\n",
      "2021-01-14 13:55:44,886 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:55:44,887 : INFO : entropies processed\n",
      "2021-01-14 13:55:44,888 : INFO : extropies processed\n",
      "2021-01-14 13:55:44,897 : INFO : token count processed\n",
      "2021-01-14 13:55:44,901 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:55:44,908 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:55:44,908 : INFO : vocab #32006\n",
      "2021-01-14 13:55:44,917 : INFO : diff #set()\n",
      "2021-01-14 13:56:04,779 : INFO : alphabet #32006\n",
      "2021-01-14 13:56:14,806 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.195041124562765, 0.4555723301991402], [0.8269928842782974, 0.17300712], [2.0, 1.2451124978365313], [4.247927513443585, 6.271631856729336, 6.451040683582233, 4.0685186865906875, 2.203113170138648, 0.17940882685289772]]\n",
      "2021-01-14 13:56:14,813 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:56:14,815 : INFO : built Dictionary(351 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 3227 corpus positions)\n",
      "2021-01-14 13:56:15,011 : INFO : token count processed\n",
      "2021-01-14 13:56:15,078 : INFO : frequencies processed\n",
      "2021-01-14 13:56:25,147 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:56:25,148 : INFO : entropies processed\n",
      "2021-01-14 13:56:25,149 : INFO : extropies processed\n",
      "2021-01-14 13:56:25,161 : INFO : token count processed\n",
      "2021-01-14 13:56:25,166 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:56:25,171 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:56:25,171 : INFO : vocab #32006\n",
      "2021-01-14 13:56:25,178 : INFO : diff #set()\n",
      "2021-01-14 13:56:45,089 : INFO : alphabet #32006\n",
      "2021-01-14 13:56:55,066 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.1945702441524204, 0.45567008058391706], [0.8083385974168777, 0.1916614], [2.321928094887362, 1.2877123795494492], [4.247927513443585, 6.873598627629562, 7.024995950975873, 4.096530190097274, 2.7770684375322876, 0.15139732334631084]]\n",
      "2021-01-14 13:56:55,069 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:56:55,071 : INFO : built Dictionary(102 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 175 corpus positions)\n",
      "2021-01-14 13:56:55,122 : INFO : token count processed\n",
      "2021-01-14 13:56:55,193 : INFO : frequencies processed\n",
      "2021-01-14 13:57:05,248 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:57:05,249 : INFO : entropies processed\n",
      "2021-01-14 13:57:05,250 : INFO : extropies processed\n",
      "2021-01-14 13:57:05,257 : INFO : token count processed\n",
      "2021-01-14 13:57:05,262 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:57:05,266 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:57:05,267 : INFO : vocab #32006\n",
      "2021-01-14 13:57:05,273 : INFO : diff #set()\n",
      "2021-01-14 13:57:25,171 : INFO : alphabet #32006\n",
      "2021-01-14 13:57:35,375 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.1891933580628056, 0.45678925359288025], [0.8294643461704254, 0.17053565], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.049830202851529, 6.344375961737107, 3.9533817545580074, 2.096448448293522, 0.29454575888557777]]\n",
      "2021-01-14 13:57:35,381 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:57:35,385 : INFO : built Dictionary(247 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1255 corpus positions)\n",
      "2021-01-14 13:57:35,513 : INFO : token count processed\n",
      "2021-01-14 13:57:35,585 : INFO : frequencies processed\n",
      "2021-01-14 13:57:45,967 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:57:45,968 : INFO : entropies processed\n",
      "2021-01-14 13:57:45,969 : INFO : extropies processed\n",
      "2021-01-14 13:57:45,976 : INFO : token count processed\n",
      "2021-01-14 13:57:45,981 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:57:45,985 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:57:45,986 : INFO : vocab #32006\n",
      "2021-01-14 13:57:45,992 : INFO : diff #set()\n",
      "2021-01-14 13:58:06,260 : INFO : alphabet #32006\n",
      "2021-01-14 13:58:16,367 : INFO : Computed distances or similarities ('284', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.1919398510244126, 0.4562168982568777], [0.8250499814748764, 0.17495002], [2.584962500721156, 1.315172029168969], [4.247927513443585, 6.778844940588858, 6.965912389887658, 4.060860064144785, 2.7179848764440724, 0.18706744929879981]]\n",
      "2021-01-14 13:58:16,371 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:58:16,372 : INFO : built Dictionary(149 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 288 corpus positions)\n",
      "2021-01-14 13:58:16,433 : INFO : token count processed\n",
      "2021-01-14 13:58:16,467 : INFO : frequencies processed\n",
      "2021-01-14 13:58:26,471 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:58:26,472 : INFO : entropies processed\n",
      "2021-01-14 13:58:26,473 : INFO : extropies processed\n",
      "2021-01-14 13:58:26,482 : INFO : token count processed\n",
      "2021-01-14 13:58:26,487 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:58:26,491 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:58:26,492 : INFO : vocab #32006\n",
      "2021-01-14 13:58:26,498 : INFO : diff #set()\n",
      "2021-01-14 13:58:46,614 : INFO : alphabet #32006\n",
      "2021-01-14 13:58:56,569 : INFO : Computed distances or similarities ('284', 'sacp-python-common/setup.py')[[1.1957970311167632, 0.4554154987136533], [0.8271840959787369, 0.1728159], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.469677430851302, 6.750962207412821, 3.9666427368820667, 2.5030346939692354, 0.2812847765615185]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 13:58:56,574 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:58:56,576 : INFO : built Dictionary(219 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1190 corpus positions)\n",
      "2021-01-14 13:58:56,676 : INFO : token count processed\n",
      "2021-01-14 13:58:56,715 : INFO : frequencies processed\n",
      "2021-01-14 13:59:06,726 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:59:06,727 : INFO : entropies processed\n",
      "2021-01-14 13:59:06,728 : INFO : extropies processed\n",
      "2021-01-14 13:59:06,735 : INFO : token count processed\n",
      "2021-01-14 13:59:06,740 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:59:06,744 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:59:06,745 : INFO : vocab #32006\n",
      "2021-01-14 13:59:06,751 : INFO : diff #set()\n",
      "2021-01-14 13:59:26,786 : INFO : alphabet #32006\n",
      "2021-01-14 13:59:36,714 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.2037071749088881, 0.4537807978237149], [0.8337668031454086, 0.1662332], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.459180448028249, 6.631846563216963, 4.075261398254871, 2.383919049773378, 0.1726661151887141]]\n",
      "2021-01-14 13:59:36,717 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 13:59:36,718 : INFO : built Dictionary(131 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 340 corpus positions)\n",
      "2021-01-14 13:59:36,772 : INFO : token count processed\n",
      "2021-01-14 13:59:36,815 : INFO : frequencies processed\n",
      "2021-01-14 13:59:46,779 : INFO : scalar_distribution processed\n",
      "2021-01-14 13:59:46,780 : INFO : entropies processed\n",
      "2021-01-14 13:59:46,781 : INFO : extropies processed\n",
      "2021-01-14 13:59:46,792 : INFO : token count processed\n",
      "2021-01-14 13:59:46,797 : INFO : alphabet_source #32006\n",
      "2021-01-14 13:59:46,802 : INFO : alphabet_target #32009\n",
      "2021-01-14 13:59:46,802 : INFO : vocab #32006\n",
      "2021-01-14 13:59:46,809 : INFO : diff #set()\n",
      "2021-01-14 14:00:06,857 : INFO : alphabet #32006\n",
      "2021-01-14 14:00:16,771 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.2172275541127822, 0.4510136986819774], [0.8623011857271194, 0.13769881], [0.0, 0.0], [4.247927513443585, 6.097125733496388, 6.328416274484993, 4.01663697245498, 2.0804887610414076, 0.23129054098860458]]\n",
      "2021-01-14 14:00:16,775 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:00:16,776 : INFO : built Dictionary(123 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 336 corpus positions)\n",
      "2021-01-14 14:00:16,837 : INFO : token count processed\n",
      "2021-01-14 14:00:16,874 : INFO : frequencies processed\n",
      "2021-01-14 14:00:26,764 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:00:26,765 : INFO : entropies processed\n",
      "2021-01-14 14:00:26,766 : INFO : extropies processed\n",
      "2021-01-14 14:00:26,777 : INFO : token count processed\n",
      "2021-01-14 14:00:26,782 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:00:26,786 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:00:26,787 : INFO : vocab #32006\n",
      "2021-01-14 14:00:26,793 : INFO : diff #set()\n",
      "2021-01-14 14:00:46,851 : INFO : alphabet #32006\n",
      "2021-01-14 14:00:56,750 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.2160180404918461, 0.45125986419228314], [0.8580401837825775, 0.14195982], [0.0, 0.0], [4.247927513443585, 6.0695858597523715, 6.29552167334876, 4.021991699847198, 2.0475941599051746, 0.22593581359638826]]\n",
      "2021-01-14 14:00:56,753 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 14:00:56,754 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:00:56,758 : INFO : built Dictionary(125 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 432 corpus positions)\n",
      "2021-01-14 14:00:56,809 : INFO : token count processed\n",
      "2021-01-14 14:00:56,851 : INFO : frequencies processed\n",
      "2021-01-14 14:01:06,902 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:01:06,903 : INFO : entropies processed\n",
      "2021-01-14 14:01:06,904 : INFO : extropies processed\n",
      "2021-01-14 14:01:06,913 : INFO : token count processed\n",
      "2021-01-14 14:01:06,918 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:01:06,924 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:01:06,925 : INFO : vocab #32006\n",
      "2021-01-14 14:01:06,933 : INFO : diff #set()\n",
      "2021-01-14 14:01:26,857 : INFO : alphabet #32006\n",
      "2021-01-14 14:01:37,082 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.2222893864240039, 0.4499863996601944], [0.8703581690788269, 0.12964183], [0.0, 0.0], [4.247927513443585, 6.104787343210121, 6.288262950946781, 4.064451905706924, 2.040335437503196, 0.1834756077366606]]\n",
      "2021-01-14 14:01:37,099 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 14:01:37,100 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:01:37,103 : INFO : built Dictionary(410 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 9117 corpus positions)\n",
      "2021-01-14 14:01:37,360 : INFO : token count processed\n",
      "2021-01-14 14:01:37,395 : INFO : frequencies processed\n",
      "2021-01-14 14:01:47,233 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:01:47,235 : INFO : entropies processed\n",
      "2021-01-14 14:01:47,235 : INFO : extropies processed\n",
      "2021-01-14 14:01:47,246 : INFO : token count processed\n",
      "2021-01-14 14:01:47,250 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:01:47,254 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:01:47,255 : INFO : vocab #32006\n",
      "2021-01-14 14:01:47,261 : INFO : diff #set()\n",
      "2021-01-14 14:02:07,701 : INFO : alphabet #32006\n",
      "2021-01-14 14:02:17,768 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.239298386454734, 0.44656844574572485], [0.8757266849279404, 0.124273315], [2.0, 1.2451124978365313], [4.247927513443585, 6.89087415148015, 7.048529590606288, 4.090272074317449, 2.8006020771627025, 0.15765543912613733]]\n",
      "2021-01-14 14:02:17,774 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:02:17,776 : INFO : built Dictionary(268 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 2277 corpus positions)\n",
      "2021-01-14 14:02:17,918 : INFO : token count processed\n",
      "2021-01-14 14:02:17,952 : INFO : frequencies processed\n",
      "2021-01-14 14:02:27,815 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:02:27,816 : INFO : entropies processed\n",
      "2021-01-14 14:02:27,817 : INFO : extropies processed\n",
      "2021-01-14 14:02:27,828 : INFO : token count processed\n",
      "2021-01-14 14:02:27,833 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:02:27,837 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:02:27,838 : INFO : vocab #32006\n",
      "2021-01-14 14:02:27,845 : INFO : diff #set()\n",
      "2021-01-14 14:02:47,979 : INFO : alphabet #32006\n",
      "2021-01-14 14:02:57,581 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.2099793667269059, 0.45249291240264017], [0.8429051488637924, 0.15709485], [2.0, 1.2451124978365313], [4.247927513443585, 6.655493573668506, 6.795698034566742, 4.107723052545349, 2.547770521123157, 0.1402044608982358]]\n",
      "2021-01-14 14:02:57,586 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:02:57,587 : INFO : built Dictionary(244 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1284 corpus positions)\n",
      "2021-01-14 14:02:57,699 : INFO : token count processed\n",
      "2021-01-14 14:02:57,734 : INFO : frequencies processed\n",
      "2021-01-14 14:03:07,645 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:03:07,646 : INFO : entropies processed\n",
      "2021-01-14 14:03:07,647 : INFO : extropies processed\n",
      "2021-01-14 14:03:07,654 : INFO : token count processed\n",
      "2021-01-14 14:03:07,659 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:03:07,664 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:03:07,664 : INFO : vocab #32006\n",
      "2021-01-14 14:03:07,671 : INFO : diff #set()\n",
      "2021-01-14 14:03:28,054 : INFO : alphabet #32006\n",
      "2021-01-14 14:03:38,043 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.2155624855141356, 0.45135265041642164], [0.8413745909929276, 0.15862541], [2.321928094887362, 1.2877123795494492], [4.247927513443585, 6.6236746347295465, 6.747094417037694, 4.124507731135438, 2.4991669035941086, 0.12341978230814732]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 14:03:38,047 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:03:38,049 : INFO : built Dictionary(246 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1223 corpus positions)\n",
      "2021-01-14 14:03:38,167 : INFO : token count processed\n",
      "2021-01-14 14:03:38,202 : INFO : frequencies processed\n",
      "2021-01-14 14:03:48,107 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:03:48,108 : INFO : entropies processed\n",
      "2021-01-14 14:03:48,109 : INFO : extropies processed\n",
      "2021-01-14 14:03:48,117 : INFO : token count processed\n",
      "2021-01-14 14:03:48,121 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:03:48,125 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:03:48,126 : INFO : vocab #32006\n",
      "2021-01-14 14:03:48,133 : INFO : diff #set()\n",
      "2021-01-14 14:04:08,315 : INFO : alphabet #32006\n",
      "2021-01-14 14:04:18,350 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.193677786875789, 0.4558554615371242], [0.8174424916505814, 0.18255751], [2.0, 1.2451124978365313], [4.247927513443585, 6.75472436518627, 6.876824954719891, 4.1258269239099645, 2.6288974412763055, 0.12210058953362068]]\n",
      "2021-01-14 14:04:18,354 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:04:18,355 : INFO : built Dictionary(200 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1018 corpus positions)\n",
      "2021-01-14 14:04:18,454 : INFO : token count processed\n",
      "2021-01-14 14:04:18,490 : INFO : frequencies processed\n",
      "2021-01-14 14:04:28,489 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:04:28,490 : INFO : entropies processed\n",
      "2021-01-14 14:04:28,491 : INFO : extropies processed\n",
      "2021-01-14 14:04:28,499 : INFO : token count processed\n",
      "2021-01-14 14:04:28,504 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:04:28,508 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:04:28,509 : INFO : vocab #32006\n",
      "2021-01-14 14:04:28,515 : INFO : diff #set()\n",
      "2021-01-14 14:04:48,582 : INFO : alphabet #32006\n",
      "2021-01-14 14:04:58,649 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.2014535239607453, 0.4542453379623704], [0.8282851278781891, 0.17171487], [1.0, 1.0], [4.247927513443585, 6.597313085495733, 6.718817064754301, 4.126423534185017, 2.470889551310716, 0.12150397925856815]]\n",
      "2021-01-14 14:04:58,653 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:04:58,655 : INFO : built Dictionary(224 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 984 corpus positions)\n",
      "2021-01-14 14:04:58,754 : INFO : token count processed\n",
      "2021-01-14 14:04:58,787 : INFO : frequencies processed\n",
      "2021-01-14 14:05:08,984 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:05:08,985 : INFO : entropies processed\n",
      "2021-01-14 14:05:08,986 : INFO : extropies processed\n",
      "2021-01-14 14:05:08,996 : INFO : token count processed\n",
      "2021-01-14 14:05:09,001 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:05:09,005 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:05:09,006 : INFO : vocab #32006\n",
      "2021-01-14 14:05:09,012 : INFO : diff #set()\n",
      "2021-01-14 14:05:29,050 : INFO : alphabet #32006\n",
      "2021-01-14 14:05:39,082 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.200699918800655, 0.45440088921572885], [0.825388565659523, 0.17461143], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.659481538516613, 6.798751028247887, 4.1086580237123105, 2.5508235148043017, 0.13926948973127384]]\n",
      "2021-01-14 14:05:39,087 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:05:39,088 : INFO : built Dictionary(248 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1146 corpus positions)\n",
      "2021-01-14 14:05:39,207 : INFO : token count processed\n",
      "2021-01-14 14:05:39,239 : INFO : frequencies processed\n",
      "2021-01-14 14:05:49,653 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:05:49,654 : INFO : entropies processed\n",
      "2021-01-14 14:05:49,655 : INFO : extropies processed\n",
      "2021-01-14 14:05:49,663 : INFO : token count processed\n",
      "2021-01-14 14:05:49,667 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:05:49,672 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:05:49,673 : INFO : vocab #32006\n",
      "2021-01-14 14:05:49,679 : INFO : diff #set()\n",
      "2021-01-14 14:06:10,061 : INFO : alphabet #32006\n",
      "2021-01-14 14:06:20,095 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.2043297541415852, 0.45365263437612224], [0.8362860977649689, 0.1637139], [2.0, 1.2451124978365313], [4.247927513443585, 6.774682571479102, 6.883147986267129, 4.139462098655557, 2.635220472823544, 0.10846541478802774]]\n",
      "2021-01-14 14:06:20,110 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:06:20,114 : INFO : built Dictionary(427 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 7883 corpus positions)\n",
      "2021-01-14 14:06:20,371 : INFO : token count processed\n",
      "2021-01-14 14:06:20,409 : INFO : frequencies processed\n",
      "2021-01-14 14:06:30,498 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:06:30,499 : INFO : entropies processed\n",
      "2021-01-14 14:06:30,500 : INFO : extropies processed\n",
      "2021-01-14 14:06:30,513 : INFO : token count processed\n",
      "2021-01-14 14:06:30,518 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:06:30,522 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:06:30,523 : INFO : vocab #32006\n",
      "2021-01-14 14:06:30,530 : INFO : diff #set()\n",
      "2021-01-14 14:06:50,509 : INFO : alphabet #32006\n",
      "2021-01-14 14:07:00,558 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.2468176491014789, 0.44507394732274264], [0.8845980614423752, 0.11540194], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.839453716525233, 6.988804983994695, 4.098576245974123, 2.7408774705511094, 0.14935126746946192]]\n",
      "2021-01-14 14:07:00,565 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:07:00,567 : INFO : built Dictionary(324 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 2325 corpus positions)\n",
      "2021-01-14 14:07:00,729 : INFO : token count processed\n",
      "2021-01-14 14:07:00,762 : INFO : frequencies processed\n",
      "2021-01-14 14:07:10,834 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:07:10,835 : INFO : entropies processed\n",
      "2021-01-14 14:07:10,836 : INFO : extropies processed\n",
      "2021-01-14 14:07:10,843 : INFO : token count processed\n",
      "2021-01-14 14:07:10,847 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:07:10,851 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:07:10,852 : INFO : vocab #32006\n",
      "2021-01-14 14:07:10,858 : INFO : diff #set()\n",
      "2021-01-14 14:07:30,921 : INFO : alphabet #32006\n",
      "2021-01-14 14:07:41,057 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.1865386965259985, 0.45734383827224884], [0.8016628175973892, 0.19833718], [2.584962500721156, 1.315172029168969], [4.247927513443585, 6.86432793886027, 6.9752095483085075, 4.137045903995348, 2.7272820348649223, 0.11088160944823766]]\n",
      "2021-01-14 14:07:41,061 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:07:41,062 : INFO : built Dictionary(161 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 527 corpus positions)\n",
      "2021-01-14 14:07:41,132 : INFO : token count processed\n",
      "2021-01-14 14:07:41,164 : INFO : frequencies processed\n",
      "2021-01-14 14:07:50,748 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:07:50,749 : INFO : entropies processed\n",
      "2021-01-14 14:07:50,750 : INFO : extropies processed\n",
      "2021-01-14 14:07:50,757 : INFO : token count processed\n",
      "2021-01-14 14:07:50,761 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:07:50,764 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:07:50,765 : INFO : vocab #32006\n",
      "2021-01-14 14:07:50,771 : INFO : diff #set()\n",
      "2021-01-14 14:08:10,770 : INFO : alphabet #32006\n",
      "2021-01-14 14:08:20,722 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.1989875031616395, 0.4547547444277103], [0.8255007565021515, 0.17449924], [1.0, 1.0], [4.247927513443585, 6.431978396403875, 6.591070113979652, 4.088835795867808, 2.3431426005360665, 0.15909171757577667]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 14:08:20,726 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:08:20,728 : INFO : built Dictionary(219 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 681 corpus positions)\n",
      "2021-01-14 14:08:20,828 : INFO : token count processed\n",
      "2021-01-14 14:08:20,861 : INFO : frequencies processed\n",
      "2021-01-14 14:08:30,834 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:08:30,835 : INFO : entropies processed\n",
      "2021-01-14 14:08:30,836 : INFO : extropies processed\n",
      "2021-01-14 14:08:30,846 : INFO : token count processed\n",
      "2021-01-14 14:08:30,851 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:08:30,856 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:08:30,856 : INFO : vocab #32006\n",
      "2021-01-14 14:08:30,863 : INFO : diff #set()\n",
      "2021-01-14 14:08:50,970 : INFO : alphabet #32006\n",
      "2021-01-14 14:09:00,926 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/test_auth_utility.py')[[1.2046482074466693, 0.4535871059256923], [0.8210792392492294, 0.17892076], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.911818353685893, 7.0457143085361595, 4.114031558593319, 2.7977867950925743, 0.13389595485026629]]\n",
      "2021-01-14 14:09:00,940 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:09:00,943 : INFO : built Dictionary(312 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 7211 corpus positions)\n",
      "2021-01-14 14:09:01,120 : INFO : token count processed\n",
      "2021-01-14 14:09:01,156 : INFO : frequencies processed\n",
      "2021-01-14 14:09:11,031 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:09:11,033 : INFO : entropies processed\n",
      "2021-01-14 14:09:11,033 : INFO : extropies processed\n",
      "2021-01-14 14:09:11,043 : INFO : token count processed\n",
      "2021-01-14 14:09:11,048 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:09:11,052 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:09:11,052 : INFO : vocab #32006\n",
      "2021-01-14 14:09:11,059 : INFO : diff #set()\n",
      "2021-01-14 14:09:31,641 : INFO : alphabet #32006\n",
      "2021-01-14 14:09:41,596 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.198146243747401, 0.4549287850362491], [0.8197895586490631, 0.18021044], [0.0, 0.0], [4.247927513443585, 6.363791471162389, 6.399349070127072, 4.212369914478903, 2.151421556683487, 0.035557598964683024]]\n",
      "2021-01-14 14:09:41,600 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:09:41,602 : INFO : built Dictionary(211 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1281 corpus positions)\n",
      "2021-01-14 14:09:41,706 : INFO : token count processed\n",
      "2021-01-14 14:09:41,742 : INFO : frequencies processed\n",
      "2021-01-14 14:09:51,815 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:09:51,816 : INFO : entropies processed\n",
      "2021-01-14 14:09:51,817 : INFO : extropies processed\n",
      "2021-01-14 14:09:51,824 : INFO : token count processed\n",
      "2021-01-14 14:09:51,829 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:09:51,833 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:09:51,834 : INFO : vocab #32006\n",
      "2021-01-14 14:09:51,841 : INFO : diff #set()\n",
      "2021-01-14 14:10:11,811 : INFO : alphabet #32006\n",
      "2021-01-14 14:10:21,781 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.2115179770989128, 0.45217810135633996], [0.8514375686645508, 0.14856243], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.29000629755059, 6.508614548379481, 4.0293192626146945, 2.260687034935896, 0.21860825082889068]]\n",
      "2021-01-14 14:10:21,786 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:10:21,787 : INFO : built Dictionary(220 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1234 corpus positions)\n",
      "2021-01-14 14:10:21,883 : INFO : token count processed\n",
      "2021-01-14 14:10:21,915 : INFO : frequencies processed\n",
      "2021-01-14 14:10:31,995 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:10:31,996 : INFO : entropies processed\n",
      "2021-01-14 14:10:31,997 : INFO : extropies processed\n",
      "2021-01-14 14:10:32,005 : INFO : token count processed\n",
      "2021-01-14 14:10:32,009 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:10:32,014 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:10:32,015 : INFO : vocab #32006\n",
      "2021-01-14 14:10:32,021 : INFO : diff #set()\n",
      "2021-01-14 14:10:52,099 : INFO : alphabet #32006\n",
      "2021-01-14 14:11:02,122 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.2086103012370681, 0.45277340209809236], [0.8486240804195404, 0.15137592], [2.0, 1.2451124978365313], [4.247927513443585, 6.361621244785958, 6.563646277592275, 4.0459024806372685, 2.3157187641486896, 0.20202503280631667]]\n",
      "2021-01-14 14:11:02,126 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:11:02,128 : INFO : built Dictionary(230 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1257 corpus positions)\n",
      "2021-01-14 14:11:02,246 : INFO : token count processed\n",
      "2021-01-14 14:11:02,278 : INFO : frequencies processed\n",
      "2021-01-14 14:11:12,413 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:11:12,414 : INFO : entropies processed\n",
      "2021-01-14 14:11:12,415 : INFO : extropies processed\n",
      "2021-01-14 14:11:12,422 : INFO : token count processed\n",
      "2021-01-14 14:11:12,427 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:11:12,431 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:11:12,432 : INFO : vocab #32006\n",
      "2021-01-14 14:11:12,438 : INFO : diff #set()\n",
      "2021-01-14 14:11:32,262 : INFO : alphabet #32006\n",
      "2021-01-14 14:11:42,381 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.2072434150551863, 0.45305379242687543], [0.8355123549699783, 0.16448765], [1.0, 1.0], [4.247927513443585, 6.620594433343389, 6.831297569203862, 4.037224377583112, 2.583370055760277, 0.21070313586047273]]\n",
      "2021-01-14 14:11:42,386 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:11:42,388 : INFO : built Dictionary(202 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1350 corpus positions)\n",
      "2021-01-14 14:11:42,478 : INFO : token count processed\n",
      "2021-01-14 14:11:42,507 : INFO : frequencies processed\n",
      "2021-01-14 14:11:52,517 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:11:52,518 : INFO : entropies processed\n",
      "2021-01-14 14:11:52,519 : INFO : extropies processed\n",
      "2021-01-14 14:11:52,526 : INFO : token count processed\n",
      "2021-01-14 14:11:52,531 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:11:52,535 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:11:52,536 : INFO : vocab #32006\n",
      "2021-01-14 14:11:52,542 : INFO : diff #set()\n",
      "2021-01-14 14:12:13,018 : INFO : alphabet #32006\n",
      "2021-01-14 14:12:23,269 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.206855080747117, 0.45313351507497096], [0.8448195457458496, 0.15518045], [1.0, 1.0], [4.247927513443585, 6.207411496248084, 6.338078088034739, 4.117260921656931, 2.0901505745911537, 0.13066659178665496]]\n",
      "2021-01-14 14:12:23,272 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:12:23,276 : INFO : built Dictionary(140 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 254 corpus positions)\n",
      "2021-01-14 14:12:23,333 : INFO : token count processed\n",
      "2021-01-14 14:12:23,403 : INFO : frequencies processed\n",
      "2021-01-14 14:12:33,364 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:12:33,366 : INFO : entropies processed\n",
      "2021-01-14 14:12:33,367 : INFO : extropies processed\n",
      "2021-01-14 14:12:33,377 : INFO : token count processed\n",
      "2021-01-14 14:12:33,382 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:12:33,387 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:12:33,387 : INFO : vocab #32006\n",
      "2021-01-14 14:12:33,394 : INFO : diff #set()\n",
      "2021-01-14 14:12:53,304 : INFO : alphabet #32006\n",
      "2021-01-14 14:13:02,972 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.1900553883817822, 0.45660945622881877], [0.8015438467264175, 0.19845615], [2.0, 1.2451124978365313], [4.247927513443585, 6.5805228788529595, 6.7676921584212675, 4.060758233875278, 2.5197646449776823, 0.18716927956830798]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 14:13:02,977 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 14:13:02,978 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:13:02,979 : INFO : built Dictionary(223 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1215 corpus positions)\n",
      "2021-01-14 14:13:03,094 : INFO : token count processed\n",
      "2021-01-14 14:13:03,166 : INFO : frequencies processed\n",
      "2021-01-14 14:13:13,208 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:13:13,209 : INFO : entropies processed\n",
      "2021-01-14 14:13:13,210 : INFO : extropies processed\n",
      "2021-01-14 14:13:13,221 : INFO : token count processed\n",
      "2021-01-14 14:13:13,226 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:13:13,231 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:13:13,231 : INFO : vocab #32006\n",
      "2021-01-14 14:13:13,238 : INFO : diff #set()\n",
      "2021-01-14 14:13:33,290 : INFO : alphabet #32006\n",
      "2021-01-14 14:13:43,244 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.1987264518320966, 0.4548087367424658], [0.8342567533254623, 0.16574325], [2.0, 1.2451124978365313], [4.247927513443585, 6.422089779976135, 6.518623149524535, 4.151394143895185, 2.2706956360809496, 0.09653336954839986]]\n",
      "2021-01-14 14:13:43,248 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:13:43,250 : INFO : built Dictionary(236 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1497 corpus positions)\n",
      "2021-01-14 14:13:43,375 : INFO : token count processed\n",
      "2021-01-14 14:13:43,445 : INFO : frequencies processed\n",
      "2021-01-14 14:13:53,393 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:13:53,394 : INFO : entropies processed\n",
      "2021-01-14 14:13:53,395 : INFO : extropies processed\n",
      "2021-01-14 14:13:53,406 : INFO : token count processed\n",
      "2021-01-14 14:13:53,411 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:13:53,418 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:13:53,419 : INFO : vocab #32006\n",
      "2021-01-14 14:13:53,426 : INFO : diff #set()\n",
      "2021-01-14 14:14:13,679 : INFO : alphabet #32006\n",
      "2021-01-14 14:14:24,231 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.2151160494017734, 0.45144361636044555], [0.8512717187404633, 0.14872828], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.485445644653597, 6.721553657627408, 4.011819500469774, 2.4736261441838225, 0.23610801297381112]]\n",
      "2021-01-14 14:14:24,236 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:14:24,237 : INFO : built Dictionary(212 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1383 corpus positions)\n",
      "2021-01-14 14:14:24,336 : INFO : token count processed\n",
      "2021-01-14 14:14:24,379 : INFO : frequencies processed\n",
      "2021-01-14 14:14:34,763 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:14:34,764 : INFO : entropies processed\n",
      "2021-01-14 14:14:34,765 : INFO : extropies processed\n",
      "2021-01-14 14:14:34,773 : INFO : token count processed\n",
      "2021-01-14 14:14:34,778 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:14:34,782 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:14:34,783 : INFO : vocab #32006\n",
      "2021-01-14 14:14:34,789 : INFO : diff #set()\n",
      "2021-01-14 14:14:54,908 : INFO : alphabet #32006\n",
      "2021-01-14 14:15:04,876 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.2086503022361774, 0.45276520189164243], [0.8469199240207672, 0.15308008], [1.0, 1.0], [4.247927513443585, 6.2276600107346916, 6.356071065942546, 4.119516458235731, 2.1081435524989605, 0.12841105520785412]]\n",
      "2021-01-14 14:15:04,881 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:15:04,883 : INFO : built Dictionary(201 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1581 corpus positions)\n",
      "2021-01-14 14:15:04,977 : INFO : token count processed\n",
      "2021-01-14 14:15:05,013 : INFO : frequencies processed\n",
      "2021-01-14 14:15:15,199 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:15:15,201 : INFO : entropies processed\n",
      "2021-01-14 14:15:15,202 : INFO : extropies processed\n",
      "2021-01-14 14:15:15,209 : INFO : token count processed\n",
      "2021-01-14 14:15:15,214 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:15:15,218 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:15:15,219 : INFO : vocab #32006\n",
      "2021-01-14 14:15:15,225 : INFO : diff #set()\n",
      "2021-01-14 14:15:35,281 : INFO : alphabet #32006\n",
      "2021-01-14 14:15:45,370 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.2138470230117793, 0.451702393889697], [0.8566282838582993, 0.14337172], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.253918170574241, 6.430761602904708, 4.071084081113119, 2.1828340894611227, 0.1768434323304673]]\n",
      "2021-01-14 14:15:45,374 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:15:45,376 : INFO : built Dictionary(174 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 678 corpus positions)\n",
      "2021-01-14 14:15:45,468 : INFO : token count processed\n",
      "2021-01-14 14:15:45,503 : INFO : frequencies processed\n",
      "2021-01-14 14:15:55,632 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:15:55,633 : INFO : entropies processed\n",
      "2021-01-14 14:15:55,634 : INFO : extropies processed\n",
      "2021-01-14 14:15:55,642 : INFO : token count processed\n",
      "2021-01-14 14:15:55,646 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:15:55,651 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:15:55,651 : INFO : vocab #32006\n",
      "2021-01-14 14:15:55,658 : INFO : diff #set()\n",
      "2021-01-14 14:16:16,020 : INFO : alphabet #32006\n",
      "2021-01-14 14:16:26,129 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.1954369599514367, 0.45549019090127746], [0.8258146494626999, 0.17418535], [1.0, 1.0], [4.247927513443585, 6.374522245625576, 6.5807429463513945, 4.041706812717766, 2.3328154329078092, 0.20622070072581877]]\n",
      "2021-01-14 14:16:26,134 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:16:26,136 : INFO : built Dictionary(295 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1960 corpus positions)\n",
      "2021-01-14 14:16:26,284 : INFO : token count processed\n",
      "2021-01-14 14:16:26,322 : INFO : frequencies processed\n",
      "2021-01-14 14:16:36,274 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:16:36,275 : INFO : entropies processed\n",
      "2021-01-14 14:16:36,276 : INFO : extropies processed\n",
      "2021-01-14 14:16:36,284 : INFO : token count processed\n",
      "2021-01-14 14:16:36,288 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:16:36,293 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:16:36,294 : INFO : vocab #32006\n",
      "2021-01-14 14:16:36,300 : INFO : diff #set()\n",
      "2021-01-14 14:16:56,598 : INFO : alphabet #32006\n",
      "2021-01-14 14:17:06,632 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.214328285291609, 0.45160422085666857], [0.8446355313062668, 0.15536447], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.731238669067808, 6.993174682320679, 3.985991500190713, 2.7452471688770936, 0.2619360132528712]]\n",
      "2021-01-14 14:17:06,636 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:17:06,638 : INFO : built Dictionary(220 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1352 corpus positions)\n",
      "2021-01-14 14:17:06,757 : INFO : token count processed\n",
      "2021-01-14 14:17:06,793 : INFO : frequencies processed\n",
      "2021-01-14 14:17:16,720 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:17:16,721 : INFO : entropies processed\n",
      "2021-01-14 14:17:16,722 : INFO : extropies processed\n",
      "2021-01-14 14:17:16,733 : INFO : token count processed\n",
      "2021-01-14 14:17:16,738 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:17:16,743 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:17:16,744 : INFO : vocab #32006\n",
      "2021-01-14 14:17:16,750 : INFO : diff #set()\n",
      "2021-01-14 14:17:37,200 : INFO : alphabet #32006\n",
      "2021-01-14 14:17:47,199 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.2035460671281557, 0.45381397508211985], [0.8357911705970764, 0.16420883], [1.584962500721156, 1.1699250014423124], [4.247927513443585, 6.503741451859337, 6.68734581784089, 4.064323147462033, 2.4394183043973046, 0.1836043659815525]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 14:17:47,204 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:17:47,206 : INFO : built Dictionary(240 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 1674 corpus positions)\n",
      "2021-01-14 14:17:47,321 : INFO : token count processed\n",
      "2021-01-14 14:17:47,355 : INFO : frequencies processed\n",
      "2021-01-14 14:17:57,169 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:17:57,170 : INFO : entropies processed\n",
      "2021-01-14 14:17:57,171 : INFO : extropies processed\n",
      "2021-01-14 14:17:57,179 : INFO : token count processed\n",
      "2021-01-14 14:17:57,183 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:17:57,188 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:17:57,189 : INFO : vocab #32006\n",
      "2021-01-14 14:17:57,195 : INFO : diff #set()\n",
      "2021-01-14 14:18:17,516 : INFO : alphabet #32006\n",
      "2021-01-14 14:18:27,500 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.2079589648560614, 0.4529069678906785], [0.8603842854499817, 0.13961571], [2.321928094887362, 1.2877123795494492], [4.247927513443585, 6.334729224484471, 6.459823898482566, 4.122832839445491, 2.2118963850389806, 0.12509467399809449]]\n",
      "2021-01-14 14:18:27,505 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:18:27,507 : INFO : built Dictionary(240 unique tokens: ['.', '28', '3)', 'att', 'fix']...) from 2 documents (total 2023 corpus positions)\n",
      "2021-01-14 14:18:27,618 : INFO : token count processed\n",
      "2021-01-14 14:18:27,651 : INFO : frequencies processed\n",
      "2021-01-14 14:18:37,627 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:18:37,628 : INFO : entropies processed\n",
      "2021-01-14 14:18:37,629 : INFO : extropies processed\n",
      "2021-01-14 14:18:37,636 : INFO : token count processed\n",
      "2021-01-14 14:18:37,641 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:18:37,645 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:18:37,646 : INFO : vocab #32006\n",
      "2021-01-14 14:18:37,653 : INFO : diff #set()\n",
      "2021-01-14 14:19:02,963 : INFO : alphabet #32006\n",
      "2021-01-14 14:19:17,527 : INFO : Computed distances or similarities ('284', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.2227163865583444, 0.44989995396956634], [0.8764464557170868, 0.123553544], [1.0, 1.0], [4.247927513443585, 6.21319712067992, 6.404135860062512, 4.056988774060994, 2.156208346618927, 0.19093873938259165]]\n",
      "2021-01-14 14:19:17,531 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 14:19:17,532 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:19:17,534 : INFO : built Dictionary(266 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1398 corpus positions)\n",
      "2021-01-14 14:19:17,592 : INFO : token count processed\n",
      "2021-01-14 14:19:17,626 : INFO : frequencies processed\n",
      "2021-01-14 14:19:30,161 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:19:30,162 : INFO : entropies processed\n",
      "2021-01-14 14:19:30,163 : INFO : extropies processed\n",
      "2021-01-14 14:19:30,171 : INFO : token count processed\n",
      "2021-01-14 14:19:30,176 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:19:30,181 : INFO : alphabet_target #32010\n",
      "2021-01-14 14:19:30,182 : INFO : vocab #32006\n",
      "2021-01-14 14:19:30,189 : INFO : diff #set()\n",
      "2021-01-14 14:19:52,701 : INFO : alphabet #32006\n",
      "2021-01-14 14:20:02,538 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.3096799454250847, 0.4329604203304259], [0.9819041192531586, 0.01809588], [0.0, 0.0], [2.807354922057604, 6.905617163738059, 7.010705995945761, 2.7022660898499016, 4.203351073888157, 0.10508883220770215]]\n",
      "2021-01-14 14:20:02,544 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:20:02,546 : INFO : built Dictionary(357 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 2288 corpus positions)\n",
      "2021-01-14 14:20:02,642 : INFO : token count processed\n",
      "2021-01-14 14:20:02,675 : INFO : frequencies processed\n",
      "2021-01-14 14:20:13,008 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:20:13,009 : INFO : entropies processed\n",
      "2021-01-14 14:20:13,010 : INFO : extropies processed\n",
      "2021-01-14 14:20:13,018 : INFO : token count processed\n",
      "2021-01-14 14:20:13,022 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:20:13,027 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:20:13,028 : INFO : vocab #32006\n",
      "2021-01-14 14:20:13,035 : INFO : diff #set()\n",
      "2021-01-14 14:20:33,024 : INFO : alphabet #32006\n",
      "2021-01-14 14:20:42,979 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.3061524351143456, 0.43362268025895573], [0.9789094813168049, 0.021090519], [0.0, 0.0], [2.807354922057604, 7.1219284286457345, 7.317301180885723, 2.6119821698176153, 4.509946258828119, 0.1953727522399884]]\n",
      "2021-01-14 14:20:42,985 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:20:42,986 : INFO : built Dictionary(276 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 2271 corpus positions)\n",
      "2021-01-14 14:20:43,048 : INFO : token count processed\n",
      "2021-01-14 14:20:43,080 : INFO : frequencies processed\n",
      "2021-01-14 14:20:53,130 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:20:53,131 : INFO : entropies processed\n",
      "2021-01-14 14:20:53,131 : INFO : extropies processed\n",
      "2021-01-14 14:20:53,139 : INFO : token count processed\n",
      "2021-01-14 14:20:53,144 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:20:53,148 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:20:53,149 : INFO : vocab #32006\n",
      "2021-01-14 14:20:53,156 : INFO : diff #set()\n",
      "2021-01-14 14:21:13,373 : INFO : alphabet #32006\n",
      "2021-01-14 14:21:23,463 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.3106493294991035, 0.4327787809398051], [0.9851399455219507, 0.0148600545], [0.0, 0.0], [2.807354922057604, 6.41099024988467, 6.456926351273763, 2.7614188206685117, 3.649571429216159, 0.045936101389092876]]\n",
      "2021-01-14 14:21:23,467 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:21:23,469 : INFO : built Dictionary(155 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 670 corpus positions)\n",
      "2021-01-14 14:21:23,520 : INFO : token count processed\n",
      "2021-01-14 14:21:23,550 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:21:23,553 : INFO : frequencies processed\n",
      "2021-01-14 14:21:23,555 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:21:23,562 : INFO : token count processed\n",
      "2021-01-14 14:21:23,568 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:21:23,575 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:21:23,577 : INFO : vocab #32006\n",
      "2021-01-14 14:21:23,585 : INFO : diff #set()\n",
      "2021-01-14 14:21:43,364 : INFO : alphabet #32006\n",
      "2021-01-14 14:21:53,338 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.3064102204722248, 0.43357421464914225], [0.9878474669530988, 0.012152533], [nan, nan], [2.807354922057604, 6.077866832717642, 6.1644426412278595, 2.720779113547387, 3.3570877191702553, 0.08657580851021773]]\n",
      "2021-01-14 14:21:53,341 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:21:53,343 : INFO : built Dictionary(130 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 435 corpus positions)\n",
      "2021-01-14 14:21:53,373 : INFO : token count processed\n",
      "2021-01-14 14:21:53,413 : INFO : frequencies processed\n",
      "2021-01-14 14:22:03,498 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:22:03,499 : INFO : entropies processed\n",
      "2021-01-14 14:22:03,500 : INFO : extropies processed\n",
      "2021-01-14 14:22:03,507 : INFO : token count processed\n",
      "2021-01-14 14:22:03,512 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:22:03,516 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:22:03,517 : INFO : vocab #32006\n",
      "2021-01-14 14:22:03,523 : INFO : diff #set()\n",
      "2021-01-14 14:22:23,791 : INFO : alphabet #32006\n",
      "2021-01-14 14:22:33,807 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.3019357654713701, 0.4344169872156397], [0.9797668159008026, 0.020233184], [0.0, 0.0], [2.807354922057604, 5.977547459003844, 6.0315773155539, 2.753325065507548, 3.2242223934962957, 0.054029856550055655]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 14:22:33,812 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:22:33,814 : INFO : built Dictionary(233 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 2146 corpus positions)\n",
      "2021-01-14 14:22:33,866 : INFO : token count processed\n",
      "2021-01-14 14:22:33,899 : INFO : frequencies processed\n",
      "2021-01-14 14:22:43,891 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:22:43,892 : INFO : entropies processed\n",
      "2021-01-14 14:22:43,893 : INFO : extropies processed\n",
      "2021-01-14 14:22:43,901 : INFO : token count processed\n",
      "2021-01-14 14:22:43,906 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:22:43,910 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:22:43,910 : INFO : vocab #32006\n",
      "2021-01-14 14:22:43,917 : INFO : diff #set()\n",
      "2021-01-14 14:23:03,720 : INFO : alphabet #32006\n",
      "2021-01-14 14:23:13,722 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.2934069849448622, 0.43603250821355716], [0.9597821794450283, 0.04021782], [1.584962500721156, 1.1699250014423124], [2.807354922057604, 6.4614394051846435, 6.533186529893295, 2.735607797348952, 3.725831607835691, 0.07174712470865163]]\n",
      "2021-01-14 14:23:13,727 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 14:23:13,728 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:23:13,729 : INFO : built Dictionary(197 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1370 corpus positions)\n",
      "2021-01-14 14:23:13,791 : INFO : token count processed\n",
      "2021-01-14 14:23:13,827 : INFO : frequencies processed\n",
      "2021-01-14 14:23:23,741 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:23:23,742 : INFO : entropies processed\n",
      "2021-01-14 14:23:23,743 : INFO : extropies processed\n",
      "2021-01-14 14:23:23,754 : INFO : token count processed\n",
      "2021-01-14 14:23:23,759 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:23:23,763 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:23:23,764 : INFO : vocab #32006\n",
      "2021-01-14 14:23:23,771 : INFO : diff #set()\n",
      "2021-01-14 14:23:43,844 : INFO : alphabet #32006\n",
      "2021-01-14 14:23:53,781 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.308002080983632, 0.433275172600285], [0.9781888574361801, 0.021811143], [0.0, 0.0], [2.807354922057604, 6.327195724598159, 6.401796463248894, 2.7327541834068683, 3.5944415411912902, 0.07460073865073547]]\n",
      "2021-01-14 14:23:53,793 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 14:23:53,794 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:23:53,800 : INFO : built Dictionary(411 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 6262 corpus positions)\n",
      "2021-01-14 14:23:53,912 : INFO : token count processed\n",
      "2021-01-14 14:23:53,946 : INFO : frequencies processed\n",
      "2021-01-14 14:24:03,747 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:24:03,748 : INFO : entropies processed\n",
      "2021-01-14 14:24:03,749 : INFO : extropies processed\n",
      "2021-01-14 14:24:03,758 : INFO : token count processed\n",
      "2021-01-14 14:24:03,763 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:24:03,767 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:24:03,768 : INFO : vocab #32006\n",
      "2021-01-14 14:24:03,774 : INFO : diff #set()\n",
      "2021-01-14 14:24:23,837 : INFO : alphabet #32006\n",
      "2021-01-14 14:24:33,813 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.3004983596312178, 0.43468842123421697], [0.970603983849287, 0.029396016], [1.584962500721156, 1.1699250014423124], [2.807354922057604, 6.9079058562486315, 7.003111431937992, 2.7121493463682445, 4.195756509880388, 0.09520557568936017]]\n",
      "2021-01-14 14:24:33,820 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:24:33,822 : INFO : built Dictionary(315 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 2656 corpus positions)\n",
      "2021-01-14 14:24:33,894 : INFO : token count processed\n",
      "2021-01-14 14:24:33,930 : INFO : frequencies processed\n",
      "2021-01-14 14:24:43,893 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:24:43,894 : INFO : entropies processed\n",
      "2021-01-14 14:24:43,895 : INFO : extropies processed\n",
      "2021-01-14 14:24:43,903 : INFO : token count processed\n",
      "2021-01-14 14:24:43,908 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:24:43,912 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:24:43,913 : INFO : vocab #32006\n",
      "2021-01-14 14:24:43,919 : INFO : diff #set()\n",
      "2021-01-14 14:25:04,068 : INFO : alphabet #32006\n",
      "2021-01-14 14:25:14,221 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.3119234152959869, 0.4325402793984738], [0.9817012548446655, 0.018298745], [0.0, 0.0], [2.807354922057604, 6.61034830706307, 6.744606092504562, 2.6730971366161116, 3.937251170446958, 0.13425778544149214]]\n",
      "2021-01-14 14:25:14,224 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:25:14,226 : INFO : built Dictionary(202 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 703 corpus positions)\n",
      "2021-01-14 14:25:14,268 : INFO : token count processed\n",
      "2021-01-14 14:25:14,293 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:25:14,294 : INFO : frequencies processed\n",
      "2021-01-14 14:25:14,294 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:25:14,300 : INFO : token count processed\n",
      "2021-01-14 14:25:14,304 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:25:14,308 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:25:14,310 : INFO : vocab #32006\n",
      "2021-01-14 14:25:14,316 : INFO : diff #set()\n",
      "2021-01-14 14:25:34,847 : INFO : alphabet #32006\n",
      "2021-01-14 14:25:44,694 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.3093485639624938, 0.4330225482653649], [0.9870178950950503, 0.012982105], [nan, nan], [2.807354922057604, 6.616715366949855, 6.707585530736312, 2.7164847582711467, 3.9002306086787075, 0.09087016378645707]]\n",
      "2021-01-14 14:25:44,701 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:25:44,704 : INFO : built Dictionary(420 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 2737 corpus positions)\n",
      "2021-01-14 14:25:44,824 : INFO : token count processed\n",
      "2021-01-14 14:25:44,858 : INFO : frequencies processed\n",
      "2021-01-14 14:25:54,807 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:25:54,808 : INFO : entropies processed\n",
      "2021-01-14 14:25:54,809 : INFO : extropies processed\n",
      "2021-01-14 14:25:54,818 : INFO : token count processed\n",
      "2021-01-14 14:25:54,822 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:25:54,826 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:25:54,827 : INFO : vocab #32006\n",
      "2021-01-14 14:25:54,833 : INFO : diff #set()\n",
      "2021-01-14 14:26:14,912 : INFO : alphabet #32006\n",
      "2021-01-14 14:26:24,855 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.291885595456404, 0.4363219534092237], [0.958400409668684, 0.04159959], [1.0, 1.0], [2.807354922057604, 7.32185870753746, 7.50192190154502, 2.627291728050044, 4.694566979487416, 0.18006319400755988]]\n",
      "2021-01-14 14:26:24,858 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:26:24,859 : INFO : built Dictionary(55 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 102 corpus positions)\n",
      "2021-01-14 14:26:24,876 : INFO : token count processed\n",
      "2021-01-14 14:26:24,904 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:26:24,907 : INFO : frequencies processed\n",
      "2021-01-14 14:26:24,908 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:26:24,916 : INFO : token count processed\n",
      "2021-01-14 14:26:24,920 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:26:24,927 : INFO : alphabet_target #32008\n",
      "2021-01-14 14:26:24,928 : INFO : vocab #32006\n",
      "2021-01-14 14:26:24,935 : INFO : diff #set()\n",
      "2021-01-14 14:26:44,847 : INFO : alphabet #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 14:26:54,829 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/fireException.py')[[1.3084170874936791, 0.4331972785237573], [0.9811711646616459, 0.018828835], [nan, nan], [2.807354922057604, 5.176618657501385, 5.4440335127319175, 2.5399400668270724, 2.6366785906743133, 0.2674148552305322]]\n",
      "2021-01-14 14:26:54,833 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:26:54,834 : INFO : built Dictionary(151 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 495 corpus positions)\n",
      "2021-01-14 14:26:54,878 : INFO : token count processed\n",
      "2021-01-14 14:26:54,933 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:26:54,939 : INFO : frequencies processed\n",
      "2021-01-14 14:26:54,940 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:26:54,945 : INFO : token count processed\n",
      "2021-01-14 14:26:54,955 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:26:54,963 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:26:54,964 : INFO : vocab #32006\n",
      "2021-01-14 14:26:54,971 : INFO : diff #set()\n",
      "2021-01-14 14:27:15,025 : INFO : alphabet #32006\n",
      "2021-01-14 14:27:25,003 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.3103091945968572, 0.4328424967267195], [0.9842397440224886, 0.015760256], [nan, nan], [2.807354922057604, 6.468846789852156, 6.5922633583378065, 2.6839383535719534, 3.7849084362802023, 0.1234165684856503]]\n",
      "2021-01-14 14:27:25,009 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:27:25,011 : INFO : built Dictionary(366 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 2544 corpus positions)\n",
      "2021-01-14 14:27:25,125 : INFO : token count processed\n",
      "2021-01-14 14:27:25,161 : INFO : frequencies processed\n",
      "2021-01-14 14:27:35,118 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:27:35,119 : INFO : entropies processed\n",
      "2021-01-14 14:27:35,120 : INFO : extropies processed\n",
      "2021-01-14 14:27:35,128 : INFO : token count processed\n",
      "2021-01-14 14:27:35,133 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:27:35,137 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:27:35,138 : INFO : vocab #32006\n",
      "2021-01-14 14:27:35,144 : INFO : diff #set()\n",
      "2021-01-14 14:27:55,153 : INFO : alphabet #32006\n",
      "2021-01-14 14:28:04,448 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.3051925855792696, 0.43380323459990267], [0.9737902469933033, 0.026209753], [1.0, 1.0], [2.807354922057604, 6.957796704012729, 7.074905047437989, 2.690246578632344, 4.267550125380385, 0.11710834342525978]]\n",
      "2021-01-14 14:28:04,455 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:28:04,457 : INFO : built Dictionary(276 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 3041 corpus positions)\n",
      "2021-01-14 14:28:04,521 : INFO : token count processed\n",
      "2021-01-14 14:28:04,552 : INFO : frequencies processed\n",
      "2021-01-14 14:28:14,432 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:28:14,433 : INFO : entropies processed\n",
      "2021-01-14 14:28:14,434 : INFO : extropies processed\n",
      "2021-01-14 14:28:14,442 : INFO : token count processed\n",
      "2021-01-14 14:28:14,447 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:28:14,451 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:28:14,453 : INFO : vocab #32006\n",
      "2021-01-14 14:28:14,459 : INFO : diff #set()\n",
      "2021-01-14 14:28:34,965 : INFO : alphabet #32006\n",
      "2021-01-14 14:28:44,989 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.3065411874236257, 0.4335495960152292], [0.9820467513054609, 0.017953249], [0.0, 0.0], [2.807354922057604, 6.441859572014148, 6.5671208917320545, 2.682093602339698, 3.7597659696744503, 0.12526131971790644]]\n",
      "2021-01-14 14:28:44,994 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:28:44,996 : INFO : built Dictionary(302 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1517 corpus positions)\n",
      "2021-01-14 14:28:45,072 : INFO : token count processed\n",
      "2021-01-14 14:28:45,107 : INFO : frequencies processed\n",
      "2021-01-14 14:28:55,105 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:28:55,106 : INFO : entropies processed\n",
      "2021-01-14 14:28:55,107 : INFO : extropies processed\n",
      "2021-01-14 14:28:55,117 : INFO : token count processed\n",
      "2021-01-14 14:28:55,122 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:28:55,126 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:28:55,127 : INFO : vocab #32006\n",
      "2021-01-14 14:28:55,133 : INFO : diff #set()\n",
      "2021-01-14 14:29:17,004 : INFO : alphabet #32006\n",
      "2021-01-14 14:29:31,578 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.3105919021216195, 0.4327895372098315], [0.9831690359860659, 0.016830964], [0.0, 0.0], [2.807354922057604, 6.998955278238291, 7.133068285517458, 2.673241914778437, 4.325713363459855, 0.13411300727916764]]\n",
      "2021-01-14 14:29:31,583 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:29:31,585 : INFO : built Dictionary(217 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1336 corpus positions)\n",
      "2021-01-14 14:29:31,633 : INFO : token count processed\n",
      "2021-01-14 14:29:31,667 : INFO : frequencies processed\n",
      "2021-01-14 14:29:46,307 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:29:46,308 : INFO : entropies processed\n",
      "2021-01-14 14:29:46,309 : INFO : extropies processed\n",
      "2021-01-14 14:29:46,317 : INFO : token count processed\n",
      "2021-01-14 14:29:46,322 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:29:46,327 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:29:46,328 : INFO : vocab #32006\n",
      "2021-01-14 14:29:46,335 : INFO : diff #set()\n",
      "2021-01-14 14:30:14,289 : INFO : alphabet #32006\n",
      "2021-01-14 14:30:27,200 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.3069785772589821, 0.4334673975118321], [0.9835021700710058, 0.01649783], [0.0, 0.0], [2.807354922057604, 6.492983191376071, 6.572104343126556, 2.728233770307119, 3.764749421068952, 0.07912115175048484]]\n",
      "2021-01-14 14:30:27,207 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:30:27,209 : INFO : built Dictionary(427 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 3275 corpus positions)\n",
      "2021-01-14 14:30:27,318 : INFO : token count processed\n",
      "2021-01-14 14:30:27,348 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:30:27,350 : INFO : frequencies processed\n",
      "2021-01-14 14:30:27,354 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:30:27,365 : INFO : token count processed\n",
      "2021-01-14 14:30:27,374 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:30:27,381 : INFO : alphabet_target #32008\n",
      "2021-01-14 14:30:27,384 : INFO : vocab #32006\n",
      "2021-01-14 14:30:27,397 : INFO : diff #set()\n",
      "2021-01-14 14:30:51,588 : INFO : alphabet #32006\n",
      "2021-01-14 14:31:03,735 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.3183921652792356, 0.4313334107042914], [0.9870312977582216, 0.012968702], [nan, nan], [2.807354922057604, 6.560342487747443, 6.7303403816413505, 2.637357028163696, 3.9229854595837463, 0.16999789389390774]]\n",
      "2021-01-14 14:31:03,743 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 14:31:03,744 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:31:03,746 : INFO : built Dictionary(443 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 3480 corpus positions)\n",
      "2021-01-14 14:31:03,859 : INFO : token count processed\n",
      "2021-01-14 14:31:03,893 : INFO : frequencies processed\n",
      "2021-01-14 14:31:14,883 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:31:14,884 : INFO : entropies processed\n",
      "2021-01-14 14:31:14,885 : INFO : extropies processed\n",
      "2021-01-14 14:31:14,894 : INFO : token count processed\n",
      "2021-01-14 14:31:14,898 : INFO : alphabet_source #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 14:31:14,903 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:31:14,904 : INFO : vocab #32006\n",
      "2021-01-14 14:31:14,911 : INFO : diff #set()\n",
      "2021-01-14 14:31:37,522 : INFO : alphabet #32006\n",
      "2021-01-14 14:31:48,942 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.3111055059631935, 0.4326933571054051], [0.9813728332519531, 0.018627167], [0.0, 0.0], [2.807354922057604, 7.046173750105238, 7.217299268706613, 2.6362294034562286, 4.409944346649009, 0.17112551860137515]]\n",
      "2021-01-14 14:31:48,953 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 14:31:48,954 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:31:48,957 : INFO : built Dictionary(496 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 5589 corpus positions)\n",
      "2021-01-14 14:31:49,122 : INFO : token count processed\n",
      "2021-01-14 14:31:49,156 : INFO : frequencies processed\n",
      "2021-01-14 14:32:00,442 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:32:00,443 : INFO : entropies processed\n",
      "2021-01-14 14:32:00,444 : INFO : extropies processed\n",
      "2021-01-14 14:32:00,453 : INFO : token count processed\n",
      "2021-01-14 14:32:00,458 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:32:00,463 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:32:00,464 : INFO : vocab #32006\n",
      "2021-01-14 14:32:00,471 : INFO : diff #set()\n",
      "2021-01-14 14:32:26,201 : INFO : alphabet #32006\n",
      "2021-01-14 14:32:41,812 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.3011169635555395, 0.4345715649563782], [0.9745125453919172, 0.025487455], [1.0, 1.0], [2.807354922057604, 7.009229588004272, 7.104500334298793, 2.712084175763083, 4.297145412241189, 0.09527074629452059]]\n",
      "2021-01-14 14:32:41,825 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:32:41,828 : INFO : built Dictionary(575 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 6530 corpus positions)\n",
      "2021-01-14 14:32:42,004 : INFO : token count processed\n",
      "2021-01-14 14:32:42,070 : INFO : frequencies processed\n",
      "2021-01-14 14:32:53,575 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:32:53,576 : INFO : entropies processed\n",
      "2021-01-14 14:32:53,576 : INFO : extropies processed\n",
      "2021-01-14 14:32:53,586 : INFO : token count processed\n",
      "2021-01-14 14:32:53,591 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:32:53,595 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:32:53,596 : INFO : vocab #32006\n",
      "2021-01-14 14:32:53,603 : INFO : diff #set()\n",
      "2021-01-14 14:33:20,432 : INFO : alphabet #32006\n",
      "2021-01-14 14:33:33,877 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.297779919880047, 0.43520268906005755], [0.9590744189918041, 0.04092558], [1.0, 1.0], [2.807354922057604, 7.376088004590871, 7.530599457082858, 2.6528434695656165, 4.723244535025254, 0.15451145249198728]]\n",
      "2021-01-14 14:33:33,881 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:33:33,882 : INFO : built Dictionary(127 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 316 corpus positions)\n",
      "2021-01-14 14:33:33,907 : INFO : token count processed\n",
      "2021-01-14 14:33:33,933 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:33:33,934 : INFO : frequencies processed\n",
      "2021-01-14 14:33:33,934 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:33:33,940 : INFO : token count processed\n",
      "2021-01-14 14:33:33,944 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:33:33,948 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:33:33,949 : INFO : vocab #32006\n",
      "2021-01-14 14:33:33,955 : INFO : diff #set()\n",
      "2021-01-14 14:33:53,555 : INFO : alphabet #32006\n",
      "2021-01-14 14:34:03,559 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.3088384898225494, 0.43311821264589934], [0.9782107658684254, 0.021789234], [nan, nan], [2.807354922057604, 6.2993628166120885, 6.40518898377717, 2.7015287548925224, 3.5978340617195657, 0.10582616716508131]]\n",
      "2021-01-14 14:34:03,562 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:34:03,563 : INFO : built Dictionary(24 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 32 corpus positions)\n",
      "2021-01-14 14:34:03,576 : INFO : token count processed\n",
      "2021-01-14 14:34:03,604 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:34:03,605 : INFO : frequencies processed\n",
      "2021-01-14 14:34:03,606 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:34:03,612 : INFO : token count processed\n",
      "2021-01-14 14:34:03,617 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:34:03,621 : INFO : alphabet_target #32008\n",
      "2021-01-14 14:34:03,622 : INFO : vocab #32006\n",
      "2021-01-14 14:34:03,628 : INFO : diff #set()\n",
      "2021-01-14 14:34:23,616 : INFO : alphabet #32006\n",
      "2021-01-14 14:34:33,719 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.324467525476067, 0.4302060532315646], [0.9951106831431389, 0.004889317], [nan, nan], [2.807354922057604, 3.8936606896881862, 4.2817276788697365, 2.4192879328760535, 1.4743727568121323, 0.38806698918155025]]\n",
      "2021-01-14 14:34:33,742 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:34:33,747 : INFO : built Dictionary(735 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 12474 corpus positions)\n",
      "2021-01-14 14:34:34,084 : INFO : token count processed\n",
      "2021-01-14 14:34:34,157 : INFO : frequencies processed\n",
      "2021-01-14 14:34:44,022 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:34:44,023 : INFO : entropies processed\n",
      "2021-01-14 14:34:44,024 : INFO : extropies processed\n",
      "2021-01-14 14:34:44,035 : INFO : token count processed\n",
      "2021-01-14 14:34:44,040 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:34:44,044 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:34:44,045 : INFO : vocab #32006\n",
      "2021-01-14 14:34:44,052 : INFO : diff #set()\n",
      "2021-01-14 14:35:04,146 : INFO : alphabet #32006\n",
      "2021-01-14 14:35:14,276 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.308607044579898, 0.433161634132487], [0.9816708136349916, 0.018329186], [0.0, 0.0], [2.807354922057604, 7.434393313070278, 7.634493350116882, 2.6072548850110007, 4.827138428059278, 0.20010003704660395]]\n",
      "2021-01-14 14:35:14,284 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:35:14,287 : INFO : built Dictionary(484 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 4101 corpus positions)\n",
      "2021-01-14 14:35:14,448 : INFO : token count processed\n",
      "2021-01-14 14:35:14,532 : INFO : frequencies processed\n",
      "2021-01-14 14:35:24,494 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:35:24,495 : INFO : entropies processed\n",
      "2021-01-14 14:35:24,496 : INFO : extropies processed\n",
      "2021-01-14 14:35:24,505 : INFO : token count processed\n",
      "2021-01-14 14:35:24,509 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:35:24,513 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:35:24,513 : INFO : vocab #32006\n",
      "2021-01-14 14:35:24,519 : INFO : diff #set()\n",
      "2021-01-14 14:35:44,664 : INFO : alphabet #32006\n",
      "2021-01-14 14:35:54,836 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.3029356213843275, 0.4342283782118432], [0.9711078573018312, 0.028892143], [1.584962500721156, 1.1699250014423124], [2.807354922057604, 7.2991514951718255, 7.47377043012034, 2.632735987109089, 4.666415508062736, 0.17461893494851477]]\n",
      "2021-01-14 14:35:54,843 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:35:54,846 : INFO : built Dictionary(450 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 3518 corpus positions)\n",
      "2021-01-14 14:35:54,971 : INFO : token count processed\n",
      "2021-01-14 14:35:55,003 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 14:36:04,978 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:36:04,980 : INFO : entropies processed\n",
      "2021-01-14 14:36:04,981 : INFO : extropies processed\n",
      "2021-01-14 14:36:04,989 : INFO : token count processed\n",
      "2021-01-14 14:36:04,994 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:36:04,998 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:36:04,999 : INFO : vocab #32006\n",
      "2021-01-14 14:36:05,005 : INFO : diff #set()\n",
      "2021-01-14 14:36:24,895 : INFO : alphabet #32006\n",
      "2021-01-14 14:36:34,861 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.303124592818971, 0.4341927497617587], [0.9717526026070118, 0.028247397], [1.584962500721156, 1.1699250014423124], [2.807354922057604, 7.170319527000998, 7.336280056798236, 2.6413943922603664, 4.528925134740632, 0.1659605297972382]]\n",
      "2021-01-14 14:36:34,864 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:36:34,866 : INFO : built Dictionary(161 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 554 corpus positions)\n",
      "2021-01-14 14:36:34,900 : INFO : token count processed\n",
      "2021-01-14 14:36:34,928 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:36:34,929 : INFO : frequencies processed\n",
      "2021-01-14 14:36:34,931 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:36:34,936 : INFO : token count processed\n",
      "2021-01-14 14:36:34,941 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:36:34,945 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:36:34,946 : INFO : vocab #32006\n",
      "2021-01-14 14:36:34,953 : INFO : diff #set()\n",
      "2021-01-14 14:36:54,915 : INFO : alphabet #32006\n",
      "2021-01-14 14:37:04,872 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.315378020147758, 0.43189491793490553], [0.9880268042907119, 0.011973196], [nan, nan], [2.807354922057604, 6.353654804387375, 6.480039803440459, 2.68096992300452, 3.6726848813828545, 0.12638499905308365]]\n",
      "2021-01-14 14:37:04,875 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:37:04,877 : INFO : built Dictionary(155 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 682 corpus positions)\n",
      "2021-01-14 14:37:04,909 : INFO : token count processed\n",
      "2021-01-14 14:37:04,950 : INFO : frequencies processed\n",
      "2021-01-14 14:37:14,934 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:37:14,935 : INFO : entropies processed\n",
      "2021-01-14 14:37:14,936 : INFO : extropies processed\n",
      "2021-01-14 14:37:14,943 : INFO : token count processed\n",
      "2021-01-14 14:37:14,947 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:37:14,951 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:37:14,952 : INFO : vocab #32006\n",
      "2021-01-14 14:37:14,958 : INFO : diff #set()\n",
      "2021-01-14 14:37:35,016 : INFO : alphabet #32006\n",
      "2021-01-14 14:37:45,021 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.3121423177376605, 0.432499328578727], [0.9842898491770029, 0.01571015], [0.0, 0.0], [2.807354922057604, 6.245180322479091, 6.33746202090629, 2.715073223630405, 3.5301070988486862, 0.09228169842719947]]\n",
      "2021-01-14 14:37:45,026 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:37:45,028 : INFO : built Dictionary(383 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1754 corpus positions)\n",
      "2021-01-14 14:37:45,142 : INFO : token count processed\n",
      "2021-01-14 14:37:45,176 : INFO : frequencies processed\n",
      "2021-01-14 14:37:54,926 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:37:54,927 : INFO : entropies processed\n",
      "2021-01-14 14:37:54,928 : INFO : extropies processed\n",
      "2021-01-14 14:37:54,939 : INFO : token count processed\n",
      "2021-01-14 14:37:54,944 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:37:54,949 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:37:54,949 : INFO : vocab #32006\n",
      "2021-01-14 14:37:54,956 : INFO : diff #set()\n",
      "2021-01-14 14:38:15,069 : INFO : alphabet #32006\n",
      "2021-01-14 14:38:25,029 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.305491831375224, 0.4337469282654109], [0.9758112505078316, 0.02418875], [1.0, 1.0], [2.807354922057604, 7.2691387000368, 7.446902807309737, 2.6295908147846667, 4.639547885252133, 0.177764107272937]]\n",
      "2021-01-14 14:38:25,034 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:38:25,036 : INFO : built Dictionary(310 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1591 corpus positions)\n",
      "2021-01-14 14:38:25,109 : INFO : token count processed\n",
      "2021-01-14 14:38:25,141 : INFO : frequencies processed\n",
      "2021-01-14 14:38:35,089 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:38:35,090 : INFO : entropies processed\n",
      "2021-01-14 14:38:35,091 : INFO : extropies processed\n",
      "2021-01-14 14:38:35,099 : INFO : token count processed\n",
      "2021-01-14 14:38:35,103 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:38:35,108 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:38:35,108 : INFO : vocab #32006\n",
      "2021-01-14 14:38:35,115 : INFO : diff #set()\n",
      "2021-01-14 14:38:54,732 : INFO : alphabet #32006\n",
      "2021-01-14 14:39:04,791 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.2989529675935585, 0.4349806255700635], [0.9696680847555399, 0.030331915], [0.0, 0.0], [2.807354922057604, 7.08857858466988, 7.227382704712117, 2.6685508020153677, 4.420027782654513, 0.13880412004223697]]\n",
      "2021-01-14 14:39:04,794 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:39:04,795 : INFO : built Dictionary(135 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 514 corpus positions)\n",
      "2021-01-14 14:39:04,822 : INFO : token count processed\n",
      "2021-01-14 14:39:04,848 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:39:04,849 : INFO : frequencies processed\n",
      "2021-01-14 14:39:04,849 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:39:04,855 : INFO : token count processed\n",
      "2021-01-14 14:39:04,859 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:39:04,864 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:39:04,864 : INFO : vocab #32006\n",
      "2021-01-14 14:39:04,871 : INFO : diff #set()\n",
      "2021-01-14 14:39:26,887 : INFO : alphabet #32006\n",
      "2021-01-14 14:39:41,529 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.3177217449778946, 0.43145817748261994], [0.9893408501520753, 0.01065915], [nan, nan], [2.807354922057604, 6.0479231618016716, 6.156945268037523, 2.698332815821752, 3.349590345979919, 0.10902210623585162]]\n",
      "2021-01-14 14:39:41,533 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:39:41,534 : INFO : built Dictionary(136 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 633 corpus positions)\n",
      "2021-01-14 14:39:41,564 : INFO : token count processed\n",
      "2021-01-14 14:39:41,601 : INFO : frequencies processed\n",
      "2021-01-14 14:39:55,131 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:39:55,132 : INFO : entropies processed\n",
      "2021-01-14 14:39:55,133 : INFO : extropies processed\n",
      "2021-01-14 14:39:55,141 : INFO : token count processed\n",
      "2021-01-14 14:39:55,146 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:39:55,151 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:39:55,152 : INFO : vocab #32006\n",
      "2021-01-14 14:39:55,159 : INFO : diff #set()\n",
      "2021-01-14 14:40:23,599 : INFO : alphabet #32006\n",
      "2021-01-14 14:40:37,152 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.3134292104681398, 0.43225874190360136], [0.9844373315572739, 0.015562668], [0.0, 0.0], [2.807354922057604, 6.036583168403119, 6.123467364310689, 2.720470726150034, 3.3161124422530848, 0.08688419590756968]]\n",
      "2021-01-14 14:40:37,167 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 14:40:37,168 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:40:37,171 : INFO : built Dictionary(550 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 6991 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 14:40:37,376 : INFO : token count processed\n",
      "2021-01-14 14:40:37,410 : INFO : frequencies processed\n",
      "2021-01-14 14:40:51,316 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:40:51,317 : INFO : entropies processed\n",
      "2021-01-14 14:40:51,318 : INFO : extropies processed\n",
      "2021-01-14 14:40:51,328 : INFO : token count processed\n",
      "2021-01-14 14:40:51,333 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:40:51,338 : INFO : alphabet_target #32010\n",
      "2021-01-14 14:40:51,339 : INFO : vocab #32006\n",
      "2021-01-14 14:40:51,346 : INFO : diff #set()\n",
      "2021-01-14 14:41:16,610 : INFO : alphabet #32006\n",
      "2021-01-14 14:41:28,949 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.302858731292481, 0.43424287665216404], [0.9737477712333202, 0.026252229], [1.0, 1.0], [2.807354922057604, 7.29352035514053, 7.440462754307451, 2.6604125228906836, 4.633107832249847, 0.14694239916692098]]\n",
      "2021-01-14 14:41:28,956 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:41:28,959 : INFO : built Dictionary(360 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 3234 corpus positions)\n",
      "2021-01-14 14:41:29,048 : INFO : token count processed\n",
      "2021-01-14 14:41:29,080 : INFO : frequencies processed\n",
      "2021-01-14 14:41:40,647 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:41:40,648 : INFO : entropies processed\n",
      "2021-01-14 14:41:40,649 : INFO : extropies processed\n",
      "2021-01-14 14:41:40,657 : INFO : token count processed\n",
      "2021-01-14 14:41:40,662 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:41:40,667 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:41:40,668 : INFO : vocab #32006\n",
      "2021-01-14 14:41:40,675 : INFO : diff #set()\n",
      "2021-01-14 14:42:06,816 : INFO : alphabet #32006\n",
      "2021-01-14 14:42:22,949 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.3130505832440733, 0.43232949907973545], [0.9862545914947987, 0.0137454085], [0.0, 0.0], [2.807354922057604, 6.8153433747477745, 6.937577249922107, 2.685121046883271, 4.130222327864503, 0.1222338751743326]]\n",
      "2021-01-14 14:42:22,952 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:42:22,954 : INFO : built Dictionary(112 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 255 corpus positions)\n",
      "2021-01-14 14:42:22,977 : INFO : token count processed\n",
      "2021-01-14 14:42:23,004 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:42:23,005 : INFO : frequencies processed\n",
      "2021-01-14 14:42:23,006 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:42:23,011 : INFO : token count processed\n",
      "2021-01-14 14:42:23,016 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:42:23,020 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:42:23,021 : INFO : vocab #32006\n",
      "2021-01-14 14:42:23,028 : INFO : diff #set()\n",
      "2021-01-14 14:42:45,423 : INFO : alphabet #32006\n",
      "2021-01-14 14:42:56,777 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.313632965680914, 0.4322206740798642], [0.9888550946488976, 0.011144905], [nan, nan], [2.807354922057604, 6.150121915859574, 6.239327944949197, 2.7181488929679816, 3.4319730228915932, 0.08920602908962305]]\n",
      "2021-01-14 14:42:56,781 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:42:56,783 : INFO : built Dictionary(274 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 966 corpus positions)\n",
      "2021-01-14 14:42:56,879 : INFO : token count processed\n",
      "2021-01-14 14:42:56,938 : INFO : frequencies processed\n",
      "2021-01-14 14:43:14,326 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:43:14,327 : INFO : entropies processed\n",
      "2021-01-14 14:43:14,328 : INFO : extropies processed\n",
      "2021-01-14 14:43:14,335 : INFO : token count processed\n",
      "2021-01-14 14:43:14,340 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:43:14,345 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:43:14,346 : INFO : vocab #32006\n",
      "2021-01-14 14:43:14,352 : INFO : diff #set()\n",
      "2021-01-14 14:43:37,711 : INFO : alphabet #32006\n",
      "2021-01-14 14:43:47,892 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.2749137876331607, 0.4395770975745012], [0.9163721054792404, 0.083627895], [1.584962500721156, 1.1699250014423124], [2.807354922057604, 7.0391145208191315, 7.229334681124081, 2.617134761752654, 4.4219797590664776, 0.19022016030494981]]\n",
      "2021-01-14 14:43:47,901 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 14:43:47,904 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:43:47,907 : INFO : built Dictionary(580 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 4341 corpus positions)\n",
      "2021-01-14 14:43:48,120 : INFO : token count processed\n",
      "2021-01-14 14:43:48,155 : INFO : frequencies processed\n",
      "2021-01-14 14:43:58,194 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:43:58,195 : INFO : entropies processed\n",
      "2021-01-14 14:43:58,196 : INFO : extropies processed\n",
      "2021-01-14 14:43:58,204 : INFO : token count processed\n",
      "2021-01-14 14:43:58,208 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:43:58,212 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:43:58,213 : INFO : vocab #32006\n",
      "2021-01-14 14:43:58,219 : INFO : diff #set()\n",
      "2021-01-14 14:44:18,457 : INFO : alphabet #32006\n",
      "2021-01-14 14:44:28,363 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.2989152038337974, 0.43498777089835455], [0.9724875278770924, 0.027512472], [1.0, 1.0], [2.807354922057604, 7.482466367279176, 7.738944923609535, 2.5508763657272455, 4.93159000155193, 0.2564785563303591]]\n",
      "2021-01-14 14:44:28,366 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:44:28,368 : INFO : built Dictionary(161 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 563 corpus positions)\n",
      "2021-01-14 14:44:28,416 : INFO : token count processed\n",
      "2021-01-14 14:44:28,441 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:44:28,444 : INFO : frequencies processed\n",
      "2021-01-14 14:44:28,446 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:44:28,452 : INFO : token count processed\n",
      "2021-01-14 14:44:28,458 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:44:28,463 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:44:28,466 : INFO : vocab #32006\n",
      "2021-01-14 14:44:28,473 : INFO : diff #set()\n",
      "2021-01-14 14:44:48,145 : INFO : alphabet #32006\n",
      "2021-01-14 14:44:58,092 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.3155333391551451, 0.4318659477236739], [0.9881086247041821, 0.011891375], [nan, nan], [2.807354922057604, 6.372162341197667, 6.495768920921872, 2.6837483423333985, 3.688413998864268, 0.12360657972420519]]\n",
      "2021-01-14 14:44:58,098 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:44:58,100 : INFO : built Dictionary(308 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1984 corpus positions)\n",
      "2021-01-14 14:44:58,184 : INFO : token count processed\n",
      "2021-01-14 14:44:58,220 : INFO : frequencies processed\n",
      "2021-01-14 14:45:08,387 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:45:08,388 : INFO : entropies processed\n",
      "2021-01-14 14:45:08,389 : INFO : extropies processed\n",
      "2021-01-14 14:45:08,397 : INFO : token count processed\n",
      "2021-01-14 14:45:08,402 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:45:08,406 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:45:08,407 : INFO : vocab #32006\n",
      "2021-01-14 14:45:08,413 : INFO : diff #set()\n",
      "2021-01-14 14:45:28,364 : INFO : alphabet #32006\n",
      "2021-01-14 14:45:38,425 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.3096478533508153, 0.43296643622498965], [0.9807458929717541, 0.019254107], [1.0, 1.0], [2.807354922057604, 6.798155919669889, 6.954814064647943, 2.650696777079551, 4.147459142590339, 0.15665814497805375]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 14:45:38,429 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:45:38,431 : INFO : built Dictionary(166 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 700 corpus positions)\n",
      "2021-01-14 14:45:38,483 : INFO : token count processed\n",
      "2021-01-14 14:45:38,513 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:45:38,516 : INFO : frequencies processed\n",
      "2021-01-14 14:45:38,518 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:45:38,526 : INFO : token count processed\n",
      "2021-01-14 14:45:38,532 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:45:38,538 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:45:38,541 : INFO : vocab #32006\n",
      "2021-01-14 14:45:38,549 : INFO : diff #set()\n",
      "2021-01-14 14:45:58,552 : INFO : alphabet #32006\n",
      "2021-01-14 14:46:08,664 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.317155318967587, 0.4315636469486007], [0.9883892191573977, 0.011610781], [nan, nan], [2.807354922057604, 6.271631856729336, 6.393698746248291, 2.685288032538648, 3.586343824190687, 0.12206688951895561]]\n",
      "2021-01-14 14:46:08,671 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:46:08,673 : INFO : built Dictionary(343 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 3215 corpus positions)\n",
      "2021-01-14 14:46:08,775 : INFO : token count processed\n",
      "2021-01-14 14:46:08,810 : INFO : frequencies processed\n",
      "2021-01-14 14:46:19,211 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:46:19,212 : INFO : entropies processed\n",
      "2021-01-14 14:46:19,213 : INFO : extropies processed\n",
      "2021-01-14 14:46:19,222 : INFO : token count processed\n",
      "2021-01-14 14:46:19,226 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:46:19,231 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:46:19,231 : INFO : vocab #32006\n",
      "2021-01-14 14:46:19,238 : INFO : diff #set()\n",
      "2021-01-14 14:46:40,997 : INFO : alphabet #32006\n",
      "2021-01-14 14:46:54,998 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.309803271896572, 0.4329373034348952], [0.982000207528472, 0.017999792], [0.0, 0.0], [2.807354922057604, 6.873598627629562, 7.007406052792375, 2.6735474968947903, 4.200051130734771, 0.1338074251628134]]\n",
      "2021-01-14 14:46:55,001 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:46:55,003 : INFO : built Dictionary(93 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 163 corpus positions)\n",
      "2021-01-14 14:46:55,023 : INFO : token count processed\n",
      "2021-01-14 14:46:55,053 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:46:55,054 : INFO : frequencies processed\n",
      "2021-01-14 14:46:55,055 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:46:55,061 : INFO : token count processed\n",
      "2021-01-14 14:46:55,066 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:46:55,071 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:46:55,072 : INFO : vocab #32006\n",
      "2021-01-14 14:46:55,080 : INFO : diff #set()\n",
      "2021-01-14 14:47:23,764 : INFO : alphabet #32006\n",
      "2021-01-14 14:47:33,933 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.308401912062486, 0.43320012636210775], [0.9836111851036549, 0.016388815], [nan, nan], [2.807354922057604, 6.049830202851529, 6.214271309126467, 2.6429138157826664, 3.4069163870688626, 0.16444110627493735]]\n",
      "2021-01-14 14:47:33,937 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:47:33,939 : INFO : built Dictionary(241 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1243 corpus positions)\n",
      "2021-01-14 14:47:33,996 : INFO : token count processed\n",
      "2021-01-14 14:47:34,025 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:47:34,026 : INFO : frequencies processed\n",
      "2021-01-14 14:47:34,027 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:47:34,033 : INFO : token count processed\n",
      "2021-01-14 14:47:34,038 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:47:34,042 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:47:34,043 : INFO : vocab #32006\n",
      "2021-01-14 14:47:34,050 : INFO : diff #set()\n",
      "2021-01-14 14:47:54,100 : INFO : alphabet #32006\n",
      "2021-01-14 14:48:04,071 : INFO : Computed distances or similarities ('282', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.3093745318193586, 0.43301767912551875], [0.984580029733479, 0.01541997], [nan, nan], [2.807354922057604, 6.778844940588858, 6.936330884711301, 2.64986897793516, 4.128975962653698, 0.15748594412244366]]\n",
      "2021-01-14 14:48:04,075 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:48:04,076 : INFO : built Dictionary(140 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 276 corpus positions)\n",
      "2021-01-14 14:48:04,119 : INFO : token count processed\n",
      "2021-01-14 14:48:04,149 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:48:04,152 : INFO : frequencies processed\n",
      "2021-01-14 14:48:04,154 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:48:04,161 : INFO : token count processed\n",
      "2021-01-14 14:48:04,167 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:48:04,174 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:48:04,176 : INFO : vocab #32006\n",
      "2021-01-14 14:48:04,184 : INFO : diff #set()\n",
      "2021-01-14 14:48:24,453 : INFO : alphabet #32006\n",
      "2021-01-14 14:48:34,435 : INFO : Computed distances or similarities ('282', 'sacp-python-common/setup.py')[[1.308027547978676, 0.4332703918009037], [0.9836497120559216, 0.016350288], [nan, nan], [2.807354922057604, 6.469677430851302, 6.666494566055002, 2.6105377868539037, 3.859139643997398, 0.19681713520370003]]\n",
      "2021-01-14 14:48:34,439 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:48:34,441 : INFO : built Dictionary(210 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1178 corpus positions)\n",
      "2021-01-14 14:48:34,497 : INFO : token count processed\n",
      "2021-01-14 14:48:34,527 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:48:34,529 : INFO : frequencies processed\n",
      "2021-01-14 14:48:34,531 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:48:34,539 : INFO : token count processed\n",
      "2021-01-14 14:48:34,545 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:48:34,551 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:48:34,554 : INFO : vocab #32006\n",
      "2021-01-14 14:48:34,562 : INFO : diff #set()\n",
      "2021-01-14 14:48:54,603 : INFO : alphabet #32006\n",
      "2021-01-14 14:49:04,501 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.309716988884291, 0.4329534764703143], [0.9830097518861294, 0.016990248], [nan, nan], [2.807354922057604, 6.459180448028249, 6.588728516446631, 2.677806853639222, 3.7813735943890268, 0.12954806841838185]]\n",
      "2021-01-14 14:49:04,504 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:49:04,505 : INFO : built Dictionary(120 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 328 corpus positions)\n",
      "2021-01-14 14:49:04,554 : INFO : token count processed\n",
      "2021-01-14 14:49:04,633 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:49:04,633 : INFO : frequencies processed\n",
      "2021-01-14 14:49:04,634 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:49:04,640 : INFO : token count processed\n",
      "2021-01-14 14:49:04,644 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:49:04,651 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:49:04,652 : INFO : vocab #32006\n",
      "2021-01-14 14:49:04,660 : INFO : diff #set()\n",
      "2021-01-14 14:49:24,811 : INFO : alphabet #32006\n",
      "2021-01-14 14:49:34,890 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.3098991319645357, 0.4329193366766255], [0.9824517648667097, 0.017548235], [nan, nan], [2.807354922057604, 6.097125733496388, 6.211824117783515, 2.6926565377704774, 3.4044691957259112, 0.1146983842871272]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 14:49:34,894 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:49:34,895 : INFO : built Dictionary(112 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 324 corpus positions)\n",
      "2021-01-14 14:49:34,936 : INFO : token count processed\n",
      "2021-01-14 14:49:34,986 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:49:34,991 : INFO : frequencies processed\n",
      "2021-01-14 14:49:34,992 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:49:34,997 : INFO : token count processed\n",
      "2021-01-14 14:49:35,005 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:49:35,011 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:49:35,015 : INFO : vocab #32006\n",
      "2021-01-14 14:49:35,021 : INFO : diff #set()\n",
      "2021-01-14 14:49:54,907 : INFO : alphabet #32006\n",
      "2021-01-14 14:50:04,971 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.3101512669994426, 0.432872086899685], [0.9825458209961653, 0.017454179], [nan, nan], [2.807354922057604, 6.0695858597523715, 6.17733172671476, 2.699609055095216, 3.369976804657156, 0.10774586696238853]]\n",
      "2021-01-14 14:50:04,975 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 14:50:04,976 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:50:04,977 : INFO : built Dictionary(114 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 420 corpus positions)\n",
      "2021-01-14 14:50:05,016 : INFO : token count processed\n",
      "2021-01-14 14:50:05,089 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:50:05,091 : INFO : frequencies processed\n",
      "2021-01-14 14:50:05,091 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:50:05,097 : INFO : token count processed\n",
      "2021-01-14 14:50:05,102 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:50:05,107 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:50:05,108 : INFO : vocab #32006\n",
      "2021-01-14 14:50:05,114 : INFO : diff #set()\n",
      "2021-01-14 14:50:25,043 : INFO : alphabet #32006\n",
      "2021-01-14 14:50:35,215 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.3084308556228157, 0.4331946948136766], [0.9772445615381002, 0.022755438], [nan, nan], [2.807354922057604, 6.104787343210121, 6.186277750352528, 2.725864514915197, 3.378922828294924, 0.0814904071424074]]\n",
      "2021-01-14 14:50:35,232 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 14:50:35,233 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:50:35,236 : INFO : built Dictionary(401 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 9105 corpus positions)\n",
      "2021-01-14 14:50:35,365 : INFO : token count processed\n",
      "2021-01-14 14:50:35,439 : INFO : frequencies processed\n",
      "2021-01-14 14:50:45,411 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:50:45,412 : INFO : entropies processed\n",
      "2021-01-14 14:50:45,413 : INFO : extropies processed\n",
      "2021-01-14 14:50:45,427 : INFO : token count processed\n",
      "2021-01-14 14:50:45,431 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:50:45,436 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:50:45,436 : INFO : vocab #32006\n",
      "2021-01-14 14:50:45,443 : INFO : diff #set()\n",
      "2021-01-14 14:51:05,398 : INFO : alphabet #32006\n",
      "2021-01-14 14:51:15,501 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.3079189471255896, 0.43329077966340873], [0.9774464555084705, 0.022553544], [0.0, 0.0], [2.807354922057604, 6.89087415148015, 7.039606687097887, 2.658622386439868, 4.232251765040283, 0.1487325356177367]]\n",
      "2021-01-14 14:51:15,507 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:51:15,509 : INFO : built Dictionary(260 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 2265 corpus positions)\n",
      "2021-01-14 14:51:15,566 : INFO : token count processed\n",
      "2021-01-14 14:51:15,595 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:51:15,596 : INFO : frequencies processed\n",
      "2021-01-14 14:51:15,596 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:51:15,603 : INFO : token count processed\n",
      "2021-01-14 14:51:15,607 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:51:15,611 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:51:15,613 : INFO : vocab #32006\n",
      "2021-01-14 14:51:15,619 : INFO : diff #set()\n",
      "2021-01-14 14:51:35,523 : INFO : alphabet #32006\n",
      "2021-01-14 14:51:45,637 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.3052598607970596, 0.4337905747659368], [0.974062392488122, 0.025937608], [nan, nan], [2.807354922057604, 6.655493573668506, 6.770420818521438, 2.6924276772046722, 3.9630658964638337, 0.11492724485293149]]\n",
      "2021-01-14 14:51:45,642 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:51:45,644 : INFO : built Dictionary(237 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1272 corpus positions)\n",
      "2021-01-14 14:51:45,695 : INFO : token count processed\n",
      "2021-01-14 14:51:45,724 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:51:45,725 : INFO : frequencies processed\n",
      "2021-01-14 14:51:45,725 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:51:45,732 : INFO : token count processed\n",
      "2021-01-14 14:51:45,736 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:51:45,741 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:51:45,742 : INFO : vocab #32006\n",
      "2021-01-14 14:51:45,749 : INFO : diff #set()\n",
      "2021-01-14 14:52:05,747 : INFO : alphabet #32006\n",
      "2021-01-14 14:52:15,823 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.3098883789922227, 0.4329213519989604], [0.9842682406306267, 0.01573176], [nan, nan], [2.807354922057604, 6.6236746347295465, 6.71196887451081, 2.71906068227634, 3.904613952453206, 0.0882942397812636]]\n",
      "2021-01-14 14:52:15,827 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:52:15,829 : INFO : built Dictionary(238 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1211 corpus positions)\n",
      "2021-01-14 14:52:15,883 : INFO : token count processed\n",
      "2021-01-14 14:52:15,912 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:52:15,913 : INFO : frequencies processed\n",
      "2021-01-14 14:52:15,914 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:52:15,920 : INFO : token count processed\n",
      "2021-01-14 14:52:15,925 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:52:15,929 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:52:15,930 : INFO : vocab #32006\n",
      "2021-01-14 14:52:15,937 : INFO : diff #set()\n",
      "2021-01-14 14:52:36,133 : INFO : alphabet #32006\n",
      "2021-01-14 14:52:46,142 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.307260929880006, 0.4334143516450942], [0.979501249268651, 0.02049875], [nan, nan], [2.807354922057604, 6.75472436518627, 6.839768353232685, 2.7223109340111886, 4.032413431175081, 0.08504398804641511]]\n",
      "2021-01-14 14:52:46,146 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:52:46,148 : INFO : built Dictionary(190 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1006 corpus positions)\n",
      "2021-01-14 14:52:46,189 : INFO : token count processed\n",
      "2021-01-14 14:52:46,218 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:52:46,219 : INFO : frequencies processed\n",
      "2021-01-14 14:52:46,220 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:52:46,226 : INFO : token count processed\n",
      "2021-01-14 14:52:46,230 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:52:46,235 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:52:46,236 : INFO : vocab #32006\n",
      "2021-01-14 14:52:46,242 : INFO : diff #set()\n",
      "2021-01-14 14:53:06,457 : INFO : alphabet #32006\n",
      "2021-01-14 14:53:16,392 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.3112154528932838, 0.4326727734310339], [0.9838035851716995, 0.016196415], [nan, nan], [2.807354922057604, 6.597313085495733, 6.670326188869883, 2.734341818683454, 3.8629712668122784, 0.07301310337414968]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 14:53:16,396 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:53:16,398 : INFO : built Dictionary(215 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 972 corpus positions)\n",
      "2021-01-14 14:53:16,442 : INFO : token count processed\n",
      "2021-01-14 14:53:16,470 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:53:16,472 : INFO : frequencies processed\n",
      "2021-01-14 14:53:16,472 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:53:16,478 : INFO : token count processed\n",
      "2021-01-14 14:53:16,483 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:53:16,488 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:53:16,489 : INFO : vocab #32006\n",
      "2021-01-14 14:53:16,495 : INFO : diff #set()\n",
      "2021-01-14 14:53:36,979 : INFO : alphabet #32006\n",
      "2021-01-14 14:53:47,111 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.3089242260970717, 0.4331021298565378], [0.9813469778746367, 0.018653022], [nan, nan], [2.807354922057604, 6.659481538516613, 6.7516746157710905, 2.715161844803127, 3.9443196937134863, 0.09219307725447745]]\n",
      "2021-01-14 14:53:47,115 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:53:47,117 : INFO : built Dictionary(240 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1134 corpus positions)\n",
      "2021-01-14 14:53:47,167 : INFO : token count processed\n",
      "2021-01-14 14:53:47,196 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:53:47,197 : INFO : frequencies processed\n",
      "2021-01-14 14:53:47,198 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:53:47,204 : INFO : token count processed\n",
      "2021-01-14 14:53:47,208 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:53:47,213 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:53:47,214 : INFO : vocab #32006\n",
      "2021-01-14 14:53:47,221 : INFO : diff #set()\n",
      "2021-01-14 14:54:07,432 : INFO : alphabet #32006\n",
      "2021-01-14 14:54:17,422 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.3122384094580253, 0.4324813548246497], [0.9869221150875092, 0.013077885], [nan, nan], [2.807354922057604, 6.774682571479102, 6.843959192719956, 2.73807830081675, 4.036604270662352, 0.06927662124085465]]\n",
      "2021-01-14 14:54:17,437 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:54:17,440 : INFO : built Dictionary(416 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 7871 corpus positions)\n",
      "2021-01-14 14:54:17,560 : INFO : token count processed\n",
      "2021-01-14 14:54:17,594 : INFO : frequencies processed\n",
      "2021-01-14 14:54:27,522 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:54:27,523 : INFO : entropies processed\n",
      "2021-01-14 14:54:27,524 : INFO : extropies processed\n",
      "2021-01-14 14:54:27,533 : INFO : token count processed\n",
      "2021-01-14 14:54:27,538 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:54:27,543 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:54:27,544 : INFO : vocab #32006\n",
      "2021-01-14 14:54:27,550 : INFO : diff #set()\n",
      "2021-01-14 14:54:47,400 : INFO : alphabet #32006\n",
      "2021-01-14 14:54:57,420 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.3083263220464023, 0.43321431222664797], [0.9799041450023651, 0.020095855], [1.0, 1.0], [2.807354922057604, 6.839453716525233, 6.977945787263225, 2.668862851319611, 4.170590865205622, 0.13849207073799263]]\n",
      "2021-01-14 14:54:57,427 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:54:57,429 : INFO : built Dictionary(317 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 2313 corpus positions)\n",
      "2021-01-14 14:54:57,503 : INFO : token count processed\n",
      "2021-01-14 14:54:57,536 : INFO : frequencies processed\n",
      "2021-01-14 14:55:07,487 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:55:07,488 : INFO : entropies processed\n",
      "2021-01-14 14:55:07,489 : INFO : extropies processed\n",
      "2021-01-14 14:55:07,497 : INFO : token count processed\n",
      "2021-01-14 14:55:07,501 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:55:07,506 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:55:07,507 : INFO : vocab #32006\n",
      "2021-01-14 14:55:07,513 : INFO : diff #set()\n",
      "2021-01-14 14:55:27,626 : INFO : alphabet #32006\n",
      "2021-01-14 14:55:37,653 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.2979768762877149, 0.43516538844179237], [0.9630541987717152, 0.0369458], [0.0, 0.0], [2.807354922057604, 6.86432793886027, 6.954537348219157, 2.7171455126987176, 4.147182426161553, 0.090209409358887]]\n",
      "2021-01-14 14:55:37,657 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:55:37,658 : INFO : built Dictionary(151 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 515 corpus positions)\n",
      "2021-01-14 14:55:37,688 : INFO : token count processed\n",
      "2021-01-14 14:55:37,717 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:55:37,718 : INFO : frequencies processed\n",
      "2021-01-14 14:55:37,718 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:55:37,724 : INFO : token count processed\n",
      "2021-01-14 14:55:37,729 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:55:37,733 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:55:37,734 : INFO : vocab #32006\n",
      "2021-01-14 14:55:37,741 : INFO : diff #set()\n",
      "2021-01-14 14:55:57,865 : INFO : alphabet #32006\n",
      "2021-01-14 14:56:07,827 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.3066100003031795, 0.4335366619708405], [0.9727731980383396, 0.027226802], [nan, nan], [2.807354922057604, 6.431978396403875, 6.513653546940432, 2.7256797715210475, 3.706298624882828, 0.08167515053655716]]\n",
      "2021-01-14 14:56:07,831 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:56:07,833 : INFO : built Dictionary(210 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 669 corpus positions)\n",
      "2021-01-14 14:56:07,876 : INFO : token count processed\n",
      "2021-01-14 14:56:07,904 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:56:07,906 : INFO : frequencies processed\n",
      "2021-01-14 14:56:07,906 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:56:07,912 : INFO : token count processed\n",
      "2021-01-14 14:56:07,917 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:56:07,921 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:56:07,923 : INFO : vocab #32006\n",
      "2021-01-14 14:56:07,929 : INFO : diff #set()\n",
      "2021-01-14 14:56:27,935 : INFO : alphabet #32006\n",
      "2021-01-14 14:56:37,904 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/test_auth_utility.py')[[1.3164452421794226, 0.4316959372884429], [0.9877906125038862, 0.0122093875], [nan, nan], [2.807354922057604, 6.911818353685893, 6.9937580430781106, 2.7254152326653864, 4.186403121020506, 0.0819396893922173]]\n",
      "2021-01-14 14:56:37,918 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:56:37,921 : INFO : built Dictionary(300 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 7199 corpus positions)\n",
      "2021-01-14 14:56:38,002 : INFO : token count processed\n",
      "2021-01-14 14:56:38,035 : INFO : frequencies processed\n",
      "2021-01-14 14:56:47,988 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:56:47,989 : INFO : entropies processed\n",
      "2021-01-14 14:56:47,990 : INFO : extropies processed\n",
      "2021-01-14 14:56:48,000 : INFO : token count processed\n",
      "2021-01-14 14:56:48,004 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:56:48,009 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:56:48,010 : INFO : vocab #32006\n",
      "2021-01-14 14:56:48,017 : INFO : diff #set()\n",
      "2021-01-14 14:57:08,015 : INFO : alphabet #32006\n",
      "2021-01-14 14:57:18,127 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.3131271216496843, 0.43231519385186945], [0.9845089642331004, 0.015491036], [0.0, 0.0], [2.807354922057604, 6.363791471162389, 6.38683378016262, 2.7843126130573737, 3.579478858105016, 0.023042309000230965]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 14:57:18,132 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:57:18,134 : INFO : built Dictionary(201 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1269 corpus positions)\n",
      "2021-01-14 14:57:18,177 : INFO : token count processed\n",
      "2021-01-14 14:57:18,211 : INFO : frequencies processed\n",
      "2021-01-14 14:57:28,153 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:57:28,154 : INFO : entropies processed\n",
      "2021-01-14 14:57:28,155 : INFO : extropies processed\n",
      "2021-01-14 14:57:28,163 : INFO : token count processed\n",
      "2021-01-14 14:57:28,167 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:57:28,172 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:57:28,173 : INFO : vocab #32006\n",
      "2021-01-14 14:57:28,179 : INFO : diff #set()\n",
      "2021-01-14 14:57:48,287 : INFO : alphabet #32006\n",
      "2021-01-14 14:57:58,277 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.3017467259582716, 0.434452665326884], [0.9717133287340403, 0.028286671], [0.0, 0.0], [2.807354922057604, 6.29000629755059, 6.461313657433326, 2.636047562174868, 3.653958735375722, 0.1713073598827357]]\n",
      "2021-01-14 14:57:58,281 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:57:58,283 : INFO : built Dictionary(212 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1222 corpus positions)\n",
      "2021-01-14 14:57:58,329 : INFO : token count processed\n",
      "2021-01-14 14:57:58,358 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:57:58,358 : INFO : frequencies processed\n",
      "2021-01-14 14:57:58,359 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:57:58,365 : INFO : token count processed\n",
      "2021-01-14 14:57:58,370 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:57:58,375 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:57:58,376 : INFO : vocab #32006\n",
      "2021-01-14 14:57:58,382 : INFO : diff #set()\n",
      "2021-01-14 14:58:18,481 : INFO : alphabet #32006\n",
      "2021-01-14 14:58:28,900 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.3112596971041526, 0.4326644908198461], [0.9842291139066219, 0.015770886], [nan, nan], [2.807354922057604, 6.361621244785958, 6.521998669728442, 2.6469774971151194, 3.7146437476708383, 0.1603774249424843]]\n",
      "2021-01-14 14:58:28,905 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:58:28,906 : INFO : built Dictionary(220 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1245 corpus positions)\n",
      "2021-01-14 14:58:28,967 : INFO : token count processed\n",
      "2021-01-14 14:58:28,997 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:58:28,999 : INFO : frequencies processed\n",
      "2021-01-14 14:58:29,001 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:58:29,009 : INFO : token count processed\n",
      "2021-01-14 14:58:29,015 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:58:29,021 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:58:29,024 : INFO : vocab #32006\n",
      "2021-01-14 14:58:29,032 : INFO : diff #set()\n",
      "2021-01-14 14:58:49,149 : INFO : alphabet #32006\n",
      "2021-01-14 14:58:59,148 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.3030344878196751, 0.4342097373221355], [0.9776093643158674, 0.022390636], [nan, nan], [2.807354922057604, 6.620594433343389, 6.789708216769706, 2.6382411386312876, 3.982353294712102, 0.169113783426317]]\n",
      "2021-01-14 14:58:59,152 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:58:59,154 : INFO : built Dictionary(192 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1338 corpus positions)\n",
      "2021-01-14 14:58:59,210 : INFO : token count processed\n",
      "2021-01-14 14:58:59,240 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 14:58:59,243 : INFO : frequencies processed\n",
      "2021-01-14 14:58:59,245 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 14:58:59,253 : INFO : token count processed\n",
      "2021-01-14 14:58:59,259 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:58:59,271 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:58:59,274 : INFO : vocab #32006\n",
      "2021-01-14 14:58:59,283 : INFO : diff #set()\n",
      "2021-01-14 14:59:19,295 : INFO : alphabet #32006\n",
      "2021-01-14 14:59:29,576 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.302316140049623, 0.43434521550044236], [0.975719902664423, 0.024280097], [nan, nan], [2.807354922057604, 6.207411496248084, 6.293595749014031, 2.7211706692916575, 3.486240826956427, 0.0861842527659471]]\n",
      "2021-01-14 14:59:29,579 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 14:59:29,581 : INFO : built Dictionary(131 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 242 corpus positions)\n",
      "2021-01-14 14:59:29,627 : INFO : token count processed\n",
      "2021-01-14 14:59:29,713 : INFO : frequencies processed\n",
      "2021-01-14 14:59:39,416 : INFO : scalar_distribution processed\n",
      "2021-01-14 14:59:39,417 : INFO : entropies processed\n",
      "2021-01-14 14:59:39,418 : INFO : extropies processed\n",
      "2021-01-14 14:59:39,428 : INFO : token count processed\n",
      "2021-01-14 14:59:39,432 : INFO : alphabet_source #32006\n",
      "2021-01-14 14:59:39,438 : INFO : alphabet_target #32009\n",
      "2021-01-14 14:59:39,439 : INFO : vocab #32006\n",
      "2021-01-14 14:59:39,447 : INFO : diff #set()\n",
      "2021-01-14 14:59:59,505 : INFO : alphabet #32006\n",
      "2021-01-14 15:00:09,453 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.3003604041725667, 0.43471449003648505], [0.9669538512825966, 0.03304615], [0.0, 0.0], [2.807354922057604, 6.5805228788529595, 6.67548413677334, 2.712393664137224, 3.868129214715736, 0.09496125792038068]]\n",
      "2021-01-14 15:00:09,458 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 15:00:09,459 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:00:09,463 : INFO : built Dictionary(215 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1203 corpus positions)\n",
      "2021-01-14 15:00:09,542 : INFO : token count processed\n",
      "2021-01-14 15:00:09,622 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 15:00:09,623 : INFO : frequencies processed\n",
      "2021-01-14 15:00:09,625 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 15:00:09,631 : INFO : token count processed\n",
      "2021-01-14 15:00:09,638 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:00:09,642 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:00:09,644 : INFO : vocab #32006\n",
      "2021-01-14 15:00:09,655 : INFO : diff #set()\n",
      "2021-01-14 15:00:29,670 : INFO : alphabet #32006\n",
      "2021-01-14 15:00:40,080 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.3189849550761004, 0.43122315123738425], [0.9874709285795689, 0.012529071], [nan, nan], [2.807354922057604, 6.422089779976135, 6.479248371484105, 2.7501963305496337, 3.6718934494265008, 0.05715859150796998]]\n",
      "2021-01-14 15:00:40,085 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:00:40,086 : INFO : built Dictionary(227 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1485 corpus positions)\n",
      "2021-01-14 15:00:40,138 : INFO : token count processed\n",
      "2021-01-14 15:00:40,165 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 15:00:40,165 : INFO : frequencies processed\n",
      "2021-01-14 15:00:40,167 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 15:00:40,174 : INFO : token count processed\n",
      "2021-01-14 15:00:40,178 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:00:40,182 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:00:40,183 : INFO : vocab #32006\n",
      "2021-01-14 15:00:40,190 : INFO : diff #set()\n",
      "2021-01-14 15:01:00,030 : INFO : alphabet #32006\n",
      "2021-01-14 15:01:10,108 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.3071939355451294, 0.43342693676235167], [0.9809744358062744, 0.019025564], [nan, nan], [2.807354922057604, 6.485445644653597, 6.685484491393025, 2.6073160753181757, 3.8781295693354205, 0.20003884673942807]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 15:01:10,112 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:01:10,114 : INFO : built Dictionary(202 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1371 corpus positions)\n",
      "2021-01-14 15:01:10,183 : INFO : token count processed\n",
      "2021-01-14 15:01:10,241 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 15:01:10,247 : INFO : frequencies processed\n",
      "2021-01-14 15:01:10,249 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 15:01:10,257 : INFO : token count processed\n",
      "2021-01-14 15:01:10,261 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:01:10,268 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:01:10,268 : INFO : vocab #32006\n",
      "2021-01-14 15:01:10,277 : INFO : diff #set()\n",
      "2021-01-14 15:01:30,418 : INFO : alphabet #32006\n",
      "2021-01-14 15:01:40,368 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.3043285297128469, 0.43396589813719605], [0.9767798837274313, 0.023220116], [nan, nan], [2.807354922057604, 6.2276600107346916, 6.311963108820451, 2.7230518239718444, 3.5046081867628467, 0.08430309808575931]]\n",
      "2021-01-14 15:01:40,373 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:01:40,375 : INFO : built Dictionary(190 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1569 corpus positions)\n",
      "2021-01-14 15:01:40,437 : INFO : token count processed\n",
      "2021-01-14 15:01:40,502 : INFO : frequencies processed\n",
      "2021-01-14 15:01:50,463 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:01:50,464 : INFO : entropies processed\n",
      "2021-01-14 15:01:50,465 : INFO : extropies processed\n",
      "2021-01-14 15:01:50,472 : INFO : token count processed\n",
      "2021-01-14 15:01:50,477 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:01:50,481 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:01:50,482 : INFO : vocab #32006\n",
      "2021-01-14 15:01:50,488 : INFO : diff #set()\n",
      "2021-01-14 15:02:10,477 : INFO : alphabet #32006\n",
      "2021-01-14 15:02:20,540 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.3045883780674739, 0.43391696734952545], [0.9757087025791407, 0.024291297], [1.0, 1.0], [2.807354922057604, 6.253918170574241, 6.388803400400466, 2.6724696922313793, 3.5814484783428617, 0.1348852298262253]]\n",
      "2021-01-14 15:02:20,543 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:02:20,545 : INFO : built Dictionary(164 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 666 corpus positions)\n",
      "2021-01-14 15:02:20,580 : INFO : token count processed\n",
      "2021-01-14 15:02:20,608 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 15:02:20,609 : INFO : frequencies processed\n",
      "2021-01-14 15:02:20,610 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 15:02:20,616 : INFO : token count processed\n",
      "2021-01-14 15:02:20,621 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:02:20,625 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:02:20,626 : INFO : vocab #32006\n",
      "2021-01-14 15:02:20,632 : INFO : diff #set()\n",
      "2021-01-14 15:02:40,684 : INFO : alphabet #32006\n",
      "2021-01-14 15:02:50,672 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.3036962527065066, 0.43408500527148314], [0.9761411137878895, 0.023858886], [nan, nan], [2.807354922057604, 6.374522245625576, 6.512953006769915, 2.6689241609132646, 3.7055980847123107, 0.13843076114433917]]\n",
      "2021-01-14 15:02:50,678 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:02:50,680 : INFO : built Dictionary(286 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1948 corpus positions)\n",
      "2021-01-14 15:02:50,769 : INFO : token count processed\n",
      "2021-01-14 15:02:50,832 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 15:02:50,841 : INFO : frequencies processed\n",
      "2021-01-14 15:02:50,842 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 15:02:50,851 : INFO : token count processed\n",
      "2021-01-14 15:02:50,855 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:02:50,861 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:02:50,861 : INFO : vocab #32006\n",
      "2021-01-14 15:02:50,868 : INFO : diff #set()\n",
      "2021-01-14 15:03:11,009 : INFO : alphabet #32006\n",
      "2021-01-14 15:03:21,013 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.3120976704194953, 0.432507680273976], [0.9839545413851738, 0.016045459], [nan, nan], [2.807354922057604, 6.731238669067808, 6.964354128213607, 2.5742394629118053, 4.156999206156003, 0.23311545914579934]]\n",
      "2021-01-14 15:03:21,018 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:03:21,019 : INFO : built Dictionary(211 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1340 corpus positions)\n",
      "2021-01-14 15:03:21,074 : INFO : token count processed\n",
      "2021-01-14 15:03:21,101 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 15:03:21,104 : INFO : frequencies processed\n",
      "2021-01-14 15:03:21,106 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 15:03:21,113 : INFO : token count processed\n",
      "2021-01-14 15:03:21,119 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:03:21,124 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:03:21,127 : INFO : vocab #32006\n",
      "2021-01-14 15:03:21,135 : INFO : diff #set()\n",
      "2021-01-14 15:03:41,164 : INFO : alphabet #32006\n",
      "2021-01-14 15:03:51,129 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.3077196585715016, 0.43332819750688806], [0.9820731971412897, 0.017926803], [nan, nan], [2.807354922057604, 6.503741451859337, 6.648706063833426, 2.6623903100835147, 3.841351141775822, 0.14496461197408905]]\n",
      "2021-01-14 15:03:51,134 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:03:51,136 : INFO : built Dictionary(233 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 1662 corpus positions)\n",
      "2021-01-14 15:03:51,192 : INFO : token count processed\n",
      "2021-01-14 15:03:51,221 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 15:03:51,222 : INFO : frequencies processed\n",
      "2021-01-14 15:03:51,223 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 15:03:51,229 : INFO : token count processed\n",
      "2021-01-14 15:03:51,234 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:03:51,238 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:03:51,239 : INFO : vocab #32006\n",
      "2021-01-14 15:03:51,245 : INFO : diff #set()\n",
      "2021-01-14 15:04:11,254 : INFO : alphabet #32006\n",
      "2021-01-14 15:04:21,195 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.3096419719695762, 0.4329675387511414], [0.9864940233528614, 0.013505977], [nan, nan], [2.807354922057604, 6.334729224484471, 6.427202028281885, 2.7148821182601903, 3.6198471062242805, 0.0924728037974134]]\n",
      "2021-01-14 15:04:21,200 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:04:21,202 : INFO : built Dictionary(230 unique tokens: ['ug', '▁*', '▁binary', '▁deb', '▁merge']...) from 2 documents (total 2011 corpus positions)\n",
      "2021-01-14 15:04:21,259 : INFO : token count processed\n",
      "2021-01-14 15:04:21,288 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 15:04:21,288 : INFO : frequencies processed\n",
      "2021-01-14 15:04:21,290 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 15:04:21,296 : INFO : token count processed\n",
      "2021-01-14 15:04:21,301 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:04:21,305 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:04:21,306 : INFO : vocab #32006\n",
      "2021-01-14 15:04:21,313 : INFO : diff #set()\n",
      "2021-01-14 15:04:41,041 : INFO : alphabet #32006\n",
      "2021-01-14 15:04:51,038 : INFO : Computed distances or similarities ('282', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.3063812994708435, 0.4335796514780238], [0.9843885051086545, 0.015611495], [nan, nan], [2.807354922057604, 6.21319712067992, 6.371739934673252, 2.6488121080642717, 3.564385012615648, 0.15854281399333203]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 15:04:51,043 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 15:04:51,044 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:04:51,047 : INFO : built Dictionary(300 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1453 corpus positions)\n",
      "2021-01-14 15:04:51,442 : INFO : token count processed\n",
      "2021-01-14 15:04:51,474 : INFO : frequencies processed\n",
      "2021-01-14 15:05:01,460 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:05:01,461 : INFO : entropies processed\n",
      "2021-01-14 15:05:01,462 : INFO : extropies processed\n",
      "2021-01-14 15:05:01,470 : INFO : token count processed\n",
      "2021-01-14 15:05:01,474 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:05:01,479 : INFO : alphabet_target #32010\n",
      "2021-01-14 15:05:01,480 : INFO : vocab #32006\n",
      "2021-01-14 15:05:01,486 : INFO : diff #set()\n",
      "2021-01-14 15:05:21,553 : INFO : alphabet #32006\n",
      "2021-01-14 15:05:31,581 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.1723816673530456, 0.4603242676129087], [0.8233251571655273, 0.17667484], [3.6644977792004623, 1.381962919072374], [5.631615665225582, 6.905617163738059, 7.155315928839128, 5.381916900124514, 1.523700263613546, 0.24969876510106914]]\n",
      "2021-01-14 15:05:31,587 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:05:31,589 : INFO : built Dictionary(383 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 2343 corpus positions)\n",
      "2021-01-14 15:05:32,159 : INFO : token count processed\n",
      "2021-01-14 15:05:32,222 : INFO : frequencies processed\n",
      "2021-01-14 15:05:42,204 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:05:42,205 : INFO : entropies processed\n",
      "2021-01-14 15:05:42,206 : INFO : extropies processed\n",
      "2021-01-14 15:05:42,218 : INFO : token count processed\n",
      "2021-01-14 15:05:42,222 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:05:42,227 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:05:42,228 : INFO : vocab #32006\n",
      "2021-01-14 15:05:42,234 : INFO : diff #set()\n",
      "2021-01-14 15:06:02,305 : INFO : alphabet #32006\n",
      "2021-01-14 15:06:12,338 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.1584209915174704, 0.46330164686591263], [0.8262512534856796, 0.17374875], [4.334962500721156, 1.4043693211870836], [5.631615665225582, 7.1219284286457345, 7.4020201980139735, 5.351523895857342, 1.7704045327883913, 0.280091769368239]]\n",
      "2021-01-14 15:06:12,344 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:06:12,346 : INFO : built Dictionary(306 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 2326 corpus positions)\n",
      "2021-01-14 15:06:12,735 : INFO : token count processed\n",
      "2021-01-14 15:06:12,779 : INFO : frequencies processed\n",
      "2021-01-14 15:06:22,761 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:06:22,762 : INFO : entropies processed\n",
      "2021-01-14 15:06:22,763 : INFO : extropies processed\n",
      "2021-01-14 15:06:22,774 : INFO : token count processed\n",
      "2021-01-14 15:06:22,779 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:06:22,787 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:06:22,788 : INFO : vocab #32006\n",
      "2021-01-14 15:06:22,796 : INFO : diff #set()\n",
      "2021-01-14 15:06:42,829 : INFO : alphabet #32006\n",
      "2021-01-14 15:06:52,813 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2173883380071826, 0.45098099546186066], [0.8889678567647934, 0.11103214], [4.058813890331201, 1.3971393108496475], [5.631615665225582, 6.41099024988467, 6.574252321687139, 5.468353593423114, 0.9426366564615565, 0.16326207180246843]]\n",
      "2021-01-14 15:06:52,817 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:06:52,818 : INFO : built Dictionary(190 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 725 corpus positions)\n",
      "2021-01-14 15:06:53,032 : INFO : token count processed\n",
      "2021-01-14 15:06:53,069 : INFO : frequencies processed\n",
      "2021-01-14 15:07:03,105 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:07:03,106 : INFO : entropies processed\n",
      "2021-01-14 15:07:03,107 : INFO : extropies processed\n",
      "2021-01-14 15:07:03,118 : INFO : token count processed\n",
      "2021-01-14 15:07:03,123 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:07:03,127 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:07:03,128 : INFO : vocab #32006\n",
      "2021-01-14 15:07:03,135 : INFO : diff #set()\n",
      "2021-01-14 15:07:23,023 : INFO : alphabet #32006\n",
      "2021-01-14 15:07:32,992 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.1669720332089688, 0.46147342221078236], [0.857599675655365, 0.14240032], [3.4182958340544896, 1.369895090630202], [5.631615665225582, 6.077866832717642, 6.454423561092824, 5.2550589368504, 0.8228078958672418, 0.3765567283751823]]\n",
      "2021-01-14 15:07:32,996 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:07:32,997 : INFO : built Dictionary(165 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 490 corpus positions)\n",
      "2021-01-14 15:07:33,150 : INFO : token count processed\n",
      "2021-01-14 15:07:33,179 : INFO : frequencies processed\n",
      "2021-01-14 15:07:43,270 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:07:43,271 : INFO : entropies processed\n",
      "2021-01-14 15:07:43,272 : INFO : extropies processed\n",
      "2021-01-14 15:07:43,282 : INFO : token count processed\n",
      "2021-01-14 15:07:43,287 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:07:43,291 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:07:43,292 : INFO : vocab #32006\n",
      "2021-01-14 15:07:43,298 : INFO : diff #set()\n",
      "2021-01-14 15:08:03,237 : INFO : alphabet #32006\n",
      "2021-01-14 15:08:13,313 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.1587236920604973, 0.46323668178464383], [0.8458955585956573, 0.15410444], [3.5465935642949384, 1.3764678303056375], [5.631615665225582, 5.977547459003844, 6.40883645828985, 5.200326665939576, 0.7772207930642674, 0.4312889992860054]]\n",
      "2021-01-14 15:08:13,319 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:08:13,321 : INFO : built Dictionary(265 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 2201 corpus positions)\n",
      "2021-01-14 15:08:13,654 : INFO : token count processed\n",
      "2021-01-14 15:08:13,737 : INFO : frequencies processed\n",
      "2021-01-14 15:08:23,636 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:08:23,637 : INFO : entropies processed\n",
      "2021-01-14 15:08:23,638 : INFO : extropies processed\n",
      "2021-01-14 15:08:23,650 : INFO : token count processed\n",
      "2021-01-14 15:08:23,654 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:08:23,659 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:08:23,662 : INFO : vocab #32006\n",
      "2021-01-14 15:08:23,670 : INFO : diff #set()\n",
      "2021-01-14 15:08:43,710 : INFO : alphabet #32006\n",
      "2021-01-14 15:08:53,712 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.1812982376964098, 0.458442583741352], [0.8623428046703339, 0.1376572], [4.058813890331201, 1.3971393108496475], [5.631615665225582, 6.4614394051846435, 6.655318449582619, 5.437736620827607, 1.0237027843570372, 0.19387904439797587]]\n",
      "2021-01-14 15:08:53,717 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 15:08:53,718 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:08:53,723 : INFO : built Dictionary(238 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1425 corpus positions)\n",
      "2021-01-14 15:08:54,011 : INFO : token count processed\n",
      "2021-01-14 15:08:54,090 : INFO : frequencies processed\n",
      "2021-01-14 15:09:04,073 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:09:04,075 : INFO : entropies processed\n",
      "2021-01-14 15:09:04,075 : INFO : extropies processed\n",
      "2021-01-14 15:09:04,083 : INFO : token count processed\n",
      "2021-01-14 15:09:04,088 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:09:04,092 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:09:04,093 : INFO : vocab #32006\n",
      "2021-01-14 15:09:04,099 : INFO : diff #set()\n",
      "2021-01-14 15:09:24,113 : INFO : alphabet #32006\n",
      "2021-01-14 15:09:33,568 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.1891760140130498, 0.45679287256892026], [0.8410888463258743, 0.15891115], [2.521640636343318, 1.2998438251349493], [5.631615665225582, 6.327195724598159, 6.591978344857263, 5.366833044966477, 0.9603626796316806, 0.26478262025910393]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 15:09:33,581 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 15:09:33,583 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:09:33,590 : INFO : built Dictionary(435 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 6317 corpus positions)\n",
      "2021-01-14 15:09:34,260 : INFO : token count processed\n",
      "2021-01-14 15:09:34,320 : INFO : frequencies processed\n",
      "2021-01-14 15:09:44,324 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:09:44,325 : INFO : entropies processed\n",
      "2021-01-14 15:09:44,326 : INFO : extropies processed\n",
      "2021-01-14 15:09:44,335 : INFO : token count processed\n",
      "2021-01-14 15:09:44,340 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:09:44,344 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:09:44,345 : INFO : vocab #32006\n",
      "2021-01-14 15:09:44,351 : INFO : diff #set()\n",
      "2021-01-14 15:10:04,380 : INFO : alphabet #32006\n",
      "2021-01-14 15:10:14,519 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.153250781661398, 0.4644140889285657], [0.8374090492725372, 0.16259095], [4.506890595608519, 1.4048681879949363], [5.631615665225582, 6.9079058562486315, 7.044966559399409, 5.494554962074805, 1.4133508941738269, 0.13706070315077756]]\n",
      "2021-01-14 15:10:14,525 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:10:14,528 : INFO : built Dictionary(341 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 2711 corpus positions)\n",
      "2021-01-14 15:10:15,021 : INFO : token count processed\n",
      "2021-01-14 15:10:15,107 : INFO : frequencies processed\n",
      "2021-01-14 15:10:25,051 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:10:25,052 : INFO : entropies processed\n",
      "2021-01-14 15:10:25,053 : INFO : extropies processed\n",
      "2021-01-14 15:10:25,064 : INFO : token count processed\n",
      "2021-01-14 15:10:25,069 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:10:25,073 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:10:25,074 : INFO : vocab #32006\n",
      "2021-01-14 15:10:25,081 : INFO : diff #set()\n",
      "2021-01-14 15:10:45,563 : INFO : alphabet #32006\n",
      "2021-01-14 15:10:55,492 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.133017850462615, 0.46881933021944344], [0.8054070621728897, 0.19459294], [4.349648912578752, 1.4051631060490313], [5.631615665225582, 6.61034830706307, 6.830736696905776, 5.411227275382876, 1.1991210316801935, 0.22038838984270548]]\n",
      "2021-01-14 15:10:55,496 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:10:55,498 : INFO : built Dictionary(235 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 758 corpus positions)\n",
      "2021-01-14 15:10:55,786 : INFO : token count processed\n",
      "2021-01-14 15:10:55,856 : INFO : frequencies processed\n",
      "2021-01-14 15:11:05,948 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:11:05,949 : INFO : entropies processed\n",
      "2021-01-14 15:11:05,950 : INFO : extropies processed\n",
      "2021-01-14 15:11:05,958 : INFO : token count processed\n",
      "2021-01-14 15:11:05,962 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:11:05,967 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:11:05,967 : INFO : vocab #32006\n",
      "2021-01-14 15:11:05,974 : INFO : diff #set()\n",
      "2021-01-14 15:11:26,032 : INFO : alphabet #32006\n",
      "2021-01-14 15:11:35,984 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.1506032222534328, 0.46498581870075767], [0.8015018552541733, 0.19849814], [3.640223928941852, 1.3797477693995936], [5.631615665225582, 6.616715366949855, 6.92950204001245, 5.318828992162986, 1.2978863747868674, 0.312786673062595]]\n",
      "2021-01-14 15:11:35,991 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:11:35,993 : INFO : built Dictionary(444 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 2792 corpus positions)\n",
      "2021-01-14 15:11:36,711 : INFO : token count processed\n",
      "2021-01-14 15:11:36,777 : INFO : frequencies processed\n",
      "2021-01-14 15:11:46,956 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:11:46,957 : INFO : entropies processed\n",
      "2021-01-14 15:11:46,958 : INFO : extropies processed\n",
      "2021-01-14 15:11:46,965 : INFO : token count processed\n",
      "2021-01-14 15:11:46,970 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:11:46,974 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:11:46,975 : INFO : vocab #32006\n",
      "2021-01-14 15:11:46,981 : INFO : diff #set()\n",
      "2021-01-14 15:12:07,047 : INFO : alphabet #32006\n",
      "2021-01-14 15:12:17,034 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.1121167601664899, 0.47345867371516615], [0.7615310847759247, 0.23846892], [4.444187891679296, 1.4030448178973773], [5.631615665225582, 7.32185870753746, 7.566600595786244, 5.3868737769768, 1.9349849305606615, 0.2447418882487833]]\n",
      "2021-01-14 15:12:17,037 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:12:17,038 : INFO : built Dictionary(98 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 157 corpus positions)\n",
      "2021-01-14 15:12:17,105 : INFO : token count processed\n",
      "2021-01-14 15:12:17,165 : INFO : frequencies processed\n",
      "2021-01-14 15:12:27,259 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:12:27,260 : INFO : entropies processed\n",
      "2021-01-14 15:12:27,261 : INFO : extropies processed\n",
      "2021-01-14 15:12:27,268 : INFO : token count processed\n",
      "2021-01-14 15:12:27,273 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:12:27,277 : INFO : alphabet_target #32008\n",
      "2021-01-14 15:12:27,278 : INFO : vocab #32006\n",
      "2021-01-14 15:12:27,285 : INFO : diff #set()\n",
      "2021-01-14 15:12:47,241 : INFO : alphabet #32006\n",
      "2021-01-14 15:12:57,354 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/fireException.py')[[1.1871643120019344, 0.457213019850662], [0.8540703803300858, 0.14592962], [1.5, 1.1225562489182657], [5.631615665225582, 5.176618657501385, 6.314054779520902, 4.494179543206066, 0.6824391142953194, 1.1374361220195164]]\n",
      "2021-01-14 15:12:57,358 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:12:57,359 : INFO : built Dictionary(188 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 550 corpus positions)\n",
      "2021-01-14 15:12:57,566 : INFO : token count processed\n",
      "2021-01-14 15:12:57,634 : INFO : frequencies processed\n",
      "2021-01-14 15:13:07,650 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:13:07,651 : INFO : entropies processed\n",
      "2021-01-14 15:13:07,652 : INFO : extropies processed\n",
      "2021-01-14 15:13:07,660 : INFO : token count processed\n",
      "2021-01-14 15:13:07,664 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:13:07,669 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:13:07,670 : INFO : vocab #32006\n",
      "2021-01-14 15:13:07,676 : INFO : diff #set()\n",
      "2021-01-14 15:13:27,569 : INFO : alphabet #32006\n",
      "2021-01-14 15:13:37,668 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.1899261976853353, 0.45663639306975734], [0.8623628914356232, 0.13763711], [3.121928094887362, 1.3519647487142497], [5.631615665225582, 6.468846789852156, 6.906359153487507, 5.194103301590232, 1.274743488261925, 0.43751236363535106]]\n",
      "2021-01-14 15:13:37,674 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:13:37,677 : INFO : built Dictionary(395 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 2599 corpus positions)\n",
      "2021-01-14 15:13:38,330 : INFO : token count processed\n",
      "2021-01-14 15:13:38,393 : INFO : frequencies processed\n",
      "2021-01-14 15:13:48,416 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:13:48,417 : INFO : entropies processed\n",
      "2021-01-14 15:13:48,418 : INFO : extropies processed\n",
      "2021-01-14 15:13:48,426 : INFO : token count processed\n",
      "2021-01-14 15:13:48,430 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:13:48,434 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:13:48,435 : INFO : vocab #32006\n",
      "2021-01-14 15:13:48,442 : INFO : diff #set()\n",
      "2021-01-14 15:14:08,538 : INFO : alphabet #32006\n",
      "2021-01-14 15:14:18,522 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.1913088326363215, 0.45634827236876474], [0.858706459403038, 0.14129354], [4.20184123230257, 1.4009137160862843], [5.631615665225582, 6.957796704012729, 7.165013324835304, 5.424399044403007, 1.5333976596097214, 0.20721662082257453]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 15:14:18,529 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:14:18,532 : INFO : built Dictionary(311 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 3096 corpus positions)\n",
      "2021-01-14 15:14:18,932 : INFO : token count processed\n",
      "2021-01-14 15:14:19,003 : INFO : frequencies processed\n",
      "2021-01-14 15:14:28,972 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:14:28,973 : INFO : entropies processed\n",
      "2021-01-14 15:14:28,975 : INFO : extropies processed\n",
      "2021-01-14 15:14:28,986 : INFO : token count processed\n",
      "2021-01-14 15:14:28,990 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:14:28,994 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:14:28,995 : INFO : vocab #32006\n",
      "2021-01-14 15:14:29,001 : INFO : diff #set()\n",
      "2021-01-14 15:14:48,820 : INFO : alphabet #32006\n",
      "2021-01-14 15:14:58,950 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.1938277843085556, 0.4558242935715107], [0.8723216503858566, 0.12767835], [3.5465935642949384, 1.3764678303056375], [5.631615665225582, 6.441859572014148, 6.664077273209033, 5.409397964030697, 1.0324616079834508, 0.22221770119488493]]\n",
      "2021-01-14 15:14:58,955 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:14:58,957 : INFO : built Dictionary(328 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1572 corpus positions)\n",
      "2021-01-14 15:14:59,422 : INFO : token count processed\n",
      "2021-01-14 15:14:59,454 : INFO : frequencies processed\n",
      "2021-01-14 15:15:09,397 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:15:09,398 : INFO : entropies processed\n",
      "2021-01-14 15:15:09,398 : INFO : extropies processed\n",
      "2021-01-14 15:15:09,406 : INFO : token count processed\n",
      "2021-01-14 15:15:09,410 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:15:09,415 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:15:09,416 : INFO : vocab #32006\n",
      "2021-01-14 15:15:09,422 : INFO : diff #set()\n",
      "2021-01-14 15:15:29,387 : INFO : alphabet #32006\n",
      "2021-01-14 15:15:39,321 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.1432284973446176, 0.46658580792433646], [0.804906815290451, 0.19509318], [4.334962500721156, 1.4043693211870836], [5.631615665225582, 6.998955278238291, 7.245806232900716, 5.384764710563157, 1.6141905676751334, 0.2468509546624249]]\n",
      "2021-01-14 15:15:39,326 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:15:39,327 : INFO : built Dictionary(246 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1391 corpus positions)\n",
      "2021-01-14 15:15:39,613 : INFO : token count processed\n",
      "2021-01-14 15:15:39,647 : INFO : frequencies processed\n",
      "2021-01-14 15:15:49,736 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:15:49,737 : INFO : entropies processed\n",
      "2021-01-14 15:15:49,738 : INFO : extropies processed\n",
      "2021-01-14 15:15:49,746 : INFO : token count processed\n",
      "2021-01-14 15:15:49,751 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:15:49,755 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:15:49,756 : INFO : vocab #32006\n",
      "2021-01-14 15:15:49,762 : INFO : diff #set()\n",
      "2021-01-14 15:16:10,134 : INFO : alphabet #32006\n",
      "2021-01-14 15:16:20,084 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.1633345261734276, 0.46224935991237137], [0.8497039675712585, 0.15029603], [4.142664355548848, 1.3998246368249863], [5.631615665225582, 6.492983191376071, 6.729174403815149, 5.395424452786505, 1.0975587385895667, 0.2361912124390777]]\n",
      "2021-01-14 15:16:20,092 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:16:20,094 : INFO : built Dictionary(464 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 3330 corpus positions)\n",
      "2021-01-14 15:16:20,839 : INFO : token count processed\n",
      "2021-01-14 15:16:20,878 : INFO : frequencies processed\n",
      "2021-01-14 15:16:30,947 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:16:30,949 : INFO : entropies processed\n",
      "2021-01-14 15:16:30,949 : INFO : extropies processed\n",
      "2021-01-14 15:16:30,961 : INFO : token count processed\n",
      "2021-01-14 15:16:30,965 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:16:30,970 : INFO : alphabet_target #32008\n",
      "2021-01-14 15:16:30,971 : INFO : vocab #32006\n",
      "2021-01-14 15:16:30,977 : INFO : diff #set()\n",
      "2021-01-14 15:16:51,143 : INFO : alphabet #32006\n",
      "2021-01-14 15:17:01,103 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.1990101189970528, 0.4547500674785846], [0.8781807571649551, 0.12181924], [3.169925001442312, 1.3594000115384994], [5.631615665225582, 6.560342487747443, 6.822979825541008, 5.368978327432018, 1.191364160315426, 0.26263733779356535]]\n",
      "2021-01-14 15:17:01,111 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 15:17:01,112 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:17:01,114 : INFO : built Dictionary(466 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 3535 corpus positions)\n",
      "2021-01-14 15:17:01,886 : INFO : token count processed\n",
      "2021-01-14 15:17:01,915 : INFO : frequencies processed\n",
      "2021-01-14 15:17:12,000 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:17:12,001 : INFO : entropies processed\n",
      "2021-01-14 15:17:12,002 : INFO : extropies processed\n",
      "2021-01-14 15:17:12,010 : INFO : token count processed\n",
      "2021-01-14 15:17:12,015 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:17:12,019 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:17:12,020 : INFO : vocab #32006\n",
      "2021-01-14 15:17:12,027 : INFO : diff #set()\n",
      "2021-01-14 15:17:31,774 : INFO : alphabet #32006\n",
      "2021-01-14 15:17:41,470 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.140681815800724, 0.46714088596391856], [0.8069376647472382, 0.19306234], [4.532665279941247, 1.4094755001990091], [5.631615665225582, 7.046173750105238, 7.2779313685848, 5.399858046746021, 1.6463157033592175, 0.23175761847956178]]\n",
      "2021-01-14 15:17:41,481 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 15:17:41,482 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:17:41,485 : INFO : built Dictionary(514 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 5644 corpus positions)\n",
      "2021-01-14 15:17:42,401 : INFO : token count processed\n",
      "2021-01-14 15:17:42,436 : INFO : frequencies processed\n",
      "2021-01-14 15:17:52,354 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:17:52,355 : INFO : entropies processed\n",
      "2021-01-14 15:17:52,356 : INFO : extropies processed\n",
      "2021-01-14 15:17:52,368 : INFO : token count processed\n",
      "2021-01-14 15:17:52,373 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:17:52,377 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:17:52,378 : INFO : vocab #32006\n",
      "2021-01-14 15:17:52,385 : INFO : diff #set()\n",
      "2021-01-14 15:18:12,406 : INFO : alphabet #32006\n",
      "2021-01-14 15:18:22,380 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.1315959558363418, 0.4691320591324942], [0.8081660568714142, 0.19183394], [4.781036112553423, 1.4120262855088896], [5.631615665225582, 7.009229588004272, 7.1432937382987385, 5.497551514931117, 1.5116780730731563, 0.13406415029446617]]\n",
      "2021-01-14 15:18:22,393 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:18:22,396 : INFO : built Dictionary(596 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 6585 corpus positions)\n",
      "2021-01-14 15:18:23,646 : INFO : token count processed\n",
      "2021-01-14 15:18:23,723 : INFO : frequencies processed\n",
      "2021-01-14 15:18:33,710 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:18:33,711 : INFO : entropies processed\n",
      "2021-01-14 15:18:33,712 : INFO : extropies processed\n",
      "2021-01-14 15:18:33,723 : INFO : token count processed\n",
      "2021-01-14 15:18:33,727 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:18:33,732 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:18:33,732 : INFO : vocab #32006\n",
      "2021-01-14 15:18:33,739 : INFO : diff #set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 15:18:54,021 : INFO : alphabet #32006\n",
      "2021-01-14 15:19:04,069 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.1517815310749973, 0.46473119392395534], [0.8017689138650894, 0.19823109], [4.616874605956221, 1.4082442100825672], [5.631615665225582, 7.376088004590871, 7.563145523272366, 5.444558146544088, 1.9315298580467841, 0.18705751868149534]]\n",
      "2021-01-14 15:19:04,072 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:19:04,073 : INFO : built Dictionary(165 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 371 corpus positions)\n",
      "2021-01-14 15:19:04,223 : INFO : token count processed\n",
      "2021-01-14 15:19:04,257 : INFO : frequencies processed\n",
      "2021-01-14 15:19:14,245 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:19:14,246 : INFO : entropies processed\n",
      "2021-01-14 15:19:14,247 : INFO : extropies processed\n",
      "2021-01-14 15:19:14,254 : INFO : token count processed\n",
      "2021-01-14 15:19:14,259 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:19:14,263 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:19:14,264 : INFO : vocab #32006\n",
      "2021-01-14 15:19:14,271 : INFO : diff #set()\n",
      "2021-01-14 15:19:34,017 : INFO : alphabet #32006\n",
      "2021-01-14 15:19:44,091 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.1712646513020926, 0.46056108333007995], [0.8430758565664291, 0.15692414], [2.94770277922009, 1.3393100707180505], [5.631615665225582, 6.2993628166120885, 6.805903957613632, 5.125074524224038, 1.1742882923880495, 0.5065411410015432]]\n",
      "2021-01-14 15:19:44,094 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:19:44,096 : INFO : built Dictionary(69 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 87 corpus positions)\n",
      "2021-01-14 15:19:44,121 : INFO : token count processed\n",
      "2021-01-14 15:19:44,153 : INFO : frequencies processed\n",
      "2021-01-14 15:19:53,663 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:19:53,664 : INFO : entropies processed\n",
      "2021-01-14 15:19:53,664 : INFO : extropies processed\n",
      "2021-01-14 15:19:53,672 : INFO : token count processed\n",
      "2021-01-14 15:19:53,677 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:19:53,681 : INFO : alphabet_target #32008\n",
      "2021-01-14 15:19:53,682 : INFO : vocab #32006\n",
      "2021-01-14 15:19:53,689 : INFO : diff #set()\n",
      "2021-01-14 15:20:13,644 : INFO : alphabet #32006\n",
      "2021-01-14 15:20:23,677 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.245192544409746, 0.4453960986508161], [0.8530857563018799, 0.14691424], [0.0, 0.0], [5.631615665225582, 3.8936606896881862, 5.888630200767226, 3.6366461541465416, 0.2570145355416438, 1.9949695110790397]]\n",
      "2021-01-14 15:20:23,700 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:20:23,705 : INFO : built Dictionary(752 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 12529 corpus positions)\n",
      "2021-01-14 15:20:25,455 : INFO : token count processed\n",
      "2021-01-14 15:20:25,490 : INFO : frequencies processed\n",
      "2021-01-14 15:20:35,576 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:20:35,577 : INFO : entropies processed\n",
      "2021-01-14 15:20:35,580 : INFO : extropies processed\n",
      "2021-01-14 15:20:35,592 : INFO : token count processed\n",
      "2021-01-14 15:20:35,601 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:20:35,607 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:20:35,608 : INFO : vocab #32006\n",
      "2021-01-14 15:20:35,616 : INFO : diff #set()\n",
      "2021-01-14 15:20:55,663 : INFO : alphabet #32006\n",
      "2021-01-14 15:21:05,684 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.1337538014768902, 0.4686576301857525], [0.7915506511926651, 0.20844935], [4.786425874087823, 1.4120364694765237], [5.631615665225582, 7.434393313070278, 7.65260598895161, 5.413402989344251, 2.0209903237260276, 0.21821267588133164]]\n",
      "2021-01-14 15:21:05,693 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:21:05,696 : INFO : built Dictionary(506 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 4156 corpus positions)\n",
      "2021-01-14 15:21:06,518 : INFO : token count processed\n",
      "2021-01-14 15:21:06,552 : INFO : frequencies processed\n",
      "2021-01-14 15:21:16,450 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:21:16,451 : INFO : entropies processed\n",
      "2021-01-14 15:21:16,452 : INFO : extropies processed\n",
      "2021-01-14 15:21:16,461 : INFO : token count processed\n",
      "2021-01-14 15:21:16,465 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:21:16,469 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:21:16,470 : INFO : vocab #32006\n",
      "2021-01-14 15:21:16,476 : INFO : diff #set()\n",
      "2021-01-14 15:21:35,960 : INFO : alphabet #32006\n",
      "2021-01-14 15:21:45,848 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.1304851533664528, 0.4693766574340429], [0.7791520655155182, 0.22084793], [4.625, 1.4080803599387752], [5.631615665225582, 7.2991514951718255, 7.519900315189657, 5.410866845207751, 1.888284649964075, 0.2207488200178318]]\n",
      "2021-01-14 15:21:45,856 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:21:45,859 : INFO : built Dictionary(476 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 3573 corpus positions)\n",
      "2021-01-14 15:21:46,609 : INFO : token count processed\n",
      "2021-01-14 15:21:46,660 : INFO : frequencies processed\n",
      "2021-01-14 15:21:56,692 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:21:56,693 : INFO : entropies processed\n",
      "2021-01-14 15:21:56,694 : INFO : extropies processed\n",
      "2021-01-14 15:21:56,705 : INFO : token count processed\n",
      "2021-01-14 15:21:56,710 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:21:56,714 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:21:56,715 : INFO : vocab #32006\n",
      "2021-01-14 15:21:56,722 : INFO : diff #set()\n",
      "2021-01-14 15:22:16,763 : INFO : alphabet #32006\n",
      "2021-01-14 15:22:26,948 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.1406487499321616, 0.46714810172929616], [0.8026162385940552, 0.19738376], [4.378783493486175, 1.4010505520916627], [5.631615665225582, 7.170319527000998, 7.394538267578691, 5.407396924647889, 1.7629226023531084, 0.22421874057769298]]\n",
      "2021-01-14 15:22:26,952 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:22:26,954 : INFO : built Dictionary(198 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 609 corpus positions)\n",
      "2021-01-14 15:22:27,149 : INFO : token count processed\n",
      "2021-01-14 15:22:27,194 : INFO : frequencies processed\n",
      "2021-01-14 15:22:37,269 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:22:37,270 : INFO : entropies processed\n",
      "2021-01-14 15:22:37,271 : INFO : extropies processed\n",
      "2021-01-14 15:22:37,282 : INFO : token count processed\n",
      "2021-01-14 15:22:37,287 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:22:37,291 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:22:37,292 : INFO : vocab #32006\n",
      "2021-01-14 15:22:37,298 : INFO : diff #set()\n",
      "2021-01-14 15:22:57,514 : INFO : alphabet #32006\n",
      "2021-01-14 15:23:07,528 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.1740705510023546, 0.45996667382249873], [0.8453681170940399, 0.15463188], [3.121928094887362, 1.3519647487142497], [5.631615665225582, 6.353654804387375, 6.781648557352874, 5.203621912260083, 1.1500328921272915, 0.4279937529654987]]\n",
      "2021-01-14 15:23:07,532 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:23:07,534 : INFO : built Dictionary(195 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 737 corpus positions)\n",
      "2021-01-14 15:23:07,726 : INFO : token count processed\n",
      "2021-01-14 15:23:07,759 : INFO : frequencies processed\n",
      "2021-01-14 15:23:17,619 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:23:17,620 : INFO : entropies processed\n",
      "2021-01-14 15:23:17,621 : INFO : extropies processed\n",
      "2021-01-14 15:23:17,628 : INFO : token count processed\n",
      "2021-01-14 15:23:17,632 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:23:17,636 : INFO : alphabet_target #32009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 15:23:17,637 : INFO : vocab #32006\n",
      "2021-01-14 15:23:17,643 : INFO : diff #set()\n",
      "2021-01-14 15:23:37,376 : INFO : alphabet #32006\n",
      "2021-01-14 15:23:47,279 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.1862007124890184, 0.4574145430871655], [0.8436628133058548, 0.15633719], [2.75, 1.3226647836567116], [5.631615665225582, 6.245180322479091, 6.630696561924118, 5.246099425780555, 0.9990808966985361, 0.38551623944502733]]\n",
      "2021-01-14 15:23:47,284 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:23:47,286 : INFO : built Dictionary(403 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1809 corpus positions)\n",
      "2021-01-14 15:23:47,920 : INFO : token count processed\n",
      "2021-01-14 15:23:47,966 : INFO : frequencies processed\n",
      "2021-01-14 15:23:57,960 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:23:57,961 : INFO : entropies processed\n",
      "2021-01-14 15:23:57,962 : INFO : extropies processed\n",
      "2021-01-14 15:23:57,973 : INFO : token count processed\n",
      "2021-01-14 15:23:57,978 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:23:57,985 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:23:57,985 : INFO : vocab #32006\n",
      "2021-01-14 15:23:57,994 : INFO : diff #set()\n",
      "2021-01-14 15:24:18,183 : INFO : alphabet #32006\n",
      "2021-01-14 15:24:28,142 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.143189781229324, 0.4665942366645685], [0.7842883765697479, 0.21571162], [4.726409765557392, 1.4125929642175312], [5.631615665225582, 7.2691387000368, 7.530643260568809, 5.370111104693574, 1.8990275953432265, 0.26150456053200877]]\n",
      "2021-01-14 15:24:28,148 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:24:28,150 : INFO : built Dictionary(339 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1646 corpus positions)\n",
      "2021-01-14 15:24:28,624 : INFO : token count processed\n",
      "2021-01-14 15:24:28,651 : INFO : frequencies processed\n",
      "2021-01-14 15:24:38,198 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:24:38,199 : INFO : entropies processed\n",
      "2021-01-14 15:24:38,200 : INFO : extropies processed\n",
      "2021-01-14 15:24:38,211 : INFO : token count processed\n",
      "2021-01-14 15:24:38,216 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:24:38,220 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:24:38,221 : INFO : vocab #32006\n",
      "2021-01-14 15:24:38,228 : INFO : diff #set()\n",
      "2021-01-14 15:24:57,888 : INFO : alphabet #32006\n",
      "2021-01-14 15:25:07,846 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.1426185677760416, 0.4667186288028684], [0.7695329487323761, 0.23046705], [4.001822825622231, 1.3874928763412115], [5.631615665225582, 7.08857858466988, 7.3360882408312085, 5.384106009064253, 1.7044725756056263, 0.2475096561613288]]\n",
      "2021-01-14 15:25:07,850 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:25:07,851 : INFO : built Dictionary(173 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 569 corpus positions)\n",
      "2021-01-14 15:25:08,014 : INFO : token count processed\n",
      "2021-01-14 15:25:08,047 : INFO : frequencies processed\n",
      "2021-01-14 15:25:18,137 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:25:18,138 : INFO : entropies processed\n",
      "2021-01-14 15:25:18,139 : INFO : extropies processed\n",
      "2021-01-14 15:25:18,146 : INFO : token count processed\n",
      "2021-01-14 15:25:18,150 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:25:18,155 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:25:18,156 : INFO : vocab #32006\n",
      "2021-01-14 15:25:18,162 : INFO : diff #set()\n",
      "2021-01-14 15:25:38,235 : INFO : alphabet #32006\n",
      "2021-01-14 15:25:48,231 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.1759241020942954, 0.45957485329452186], [0.8493388593196869, 0.15066114], [2.94770277922009, 1.3393100707180505], [5.631615665225582, 6.0479231618016716, 6.507491656252805, 5.17204717077445, 0.8758759910272227, 0.45956849445113335]]\n",
      "2021-01-14 15:25:48,235 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:25:48,236 : INFO : built Dictionary(176 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 688 corpus positions)\n",
      "2021-01-14 15:25:48,398 : INFO : token count processed\n",
      "2021-01-14 15:25:48,431 : INFO : frequencies processed\n",
      "2021-01-14 15:25:58,526 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:25:58,527 : INFO : entropies processed\n",
      "2021-01-14 15:25:58,528 : INFO : extropies processed\n",
      "2021-01-14 15:25:58,535 : INFO : token count processed\n",
      "2021-01-14 15:25:58,540 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:25:58,544 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:25:58,545 : INFO : vocab #32006\n",
      "2021-01-14 15:25:58,552 : INFO : diff #set()\n",
      "2021-01-14 15:26:18,522 : INFO : alphabet #32006\n",
      "2021-01-14 15:26:28,016 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.1890304439481656, 0.4568232491990318], [0.8473046720027924, 0.15269533], [2.75, 1.3226647836567116], [5.631615665225582, 6.036583168403119, 6.448720658319642, 5.219478175309058, 0.8171049930940599, 0.4121374899165229]]\n",
      "2021-01-14 15:26:28,030 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 15:26:28,031 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:26:28,038 : INFO : built Dictionary(578 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 7046 corpus positions)\n",
      "2021-01-14 15:26:29,140 : INFO : token count processed\n",
      "2021-01-14 15:26:29,223 : INFO : frequencies processed\n",
      "2021-01-14 15:26:39,375 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:26:39,376 : INFO : entropies processed\n",
      "2021-01-14 15:26:39,377 : INFO : extropies processed\n",
      "2021-01-14 15:26:39,390 : INFO : token count processed\n",
      "2021-01-14 15:26:39,394 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:26:39,399 : INFO : alphabet_target #32010\n",
      "2021-01-14 15:26:39,399 : INFO : vocab #32006\n",
      "2021-01-14 15:26:39,406 : INFO : diff #set()\n",
      "2021-01-14 15:26:59,271 : INFO : alphabet #32006\n",
      "2021-01-14 15:27:09,446 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.1713348996443524, 0.4605461829788634], [0.8444428592920303, 0.15555714], [4.2626923908396215, 1.4023109474566753], [5.631615665225582, 7.29352035514053, 7.48015833918836, 5.444977681177752, 1.8485426739627782, 0.18663798404783005]]\n",
      "2021-01-14 15:27:09,454 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:27:09,457 : INFO : built Dictionary(386 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 3289 corpus positions)\n",
      "2021-01-14 15:27:10,040 : INFO : token count processed\n",
      "2021-01-14 15:27:10,072 : INFO : frequencies processed\n",
      "2021-01-14 15:27:20,157 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:27:20,158 : INFO : entropies processed\n",
      "2021-01-14 15:27:20,159 : INFO : extropies processed\n",
      "2021-01-14 15:27:20,171 : INFO : token count processed\n",
      "2021-01-14 15:27:20,175 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:27:20,180 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:27:20,181 : INFO : vocab #32006\n",
      "2021-01-14 15:27:20,187 : INFO : diff #set()\n",
      "2021-01-14 15:27:40,231 : INFO : alphabet #32006\n",
      "2021-01-14 15:27:50,285 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.1311249270100618, 0.4692357483720984], [0.7950020879507065, 0.20499791], [4.349648912578752, 1.4051631060490313], [5.631615665225582, 6.8153433747477745, 7.008294933715717, 5.43866410625764, 1.3766792684901352, 0.19295155896794292]]\n",
      "2021-01-14 15:27:50,288 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:27:50,289 : INFO : built Dictionary(152 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 310 corpus positions)\n",
      "2021-01-14 15:27:50,432 : INFO : token count processed\n",
      "2021-01-14 15:27:50,467 : INFO : frequencies processed\n",
      "2021-01-14 15:28:00,313 : INFO : scalar_distribution processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 15:28:00,314 : INFO : entropies processed\n",
      "2021-01-14 15:28:00,315 : INFO : extropies processed\n",
      "2021-01-14 15:28:00,322 : INFO : token count processed\n",
      "2021-01-14 15:28:00,326 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:28:00,330 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:28:00,331 : INFO : vocab #32006\n",
      "2021-01-14 15:28:00,337 : INFO : diff #set()\n",
      "2021-01-14 15:28:20,349 : INFO : alphabet #32006\n",
      "2021-01-14 15:28:30,706 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.171379962269225, 0.46053662526890904], [0.8491539359092712, 0.15084606], [2.521640636343318, 1.2998438251349493], [5.631615665225582, 6.150121915859574, 6.7113656856972135, 5.070371895387943, 1.0797500204716313, 0.5612437698376391]]\n",
      "2021-01-14 15:28:30,710 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:28:30,712 : INFO : built Dictionary(297 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1021 corpus positions)\n",
      "2021-01-14 15:28:31,097 : INFO : token count processed\n",
      "2021-01-14 15:28:31,152 : INFO : frequencies processed\n",
      "2021-01-14 15:28:41,141 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:28:41,142 : INFO : entropies processed\n",
      "2021-01-14 15:28:41,143 : INFO : extropies processed\n",
      "2021-01-14 15:28:41,154 : INFO : token count processed\n",
      "2021-01-14 15:28:41,159 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:28:41,163 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:28:41,164 : INFO : vocab #32006\n",
      "2021-01-14 15:28:41,171 : INFO : diff #set()\n",
      "2021-01-14 15:29:01,253 : INFO : alphabet #32006\n",
      "2021-01-14 15:29:11,240 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.1048067738096299, 0.47510299398649003], [0.7466769814491272, 0.25332302], [4.567099536193327, 1.4065409452947621], [5.631615665225582, 7.0391145208191315, 7.359511708664221, 5.311218477380494, 1.7278960434386388, 0.3203971878450895]]\n",
      "2021-01-14 15:29:11,249 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 15:29:11,250 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:29:11,253 : INFO : built Dictionary(601 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 4396 corpus positions)\n",
      "2021-01-14 15:29:12,484 : INFO : token count processed\n",
      "2021-01-14 15:29:12,518 : INFO : frequencies processed\n",
      "2021-01-14 15:29:22,468 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:29:22,469 : INFO : entropies processed\n",
      "2021-01-14 15:29:22,470 : INFO : extropies processed\n",
      "2021-01-14 15:29:22,483 : INFO : token count processed\n",
      "2021-01-14 15:29:22,488 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:29:22,492 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:29:22,493 : INFO : vocab #32006\n",
      "2021-01-14 15:29:22,499 : INFO : diff #set()\n",
      "2021-01-14 15:29:41,765 : INFO : alphabet #32006\n",
      "2021-01-14 15:29:51,600 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.133462392885447, 0.4687216439037055], [0.7531692236661911, 0.24683078], [4.62015169511603, 1.4081348086325465], [5.631615665225582, 7.482466367279176, 7.779447552176525, 5.334634480328233, 2.147831886950943, 0.2969811848973496]]\n",
      "2021-01-14 15:29:51,604 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:29:51,605 : INFO : built Dictionary(199 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 618 corpus positions)\n",
      "2021-01-14 15:29:51,817 : INFO : token count processed\n",
      "2021-01-14 15:29:51,873 : INFO : frequencies processed\n",
      "2021-01-14 15:30:01,938 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:30:01,939 : INFO : entropies processed\n",
      "2021-01-14 15:30:01,940 : INFO : extropies processed\n",
      "2021-01-14 15:30:01,951 : INFO : token count processed\n",
      "2021-01-14 15:30:01,956 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:30:01,960 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:30:01,961 : INFO : vocab #32006\n",
      "2021-01-14 15:30:01,968 : INFO : diff #set()\n",
      "2021-01-14 15:30:22,009 : INFO : alphabet #32006\n",
      "2021-01-14 15:30:31,965 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.1935179777866878, 0.45588867295677415], [0.8503729999065399, 0.149627], [2.94770277922009, 1.3393100707180505], [5.631615665225582, 6.372162341197667, 6.801270007782346, 5.202507998640904, 1.1696543425567638, 0.42910766658467914]]\n",
      "2021-01-14 15:30:31,971 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:30:31,973 : INFO : built Dictionary(334 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 2039 corpus positions)\n",
      "2021-01-14 15:30:32,435 : INFO : token count processed\n",
      "2021-01-14 15:30:32,477 : INFO : frequencies processed\n",
      "2021-01-14 15:30:42,513 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:30:42,514 : INFO : entropies processed\n",
      "2021-01-14 15:30:42,515 : INFO : extropies processed\n",
      "2021-01-14 15:30:42,526 : INFO : token count processed\n",
      "2021-01-14 15:30:42,531 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:30:42,535 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:30:42,536 : INFO : vocab #32006\n",
      "2021-01-14 15:30:42,543 : INFO : diff #set()\n",
      "2021-01-14 15:31:01,888 : INFO : alphabet #32006\n",
      "2021-01-14 15:31:11,956 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.1507782340201704, 0.4649479821686823], [0.8182355910539627, 0.18176441], [4.41829583405449, 1.4069837232164877], [5.631615665225582, 6.798155919669889, 7.057083658728814, 5.372687926166657, 1.425467993503232, 0.2589277390589251]]\n",
      "2021-01-14 15:31:11,961 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:31:11,962 : INFO : built Dictionary(203 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 755 corpus positions)\n",
      "2021-01-14 15:31:12,176 : INFO : token count processed\n",
      "2021-01-14 15:31:12,210 : INFO : frequencies processed\n",
      "2021-01-14 15:31:22,270 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:31:22,271 : INFO : entropies processed\n",
      "2021-01-14 15:31:22,272 : INFO : extropies processed\n",
      "2021-01-14 15:31:22,283 : INFO : token count processed\n",
      "2021-01-14 15:31:22,288 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:31:22,295 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:31:22,295 : INFO : vocab #32006\n",
      "2021-01-14 15:31:22,303 : INFO : diff #set()\n",
      "2021-01-14 15:31:42,637 : INFO : alphabet #32006\n",
      "2021-01-14 15:31:52,870 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.1532117306712668, 0.4644225116162861], [0.824772521853447, 0.17522748], [3.121928094887362, 1.3519647487142497], [5.631615665225582, 6.271631856729336, 6.66048081160214, 5.242766710352777, 1.0288651463765577, 0.38884895487280424]]\n",
      "2021-01-14 15:31:52,877 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:31:52,880 : INFO : built Dictionary(369 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 3270 corpus positions)\n",
      "2021-01-14 15:31:53,462 : INFO : token count processed\n",
      "2021-01-14 15:31:53,494 : INFO : frequencies processed\n",
      "2021-01-14 15:32:03,579 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:32:03,582 : INFO : entropies processed\n",
      "2021-01-14 15:32:03,583 : INFO : extropies processed\n",
      "2021-01-14 15:32:03,595 : INFO : token count processed\n",
      "2021-01-14 15:32:03,599 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:32:03,604 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:32:03,605 : INFO : vocab #32006\n",
      "2021-01-14 15:32:03,612 : INFO : diff #set()\n",
      "2021-01-14 15:32:23,758 : INFO : alphabet #32006\n",
      "2021-01-14 15:32:33,803 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.1474433580530736, 0.4656700239612491], [0.7883465439081192, 0.21165346], [4.238901256602631, 1.3964489380809906], [5.631615665225582, 6.873598627629562, 7.075060231956614, 5.43015406089853, 1.4434445667310314, 0.20146160432705162]]\n",
      "2021-01-14 15:32:33,807 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:32:33,808 : INFO : built Dictionary(133 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 218 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 15:32:33,926 : INFO : token count processed\n",
      "2021-01-14 15:32:33,962 : INFO : frequencies processed\n",
      "2021-01-14 15:32:43,850 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:32:43,851 : INFO : entropies processed\n",
      "2021-01-14 15:32:43,852 : INFO : extropies processed\n",
      "2021-01-14 15:32:43,859 : INFO : token count processed\n",
      "2021-01-14 15:32:43,863 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:32:43,867 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:32:43,868 : INFO : vocab #32006\n",
      "2021-01-14 15:32:43,874 : INFO : diff #set()\n",
      "2021-01-14 15:33:03,651 : INFO : alphabet #32006\n",
      "2021-01-14 15:33:13,645 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.1553627373032525, 0.46395902772782477], [0.8241252303123474, 0.17587477], [2.5, 1.2968140217166515], [5.631615665225582, 6.049830202851529, 6.734374300060908, 4.947071568016204, 1.1027586348353262, 0.684544097209379]]\n",
      "2021-01-14 15:33:13,650 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:33:13,652 : INFO : built Dictionary(273 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1298 corpus positions)\n",
      "2021-01-14 15:33:13,996 : INFO : token count processed\n",
      "2021-01-14 15:33:14,049 : INFO : frequencies processed\n",
      "2021-01-14 15:33:24,096 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:33:24,097 : INFO : entropies processed\n",
      "2021-01-14 15:33:24,098 : INFO : extropies processed\n",
      "2021-01-14 15:33:24,109 : INFO : token count processed\n",
      "2021-01-14 15:33:24,114 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:33:24,118 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:33:24,119 : INFO : vocab #32006\n",
      "2021-01-14 15:33:24,126 : INFO : diff #set()\n",
      "2021-01-14 15:33:44,353 : INFO : alphabet #32006\n",
      "2021-01-14 15:33:54,458 : INFO : Computed distances or similarities ('281', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.1472553646259114, 0.46571079363642304], [0.7657811492681503, 0.23421885], [3.6219280948873624, 1.3709563519274055], [5.631615665225582, 6.778844940588858, 7.075284985098913, 5.3351756207155265, 1.4436693198733304, 0.2964400445100548]]\n",
      "2021-01-14 15:33:54,462 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:33:54,463 : INFO : built Dictionary(177 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 331 corpus positions)\n",
      "2021-01-14 15:33:54,628 : INFO : token count processed\n",
      "2021-01-14 15:33:54,660 : INFO : frequencies processed\n",
      "2021-01-14 15:34:04,667 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:34:04,668 : INFO : entropies processed\n",
      "2021-01-14 15:34:04,669 : INFO : extropies processed\n",
      "2021-01-14 15:34:04,677 : INFO : token count processed\n",
      "2021-01-14 15:34:04,681 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:34:04,685 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:34:04,686 : INFO : vocab #32006\n",
      "2021-01-14 15:34:04,693 : INFO : diff #set()\n",
      "2021-01-14 15:34:24,774 : INFO : alphabet #32006\n",
      "2021-01-14 15:34:34,732 : INFO : Computed distances or similarities ('281', 'sacp-python-common/setup.py')[[1.135182627216028, 0.46834401294462413], [0.8062380403280258, 0.19376196], [3.095795255000934, 1.3487605247277434], [5.631615665225582, 6.469677430851302, 7.012210274273011, 5.089082821803872, 1.3805946090474288, 0.5425328434217089]]\n",
      "2021-01-14 15:34:34,737 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:34:34,739 : INFO : built Dictionary(239 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1233 corpus positions)\n",
      "2021-01-14 15:34:35,023 : INFO : token count processed\n",
      "2021-01-14 15:34:35,056 : INFO : frequencies processed\n",
      "2021-01-14 15:34:44,984 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:34:44,985 : INFO : entropies processed\n",
      "2021-01-14 15:34:44,986 : INFO : extropies processed\n",
      "2021-01-14 15:34:44,997 : INFO : token count processed\n",
      "2021-01-14 15:34:45,002 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:34:45,006 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:34:45,007 : INFO : vocab #32006\n",
      "2021-01-14 15:34:45,013 : INFO : diff #set()\n",
      "2021-01-14 15:35:04,923 : INFO : alphabet #32006\n",
      "2021-01-14 15:35:15,178 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.1406792150297875, 0.4671414535064214], [0.8119095712900162, 0.18809043], [4.037401197654112, 1.3956040576270576], [5.631615665225582, 6.459180448028249, 6.750575770343868, 5.340220342909962, 1.118960105118286, 0.29139532231561915]]\n",
      "2021-01-14 15:35:15,182 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:35:15,184 : INFO : built Dictionary(155 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 383 corpus positions)\n",
      "2021-01-14 15:35:15,328 : INFO : token count processed\n",
      "2021-01-14 15:35:15,361 : INFO : frequencies processed\n",
      "2021-01-14 15:35:25,567 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:35:25,568 : INFO : entropies processed\n",
      "2021-01-14 15:35:25,568 : INFO : extropies processed\n",
      "2021-01-14 15:35:25,579 : INFO : token count processed\n",
      "2021-01-14 15:35:25,583 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:35:25,588 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:35:25,589 : INFO : vocab #32006\n",
      "2021-01-14 15:35:25,596 : INFO : diff #set()\n",
      "2021-01-14 15:35:45,507 : INFO : alphabet #32006\n",
      "2021-01-14 15:35:55,432 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.14294472529402, 0.46664759393772803], [0.8252151757478714, 0.17478482], [3.4182958340544896, 1.369895090630202], [5.631615665225582, 6.097125733496388, 6.606011135126727, 5.122730263595243, 0.974395469901145, 0.508885401630339]]\n",
      "2021-01-14 15:35:55,436 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:35:55,437 : INFO : built Dictionary(146 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 379 corpus positions)\n",
      "2021-01-14 15:35:55,568 : INFO : token count processed\n",
      "2021-01-14 15:35:55,636 : INFO : frequencies processed\n",
      "2021-01-14 15:36:05,389 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:36:05,390 : INFO : entropies processed\n",
      "2021-01-14 15:36:05,391 : INFO : extropies processed\n",
      "2021-01-14 15:36:05,401 : INFO : token count processed\n",
      "2021-01-14 15:36:05,405 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:36:05,410 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:36:05,410 : INFO : vocab #32006\n",
      "2021-01-14 15:36:05,417 : INFO : diff #set()\n",
      "2021-01-14 15:36:25,012 : INFO : alphabet #32006\n",
      "2021-01-14 15:36:35,259 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.1279025337798252, 0.46994633641592826], [0.8164345175027847, 0.18356548], [3.5465935642949384, 1.3764678303056375], [5.631615665225582, 6.0695858597523715, 6.566357974979065, 5.134843549998889, 0.9347423097534824, 0.49677211522669307]]\n",
      "2021-01-14 15:36:35,263 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 15:36:35,264 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:36:35,269 : INFO : built Dictionary(149 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 475 corpus positions)\n",
      "2021-01-14 15:36:35,414 : INFO : token count processed\n",
      "2021-01-14 15:36:35,451 : INFO : frequencies processed\n",
      "2021-01-14 15:36:45,379 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:36:45,380 : INFO : entropies processed\n",
      "2021-01-14 15:36:45,381 : INFO : extropies processed\n",
      "2021-01-14 15:36:45,392 : INFO : token count processed\n",
      "2021-01-14 15:36:45,396 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:36:45,401 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:36:45,402 : INFO : vocab #32006\n",
      "2021-01-14 15:36:45,409 : INFO : diff #set()\n",
      "2021-01-14 15:37:05,668 : INFO : alphabet #32006\n",
      "2021-01-14 15:37:15,795 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.1040603397024564, 0.47527154099649727], [0.7921065390110016, 0.20789346], [3.3927474104487847, 1.3672090515720436], [5.631615665225582, 6.104787343210121, 6.525871981361789, 5.210531027073914, 0.8942563161362065, 0.42108463815166797]]\n",
      "2021-01-14 15:37:15,812 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 15:37:15,813 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:37:15,820 : INFO : built Dictionary(430 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 9160 corpus positions)\n",
      "2021-01-14 15:37:16,482 : INFO : token count processed\n",
      "2021-01-14 15:37:16,517 : INFO : frequencies processed\n",
      "2021-01-14 15:37:26,708 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:37:26,709 : INFO : entropies processed\n",
      "2021-01-14 15:37:26,709 : INFO : extropies processed\n",
      "2021-01-14 15:37:26,723 : INFO : token count processed\n",
      "2021-01-14 15:37:26,728 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:37:26,732 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:37:26,733 : INFO : vocab #32006\n",
      "2021-01-14 15:37:26,740 : INFO : diff #set()\n",
      "2021-01-14 15:37:46,786 : INFO : alphabet #32006\n",
      "2021-01-14 15:37:56,727 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.140485968342009, 0.46718362782568773], [0.824166551232338, 0.17583345], [4.095795255000934, 1.3969025219274842], [5.631615665225582, 6.89087415148015, 7.0722044126562364, 5.450285404049496, 1.4405887474306542, 0.18133026117608608]]\n",
      "2021-01-14 15:37:56,734 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:37:56,739 : INFO : built Dictionary(289 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 2320 corpus positions)\n",
      "2021-01-14 15:37:57,090 : INFO : token count processed\n",
      "2021-01-14 15:37:57,127 : INFO : frequencies processed\n",
      "2021-01-14 15:38:06,947 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:38:06,948 : INFO : entropies processed\n",
      "2021-01-14 15:38:06,949 : INFO : extropies processed\n",
      "2021-01-14 15:38:06,956 : INFO : token count processed\n",
      "2021-01-14 15:38:06,961 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:38:06,965 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:38:06,965 : INFO : vocab #32006\n",
      "2021-01-14 15:38:06,972 : INFO : diff #set()\n",
      "2021-01-14 15:38:26,474 : INFO : alphabet #32006\n",
      "2021-01-14 15:38:36,389 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.1174832438975077, 0.47225875476557155], [0.7571844309568405, 0.24281557], [4.021928094887363, 1.394616085503868], [5.631615665225582, 6.655493573668506, 6.868926293849018, 5.418182945045071, 1.2373106286234359, 0.21343272018051174]]\n",
      "2021-01-14 15:38:36,394 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:38:36,395 : INFO : built Dictionary(264 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1327 corpus positions)\n",
      "2021-01-14 15:38:36,715 : INFO : token count processed\n",
      "2021-01-14 15:38:36,747 : INFO : frequencies processed\n",
      "2021-01-14 15:38:46,705 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:38:46,706 : INFO : entropies processed\n",
      "2021-01-14 15:38:46,707 : INFO : extropies processed\n",
      "2021-01-14 15:38:46,714 : INFO : token count processed\n",
      "2021-01-14 15:38:46,719 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:38:46,723 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:38:46,724 : INFO : vocab #32006\n",
      "2021-01-14 15:38:46,731 : INFO : diff #set()\n",
      "2021-01-14 15:39:06,969 : INFO : alphabet #32006\n",
      "2021-01-14 15:39:17,005 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.1674920729650626, 0.46136270230138865], [0.8270661383867264, 0.17293386], [4.20184123230257, 1.4009137160862843], [5.631615665225582, 6.6236746347295465, 6.868790144645307, 5.386500155309821, 1.2371744794197248, 0.24511550991576048]]\n",
      "2021-01-14 15:39:17,010 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:39:17,011 : INFO : built Dictionary(263 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1266 corpus positions)\n",
      "2021-01-14 15:39:17,325 : INFO : token count processed\n",
      "2021-01-14 15:39:17,357 : INFO : frequencies processed\n",
      "2021-01-14 15:39:27,516 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:39:27,517 : INFO : entropies processed\n",
      "2021-01-14 15:39:27,518 : INFO : extropies processed\n",
      "2021-01-14 15:39:27,525 : INFO : token count processed\n",
      "2021-01-14 15:39:27,529 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:39:27,534 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:39:27,535 : INFO : vocab #32006\n",
      "2021-01-14 15:39:27,541 : INFO : diff #set()\n",
      "2021-01-14 15:39:47,820 : INFO : alphabet #32006\n",
      "2021-01-14 15:39:57,599 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.1136869565404746, 0.4731069550794435], [0.7182020843029022, 0.28179792], [4.334962500721156, 1.4043693211870838], [5.631615665225582, 6.75472436518627, 6.973983948576997, 5.412356081834854, 1.3423682833514148, 0.21925958339072693]]\n",
      "2021-01-14 15:39:57,603 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:39:57,605 : INFO : built Dictionary(222 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1061 corpus positions)\n",
      "2021-01-14 15:39:57,842 : INFO : token count processed\n",
      "2021-01-14 15:39:57,875 : INFO : frequencies processed\n",
      "2021-01-14 15:40:08,066 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:40:08,067 : INFO : entropies processed\n",
      "2021-01-14 15:40:08,067 : INFO : extropies processed\n",
      "2021-01-14 15:40:08,075 : INFO : token count processed\n",
      "2021-01-14 15:40:08,079 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:40:08,084 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:40:08,084 : INFO : vocab #32006\n",
      "2021-01-14 15:40:08,091 : INFO : diff #set()\n",
      "2021-01-14 15:40:28,230 : INFO : alphabet #32006\n",
      "2021-01-14 15:40:38,251 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.1451047515065302, 0.4661777003186857], [0.7565059661865234, 0.24349403], [3.75, 1.3846096858033596], [5.631615665225582, 6.597313085495733, 6.851656893050711, 5.377271857670605, 1.2200412278251287, 0.254343807554978]]\n",
      "2021-01-14 15:40:38,255 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:40:38,257 : INFO : built Dictionary(242 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1027 corpus positions)\n",
      "2021-01-14 15:40:38,545 : INFO : token count processed\n",
      "2021-01-14 15:40:38,577 : INFO : frequencies processed\n",
      "2021-01-14 15:40:48,608 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:40:48,609 : INFO : entropies processed\n",
      "2021-01-14 15:40:48,610 : INFO : extropies processed\n",
      "2021-01-14 15:40:48,620 : INFO : token count processed\n",
      "2021-01-14 15:40:48,625 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:40:48,630 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:40:48,631 : INFO : vocab #32006\n",
      "2021-01-14 15:40:48,638 : INFO : diff #set()\n",
      "2021-01-14 15:41:07,976 : INFO : alphabet #32006\n",
      "2021-01-14 15:41:17,991 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.134501800024205, 0.46849339737668994], [0.7821775376796722, 0.21782246], [4.20184123230257, 1.4009137160862843], [5.631615665225582, 6.659481538516613, 6.921441504926349, 5.369655698815846, 1.2898258397007671, 0.2619599664097363]]\n",
      "2021-01-14 15:41:17,996 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:41:17,998 : INFO : built Dictionary(271 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1189 corpus positions)\n",
      "2021-01-14 15:41:18,335 : INFO : token count processed\n",
      "2021-01-14 15:41:18,368 : INFO : frequencies processed\n",
      "2021-01-14 15:41:28,277 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:41:28,278 : INFO : entropies processed\n",
      "2021-01-14 15:41:28,280 : INFO : extropies processed\n",
      "2021-01-14 15:41:28,290 : INFO : token count processed\n",
      "2021-01-14 15:41:28,295 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:41:28,299 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:41:28,300 : INFO : vocab #32006\n",
      "2021-01-14 15:41:28,307 : INFO : diff #set()\n",
      "2021-01-14 15:41:48,381 : INFO : alphabet #32006\n",
      "2021-01-14 15:41:58,518 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.1404407322008339, 0.46719350129904536], [0.792707547545433, 0.20729245], [3.852168723603281, 1.3887904911367783], [5.631615665225582, 6.774682571479102, 7.002008997095314, 5.40428923960937, 1.3703933318697317, 0.2273264256162122]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 15:41:58,533 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:41:58,536 : INFO : built Dictionary(443 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 7926 corpus positions)\n",
      "2021-01-14 15:41:59,350 : INFO : token count processed\n",
      "2021-01-14 15:41:59,429 : INFO : frequencies processed\n",
      "2021-01-14 15:42:09,516 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:42:09,518 : INFO : entropies processed\n",
      "2021-01-14 15:42:09,518 : INFO : extropies processed\n",
      "2021-01-14 15:42:09,529 : INFO : token count processed\n",
      "2021-01-14 15:42:09,533 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:42:09,538 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:42:09,539 : INFO : vocab #32006\n",
      "2021-01-14 15:42:09,546 : INFO : diff #set()\n",
      "2021-01-14 15:42:29,820 : INFO : alphabet #32006\n",
      "2021-01-14 15:42:39,819 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.1540917033236207, 0.46423278937338935], [0.8400388062000275, 0.1599612], [4.286790198827113, 1.4011749129952065], [5.631615665225582, 6.839453716525233, 7.014945316083447, 5.456124065667368, 1.3833296508578652, 0.1754915995582147]]\n",
      "2021-01-14 15:42:39,825 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:42:39,828 : INFO : built Dictionary(341 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 2368 corpus positions)\n",
      "2021-01-14 15:42:40,327 : INFO : token count processed\n",
      "2021-01-14 15:42:40,409 : INFO : frequencies processed\n",
      "2021-01-14 15:42:50,348 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:42:50,349 : INFO : entropies processed\n",
      "2021-01-14 15:42:50,350 : INFO : extropies processed\n",
      "2021-01-14 15:42:50,362 : INFO : token count processed\n",
      "2021-01-14 15:42:50,366 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:42:50,370 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:42:50,371 : INFO : vocab #32006\n",
      "2021-01-14 15:42:50,378 : INFO : diff #set()\n",
      "2021-01-14 15:43:10,221 : INFO : alphabet #32006\n",
      "2021-01-14 15:43:20,093 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.1120655104769153, 0.47347016228402633], [0.7233085334300995, 0.27669147], [4.375222374437916, 1.4012677219702252], [5.631615665225582, 6.86432793886027, 7.034964117167931, 5.460979486917921, 1.4033484519423487, 0.1706361783076611]]\n",
      "2021-01-14 15:43:20,096 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:43:20,098 : INFO : built Dictionary(189 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 570 corpus positions)\n",
      "2021-01-14 15:43:20,314 : INFO : token count processed\n",
      "2021-01-14 15:43:20,348 : INFO : frequencies processed\n",
      "2021-01-14 15:43:30,633 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:43:30,634 : INFO : entropies processed\n",
      "2021-01-14 15:43:30,634 : INFO : extropies processed\n",
      "2021-01-14 15:43:30,641 : INFO : token count processed\n",
      "2021-01-14 15:43:30,646 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:43:30,650 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:43:30,651 : INFO : vocab #32006\n",
      "2021-01-14 15:43:30,657 : INFO : diff #set()\n",
      "2021-01-14 15:43:50,943 : INFO : alphabet #32006\n",
      "2021-01-14 15:44:00,985 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.1419214422362955, 0.46687053048777527], [0.7283541560173035, 0.27164584], [2.9139770731827523, 1.3356231683419404], [5.631615665225582, 6.431978396403875, 6.81075737412148, 5.252836687507977, 1.179141708895898, 0.37877897771760516]]\n",
      "2021-01-14 15:44:00,989 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:44:00,990 : INFO : built Dictionary(241 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 724 corpus positions)\n",
      "2021-01-14 15:44:01,273 : INFO : token count processed\n",
      "2021-01-14 15:44:01,305 : INFO : frequencies processed\n",
      "2021-01-14 15:44:11,605 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:44:11,606 : INFO : entropies processed\n",
      "2021-01-14 15:44:11,607 : INFO : extropies processed\n",
      "2021-01-14 15:44:11,614 : INFO : token count processed\n",
      "2021-01-14 15:44:11,619 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:44:11,624 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:44:11,625 : INFO : vocab #32006\n",
      "2021-01-14 15:44:11,632 : INFO : diff #set()\n",
      "2021-01-14 15:44:31,457 : INFO : alphabet #32006\n",
      "2021-01-14 15:44:41,212 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/test_auth_utility.py')[[1.1627839270147675, 0.4623670388471368], [0.7910000532865524, 0.20899995], [3.8365916681089787, 1.3877044860218604], [5.631615665225582, 6.911818353685893, 7.1947686664902, 5.348665352421277, 1.5631530012646175, 0.2829503128043065]]\n",
      "2021-01-14 15:44:41,225 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:44:41,228 : INFO : built Dictionary(336 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 7254 corpus positions)\n",
      "2021-01-14 15:44:41,721 : INFO : token count processed\n",
      "2021-01-14 15:44:41,755 : INFO : frequencies processed\n",
      "2021-01-14 15:44:51,892 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:44:51,893 : INFO : entropies processed\n",
      "2021-01-14 15:44:51,893 : INFO : extropies processed\n",
      "2021-01-14 15:44:51,902 : INFO : token count processed\n",
      "2021-01-14 15:44:51,906 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:44:51,910 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:44:51,911 : INFO : vocab #32006\n",
      "2021-01-14 15:44:51,917 : INFO : diff #set()\n",
      "2021-01-14 15:45:11,692 : INFO : alphabet #32006\n",
      "2021-01-14 15:45:21,639 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.1637365917923623, 0.46216346471805775], [0.817992091178894, 0.18200791], [3.4182958340544896, 1.369895090630202], [5.631615665225582, 6.363791471162389, 6.437673589469281, 5.557733546918691, 0.8060579242436985, 0.07388211830689162]]\n",
      "2021-01-14 15:45:21,644 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:45:21,645 : INFO : built Dictionary(227 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1324 corpus positions)\n",
      "2021-01-14 15:45:21,899 : INFO : token count processed\n",
      "2021-01-14 15:45:21,933 : INFO : frequencies processed\n",
      "2021-01-14 15:45:32,118 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:45:32,119 : INFO : entropies processed\n",
      "2021-01-14 15:45:32,119 : INFO : extropies processed\n",
      "2021-01-14 15:45:32,127 : INFO : token count processed\n",
      "2021-01-14 15:45:32,131 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:45:32,136 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:45:32,137 : INFO : vocab #32006\n",
      "2021-01-14 15:45:32,144 : INFO : diff #set()\n",
      "2021-01-14 15:45:52,205 : INFO : alphabet #32006\n",
      "2021-01-14 15:46:02,525 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.1254841039346855, 0.47048105330395323], [0.8195534497499466, 0.18044655], [4.349648912578752, 1.4051631060490313], [5.631615665225582, 6.29000629755059, 6.621539287632769, 5.300082675143402, 0.9899236224071872, 0.33153299008217907]]\n",
      "2021-01-14 15:46:02,529 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:46:02,531 : INFO : built Dictionary(239 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1277 corpus positions)\n",
      "2021-01-14 15:46:02,812 : INFO : token count processed\n",
      "2021-01-14 15:46:02,868 : INFO : frequencies processed\n",
      "2021-01-14 15:46:12,980 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:46:12,981 : INFO : entropies processed\n",
      "2021-01-14 15:46:12,982 : INFO : extropies processed\n",
      "2021-01-14 15:46:12,993 : INFO : token count processed\n",
      "2021-01-14 15:46:12,997 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:46:13,001 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:46:13,002 : INFO : vocab #32006\n",
      "2021-01-14 15:46:13,008 : INFO : diff #set()\n",
      "2021-01-14 15:46:32,675 : INFO : alphabet #32006\n",
      "2021-01-14 15:46:42,610 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.142404146750916, 0.4667653400113885], [0.8170307725667953, 0.18296923], [4.20184123230257, 1.4009137160862843], [5.631615665225582, 6.361621244785958, 6.684640671748207, 5.308596238263332, 1.053025006522625, 0.32301942696224906]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 15:46:42,615 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:46:42,617 : INFO : built Dictionary(249 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1300 corpus positions)\n",
      "2021-01-14 15:46:42,898 : INFO : token count processed\n",
      "2021-01-14 15:46:42,946 : INFO : frequencies processed\n",
      "2021-01-14 15:46:52,970 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:46:52,971 : INFO : entropies processed\n",
      "2021-01-14 15:46:52,972 : INFO : extropies processed\n",
      "2021-01-14 15:46:52,981 : INFO : token count processed\n",
      "2021-01-14 15:46:52,986 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:46:52,992 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:46:52,993 : INFO : vocab #32006\n",
      "2021-01-14 15:46:53,002 : INFO : diff #set()\n",
      "2021-01-14 15:47:13,306 : INFO : alphabet #32006\n",
      "2021-01-14 15:47:23,405 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.1086357431267093, 0.4742402775157309], [0.7380234003067017, 0.2619766], [4.037401197654112, 1.3956040576270576], [5.631615665225582, 6.620594433343389, 6.93809832849486, 5.314111770074112, 1.3064826632692776, 0.3175038951514706]]\n",
      "2021-01-14 15:47:23,410 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:47:23,412 : INFO : built Dictionary(223 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1393 corpus positions)\n",
      "2021-01-14 15:47:23,675 : INFO : token count processed\n",
      "2021-01-14 15:47:23,762 : INFO : frequencies processed\n",
      "2021-01-14 15:47:33,680 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:47:33,681 : INFO : entropies processed\n",
      "2021-01-14 15:47:33,682 : INFO : extropies processed\n",
      "2021-01-14 15:47:33,690 : INFO : token count processed\n",
      "2021-01-14 15:47:33,694 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:47:33,699 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:47:33,700 : INFO : vocab #32006\n",
      "2021-01-14 15:47:33,706 : INFO : diff #set()\n",
      "2021-01-14 15:47:53,817 : INFO : alphabet #32006\n",
      "2021-01-14 15:48:03,544 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.1157755429505745, 0.4726399278656188], [0.7455891370773315, 0.25441086], [3.852168723603281, 1.3887904911367783], [5.631615665225582, 6.207411496248084, 6.459889393424238, 5.379137768049429, 0.8282737281986554, 0.25247789717615365]]\n",
      "2021-01-14 15:48:03,547 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:48:03,549 : INFO : built Dictionary(163 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 297 corpus positions)\n",
      "2021-01-14 15:48:03,722 : INFO : token count processed\n",
      "2021-01-14 15:48:03,803 : INFO : frequencies processed\n",
      "2021-01-14 15:48:13,510 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:48:13,511 : INFO : entropies processed\n",
      "2021-01-14 15:48:13,512 : INFO : extropies processed\n",
      "2021-01-14 15:48:13,522 : INFO : token count processed\n",
      "2021-01-14 15:48:13,527 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:48:13,531 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:48:13,532 : INFO : vocab #32006\n",
      "2021-01-14 15:48:13,538 : INFO : diff #set()\n",
      "2021-01-14 15:48:33,509 : INFO : alphabet #32006\n",
      "2021-01-14 15:48:43,541 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.1305247413106898, 0.469367935800081], [0.7222508490085602, 0.27774915], [3.852168723603281, 1.3887904911367783], [5.631615665225582, 6.5805228788529595, 7.009966831657923, 5.202171712420618, 1.3783511664323411, 0.42944395280496384]]\n",
      "2021-01-14 15:48:43,546 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 15:48:43,547 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:48:43,548 : INFO : built Dictionary(252 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1258 corpus positions)\n",
      "2021-01-14 15:48:43,852 : INFO : token count processed\n",
      "2021-01-14 15:48:43,902 : INFO : frequencies processed\n",
      "2021-01-14 15:48:53,922 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:48:53,923 : INFO : entropies processed\n",
      "2021-01-14 15:48:53,924 : INFO : extropies processed\n",
      "2021-01-14 15:48:53,935 : INFO : token count processed\n",
      "2021-01-14 15:48:53,940 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:48:53,947 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:48:53,947 : INFO : vocab #32006\n",
      "2021-01-14 15:48:53,956 : INFO : diff #set()\n",
      "2021-01-14 15:49:14,227 : INFO : alphabet #32006\n",
      "2021-01-14 15:49:24,254 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.1846366936323822, 0.45774201399927333], [0.8436872065067291, 0.1563128], [3.0220552088742005, 1.3359632893587228], [5.631615665225582, 6.422089779976135, 6.6629127540609785, 5.39079269114074, 1.0312970888353963, 0.24082297408484354]]\n",
      "2021-01-14 15:49:24,259 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:49:24,261 : INFO : built Dictionary(255 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1540 corpus positions)\n",
      "2021-01-14 15:49:24,566 : INFO : token count processed\n",
      "2021-01-14 15:49:24,623 : INFO : frequencies processed\n",
      "2021-01-14 15:49:34,713 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:49:34,714 : INFO : entropies processed\n",
      "2021-01-14 15:49:34,715 : INFO : extropies processed\n",
      "2021-01-14 15:49:34,723 : INFO : token count processed\n",
      "2021-01-14 15:49:34,727 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:49:34,732 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:49:34,732 : INFO : vocab #32006\n",
      "2021-01-14 15:49:34,739 : INFO : diff #set()\n",
      "2021-01-14 15:49:54,090 : INFO : alphabet #32006\n",
      "2021-01-14 15:50:03,850 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.121451545741228, 0.47137536655384865], [0.7719706892967224, 0.22802931], [4.1219280948873624, 1.3984144061464991], [5.631615665225582, 6.485445644653597, 6.8228049503870905, 5.294256359492089, 1.1911892851615082, 0.3373593057334938]]\n",
      "2021-01-14 15:50:03,855 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:50:03,857 : INFO : built Dictionary(233 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1426 corpus positions)\n",
      "2021-01-14 15:50:04,119 : INFO : token count processed\n",
      "2021-01-14 15:50:04,166 : INFO : frequencies processed\n",
      "2021-01-14 15:50:14,195 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:50:14,196 : INFO : entropies processed\n",
      "2021-01-14 15:50:14,197 : INFO : extropies processed\n",
      "2021-01-14 15:50:14,204 : INFO : token count processed\n",
      "2021-01-14 15:50:14,209 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:50:14,213 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:50:14,214 : INFO : vocab #32006\n",
      "2021-01-14 15:50:14,221 : INFO : diff #set()\n",
      "2021-01-14 15:50:34,145 : INFO : alphabet #32006\n",
      "2021-01-14 15:50:44,474 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.1180652687541845, 0.4721289824029764], [0.746801346540451, 0.25319865], [3.852168723603281, 1.3887904911367783], [5.631615665225582, 6.2276600107346916, 6.475411187144835, 5.383864488815438, 0.8437955219192528, 0.24775117641014344]]\n",
      "2021-01-14 15:50:44,479 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:50:44,481 : INFO : built Dictionary(218 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1624 corpus positions)\n",
      "2021-01-14 15:50:44,731 : INFO : token count processed\n",
      "2021-01-14 15:50:44,762 : INFO : frequencies processed\n",
      "2021-01-14 15:50:54,788 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:50:54,789 : INFO : entropies processed\n",
      "2021-01-14 15:50:54,790 : INFO : extropies processed\n",
      "2021-01-14 15:50:54,797 : INFO : token count processed\n",
      "2021-01-14 15:50:54,802 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:50:54,806 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:50:54,807 : INFO : vocab #32006\n",
      "2021-01-14 15:50:54,813 : INFO : diff #set()\n",
      "2021-01-14 15:51:15,117 : INFO : alphabet #32006\n",
      "2021-01-14 15:51:25,197 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.1227485450665697, 0.47108735621281034], [0.8069286942481995, 0.1930713], [4.277613436819116, 1.4031503174783797], [5.631615665225582, 6.253918170574241, 6.534614133393109, 5.350919702406713, 0.9029984681675272, 0.28069596281886877]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 15:51:25,201 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:51:25,203 : INFO : built Dictionary(195 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 721 corpus positions)\n",
      "2021-01-14 15:51:25,411 : INFO : token count processed\n",
      "2021-01-14 15:51:25,442 : INFO : frequencies processed\n",
      "2021-01-14 15:51:35,498 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:51:35,499 : INFO : entropies processed\n",
      "2021-01-14 15:51:35,500 : INFO : extropies processed\n",
      "2021-01-14 15:51:35,507 : INFO : token count processed\n",
      "2021-01-14 15:51:35,512 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:51:35,516 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:51:35,516 : INFO : vocab #32006\n",
      "2021-01-14 15:51:35,522 : INFO : diff #set()\n",
      "2021-01-14 15:51:55,739 : INFO : alphabet #32006\n",
      "2021-01-14 15:52:05,715 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.0723620939558764, 0.48254115577414697], [0.6864123046398163, 0.3135877], [3.852168723603281, 1.3887904911367783], [5.631615665225582, 6.374522245625576, 6.742442363900848, 5.263695546950311, 1.1108266986752655, 0.36792011827527205]]\n",
      "2021-01-14 15:52:05,720 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:52:05,722 : INFO : built Dictionary(315 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 2003 corpus positions)\n",
      "2021-01-14 15:52:06,170 : INFO : token count processed\n",
      "2021-01-14 15:52:06,232 : INFO : frequencies processed\n",
      "2021-01-14 15:52:15,703 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:52:15,705 : INFO : entropies processed\n",
      "2021-01-14 15:52:15,705 : INFO : extropies processed\n",
      "2021-01-14 15:52:15,718 : INFO : token count processed\n",
      "2021-01-14 15:52:15,722 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:52:15,727 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:52:15,730 : INFO : vocab #32006\n",
      "2021-01-14 15:52:15,736 : INFO : diff #set()\n",
      "2021-01-14 15:52:35,715 : INFO : alphabet #32006\n",
      "2021-01-14 15:52:45,677 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.1616713119462898, 0.4626050197703908], [0.8209828436374664, 0.17901716], [4.037401197654112, 1.3956040576270576], [5.631615665225582, 6.731238669067808, 7.075838923756665, 5.287015410536725, 1.4442232585310828, 0.34460025468885735]]\n",
      "2021-01-14 15:52:45,681 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:52:45,683 : INFO : built Dictionary(238 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1395 corpus positions)\n",
      "2021-01-14 15:52:45,986 : INFO : token count processed\n",
      "2021-01-14 15:52:46,066 : INFO : frequencies processed\n",
      "2021-01-14 15:52:56,121 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:52:56,122 : INFO : entropies processed\n",
      "2021-01-14 15:52:56,123 : INFO : extropies processed\n",
      "2021-01-14 15:52:56,135 : INFO : token count processed\n",
      "2021-01-14 15:52:56,139 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:52:56,146 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:52:56,147 : INFO : vocab #32006\n",
      "2021-01-14 15:52:56,155 : INFO : diff #set()\n",
      "2021-01-14 15:53:16,519 : INFO : alphabet #32006\n",
      "2021-01-14 15:53:26,581 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.1237158834211536, 0.4708727790786552], [0.8009459227323532, 0.19905408], [4.20184123230257, 1.4009137160862843], [5.631615665225582, 6.503741451859337, 6.795842012142667, 5.339515104942252, 1.1642263469170846, 0.29210056028332954]]\n",
      "2021-01-14 15:53:26,587 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:53:26,589 : INFO : built Dictionary(258 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1717 corpus positions)\n",
      "2021-01-14 15:53:26,902 : INFO : token count processed\n",
      "2021-01-14 15:53:26,973 : INFO : frequencies processed\n",
      "2021-01-14 15:53:37,103 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:53:37,104 : INFO : entropies processed\n",
      "2021-01-14 15:53:37,105 : INFO : extropies processed\n",
      "2021-01-14 15:53:37,116 : INFO : token count processed\n",
      "2021-01-14 15:53:37,120 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:53:37,126 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:53:37,127 : INFO : vocab #32006\n",
      "2021-01-14 15:53:37,134 : INFO : diff #set()\n",
      "2021-01-14 15:53:57,063 : INFO : alphabet #32006\n",
      "2021-01-14 15:54:06,830 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.1321241494245025, 0.469015840503433], [0.7819829881191254, 0.21801701], [4.349648912578752, 1.4051631060490313], [5.631615665225582, 6.334729224484471, 6.556484703485775, 5.40986018622428, 0.9248690382601925, 0.22175547900130343]]\n",
      "2021-01-14 15:54:06,836 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:54:06,837 : INFO : built Dictionary(258 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 2066 corpus positions)\n",
      "2021-01-14 15:54:07,137 : INFO : token count processed\n",
      "2021-01-14 15:54:07,169 : INFO : frequencies processed\n",
      "2021-01-14 15:54:17,147 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:54:17,148 : INFO : entropies processed\n",
      "2021-01-14 15:54:17,149 : INFO : extropies processed\n",
      "2021-01-14 15:54:17,160 : INFO : token count processed\n",
      "2021-01-14 15:54:17,165 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:54:17,170 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:54:17,170 : INFO : vocab #32006\n",
      "2021-01-14 15:54:17,177 : INFO : diff #set()\n",
      "2021-01-14 15:54:37,138 : INFO : alphabet #32006\n",
      "2021-01-14 15:54:47,181 : INFO : Computed distances or similarities ('281', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.1052048188527093, 0.475013163111121], [0.7620690762996674, 0.23793092], [4.1219280948873624, 1.3984144061464991], [5.631615665225582, 6.21319712067992, 6.488225347245011, 5.356587438660492, 0.8566096820194291, 0.27502822656509096]]\n",
      "2021-01-14 15:54:47,186 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 15:54:47,187 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:54:47,192 : INFO : built Dictionary(271 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1407 corpus positions)\n",
      "2021-01-14 15:54:47,293 : INFO : token count processed\n",
      "2021-01-14 15:54:47,330 : INFO : frequencies processed\n",
      "2021-01-14 15:54:57,467 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:54:57,469 : INFO : entropies processed\n",
      "2021-01-14 15:54:57,470 : INFO : extropies processed\n",
      "2021-01-14 15:54:57,477 : INFO : token count processed\n",
      "2021-01-14 15:54:57,482 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:54:57,486 : INFO : alphabet_target #32010\n",
      "2021-01-14 15:54:57,487 : INFO : vocab #32006\n",
      "2021-01-14 15:54:57,494 : INFO : diff #set()\n",
      "2021-01-14 15:55:17,119 : INFO : alphabet #32006\n",
      "2021-01-14 15:55:27,178 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.2896365031550134, 0.43675054910333855], [0.972181923687458, 0.027818076], [1.584962500721156, 1.1699250014423124], [3.702819531114783, 6.905617163738059, 7.03504374995094, 3.573392944901901, 3.3322242188361573, 0.1294265862128814]]\n",
      "2021-01-14 15:55:27,184 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:55:27,186 : INFO : built Dictionary(361 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 2297 corpus positions)\n",
      "2021-01-14 15:55:27,343 : INFO : token count processed\n",
      "2021-01-14 15:55:27,375 : INFO : frequencies processed\n",
      "2021-01-14 15:55:37,070 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:55:37,071 : INFO : entropies processed\n",
      "2021-01-14 15:55:37,072 : INFO : extropies processed\n",
      "2021-01-14 15:55:37,080 : INFO : token count processed\n",
      "2021-01-14 15:55:37,084 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:55:37,088 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:55:37,089 : INFO : vocab #32006\n",
      "2021-01-14 15:55:37,095 : INFO : diff #set()\n",
      "2021-01-14 15:55:57,180 : INFO : alphabet #32006\n",
      "2021-01-14 15:56:07,301 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.2817102448559798, 0.43826774335367874], [0.967256922274828, 0.032743078], [2.0, 1.2451124978365313], [3.702819531114783, 7.1219284286457345, 7.331574698809904, 3.4931732609506145, 3.628755167695121, 0.20964627016416948]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 15:56:07,306 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:56:07,308 : INFO : built Dictionary(281 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 2280 corpus positions)\n",
      "2021-01-14 15:56:07,411 : INFO : token count processed\n",
      "2021-01-14 15:56:07,444 : INFO : frequencies processed\n",
      "2021-01-14 15:56:17,684 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:56:17,685 : INFO : entropies processed\n",
      "2021-01-14 15:56:17,686 : INFO : extropies processed\n",
      "2021-01-14 15:56:17,697 : INFO : token count processed\n",
      "2021-01-14 15:56:17,702 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:56:17,706 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:56:17,707 : INFO : vocab #32006\n",
      "2021-01-14 15:56:17,714 : INFO : diff #set()\n",
      "2021-01-14 15:56:37,827 : INFO : alphabet #32006\n",
      "2021-01-14 15:56:47,974 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2943825298234453, 0.43584711224110956], [0.9746644739061594, 0.025335526], [1.5, 1.1225562489182657], [3.702819531114783, 6.41099024988467, 6.475818737806199, 3.6379910431932547, 2.7729992066914155, 0.0648284879215284]]\n",
      "2021-01-14 15:56:47,978 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:56:47,979 : INFO : built Dictionary(161 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 679 corpus positions)\n",
      "2021-01-14 15:56:48,039 : INFO : token count processed\n",
      "2021-01-14 15:56:48,081 : INFO : frequencies processed\n",
      "2021-01-14 15:56:58,025 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:56:58,026 : INFO : entropies processed\n",
      "2021-01-14 15:56:58,027 : INFO : extropies processed\n",
      "2021-01-14 15:56:58,035 : INFO : token count processed\n",
      "2021-01-14 15:56:58,040 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:56:58,044 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:56:58,045 : INFO : vocab #32006\n",
      "2021-01-14 15:56:58,052 : INFO : diff #set()\n",
      "2021-01-14 15:57:18,015 : INFO : alphabet #32006\n",
      "2021-01-14 15:57:28,023 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.2885459891943387, 0.43695866489973423], [0.9766394961625338, 0.023360504], [0.0, 0.0], [3.702819531114783, 6.077866832717642, 6.217293284913167, 3.5633930789192583, 2.5144737537983834, 0.13942645219552485]]\n",
      "2021-01-14 15:57:28,027 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:57:28,028 : INFO : built Dictionary(136 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 444 corpus positions)\n",
      "2021-01-14 15:57:28,074 : INFO : token count processed\n",
      "2021-01-14 15:57:28,110 : INFO : frequencies processed\n",
      "2021-01-14 15:57:38,053 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:57:38,054 : INFO : entropies processed\n",
      "2021-01-14 15:57:38,055 : INFO : extropies processed\n",
      "2021-01-14 15:57:38,063 : INFO : token count processed\n",
      "2021-01-14 15:57:38,068 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:57:38,072 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:57:38,073 : INFO : vocab #32006\n",
      "2021-01-14 15:57:38,081 : INFO : diff #set()\n",
      "2021-01-14 15:57:58,100 : INFO : alphabet #32006\n",
      "2021-01-14 15:58:07,968 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.2820941265104489, 0.4381940203006001], [0.9630578868091106, 0.036942113], [1.0, 1.0], [3.702819531114783, 5.977547459003844, 6.102230737640012, 3.5781362524786164, 2.3994112065252287, 0.12468327863616757]]\n",
      "2021-01-14 15:58:07,973 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:58:07,975 : INFO : built Dictionary(240 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 2155 corpus positions)\n",
      "2021-01-14 15:58:08,085 : INFO : token count processed\n",
      "2021-01-14 15:58:08,166 : INFO : frequencies processed\n",
      "2021-01-14 15:58:18,327 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:58:18,328 : INFO : entropies processed\n",
      "2021-01-14 15:58:18,329 : INFO : extropies processed\n",
      "2021-01-14 15:58:18,340 : INFO : token count processed\n",
      "2021-01-14 15:58:18,344 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:58:18,349 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:58:18,350 : INFO : vocab #32006\n",
      "2021-01-14 15:58:18,356 : INFO : diff #set()\n",
      "2021-01-14 15:58:38,783 : INFO : alphabet #32006\n",
      "2021-01-14 15:58:48,737 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.2871759260384348, 0.4372204116943803], [0.9678570553660393, 0.032142945], [1.3709505944546687, 1.0438561897747245], [3.702819531114783, 6.4614394051846435, 6.556381365046111, 3.607877571253315, 2.8535618339313276, 0.09494195986146714]]\n",
      "2021-01-14 15:58:48,742 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 15:58:48,743 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:58:48,747 : INFO : built Dictionary(203 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1379 corpus positions)\n",
      "2021-01-14 15:58:48,829 : INFO : token count processed\n",
      "2021-01-14 15:58:48,865 : INFO : frequencies processed\n",
      "2021-01-14 15:58:58,855 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:58:58,856 : INFO : entropies processed\n",
      "2021-01-14 15:58:58,857 : INFO : extropies processed\n",
      "2021-01-14 15:58:58,865 : INFO : token count processed\n",
      "2021-01-14 15:58:58,869 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:58:58,874 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:58:58,875 : INFO : vocab #32006\n",
      "2021-01-14 15:58:58,881 : INFO : diff #set()\n",
      "2021-01-14 15:59:19,034 : INFO : alphabet #32006\n",
      "2021-01-14 15:59:29,005 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.2547379103031207, 0.4435105275120703], [0.9375523999333382, 0.0624476], [0.8112781244591328, 0.8112781244591328], [3.702819531114783, 6.327195724598159, 6.427382185385242, 3.602633070327701, 2.724562654270459, 0.10018646078708304]]\n",
      "2021-01-14 15:59:29,018 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 15:59:29,019 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 15:59:29,021 : INFO : built Dictionary(417 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 6271 corpus positions)\n",
      "2021-01-14 15:59:29,218 : INFO : token count processed\n",
      "2021-01-14 15:59:29,254 : INFO : frequencies processed\n",
      "2021-01-14 15:59:39,180 : INFO : scalar_distribution processed\n",
      "2021-01-14 15:59:39,181 : INFO : entropies processed\n",
      "2021-01-14 15:59:39,182 : INFO : extropies processed\n",
      "2021-01-14 15:59:39,191 : INFO : token count processed\n",
      "2021-01-14 15:59:39,196 : INFO : alphabet_source #32006\n",
      "2021-01-14 15:59:39,200 : INFO : alphabet_target #32009\n",
      "2021-01-14 15:59:39,201 : INFO : vocab #32006\n",
      "2021-01-14 15:59:39,207 : INFO : diff #set()\n",
      "2021-01-14 15:59:59,224 : INFO : alphabet #32006\n",
      "2021-01-14 16:00:08,957 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.2707535921857953, 0.4403824366682666], [0.9520529396831989, 0.04794706], [1.7924812503605778, 1.1575860145844845], [3.702819531114783, 6.9079058562486315, 7.011147198545312, 3.5995781888181027, 3.308327667430529, 0.10324134229668047]]\n",
      "2021-01-14 16:00:08,963 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:00:08,965 : INFO : built Dictionary(321 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 2665 corpus positions)\n",
      "2021-01-14 16:00:09,094 : INFO : token count processed\n",
      "2021-01-14 16:00:09,127 : INFO : frequencies processed\n",
      "2021-01-14 16:00:19,123 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:00:19,124 : INFO : entropies processed\n",
      "2021-01-14 16:00:19,125 : INFO : extropies processed\n",
      "2021-01-14 16:00:19,136 : INFO : token count processed\n",
      "2021-01-14 16:00:19,141 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:00:19,145 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:00:19,146 : INFO : vocab #32006\n",
      "2021-01-14 16:00:19,153 : INFO : diff #set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 16:00:39,160 : INFO : alphabet #32006\n",
      "2021-01-14 16:00:49,178 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.2373454198425138, 0.4469582529059771], [0.9303554370999336, 0.06964456], [1.0, 1.0], [3.702819531114783, 6.61034830706307, 6.761099169190522, 3.5520686689873324, 3.0582796380757387, 0.1507508621274516]]\n",
      "2021-01-14 16:00:49,182 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:00:49,183 : INFO : built Dictionary(209 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 712 corpus positions)\n",
      "2021-01-14 16:00:49,251 : INFO : token count processed\n",
      "2021-01-14 16:00:49,277 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:00:49,277 : INFO : frequencies processed\n",
      "2021-01-14 16:00:49,278 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:00:49,283 : INFO : token count processed\n",
      "2021-01-14 16:00:49,287 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:00:49,292 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:00:49,292 : INFO : vocab #32006\n",
      "2021-01-14 16:00:49,299 : INFO : diff #set()\n",
      "2021-01-14 16:01:09,217 : INFO : alphabet #32006\n",
      "2021-01-14 16:01:19,475 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.3025259838808978, 0.4343056308595937], [0.9873287258669734, 0.012671274], [nan, nan], [3.702819531114783, 6.616715366949855, 6.756350145311799, 3.5631847527528393, 3.053530614197016, 0.1396347783619447]]\n",
      "2021-01-14 16:01:19,482 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:01:19,484 : INFO : built Dictionary(425 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 2746 corpus positions)\n",
      "2021-01-14 16:01:19,675 : INFO : token count processed\n",
      "2021-01-14 16:01:19,711 : INFO : frequencies processed\n",
      "2021-01-14 16:01:29,712 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:01:29,713 : INFO : entropies processed\n",
      "2021-01-14 16:01:29,714 : INFO : extropies processed\n",
      "2021-01-14 16:01:29,723 : INFO : token count processed\n",
      "2021-01-14 16:01:29,728 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:01:29,732 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:01:29,733 : INFO : vocab #32006\n",
      "2021-01-14 16:01:29,739 : INFO : diff #set()\n",
      "2021-01-14 16:01:49,663 : INFO : alphabet #32006\n",
      "2021-01-14 16:01:59,615 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.2782181723644772, 0.43893952393599667], [0.9565056450664997, 0.043494355], [1.7924812503605778, 1.1575860145844845], [3.702819531114783, 7.32185870753746, 7.515166015330113, 3.5095122233221296, 3.81234648421533, 0.1933073077926526]]\n",
      "2021-01-14 16:01:59,618 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:01:59,619 : INFO : built Dictionary(62 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 111 corpus positions)\n",
      "2021-01-14 16:01:59,639 : INFO : token count processed\n",
      "2021-01-14 16:01:59,667 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:01:59,668 : INFO : frequencies processed\n",
      "2021-01-14 16:01:59,669 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:01:59,675 : INFO : token count processed\n",
      "2021-01-14 16:01:59,680 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:01:59,684 : INFO : alphabet_target #32008\n",
      "2021-01-14 16:01:59,685 : INFO : vocab #32006\n",
      "2021-01-14 16:01:59,691 : INFO : diff #set()\n",
      "2021-01-14 16:02:19,668 : INFO : alphabet #32006\n",
      "2021-01-14 16:02:29,815 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/fireException.py')[[1.3069478327266781, 0.43347317430150045], [0.9908281825482845, 0.009171817], [nan, nan], [3.702819531114783, 5.176618657501385, 5.628764195574949, 3.25067399304122, 1.9259446644601654, 0.4521455380735633]]\n",
      "2021-01-14 16:02:29,818 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:02:29,820 : INFO : built Dictionary(157 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 504 corpus positions)\n",
      "2021-01-14 16:02:29,882 : INFO : token count processed\n",
      "2021-01-14 16:02:29,917 : INFO : frequencies processed\n",
      "2021-01-14 16:02:39,903 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:02:39,904 : INFO : entropies processed\n",
      "2021-01-14 16:02:39,905 : INFO : extropies processed\n",
      "2021-01-14 16:02:39,916 : INFO : token count processed\n",
      "2021-01-14 16:02:39,921 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:02:39,925 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:02:39,926 : INFO : vocab #32006\n",
      "2021-01-14 16:02:39,933 : INFO : diff #set()\n",
      "2021-01-14 16:03:00,389 : INFO : alphabet #32006\n",
      "2021-01-14 16:03:10,573 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.2901867431877596, 0.4366456154610688], [0.9723104387521744, 0.027689561], [0.0, 0.0], [3.702819531114783, 6.468846789852156, 6.646905617296927, 3.5247607036700117, 2.9440860861821436, 0.17805882744477053]]\n",
      "2021-01-14 16:03:10,579 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:03:10,581 : INFO : built Dictionary(372 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 2553 corpus positions)\n",
      "2021-01-14 16:03:10,742 : INFO : token count processed\n",
      "2021-01-14 16:03:10,773 : INFO : frequencies processed\n",
      "2021-01-14 16:03:20,742 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:03:20,743 : INFO : entropies processed\n",
      "2021-01-14 16:03:20,744 : INFO : extropies processed\n",
      "2021-01-14 16:03:20,753 : INFO : token count processed\n",
      "2021-01-14 16:03:20,757 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:03:20,761 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:03:20,762 : INFO : vocab #32006\n",
      "2021-01-14 16:03:20,768 : INFO : diff #set()\n",
      "2021-01-14 16:03:40,712 : INFO : alphabet #32006\n",
      "2021-01-14 16:03:50,686 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.290860194515956, 0.436517253385379], [0.9665839336812496, 0.033416066], [1.5, 1.1225562489182657], [3.702819531114783, 6.957796704012729, 7.091754201517425, 3.5688620336100882, 3.3889346704026417, 0.13395749750469577]]\n",
      "2021-01-14 16:03:50,692 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:03:50,695 : INFO : built Dictionary(281 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 3050 corpus positions)\n",
      "2021-01-14 16:03:50,799 : INFO : token count processed\n",
      "2021-01-14 16:03:50,832 : INFO : frequencies processed\n",
      "2021-01-14 16:04:00,775 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:04:00,777 : INFO : entropies processed\n",
      "2021-01-14 16:04:00,777 : INFO : extropies processed\n",
      "2021-01-14 16:04:00,786 : INFO : token count processed\n",
      "2021-01-14 16:04:00,790 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:04:00,795 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:04:00,795 : INFO : vocab #32006\n",
      "2021-01-14 16:04:00,802 : INFO : diff #set()\n",
      "2021-01-14 16:04:20,782 : INFO : alphabet #32006\n",
      "2021-01-14 16:04:30,715 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.2796013475754437, 0.4386731921630016], [0.9670219719409943, 0.032978028], [1.3709505944546687, 1.0438561897747245], [3.702819531114783, 6.441859572014148, 6.581248465284906, 3.563430637844025, 2.878428934170123, 0.1393888932707581]]\n",
      "2021-01-14 16:04:30,720 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:04:30,722 : INFO : built Dictionary(308 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1526 corpus positions)\n",
      "2021-01-14 16:04:30,859 : INFO : token count processed\n",
      "2021-01-14 16:04:30,894 : INFO : frequencies processed\n",
      "2021-01-14 16:04:40,909 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:04:40,910 : INFO : entropies processed\n",
      "2021-01-14 16:04:40,911 : INFO : extropies processed\n",
      "2021-01-14 16:04:40,921 : INFO : token count processed\n",
      "2021-01-14 16:04:40,925 : INFO : alphabet_source #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 16:04:40,929 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:04:40,930 : INFO : vocab #32006\n",
      "2021-01-14 16:04:40,937 : INFO : diff #set()\n",
      "2021-01-14 16:05:00,776 : INFO : alphabet #32006\n",
      "2021-01-14 16:05:10,685 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.2916836979447062, 0.43636039340719174], [0.9756045658141375, 0.024395434], [1.0, 1.0], [3.702819531114783, 6.998955278238291, 7.156474603208903, 3.545300206144171, 3.4536550720941195, 0.15751932497061194]]\n",
      "2021-01-14 16:05:10,689 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:05:10,691 : INFO : built Dictionary(223 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1345 corpus positions)\n",
      "2021-01-14 16:05:10,774 : INFO : token count processed\n",
      "2021-01-14 16:05:10,806 : INFO : frequencies processed\n",
      "2021-01-14 16:05:20,620 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:05:20,621 : INFO : entropies processed\n",
      "2021-01-14 16:05:20,622 : INFO : extropies processed\n",
      "2021-01-14 16:05:20,629 : INFO : token count processed\n",
      "2021-01-14 16:05:20,634 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:05:20,638 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:05:20,639 : INFO : vocab #32006\n",
      "2021-01-14 16:05:20,645 : INFO : diff #set()\n",
      "2021-01-14 16:05:40,578 : INFO : alphabet #32006\n",
      "2021-01-14 16:05:50,546 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.2910486932015413, 0.43648133842261855], [0.9761480949819088, 0.023851905], [1.0, 1.0], [3.702819531114783, 6.492983191376071, 6.60264353392881, 3.5931591885620433, 2.899824002814027, 0.10966034255273893]]\n",
      "2021-01-14 16:05:50,553 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:05:50,555 : INFO : built Dictionary(434 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 3284 corpus positions)\n",
      "2021-01-14 16:05:50,746 : INFO : token count processed\n",
      "2021-01-14 16:05:50,772 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:05:50,772 : INFO : frequencies processed\n",
      "2021-01-14 16:05:50,773 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:05:50,779 : INFO : token count processed\n",
      "2021-01-14 16:05:50,784 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:05:50,788 : INFO : alphabet_target #32008\n",
      "2021-01-14 16:05:50,789 : INFO : vocab #32006\n",
      "2021-01-14 16:05:50,795 : INFO : diff #set()\n",
      "2021-01-14 16:06:10,808 : INFO : alphabet #32006\n",
      "2021-01-14 16:06:20,838 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.3104905500106532, 0.4328085219804899], [0.9929872369393706, 0.007012763], [nan, nan], [3.702819531114783, 6.560342487747443, 6.747559693344651, 3.515602325517574, 3.0447401622298678, 0.18721720559720811]]\n",
      "2021-01-14 16:06:20,846 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:06:20,847 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:06:20,853 : INFO : built Dictionary(449 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 3489 corpus positions)\n",
      "2021-01-14 16:06:21,079 : INFO : token count processed\n",
      "2021-01-14 16:06:21,131 : INFO : frequencies processed\n",
      "2021-01-14 16:06:31,129 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:06:31,130 : INFO : entropies processed\n",
      "2021-01-14 16:06:31,131 : INFO : extropies processed\n",
      "2021-01-14 16:06:31,143 : INFO : token count processed\n",
      "2021-01-14 16:06:31,148 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:06:31,152 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:06:31,153 : INFO : vocab #32006\n",
      "2021-01-14 16:06:31,159 : INFO : diff #set()\n",
      "2021-01-14 16:06:51,227 : INFO : alphabet #32006\n",
      "2021-01-14 16:07:01,222 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.2589918773922237, 0.44267534115899443], [0.9469562321901321, 0.053043768], [1.0, 1.0], [3.702819531114783, 7.046173750105238, 7.2297294826555545, 3.5192637985644657, 3.5269099515407714, 0.18355573255031654]]\n",
      "2021-01-14 16:07:01,233 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:07:01,234 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:07:01,237 : INFO : built Dictionary(501 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 5598 corpus positions)\n",
      "2021-01-14 16:07:01,532 : INFO : token count processed\n",
      "2021-01-14 16:07:01,597 : INFO : frequencies processed\n",
      "2021-01-14 16:07:11,674 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:07:11,675 : INFO : entropies processed\n",
      "2021-01-14 16:07:11,676 : INFO : extropies processed\n",
      "2021-01-14 16:07:11,685 : INFO : token count processed\n",
      "2021-01-14 16:07:11,690 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:07:11,694 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:07:11,695 : INFO : vocab #32006\n",
      "2021-01-14 16:07:11,701 : INFO : diff #set()\n",
      "2021-01-14 16:07:32,119 : INFO : alphabet #32006\n",
      "2021-01-14 16:07:42,224 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.2720365030151313, 0.44013377367526396], [0.959263626486063, 0.040736374], [2.0, 1.2451124978365313], [3.702819531114783, 7.009229588004272, 7.113631990324737, 3.598417128794318, 3.4108124592099536, 0.10440240232046438]]\n",
      "2021-01-14 16:07:42,237 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:07:42,241 : INFO : built Dictionary(580 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 6539 corpus positions)\n",
      "2021-01-14 16:07:42,565 : INFO : token count processed\n",
      "2021-01-14 16:07:42,644 : INFO : frequencies processed\n",
      "2021-01-14 16:07:52,546 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:07:52,547 : INFO : entropies processed\n",
      "2021-01-14 16:07:52,548 : INFO : extropies processed\n",
      "2021-01-14 16:07:52,561 : INFO : token count processed\n",
      "2021-01-14 16:07:52,566 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:07:52,571 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:07:52,571 : INFO : vocab #32006\n",
      "2021-01-14 16:07:52,578 : INFO : diff #set()\n",
      "2021-01-14 16:08:12,359 : INFO : alphabet #32006\n",
      "2021-01-14 16:08:22,428 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.2766227509294947, 0.4392471258541724], [0.9551393464207649, 0.044860654], [1.7924812503605778, 1.1575860145844845], [3.702819531114783, 7.376088004590871, 7.536971589783481, 3.541935945922172, 3.834152058668698, 0.16088358519261003]]\n",
      "2021-01-14 16:08:22,431 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:08:22,433 : INFO : built Dictionary(134 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 325 corpus positions)\n",
      "2021-01-14 16:08:22,498 : INFO : token count processed\n",
      "2021-01-14 16:08:22,571 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:08:22,573 : INFO : frequencies processed\n",
      "2021-01-14 16:08:22,576 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:08:22,582 : INFO : token count processed\n",
      "2021-01-14 16:08:22,588 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:08:22,592 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:08:22,593 : INFO : vocab #32006\n",
      "2021-01-14 16:08:22,602 : INFO : diff #set()\n",
      "2021-01-14 16:08:42,570 : INFO : alphabet #32006\n",
      "2021-01-14 16:08:52,331 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.2939458825948427, 0.4359300747186024], [0.976794483140111, 0.023205517], [nan, nan], [3.702819531114783, 6.2993628166120885, 6.486798602923472, 3.5153837448034, 2.783979071808689, 0.1874357863113838]]\n",
      "2021-01-14 16:08:52,334 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:08:52,335 : INFO : built Dictionary(30 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 41 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 16:08:52,350 : INFO : token count processed\n",
      "2021-01-14 16:08:52,384 : INFO : frequencies processed\n",
      "2021-01-14 16:09:02,535 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:09:02,536 : INFO : entropies processed\n",
      "2021-01-14 16:09:02,537 : INFO : extropies processed\n",
      "2021-01-14 16:09:02,548 : INFO : token count processed\n",
      "2021-01-14 16:09:02,553 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:09:02,557 : INFO : alphabet_target #32008\n",
      "2021-01-14 16:09:02,559 : INFO : vocab #32006\n",
      "2021-01-14 16:09:02,567 : INFO : diff #set()\n",
      "2021-01-14 16:09:22,582 : INFO : alphabet #32006\n",
      "2021-01-14 16:09:32,966 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2631903197253274, 0.4418541345304823], [0.9467709623277187, 0.053229038], [0.0, 0.0], [3.702819531114783, 3.8936606896881862, 4.631305423879507, 2.9651747969234625, 0.9284858927647237, 0.7376447341913206]]\n",
      "2021-01-14 16:09:32,989 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:09:32,994 : INFO : built Dictionary(739 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 12483 corpus positions)\n",
      "2021-01-14 16:09:33,467 : INFO : token count processed\n",
      "2021-01-14 16:09:33,500 : INFO : frequencies processed\n",
      "2021-01-14 16:09:43,293 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:09:43,294 : INFO : entropies processed\n",
      "2021-01-14 16:09:43,295 : INFO : extropies processed\n",
      "2021-01-14 16:09:43,306 : INFO : token count processed\n",
      "2021-01-14 16:09:43,311 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:09:43,315 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:09:43,316 : INFO : vocab #32006\n",
      "2021-01-14 16:09:43,323 : INFO : diff #set()\n",
      "2021-01-14 16:10:02,820 : INFO : alphabet #32006\n",
      "2021-01-14 16:10:12,664 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.2921617762077393, 0.43626938132370713], [0.9767949506640434, 0.02320505], [2.0, 1.2451124978365313], [3.702819531114783, 7.434393313070278, 7.638571812449493, 3.498641031735568, 3.93575228133471, 0.20417849937921506]]\n",
      "2021-01-14 16:10:12,673 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:10:12,675 : INFO : built Dictionary(488 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 4110 corpus positions)\n",
      "2021-01-14 16:10:12,914 : INFO : token count processed\n",
      "2021-01-14 16:10:12,946 : INFO : frequencies processed\n",
      "2021-01-14 16:10:22,941 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:10:22,942 : INFO : entropies processed\n",
      "2021-01-14 16:10:22,943 : INFO : extropies processed\n",
      "2021-01-14 16:10:22,951 : INFO : token count processed\n",
      "2021-01-14 16:10:22,956 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:10:22,960 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:10:22,960 : INFO : vocab #32006\n",
      "2021-01-14 16:10:22,966 : INFO : diff #set()\n",
      "2021-01-14 16:10:42,901 : INFO : alphabet #32006\n",
      "2021-01-14 16:10:52,976 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.2859440636847046, 0.437456023481215], [0.9679119884967804, 0.03208801], [2.5216406363433186, 1.2998438251349493], [3.702819531114783, 7.2991514951718255, 7.483168654255607, 3.5188023720310007, 3.780349123140824, 0.18401715908378158]]\n",
      "2021-01-14 16:10:52,984 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:10:52,986 : INFO : built Dictionary(457 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 3527 corpus positions)\n",
      "2021-01-14 16:10:53,208 : INFO : token count processed\n",
      "2021-01-14 16:10:53,245 : INFO : frequencies processed\n",
      "2021-01-14 16:11:03,261 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:11:03,262 : INFO : entropies processed\n",
      "2021-01-14 16:11:03,263 : INFO : extropies processed\n",
      "2021-01-14 16:11:03,271 : INFO : token count processed\n",
      "2021-01-14 16:11:03,276 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:11:03,280 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:11:03,281 : INFO : vocab #32006\n",
      "2021-01-14 16:11:03,287 : INFO : diff #set()\n",
      "2021-01-14 16:11:23,483 : INFO : alphabet #32006\n",
      "2021-01-14 16:11:33,292 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.2868222454655538, 0.43728803232645613], [0.9699447490274906, 0.030055251], [1.5, 1.1225562489182657], [3.702819531114783, 7.170319527000998, 7.349498410996274, 3.5236406471195068, 3.646678879881491, 0.17917888399527637]]\n",
      "2021-01-14 16:11:33,301 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:11:33,307 : INFO : built Dictionary(168 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 563 corpus positions)\n",
      "2021-01-14 16:11:33,366 : INFO : token count processed\n",
      "2021-01-14 16:11:33,397 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:11:33,400 : INFO : frequencies processed\n",
      "2021-01-14 16:11:33,402 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:11:33,410 : INFO : token count processed\n",
      "2021-01-14 16:11:33,416 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:11:33,422 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:11:33,425 : INFO : vocab #32006\n",
      "2021-01-14 16:11:33,433 : INFO : diff #set()\n",
      "2021-01-14 16:11:53,591 : INFO : alphabet #32006\n",
      "2021-01-14 16:12:03,985 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.3009926447824662, 0.4345950441291128], [0.985302397981286, 0.014697602], [nan, nan], [3.702819531114783, 6.353654804387375, 6.540320085154361, 3.516154250347798, 2.837500554039578, 0.18666528076698619]]\n",
      "2021-01-14 16:12:03,989 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:12:03,991 : INFO : built Dictionary(161 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 691 corpus positions)\n",
      "2021-01-14 16:12:04,050 : INFO : token count processed\n",
      "2021-01-14 16:12:04,091 : INFO : frequencies processed\n",
      "2021-01-14 16:12:14,033 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:12:14,034 : INFO : entropies processed\n",
      "2021-01-14 16:12:14,035 : INFO : extropies processed\n",
      "2021-01-14 16:12:14,046 : INFO : token count processed\n",
      "2021-01-14 16:12:14,051 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:12:14,055 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:12:14,056 : INFO : vocab #32006\n",
      "2021-01-14 16:12:14,063 : INFO : diff #set()\n",
      "2021-01-14 16:12:34,027 : INFO : alphabet #32006\n",
      "2021-01-14 16:12:44,174 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.29275436715198, 0.4361566220642217], [0.9795759040862322, 0.020424096], [1.0, 1.0], [3.702819531114783, 6.245180322479091, 6.386406971371109, 3.561592882222765, 2.683587440256326, 0.14122664889201797]]\n",
      "2021-01-14 16:12:44,179 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:12:44,182 : INFO : built Dictionary(390 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1763 corpus positions)\n",
      "2021-01-14 16:12:44,365 : INFO : token count processed\n",
      "2021-01-14 16:12:44,398 : INFO : frequencies processed\n",
      "2021-01-14 16:12:54,347 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:12:54,348 : INFO : entropies processed\n",
      "2021-01-14 16:12:54,349 : INFO : extropies processed\n",
      "2021-01-14 16:12:54,357 : INFO : token count processed\n",
      "2021-01-14 16:12:54,362 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:12:54,366 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:12:54,367 : INFO : vocab #32006\n",
      "2021-01-14 16:12:54,373 : INFO : diff #set()\n",
      "2021-01-14 16:13:14,237 : INFO : alphabet #32006\n",
      "2021-01-14 16:13:24,168 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.2953216907388059, 0.4356687796899289], [0.9782143365591764, 0.021785663], [1.0, 1.0], [3.702819531114783, 7.2691387000368, 7.47069616389417, 3.5012620672574126, 3.7678766327793864, 0.2015574638573696]]\n",
      "2021-01-14 16:13:24,173 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 16:13:24,175 : INFO : built Dictionary(316 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1600 corpus positions)\n",
      "2021-01-14 16:13:24,308 : INFO : token count processed\n",
      "2021-01-14 16:13:24,341 : INFO : frequencies processed\n",
      "2021-01-14 16:13:34,377 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:13:34,378 : INFO : entropies processed\n",
      "2021-01-14 16:13:34,379 : INFO : extropies processed\n",
      "2021-01-14 16:13:34,390 : INFO : token count processed\n",
      "2021-01-14 16:13:34,395 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:13:34,402 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:13:34,403 : INFO : vocab #32006\n",
      "2021-01-14 16:13:34,411 : INFO : diff #set()\n",
      "2021-01-14 16:13:54,704 : INFO : alphabet #32006\n",
      "2021-01-14 16:14:04,657 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.2760156552546318, 0.4393642889455977], [0.9594858475029469, 0.040514152], [1.0, 1.0], [3.702819531114783, 7.08857858466988, 7.249781209369658, 3.5416169064150047, 3.546961678254875, 0.16120262469977842]]\n",
      "2021-01-14 16:14:04,661 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:14:04,662 : INFO : built Dictionary(142 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 523 corpus positions)\n",
      "2021-01-14 16:14:04,712 : INFO : token count processed\n",
      "2021-01-14 16:14:04,748 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:14:04,749 : INFO : frequencies processed\n",
      "2021-01-14 16:14:04,750 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:14:04,756 : INFO : token count processed\n",
      "2021-01-14 16:14:04,760 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:14:04,765 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:14:04,766 : INFO : vocab #32006\n",
      "2021-01-14 16:14:04,772 : INFO : diff #set()\n",
      "2021-01-14 16:14:24,650 : INFO : alphabet #32006\n",
      "2021-01-14 16:14:34,633 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.3034061042980516, 0.43413968476251114], [0.9880772288888693, 0.011922771], [nan, nan], [3.702819531114783, 6.0479231618016716, 6.2258276274171065, 3.5249150654993473, 2.5230080963023234, 0.17790446561543494]]\n",
      "2021-01-14 16:14:34,637 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:14:34,638 : INFO : built Dictionary(142 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 642 corpus positions)\n",
      "2021-01-14 16:14:34,688 : INFO : token count processed\n",
      "2021-01-14 16:14:34,721 : INFO : frequencies processed\n",
      "2021-01-14 16:14:44,831 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:14:44,832 : INFO : entropies processed\n",
      "2021-01-14 16:14:44,833 : INFO : extropies processed\n",
      "2021-01-14 16:14:44,844 : INFO : token count processed\n",
      "2021-01-14 16:14:44,849 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:14:44,854 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:14:44,855 : INFO : vocab #32006\n",
      "2021-01-14 16:14:44,861 : INFO : diff #set()\n",
      "2021-01-14 16:15:04,620 : INFO : alphabet #32006\n",
      "2021-01-14 16:15:14,264 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.2926521136294562, 0.4361760748851329], [0.9799927249550819, 0.020007275], [1.0, 1.0], [3.702819531114783, 6.036583168403119, 6.177799435962067, 3.5616032635558366, 2.4749799048472836, 0.14121626755894745]]\n",
      "2021-01-14 16:15:14,278 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:15:14,279 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:15:14,286 : INFO : built Dictionary(556 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 7000 corpus positions)\n",
      "2021-01-14 16:15:14,597 : INFO : token count processed\n",
      "2021-01-14 16:15:14,630 : INFO : frequencies processed\n",
      "2021-01-14 16:15:24,659 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:15:24,660 : INFO : entropies processed\n",
      "2021-01-14 16:15:24,661 : INFO : extropies processed\n",
      "2021-01-14 16:15:24,671 : INFO : token count processed\n",
      "2021-01-14 16:15:24,675 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:15:24,679 : INFO : alphabet_target #32010\n",
      "2021-01-14 16:15:24,680 : INFO : vocab #32006\n",
      "2021-01-14 16:15:24,686 : INFO : diff #set()\n",
      "2021-01-14 16:15:44,767 : INFO : alphabet #32006\n",
      "2021-01-14 16:15:54,760 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.2891313103764819, 0.43684693641953415], [0.9719717036932707, 0.028028296], [1.3709505944546687, 1.0438561897747245], [3.702819531114783, 7.29352035514053, 7.447589627146748, 3.548750259108566, 3.7447700960319645, 0.15406927200621734]]\n",
      "2021-01-14 16:15:54,767 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:15:54,769 : INFO : built Dictionary(366 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 3243 corpus positions)\n",
      "2021-01-14 16:15:54,918 : INFO : token count processed\n",
      "2021-01-14 16:15:54,949 : INFO : frequencies processed\n",
      "2021-01-14 16:16:05,067 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:16:05,068 : INFO : entropies processed\n",
      "2021-01-14 16:16:05,069 : INFO : extropies processed\n",
      "2021-01-14 16:16:05,076 : INFO : token count processed\n",
      "2021-01-14 16:16:05,081 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:16:05,085 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:16:05,086 : INFO : vocab #32006\n",
      "2021-01-14 16:16:05,092 : INFO : diff #set()\n",
      "2021-01-14 16:16:25,070 : INFO : alphabet #32006\n",
      "2021-01-14 16:16:35,206 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.2757153344720165, 0.4394222708140286], [0.9635166153311729, 0.036483385], [1.0, 1.0], [3.702819531114783, 6.8153433747477745, 6.95164838771625, 3.566514518146307, 3.2488288566014667, 0.1363050129684753]]\n",
      "2021-01-14 16:16:35,209 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:16:35,211 : INFO : built Dictionary(119 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 264 corpus positions)\n",
      "2021-01-14 16:16:35,265 : INFO : token count processed\n",
      "2021-01-14 16:16:35,295 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:16:35,298 : INFO : frequencies processed\n",
      "2021-01-14 16:16:35,300 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:16:35,307 : INFO : token count processed\n",
      "2021-01-14 16:16:35,313 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:16:35,320 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:16:35,322 : INFO : vocab #32006\n",
      "2021-01-14 16:16:35,330 : INFO : diff #set()\n",
      "2021-01-14 16:16:55,316 : INFO : alphabet #32006\n",
      "2021-01-14 16:17:05,080 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.301310807689848, 0.4345349601012137], [0.987390922382474, 0.012609078], [nan, nan], [3.702819531114783, 6.150121915859574, 6.334449210997818, 3.51849223597654, 2.6316296798830345, 0.1843272951382433]]\n",
      "2021-01-14 16:17:05,084 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:17:05,086 : INFO : built Dictionary(283 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 975 corpus positions)\n",
      "2021-01-14 16:17:05,196 : INFO : token count processed\n",
      "2021-01-14 16:17:05,235 : INFO : frequencies processed\n",
      "2021-01-14 16:17:15,326 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:17:15,327 : INFO : entropies processed\n",
      "2021-01-14 16:17:15,328 : INFO : extropies processed\n",
      "2021-01-14 16:17:15,337 : INFO : token count processed\n",
      "2021-01-14 16:17:15,342 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:17:15,346 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:17:15,347 : INFO : vocab #32006\n",
      "2021-01-14 16:17:15,353 : INFO : diff #set()\n",
      "2021-01-14 16:17:35,290 : INFO : alphabet #32006\n",
      "2021-01-14 16:17:45,431 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.2901622902246725, 0.43665027769796033], [0.9729006607085466, 0.02709934], [0.0, 0.0], [3.702819531114783, 7.0391145208191315, 7.275903841735405, 3.4660302101985083, 3.5730843106206223, 0.23678932091627392]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 16:17:45,440 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:17:45,441 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:17:45,444 : INFO : built Dictionary(587 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 4350 corpus positions)\n",
      "2021-01-14 16:17:45,769 : INFO : token count processed\n",
      "2021-01-14 16:17:45,814 : INFO : frequencies processed\n",
      "2021-01-14 16:17:55,501 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:17:55,503 : INFO : entropies processed\n",
      "2021-01-14 16:17:55,503 : INFO : extropies processed\n",
      "2021-01-14 16:17:55,512 : INFO : token count processed\n",
      "2021-01-14 16:17:55,517 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:17:55,521 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:17:55,522 : INFO : vocab #32006\n",
      "2021-01-14 16:17:55,528 : INFO : diff #set()\n",
      "2021-01-14 16:18:15,653 : INFO : alphabet #32006\n",
      "2021-01-14 16:18:25,726 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.2900395819844077, 0.4366736749298724], [0.9712462313473225, 0.028753769], [1.0, 1.0], [3.702819531114783, 7.482466367279176, 7.750479143653459, 3.4348067547404995, 4.047659612538676, 0.26801277637428367]]\n",
      "2021-01-14 16:18:25,730 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:18:25,731 : INFO : built Dictionary(168 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 572 corpus positions)\n",
      "2021-01-14 16:18:25,787 : INFO : token count processed\n",
      "2021-01-14 16:18:25,816 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:18:25,817 : INFO : frequencies processed\n",
      "2021-01-14 16:18:25,817 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:18:25,823 : INFO : token count processed\n",
      "2021-01-14 16:18:25,828 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:18:25,832 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:18:25,833 : INFO : vocab #32006\n",
      "2021-01-14 16:18:25,840 : INFO : diff #set()\n",
      "2021-01-14 16:18:45,813 : INFO : alphabet #32006\n",
      "2021-01-14 16:18:55,675 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.3013209083309834, 0.4345330529001463], [0.9855643631890416, 0.014435637], [nan, nan], [3.702819531114783, 6.372162341197667, 6.555172634658407, 3.5198092376540417, 2.8523531035436243, 0.1830102934607405]]\n",
      "2021-01-14 16:18:55,680 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:18:55,682 : INFO : built Dictionary(314 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1993 corpus positions)\n",
      "2021-01-14 16:18:55,814 : INFO : token count processed\n",
      "2021-01-14 16:18:55,850 : INFO : frequencies processed\n",
      "2021-01-14 16:19:06,227 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:19:06,228 : INFO : entropies processed\n",
      "2021-01-14 16:19:06,229 : INFO : extropies processed\n",
      "2021-01-14 16:19:06,238 : INFO : token count processed\n",
      "2021-01-14 16:19:06,243 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:19:06,247 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:19:06,248 : INFO : vocab #32006\n",
      "2021-01-14 16:19:06,254 : INFO : diff #set()\n",
      "2021-01-14 16:19:26,379 : INFO : alphabet #32006\n",
      "2021-01-14 16:19:36,311 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.2940001548788043, 0.4359197613274928], [0.9781975001096725, 0.0218025], [1.584962500721156, 1.1699250014423124], [3.702819531114783, 6.798155919669889, 6.975654096129774, 3.5253213546548983, 3.272834565014991, 0.17749817645988486]]\n",
      "2021-01-14 16:19:36,315 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:19:36,316 : INFO : built Dictionary(172 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 709 corpus positions)\n",
      "2021-01-14 16:19:36,389 : INFO : token count processed\n",
      "2021-01-14 16:19:36,425 : INFO : frequencies processed\n",
      "2021-01-14 16:19:46,177 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:19:46,178 : INFO : entropies processed\n",
      "2021-01-14 16:19:46,179 : INFO : extropies processed\n",
      "2021-01-14 16:19:46,187 : INFO : token count processed\n",
      "2021-01-14 16:19:46,191 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:19:46,195 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:19:46,196 : INFO : vocab #32006\n",
      "2021-01-14 16:19:46,203 : INFO : diff #set()\n",
      "2021-01-14 16:20:06,409 : INFO : alphabet #32006\n",
      "2021-01-14 16:20:16,049 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.302978054733097, 0.4342203773695511], [0.9881268758326769, 0.011873124], [0.0, 0.0], [3.702819531114783, 6.271631856729336, 6.444231250389531, 3.530220137454589, 2.7414117192747476, 0.17259939366019506]]\n",
      "2021-01-14 16:20:16,056 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:20:16,058 : INFO : built Dictionary(350 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 3224 corpus positions)\n",
      "2021-01-14 16:20:16,217 : INFO : token count processed\n",
      "2021-01-14 16:20:16,251 : INFO : frequencies processed\n",
      "2021-01-14 16:20:26,197 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:20:26,198 : INFO : entropies processed\n",
      "2021-01-14 16:20:26,199 : INFO : extropies processed\n",
      "2021-01-14 16:20:26,210 : INFO : token count processed\n",
      "2021-01-14 16:20:26,215 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:20:26,219 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:20:26,220 : INFO : vocab #32006\n",
      "2021-01-14 16:20:26,226 : INFO : diff #set()\n",
      "2021-01-14 16:20:45,951 : INFO : alphabet #32006\n",
      "2021-01-14 16:20:56,005 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.3000727093257927, 0.4347688644560825], [0.9849595008417964, 0.015040499], [0.0, 0.0], [3.702819531114783, 6.873598627629562, 7.02393080099675, 3.552487357747596, 3.3211112698819667, 0.15033217336718785]]\n",
      "2021-01-14 16:20:56,008 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:20:56,010 : INFO : built Dictionary(100 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 172 corpus positions)\n",
      "2021-01-14 16:20:56,061 : INFO : token count processed\n",
      "2021-01-14 16:20:56,092 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:20:56,094 : INFO : frequencies processed\n",
      "2021-01-14 16:20:56,096 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:20:56,103 : INFO : token count processed\n",
      "2021-01-14 16:20:56,110 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:20:56,116 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:20:56,118 : INFO : vocab #32006\n",
      "2021-01-14 16:20:56,126 : INFO : diff #set()\n",
      "2021-01-14 16:21:16,219 : INFO : alphabet #32006\n",
      "2021-01-14 16:21:26,178 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.2948790929187244, 0.4357528041828808], [0.9822654705494642, 0.01773453], [nan, nan], [3.702819531114783, 6.049830202851529, 6.326030292536711, 3.426619441429601, 2.6232107614219276, 0.2762000896851813]]\n",
      "2021-01-14 16:21:26,182 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:21:26,184 : INFO : built Dictionary(248 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1252 corpus positions)\n",
      "2021-01-14 16:21:26,277 : INFO : token count processed\n",
      "2021-01-14 16:21:26,306 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:21:26,307 : INFO : frequencies processed\n",
      "2021-01-14 16:21:26,308 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:21:26,314 : INFO : token count processed\n",
      "2021-01-14 16:21:26,319 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:21:26,323 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:21:26,324 : INFO : vocab #32006\n",
      "2021-01-14 16:21:26,331 : INFO : diff #set()\n",
      "2021-01-14 16:21:46,267 : INFO : alphabet #32006\n",
      "2021-01-14 16:21:56,319 : INFO : Computed distances or similarities ('280', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.3011876264120674, 0.43455822051292947], [0.9888832103461027, 0.01111679], [nan, nan], [3.702819531114783, 6.778844940588858, 6.968969419232041, 3.512695052471601, 3.2661498881172575, 0.19012447864318283]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 16:21:56,322 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:21:56,323 : INFO : built Dictionary(147 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 285 corpus positions)\n",
      "2021-01-14 16:21:56,383 : INFO : token count processed\n",
      "2021-01-14 16:21:56,413 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:21:56,416 : INFO : frequencies processed\n",
      "2021-01-14 16:21:56,418 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:21:56,425 : INFO : token count processed\n",
      "2021-01-14 16:21:56,431 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:21:56,437 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:21:56,440 : INFO : vocab #32006\n",
      "2021-01-14 16:21:56,448 : INFO : diff #set()\n",
      "2021-01-14 16:22:16,427 : INFO : alphabet #32006\n",
      "2021-01-14 16:22:26,280 : INFO : Computed distances or similarities ('280', 'sacp-python-common/setup.py')[[1.3021819221789186, 0.4343705379519017], [0.9867130909115076, 0.013286909], [nan, nan], [3.702819531114783, 6.469677430851302, 6.744412442867361, 3.428084519098725, 3.041592911752578, 0.274735012016059]]\n",
      "2021-01-14 16:22:26,285 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:22:26,286 : INFO : built Dictionary(216 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1187 corpus positions)\n",
      "2021-01-14 16:22:26,363 : INFO : token count processed\n",
      "2021-01-14 16:22:26,402 : INFO : frequencies processed\n",
      "2021-01-14 16:22:36,433 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:22:36,434 : INFO : entropies processed\n",
      "2021-01-14 16:22:36,435 : INFO : extropies processed\n",
      "2021-01-14 16:22:36,442 : INFO : token count processed\n",
      "2021-01-14 16:22:36,447 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:22:36,451 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:22:36,452 : INFO : vocab #32006\n",
      "2021-01-14 16:22:36,459 : INFO : diff #set()\n",
      "2021-01-14 16:22:56,490 : INFO : alphabet #32006\n",
      "2021-01-14 16:23:06,567 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.2945746367210993, 0.43581062215042166], [0.975750146433711, 0.024249854], [0.0, 0.0], [3.702819531114783, 6.459180448028249, 6.622003563963564, 3.539996415179469, 2.919184032848781, 0.16282311593531507]]\n",
      "2021-01-14 16:23:06,571 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:23:06,572 : INFO : built Dictionary(127 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 337 corpus positions)\n",
      "2021-01-14 16:23:06,613 : INFO : token count processed\n",
      "2021-01-14 16:23:06,642 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:23:06,643 : INFO : frequencies processed\n",
      "2021-01-14 16:23:06,644 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:23:06,650 : INFO : token count processed\n",
      "2021-01-14 16:23:06,655 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:23:06,659 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:23:06,661 : INFO : vocab #32006\n",
      "2021-01-14 16:23:06,668 : INFO : diff #set()\n",
      "2021-01-14 16:23:26,247 : INFO : alphabet #32006\n",
      "2021-01-14 16:23:36,360 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.2949691376600987, 0.4357357071126362], [0.9767155554145575, 0.023284445], [nan, nan], [3.702819531114783, 6.097125733496388, 6.297408959868459, 3.502536304742712, 2.594589428753676, 0.20028322637207108]]\n",
      "2021-01-14 16:23:36,364 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:23:36,365 : INFO : built Dictionary(119 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 333 corpus positions)\n",
      "2021-01-14 16:23:36,402 : INFO : token count processed\n",
      "2021-01-14 16:23:36,430 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:23:36,431 : INFO : frequencies processed\n",
      "2021-01-14 16:23:36,432 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:23:36,438 : INFO : token count processed\n",
      "2021-01-14 16:23:36,442 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:23:36,447 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:23:36,448 : INFO : vocab #32006\n",
      "2021-01-14 16:23:36,455 : INFO : diff #set()\n",
      "2021-01-14 16:23:56,619 : INFO : alphabet #32006\n",
      "2021-01-14 16:24:06,579 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.297952691786717, 0.4351699682827127], [0.9792404193431139, 0.02075958], [nan, nan], [3.702819531114783, 6.0695858597523715, 6.26434012616813, 3.5080652646990247, 2.561520595053347, 0.19475426641575844]]\n",
      "2021-01-14 16:24:06,583 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:24:06,584 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:24:06,586 : INFO : built Dictionary(121 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 429 corpus positions)\n",
      "2021-01-14 16:24:06,624 : INFO : token count processed\n",
      "2021-01-14 16:24:06,654 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:24:06,655 : INFO : frequencies processed\n",
      "2021-01-14 16:24:06,656 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:24:06,661 : INFO : token count processed\n",
      "2021-01-14 16:24:06,666 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:24:06,671 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:24:06,672 : INFO : vocab #32006\n",
      "2021-01-14 16:24:06,678 : INFO : diff #set()\n",
      "2021-01-14 16:24:26,379 : INFO : alphabet #32006\n",
      "2021-01-14 16:24:36,263 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.2983715410773664, 0.43509066403217295], [0.9776105564087629, 0.022389444], [nan, nan], [3.702819531114783, 6.104787343210121, 6.261460170639433, 3.546146703685471, 2.5586406395246497, 0.1566728274293121]]\n",
      "2021-01-14 16:24:36,280 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:24:36,281 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:24:36,288 : INFO : built Dictionary(408 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 9114 corpus positions)\n",
      "2021-01-14 16:24:36,474 : INFO : token count processed\n",
      "2021-01-14 16:24:36,510 : INFO : frequencies processed\n",
      "2021-01-14 16:24:46,527 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:24:46,528 : INFO : entropies processed\n",
      "2021-01-14 16:24:46,529 : INFO : extropies processed\n",
      "2021-01-14 16:24:46,544 : INFO : token count processed\n",
      "2021-01-14 16:24:46,548 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:24:46,554 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:24:46,555 : INFO : vocab #32006\n",
      "2021-01-14 16:24:46,564 : INFO : diff #set()\n",
      "2021-01-14 16:25:06,525 : INFO : alphabet #32006\n",
      "2021-01-14 16:25:16,218 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.297835682509108, 0.43519212779743066], [0.9787233043462038, 0.021276696], [0.0, 0.0], [3.702819531114783, 6.89087415148015, 7.046716464345698, 3.5469772182492356, 3.3438969332309147, 0.1558423128655475]]\n",
      "2021-01-14 16:25:16,224 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:25:16,226 : INFO : built Dictionary(266 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 2274 corpus positions)\n",
      "2021-01-14 16:25:16,322 : INFO : token count processed\n",
      "2021-01-14 16:25:16,379 : INFO : frequencies processed\n",
      "2021-01-14 16:25:26,495 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:25:26,496 : INFO : entropies processed\n",
      "2021-01-14 16:25:26,497 : INFO : extropies processed\n",
      "2021-01-14 16:25:26,506 : INFO : token count processed\n",
      "2021-01-14 16:25:26,511 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:25:26,515 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:25:26,516 : INFO : vocab #32006\n",
      "2021-01-14 16:25:26,523 : INFO : diff #set()\n",
      "2021-01-14 16:25:46,252 : INFO : alphabet #32006\n",
      "2021-01-14 16:25:56,277 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.2706803512552443, 0.440396641230103], [0.9498883113265038, 0.05011169], [0.0, 0.0], [3.702819531114783, 6.655493573668506, 6.789391969765691, 3.5689211350175976, 3.086572438650908, 0.13389839609718468]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 16:25:56,282 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:25:56,283 : INFO : built Dictionary(243 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1281 corpus positions)\n",
      "2021-01-14 16:25:56,386 : INFO : token count processed\n",
      "2021-01-14 16:25:56,449 : INFO : frequencies processed\n",
      "2021-01-14 16:26:06,475 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:26:06,476 : INFO : entropies processed\n",
      "2021-01-14 16:26:06,477 : INFO : extropies processed\n",
      "2021-01-14 16:26:06,488 : INFO : token count processed\n",
      "2021-01-14 16:26:06,493 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:26:06,497 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:26:06,498 : INFO : vocab #32006\n",
      "2021-01-14 16:26:06,504 : INFO : diff #set()\n",
      "2021-01-14 16:26:26,679 : INFO : alphabet #32006\n",
      "2021-01-14 16:26:36,861 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.2893021108997933, 0.4368143440915089], [0.9722213670611382, 0.027778633], [0.0, 0.0], [3.702819531114783, 6.6236746347295465, 6.7415490869126, 3.5849450789317308, 3.0387295557978167, 0.11787445218305326]]\n",
      "2021-01-14 16:26:36,866 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:26:36,868 : INFO : built Dictionary(244 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1220 corpus positions)\n",
      "2021-01-14 16:26:36,978 : INFO : token count processed\n",
      "2021-01-14 16:26:37,062 : INFO : frequencies processed\n",
      "2021-01-14 16:26:46,984 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:26:46,985 : INFO : entropies processed\n",
      "2021-01-14 16:26:46,986 : INFO : extropies processed\n",
      "2021-01-14 16:26:46,994 : INFO : token count processed\n",
      "2021-01-14 16:26:46,998 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:26:47,002 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:26:47,003 : INFO : vocab #32006\n",
      "2021-01-14 16:26:47,009 : INFO : diff #set()\n",
      "2021-01-14 16:27:07,051 : INFO : alphabet #32006\n",
      "2021-01-14 16:27:17,075 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.285621120300023, 0.4375178331694513], [0.9625446982681751, 0.0374553], [0.0, 0.0], [3.702819531114783, 6.75472436518627, 6.87009590589734, 3.5874479904037138, 3.167276374782557, 0.11537154071107025]]\n",
      "2021-01-14 16:27:17,079 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:27:17,081 : INFO : built Dictionary(196 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1015 corpus positions)\n",
      "2021-01-14 16:27:17,153 : INFO : token count processed\n",
      "2021-01-14 16:27:17,184 : INFO : frequencies processed\n",
      "2021-01-14 16:27:27,162 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:27:27,163 : INFO : entropies processed\n",
      "2021-01-14 16:27:27,164 : INFO : extropies processed\n",
      "2021-01-14 16:27:27,174 : INFO : token count processed\n",
      "2021-01-14 16:27:27,178 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:27:27,183 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:27:27,183 : INFO : vocab #32006\n",
      "2021-01-14 16:27:27,190 : INFO : diff #set()\n",
      "2021-01-14 16:27:47,013 : INFO : alphabet #32006\n",
      "2021-01-14 16:27:57,145 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.2964136112856681, 0.4354616237621675], [0.9731836747378111, 0.026816325], [0.0, 0.0], [3.702819531114783, 6.597313085495733, 6.707485221370902, 3.592647395239613, 3.004665690256119, 0.11017213587516927]]\n",
      "2021-01-14 16:27:57,150 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:27:57,151 : INFO : built Dictionary(220 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 981 corpus positions)\n",
      "2021-01-14 16:27:57,230 : INFO : token count processed\n",
      "2021-01-14 16:27:57,262 : INFO : frequencies processed\n",
      "2021-01-14 16:28:07,236 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:28:07,237 : INFO : entropies processed\n",
      "2021-01-14 16:28:07,238 : INFO : extropies processed\n",
      "2021-01-14 16:28:07,245 : INFO : token count processed\n",
      "2021-01-14 16:28:07,250 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:28:07,254 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:28:07,255 : INFO : vocab #32006\n",
      "2021-01-14 16:28:07,262 : INFO : diff #set()\n",
      "2021-01-14 16:28:27,511 : INFO : alphabet #32006\n",
      "2021-01-14 16:28:37,369 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.285527458564926, 0.4375357628071974], [0.9640562497079372, 0.03594375], [1.0, 1.0], [3.702819531114783, 6.659481538516613, 6.7841129287497886, 3.5781881408816076, 3.0812933976350054, 0.12463139023317549]]\n",
      "2021-01-14 16:28:37,373 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:28:37,375 : INFO : built Dictionary(246 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1143 corpus positions)\n",
      "2021-01-14 16:28:37,467 : INFO : token count processed\n",
      "2021-01-14 16:28:37,500 : INFO : frequencies processed\n",
      "2021-01-14 16:28:47,416 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:28:47,417 : INFO : entropies processed\n",
      "2021-01-14 16:28:47,420 : INFO : extropies processed\n",
      "2021-01-14 16:28:47,428 : INFO : token count processed\n",
      "2021-01-14 16:28:47,432 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:28:47,439 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:28:47,439 : INFO : vocab #32006\n",
      "2021-01-14 16:28:47,448 : INFO : diff #set()\n",
      "2021-01-14 16:29:07,400 : INFO : alphabet #32006\n",
      "2021-01-14 16:29:17,571 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.2915622279092804, 0.4363835237903862], [0.9716237187385559, 0.028376281], [0.0, 0.0], [3.702819531114783, 6.774682571479102, 6.874877564852289, 3.6026245377415957, 3.172058033737506, 0.10019499337318738]]\n",
      "2021-01-14 16:29:17,585 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:29:17,588 : INFO : built Dictionary(423 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 7880 corpus positions)\n",
      "2021-01-14 16:29:17,792 : INFO : token count processed\n",
      "2021-01-14 16:29:17,825 : INFO : frequencies processed\n",
      "2021-01-14 16:29:27,937 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:29:27,938 : INFO : entropies processed\n",
      "2021-01-14 16:29:27,939 : INFO : extropies processed\n",
      "2021-01-14 16:29:27,949 : INFO : token count processed\n",
      "2021-01-14 16:29:27,953 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:29:27,958 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:29:27,959 : INFO : vocab #32006\n",
      "2021-01-14 16:29:27,965 : INFO : diff #set()\n",
      "2021-01-14 16:29:47,644 : INFO : alphabet #32006\n",
      "2021-01-14 16:29:57,673 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.299862992486694, 0.4348085095794182], [0.9825085792690516, 0.01749142], [1.0, 1.0], [3.702819531114783, 6.839453716525233, 6.986284554001522, 3.5559886936384926, 3.2834650228867392, 0.14683083747628967]]\n",
      "2021-01-14 16:29:57,679 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:29:57,682 : INFO : built Dictionary(322 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 2322 corpus positions)\n",
      "2021-01-14 16:29:57,826 : INFO : token count processed\n",
      "2021-01-14 16:29:57,861 : INFO : frequencies processed\n",
      "2021-01-14 16:30:07,551 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:30:07,552 : INFO : entropies processed\n",
      "2021-01-14 16:30:07,553 : INFO : extropies processed\n",
      "2021-01-14 16:30:07,561 : INFO : token count processed\n",
      "2021-01-14 16:30:07,565 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:30:07,569 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:30:07,570 : INFO : vocab #32006\n",
      "2021-01-14 16:30:07,576 : INFO : diff #set()\n",
      "2021-01-14 16:30:27,027 : INFO : alphabet #32006\n",
      "2021-01-14 16:30:36,808 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.2655267908199834, 0.4413984438639371], [0.9416049271821976, 0.058395073], [1.584962500721156, 1.1699250014423124], [3.702819531114783, 6.86432793886027, 6.971083128481119, 3.5960643414939337, 3.268263597366336, 0.1067551896208494]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 16:30:36,812 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:30:36,813 : INFO : built Dictionary(157 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 524 corpus positions)\n",
      "2021-01-14 16:30:36,878 : INFO : token count processed\n",
      "2021-01-14 16:30:36,915 : INFO : frequencies processed\n",
      "2021-01-14 16:30:46,804 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:30:46,805 : INFO : entropies processed\n",
      "2021-01-14 16:30:46,806 : INFO : extropies processed\n",
      "2021-01-14 16:30:46,816 : INFO : token count processed\n",
      "2021-01-14 16:30:46,821 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:30:46,826 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:30:46,826 : INFO : vocab #32006\n",
      "2021-01-14 16:30:46,833 : INFO : diff #set()\n",
      "2021-01-14 16:31:06,952 : INFO : alphabet #32006\n",
      "2021-01-14 16:31:17,079 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.286649425776692, 0.4373210815472233], [0.9664356298744678, 0.03356437], [0.0, 0.0], [3.702819531114783, 6.431978396403875, 6.567340499835954, 3.5674574276827045, 2.8645209687211706, 0.13536210343207866]]\n",
      "2021-01-14 16:31:17,083 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:31:17,085 : INFO : built Dictionary(216 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 678 corpus positions)\n",
      "2021-01-14 16:31:17,159 : INFO : token count processed\n",
      "2021-01-14 16:31:17,193 : INFO : frequencies processed\n",
      "2021-01-14 16:31:27,152 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:31:27,153 : INFO : entropies processed\n",
      "2021-01-14 16:31:27,154 : INFO : extropies processed\n",
      "2021-01-14 16:31:27,165 : INFO : token count processed\n",
      "2021-01-14 16:31:27,170 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:31:27,175 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:31:27,175 : INFO : vocab #32006\n",
      "2021-01-14 16:31:27,182 : INFO : diff #set()\n",
      "2021-01-14 16:31:47,351 : INFO : alphabet #32006\n",
      "2021-01-14 16:31:57,142 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/test_auth_utility.py')[[1.2965764736594552, 0.435430742877271], [0.9748559333384037, 0.025144067], [0.0, 0.0], [3.702819531114783, 6.911818353685893, 7.0350058350110665, 3.579632049789609, 3.3321863038962833, 0.12318748132517321]]\n",
      "2021-01-14 16:31:57,156 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:31:57,159 : INFO : built Dictionary(308 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 7208 corpus positions)\n",
      "2021-01-14 16:31:57,289 : INFO : token count processed\n",
      "2021-01-14 16:31:57,318 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:31:57,319 : INFO : frequencies processed\n",
      "2021-01-14 16:31:57,319 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:31:57,328 : INFO : token count processed\n",
      "2021-01-14 16:31:57,332 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:31:57,337 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:31:57,338 : INFO : vocab #32006\n",
      "2021-01-14 16:31:57,344 : INFO : diff #set()\n",
      "2021-01-14 16:32:17,384 : INFO : alphabet #32006\n",
      "2021-01-14 16:32:27,483 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.3077290675620146, 0.4333264307566414], [0.9902041992172599, 0.009795801], [nan, nan], [3.702819531114783, 6.363791471162389, 6.396761051499016, 3.669849950778156, 2.693941520384233, 0.032969580336627224]]\n",
      "2021-01-14 16:32:27,488 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:32:27,490 : INFO : built Dictionary(208 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1278 corpus positions)\n",
      "2021-01-14 16:32:27,563 : INFO : token count processed\n",
      "2021-01-14 16:32:27,592 : INFO : frequencies processed\n",
      "2021-01-14 16:32:37,590 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:32:37,592 : INFO : entropies processed\n",
      "2021-01-14 16:32:37,592 : INFO : extropies processed\n",
      "2021-01-14 16:32:37,603 : INFO : token count processed\n",
      "2021-01-14 16:32:37,608 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:32:37,612 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:32:37,613 : INFO : vocab #32006\n",
      "2021-01-14 16:32:37,620 : INFO : diff #set()\n",
      "2021-01-14 16:32:57,487 : INFO : alphabet #32006\n",
      "2021-01-14 16:33:07,455 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.3007904503291015, 0.43463323652832514], [0.9838926587253809, 0.016107341], [0.0, 0.0], [3.702819531114783, 6.29000629755059, 6.497472783991762, 3.495353044673612, 2.794653252876979, 0.20746648644117194]]\n",
      "2021-01-14 16:33:07,459 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:33:07,461 : INFO : built Dictionary(218 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1231 corpus positions)\n",
      "2021-01-14 16:33:07,545 : INFO : token count processed\n",
      "2021-01-14 16:33:07,579 : INFO : frequencies processed\n",
      "2021-01-14 16:33:17,606 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:33:17,607 : INFO : entropies processed\n",
      "2021-01-14 16:33:17,608 : INFO : extropies processed\n",
      "2021-01-14 16:33:17,616 : INFO : token count processed\n",
      "2021-01-14 16:33:17,620 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:33:17,625 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:33:17,625 : INFO : vocab #32006\n",
      "2021-01-14 16:33:17,632 : INFO : diff #set()\n",
      "2021-01-14 16:33:37,739 : INFO : alphabet #32006\n",
      "2021-01-14 16:33:47,665 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.2964254613874668, 0.43545937667657386], [0.9778923485428095, 0.022107651], [0.0, 0.0], [3.702819531114783, 6.361621244785958, 6.554409208571975, 3.5100315673287668, 2.8515896774571923, 0.19278796378601726]]\n",
      "2021-01-14 16:33:47,671 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:33:47,673 : INFO : built Dictionary(227 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1254 corpus positions)\n",
      "2021-01-14 16:33:47,773 : INFO : token count processed\n",
      "2021-01-14 16:33:47,856 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:33:47,857 : INFO : frequencies processed\n",
      "2021-01-14 16:33:47,858 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:33:47,864 : INFO : token count processed\n",
      "2021-01-14 16:33:47,868 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:33:47,873 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:33:47,873 : INFO : vocab #32006\n",
      "2021-01-14 16:33:47,881 : INFO : diff #set()\n",
      "2021-01-14 16:34:08,080 : INFO : alphabet #32006\n",
      "2021-01-14 16:34:18,512 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.2940647079106689, 0.4359074949157625], [0.9707352798432112, 0.02926472], [nan, nan], [3.702819531114783, 6.620594433343389, 6.823645484374935, 3.4997684800832376, 3.1208259532601517, 0.20305105103154553]]\n",
      "2021-01-14 16:34:18,516 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:34:18,518 : INFO : built Dictionary(199 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1347 corpus positions)\n",
      "2021-01-14 16:34:18,599 : INFO : token count processed\n",
      "2021-01-14 16:34:18,651 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:34:18,661 : INFO : frequencies processed\n",
      "2021-01-14 16:34:18,662 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:34:18,669 : INFO : token count processed\n",
      "2021-01-14 16:34:18,674 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:34:18,680 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:34:18,681 : INFO : vocab #32006\n",
      "2021-01-14 16:34:18,689 : INFO : diff #set()\n",
      "2021-01-14 16:34:38,666 : INFO : alphabet #32006\n",
      "2021-01-14 16:34:48,782 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.2926767215043835, 0.43617139329780036], [0.9671789258718491, 0.032821074], [nan, nan], [3.702819531114783, 6.207411496248084, 6.329282150850416, 3.5809488765124513, 2.6264626197356327, 0.12187065460233182]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 16:34:48,785 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:34:48,787 : INFO : built Dictionary(139 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 251 corpus positions)\n",
      "2021-01-14 16:34:48,854 : INFO : token count processed\n",
      "2021-01-14 16:34:48,934 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:34:48,935 : INFO : frequencies processed\n",
      "2021-01-14 16:34:48,937 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:34:48,944 : INFO : token count processed\n",
      "2021-01-14 16:34:48,948 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:34:48,953 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:34:48,954 : INFO : vocab #32006\n",
      "2021-01-14 16:34:48,962 : INFO : diff #set()\n",
      "2021-01-14 16:35:08,786 : INFO : alphabet #32006\n",
      "2021-01-14 16:35:19,031 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.2941926012988427, 0.4358831945643345], [0.9725131057202816, 0.027486894], [nan, nan], [3.702819531114783, 6.5805228788529595, 6.764212138620637, 3.5191302713471053, 3.061392607505854, 0.1836892597676778]]\n",
      "2021-01-14 16:35:19,036 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:35:19,037 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:35:19,038 : INFO : built Dictionary(222 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1212 corpus positions)\n",
      "2021-01-14 16:35:19,122 : INFO : token count processed\n",
      "2021-01-14 16:35:19,150 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:35:19,150 : INFO : frequencies processed\n",
      "2021-01-14 16:35:19,151 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:35:19,157 : INFO : token count processed\n",
      "2021-01-14 16:35:19,162 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:35:19,166 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:35:19,167 : INFO : vocab #32006\n",
      "2021-01-14 16:35:19,174 : INFO : diff #set()\n",
      "2021-01-14 16:35:38,970 : INFO : alphabet #32006\n",
      "2021-01-14 16:35:49,026 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.3043935035371699, 0.4339536621957284], [0.9879056643694639, 0.012094336], [nan, nan], [3.702819531114783, 6.422089779976135, 6.51548318387132, 3.6094261272195975, 2.8126636527565365, 0.09339340389518469]]\n",
      "2021-01-14 16:35:49,031 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:35:49,033 : INFO : built Dictionary(234 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1494 corpus positions)\n",
      "2021-01-14 16:35:49,113 : INFO : token count processed\n",
      "2021-01-14 16:35:49,141 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:35:49,142 : INFO : frequencies processed\n",
      "2021-01-14 16:35:49,142 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:35:49,149 : INFO : token count processed\n",
      "2021-01-14 16:35:49,153 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:35:49,158 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:35:49,159 : INFO : vocab #32006\n",
      "2021-01-14 16:35:49,165 : INFO : diff #set()\n",
      "2021-01-14 16:36:08,762 : INFO : alphabet #32006\n",
      "2021-01-14 16:36:18,710 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.2981358482106902, 0.4351352861836221], [0.9780634008347988, 0.0219366], [nan, nan], [3.702819531114783, 6.485445644653597, 6.716620484697547, 3.471644691070832, 3.0138009535827637, 0.23117484004395017]]\n",
      "2021-01-14 16:36:18,715 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:36:18,716 : INFO : built Dictionary(209 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1380 corpus positions)\n",
      "2021-01-14 16:36:18,790 : INFO : token count processed\n",
      "2021-01-14 16:36:18,820 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:36:18,821 : INFO : frequencies processed\n",
      "2021-01-14 16:36:18,822 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:36:18,828 : INFO : token count processed\n",
      "2021-01-14 16:36:18,832 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:36:18,837 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:36:18,838 : INFO : vocab #32006\n",
      "2021-01-14 16:36:18,845 : INFO : diff #set()\n",
      "2021-01-14 16:36:38,953 : INFO : alphabet #32006\n",
      "2021-01-14 16:36:49,056 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.292853501898742, 0.4361377642190776], [0.9677232913672924, 0.03227671], [nan, nan], [3.702819531114783, 6.2276600107346916, 6.346859778854036, 3.58361976299544, 2.6440402477392526, 0.11919976811934418]]\n",
      "2021-01-14 16:36:49,061 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:36:49,063 : INFO : built Dictionary(197 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1578 corpus positions)\n",
      "2021-01-14 16:36:49,135 : INFO : token count processed\n",
      "2021-01-14 16:36:49,168 : INFO : frequencies processed\n",
      "2021-01-14 16:36:59,079 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:36:59,080 : INFO : entropies processed\n",
      "2021-01-14 16:36:59,081 : INFO : extropies processed\n",
      "2021-01-14 16:36:59,088 : INFO : token count processed\n",
      "2021-01-14 16:36:59,093 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:36:59,098 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:36:59,099 : INFO : vocab #32006\n",
      "2021-01-14 16:36:59,105 : INFO : diff #set()\n",
      "2021-01-14 16:37:19,115 : INFO : alphabet #32006\n",
      "2021-01-14 16:37:29,062 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.293827158547594, 0.4359526376142395], [0.9763240218162537, 0.023675978], [1.0, 1.0], [3.702819531114783, 6.253918170574241, 6.418661874352779, 3.538075827336245, 2.7158423432379957, 0.1647437037785382]]\n",
      "2021-01-14 16:37:29,065 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:37:29,067 : INFO : built Dictionary(171 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 675 corpus positions)\n",
      "2021-01-14 16:37:29,124 : INFO : token count processed\n",
      "2021-01-14 16:37:29,153 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:37:29,154 : INFO : frequencies processed\n",
      "2021-01-14 16:37:29,154 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:37:29,160 : INFO : token count processed\n",
      "2021-01-14 16:37:29,165 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:37:29,170 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:37:29,171 : INFO : vocab #32006\n",
      "2021-01-14 16:37:29,177 : INFO : diff #set()\n",
      "2021-01-14 16:37:49,400 : INFO : alphabet #32006\n",
      "2021-01-14 16:37:59,279 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.2901894820028827, 0.436645093280863], [0.966740645468235, 0.033259355], [nan, nan], [3.702819531114783, 6.374522245625576, 6.56665504635553, 3.510686730384828, 2.863835515240747, 0.1921328007299543]]\n",
      "2021-01-14 16:37:59,285 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:37:59,287 : INFO : built Dictionary(293 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1957 corpus positions)\n",
      "2021-01-14 16:37:59,403 : INFO : token count processed\n",
      "2021-01-14 16:37:59,430 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:37:59,431 : INFO : frequencies processed\n",
      "2021-01-14 16:37:59,432 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:37:59,438 : INFO : token count processed\n",
      "2021-01-14 16:37:59,443 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:37:59,447 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:37:59,448 : INFO : vocab #32006\n",
      "2021-01-14 16:37:59,454 : INFO : diff #set()\n",
      "2021-01-14 16:38:19,502 : INFO : alphabet #32006\n",
      "2021-01-14 16:38:29,501 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.3005901223866474, 0.43467108298395785], [0.9820645991712809, 0.0179354], [nan, nan], [3.702819531114783, 6.731238669067808, 6.988654444610897, 3.445403755571694, 3.285834913496114, 0.25741577554308925]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 16:38:29,506 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:38:29,507 : INFO : built Dictionary(218 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1349 corpus positions)\n",
      "2021-01-14 16:38:29,607 : INFO : token count processed\n",
      "2021-01-14 16:38:29,639 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:38:29,640 : INFO : frequencies processed\n",
      "2021-01-14 16:38:29,641 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:38:29,647 : INFO : token count processed\n",
      "2021-01-14 16:38:29,652 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:38:29,658 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:38:29,661 : INFO : vocab #32006\n",
      "2021-01-14 16:38:29,668 : INFO : diff #set()\n",
      "2021-01-14 16:38:49,755 : INFO : alphabet #32006\n",
      "2021-01-14 16:38:59,474 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.3038162400748903, 0.43406239725417206], [0.9853604640811682, 0.014639536], [nan, nan], [3.702819531114783, 6.503741451859337, 6.681924732305625, 3.5246362506684967, 2.9791052011908414, 0.17818328044628728]]\n",
      "2021-01-14 16:38:59,479 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:38:59,481 : INFO : built Dictionary(239 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 1671 corpus positions)\n",
      "2021-01-14 16:38:59,569 : INFO : token count processed\n",
      "2021-01-14 16:38:59,605 : INFO : frequencies processed\n",
      "2021-01-14 16:39:09,542 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:39:09,543 : INFO : entropies processed\n",
      "2021-01-14 16:39:09,544 : INFO : extropies processed\n",
      "2021-01-14 16:39:09,551 : INFO : token count processed\n",
      "2021-01-14 16:39:09,556 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:39:09,560 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:39:09,561 : INFO : vocab #32006\n",
      "2021-01-14 16:39:09,567 : INFO : diff #set()\n",
      "2021-01-14 16:39:29,719 : INFO : alphabet #32006\n",
      "2021-01-14 16:39:39,693 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.3010234204515996, 0.4345892315184431], [0.9814156834036112, 0.018584317], [0.0, 0.0], [3.702819531114783, 6.334729224484471, 6.454053724855596, 3.5834950307436575, 2.751234193740813, 0.11932450037112474]]\n",
      "2021-01-14 16:39:39,698 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:39:39,700 : INFO : built Dictionary(237 unique tokens: ['dir', 'lp', 'missions', '▁*', '▁assistant']...) from 2 documents (total 2020 corpus positions)\n",
      "2021-01-14 16:39:39,791 : INFO : token count processed\n",
      "2021-01-14 16:39:39,820 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 16:39:39,821 : INFO : frequencies processed\n",
      "2021-01-14 16:39:39,822 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 16:39:39,828 : INFO : token count processed\n",
      "2021-01-14 16:39:39,832 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:39:39,837 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:39:39,838 : INFO : vocab #32006\n",
      "2021-01-14 16:39:39,845 : INFO : diff #set()\n",
      "2021-01-14 16:39:59,777 : INFO : alphabet #32006\n",
      "2021-01-14 16:40:09,692 : INFO : Computed distances or similarities ('280', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.301782849679978, 0.4344458471132636], [0.9844312779605389, 0.015568722], [nan, nan], [3.702819531114783, 6.21319712067992, 6.398428228454732, 3.517588423339971, 2.6956086973399485, 0.18523110777481122]]\n",
      "2021-01-14 16:40:09,697 : INFO : Removed 1 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:40:09,698 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:40:09,703 : INFO : built Dictionary(297 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1480 corpus positions)\n",
      "2021-01-14 16:40:10,106 : INFO : token count processed\n",
      "2021-01-14 16:40:10,147 : INFO : frequencies processed\n",
      "2021-01-14 16:40:20,150 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:40:20,151 : INFO : entropies processed\n",
      "2021-01-14 16:40:20,152 : INFO : extropies processed\n",
      "2021-01-14 16:40:20,160 : INFO : token count processed\n",
      "2021-01-14 16:40:20,164 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:40:20,168 : INFO : alphabet_target #32010\n",
      "2021-01-14 16:40:20,169 : INFO : vocab #32006\n",
      "2021-01-14 16:40:20,175 : INFO : diff #set()\n",
      "2021-01-14 16:40:39,960 : INFO : alphabet #32006\n",
      "2021-01-14 16:40:49,912 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.0749175114698495, 0.48194686992236646], [0.6696670651435852, 0.33033293], [3.7582890779432674, 1.3486438809564474], [5.476474452049073, 6.905617163738059, 7.133548119180386, 5.248543496606747, 1.6570736671313124, 0.22793095544232678]]\n",
      "2021-01-14 16:40:49,918 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:40:49,919 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:40:49,924 : INFO : built Dictionary(379 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 2370 corpus positions)\n",
      "2021-01-14 16:40:50,516 : INFO : token count processed\n",
      "2021-01-14 16:40:50,548 : INFO : frequencies processed\n",
      "2021-01-14 16:41:00,356 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:41:00,357 : INFO : entropies processed\n",
      "2021-01-14 16:41:00,358 : INFO : extropies processed\n",
      "2021-01-14 16:41:00,369 : INFO : token count processed\n",
      "2021-01-14 16:41:00,374 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:41:00,381 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:41:00,382 : INFO : vocab #32006\n",
      "2021-01-14 16:41:00,388 : INFO : diff #set()\n",
      "2021-01-14 16:41:20,601 : INFO : alphabet #32006\n",
      "2021-01-14 16:41:30,681 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.0963999974498178, 0.4770082051213785], [0.7018168568611145, 0.29818314], [4.611416824235503, 1.4049382863190953], [5.476474452049073, 7.1219284286457345, 7.392121265000349, 5.206281615694459, 1.9156468129512758, 0.27019283635461466]]\n",
      "2021-01-14 16:41:30,686 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:41:30,687 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:41:30,689 : INFO : built Dictionary(305 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 2353 corpus positions)\n",
      "2021-01-14 16:41:31,093 : INFO : token count processed\n",
      "2021-01-14 16:41:31,176 : INFO : frequencies processed\n",
      "2021-01-14 16:41:41,141 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:41:41,142 : INFO : entropies processed\n",
      "2021-01-14 16:41:41,143 : INFO : extropies processed\n",
      "2021-01-14 16:41:41,150 : INFO : token count processed\n",
      "2021-01-14 16:41:41,155 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:41:41,159 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:41:41,159 : INFO : vocab #32006\n",
      "2021-01-14 16:41:41,165 : INFO : diff #set()\n",
      "2021-01-14 16:42:00,966 : INFO : alphabet #32006\n",
      "2021-01-14 16:42:10,943 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.1284069812015143, 0.4698349558294939], [0.7510831207036972, 0.24891688], [4.2629790410707376, 1.3941473527224613], [5.476474452049073, 6.41099024988467, 6.585571919259342, 5.301892782674402, 1.109097467210269, 0.174581669374672]]\n",
      "2021-01-14 16:42:10,947 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:42:10,948 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:42:10,951 : INFO : built Dictionary(186 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 752 corpus positions)\n",
      "2021-01-14 16:42:11,144 : INFO : token count processed\n",
      "2021-01-14 16:42:11,176 : INFO : frequencies processed\n",
      "2021-01-14 16:42:21,215 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:42:21,216 : INFO : entropies processed\n",
      "2021-01-14 16:42:21,217 : INFO : extropies processed\n",
      "2021-01-14 16:42:21,224 : INFO : token count processed\n",
      "2021-01-14 16:42:21,228 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:42:21,233 : INFO : alphabet_target #32009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 16:42:21,234 : INFO : vocab #32006\n",
      "2021-01-14 16:42:21,240 : INFO : diff #set()\n",
      "2021-01-14 16:42:41,212 : INFO : alphabet #32006\n",
      "2021-01-14 16:42:51,242 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.0666617573158905, 0.4838721171764293], [0.7145834267139435, 0.28541657], [4.182005814760213, 1.397385805051587], [5.476474452049073, 6.077866832717642, 6.446155511131115, 5.108185773635599, 0.9696810590820419, 0.36828867841347357]]\n",
      "2021-01-14 16:42:51,246 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:42:51,247 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:42:51,248 : INFO : built Dictionary(165 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 517 corpus positions)\n",
      "2021-01-14 16:42:51,408 : INFO : token count processed\n",
      "2021-01-14 16:42:51,441 : INFO : frequencies processed\n",
      "2021-01-14 16:43:01,311 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:43:01,312 : INFO : entropies processed\n",
      "2021-01-14 16:43:01,313 : INFO : extropies processed\n",
      "2021-01-14 16:43:01,319 : INFO : token count processed\n",
      "2021-01-14 16:43:01,324 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:43:01,328 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:43:01,330 : INFO : vocab #32006\n",
      "2021-01-14 16:43:01,336 : INFO : diff #set()\n",
      "2021-01-14 16:43:21,571 : INFO : alphabet #32006\n",
      "2021-01-14 16:43:31,899 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.0722922499366037, 0.4825574192204754], [0.6962788105010986, 0.3037212], [3.9690016298759927, 1.3907663955465712], [5.476474452049073, 5.977547459003844, 6.40356078749627, 5.050461123556648, 0.9270863354471963, 0.42601332849242546]]\n",
      "2021-01-14 16:43:31,904 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:43:31,907 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:43:31,911 : INFO : built Dictionary(264 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 2228 corpus positions)\n",
      "2021-01-14 16:43:32,287 : INFO : token count processed\n",
      "2021-01-14 16:43:32,368 : INFO : frequencies processed\n",
      "2021-01-14 16:43:42,344 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:43:42,346 : INFO : entropies processed\n",
      "2021-01-14 16:43:42,346 : INFO : extropies processed\n",
      "2021-01-14 16:43:42,354 : INFO : token count processed\n",
      "2021-01-14 16:43:42,358 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:43:42,362 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:43:42,363 : INFO : vocab #32006\n",
      "2021-01-14 16:43:42,369 : INFO : diff #set()\n",
      "2021-01-14 16:44:02,240 : INFO : alphabet #32006\n",
      "2021-01-14 16:44:12,361 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.1042106690096578, 0.4752375865818834], [0.7374016642570496, 0.26259834], [4.390319531114783, 1.403816968419779], [5.476474452049073, 6.4614394051846435, 6.6704341138039265, 5.26747974342979, 1.1939596617548531, 0.20899470861928293]]\n",
      "2021-01-14 16:44:12,366 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:44:12,367 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:44:12,368 : INFO : built Dictionary(234 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1452 corpus positions)\n",
      "2021-01-14 16:44:12,658 : INFO : token count processed\n",
      "2021-01-14 16:44:12,742 : INFO : frequencies processed\n",
      "2021-01-14 16:44:22,732 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:44:22,733 : INFO : entropies processed\n",
      "2021-01-14 16:44:22,734 : INFO : extropies processed\n",
      "2021-01-14 16:44:22,745 : INFO : token count processed\n",
      "2021-01-14 16:44:22,749 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:44:22,754 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:44:22,754 : INFO : vocab #32006\n",
      "2021-01-14 16:44:22,761 : INFO : diff #set()\n",
      "2021-01-14 16:44:43,137 : INFO : alphabet #32006\n",
      "2021-01-14 16:44:52,999 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.1098128656432806, 0.47397568584600547], [0.7356683015823364, 0.2643317], [3.8208888513501886, 1.3871680326029323], [5.476474452049073, 6.327195724598159, 6.604234084661066, 5.199436091986167, 1.1277596326119923, 0.2770383600629067]]\n",
      "2021-01-14 16:44:53,011 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:44:53,012 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:44:53,019 : INFO : built Dictionary(433 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 6344 corpus positions)\n",
      "2021-01-14 16:44:53,760 : INFO : token count processed\n",
      "2021-01-14 16:44:53,830 : INFO : frequencies processed\n",
      "2021-01-14 16:45:03,879 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:45:03,880 : INFO : entropies processed\n",
      "2021-01-14 16:45:03,881 : INFO : extropies processed\n",
      "2021-01-14 16:45:03,893 : INFO : token count processed\n",
      "2021-01-14 16:45:03,898 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:45:03,903 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:45:03,904 : INFO : vocab #32006\n",
      "2021-01-14 16:45:03,910 : INFO : diff #set()\n",
      "2021-01-14 16:45:24,103 : INFO : alphabet #32006\n",
      "2021-01-14 16:45:33,806 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.0414775427938319, 0.48984129339550114], [0.6993960440158844, 0.30060396], [4.7084573552056845, 1.4052454116650057], [5.476474452049073, 6.9079058562486315, 7.047306984676688, 5.337073323621016, 1.5708325326276142, 0.13940112842805608]]\n",
      "2021-01-14 16:45:33,812 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:45:33,814 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:45:33,816 : INFO : built Dictionary(343 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 2738 corpus positions)\n",
      "2021-01-14 16:45:34,283 : INFO : token count processed\n",
      "2021-01-14 16:45:34,317 : INFO : frequencies processed\n",
      "2021-01-14 16:45:44,335 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:45:44,336 : INFO : entropies processed\n",
      "2021-01-14 16:45:44,337 : INFO : extropies processed\n",
      "2021-01-14 16:45:44,345 : INFO : token count processed\n",
      "2021-01-14 16:45:44,349 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:45:44,353 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:45:44,354 : INFO : vocab #32006\n",
      "2021-01-14 16:45:44,360 : INFO : diff #set()\n",
      "2021-01-14 16:46:04,051 : INFO : alphabet #32006\n",
      "2021-01-14 16:46:13,958 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.0870380456082926, 0.47914794946085315], [0.7106556296348572, 0.28934437], [4.33379987011787, 1.399295085005141], [5.476474452049073, 6.61034830706307, 6.836284080522571, 5.250538678589573, 1.3598096284734975, 0.22593577345950067]]\n",
      "2021-01-14 16:46:13,962 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:46:13,963 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:46:13,967 : INFO : built Dictionary(242 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 785 corpus positions)\n",
      "2021-01-14 16:46:14,260 : INFO : token count processed\n",
      "2021-01-14 16:46:14,319 : INFO : frequencies processed\n",
      "2021-01-14 16:46:24,254 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:46:24,255 : INFO : entropies processed\n",
      "2021-01-14 16:46:24,256 : INFO : extropies processed\n",
      "2021-01-14 16:46:24,265 : INFO : token count processed\n",
      "2021-01-14 16:46:24,270 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:46:24,274 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:46:24,275 : INFO : vocab #32006\n",
      "2021-01-14 16:46:24,281 : INFO : diff #set()\n",
      "2021-01-14 16:46:44,465 : INFO : alphabet #32006\n",
      "2021-01-14 16:46:54,410 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.0988629621086587, 0.4764484475896095], [0.6990433633327484, 0.30095664], [3.2927701939369904, 1.3562369015699551], [5.476474452049073, 6.616715366949855, 6.959117917825337, 5.134071901173591, 1.4826434657762633, 0.342402550875482]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 16:46:54,417 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:46:54,418 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:46:54,420 : INFO : built Dictionary(445 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 2819 corpus positions)\n",
      "2021-01-14 16:46:55,179 : INFO : token count processed\n",
      "2021-01-14 16:46:55,208 : INFO : frequencies processed\n",
      "2021-01-14 16:47:05,433 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:47:05,434 : INFO : entropies processed\n",
      "2021-01-14 16:47:05,435 : INFO : extropies processed\n",
      "2021-01-14 16:47:05,443 : INFO : token count processed\n",
      "2021-01-14 16:47:05,448 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:47:05,452 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:47:05,453 : INFO : vocab #32006\n",
      "2021-01-14 16:47:05,459 : INFO : diff #set()\n",
      "2021-01-14 16:47:25,224 : INFO : alphabet #32006\n",
      "2021-01-14 16:47:35,208 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.0841694781870757, 0.4798074295137718], [0.7096389830112457, 0.29036102], [4.561641344913578, 1.4055346460522404], [5.476474452049073, 7.32185870753746, 7.568724895208157, 5.229608264378377, 2.0922504431590836, 0.24686618767069657]]\n",
      "2021-01-14 16:47:35,212 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:47:35,213 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:47:35,217 : INFO : built Dictionary(100 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 184 corpus positions)\n",
      "2021-01-14 16:47:35,283 : INFO : token count processed\n",
      "2021-01-14 16:47:35,314 : INFO : frequencies processed\n",
      "2021-01-14 16:47:45,203 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:47:45,204 : INFO : entropies processed\n",
      "2021-01-14 16:47:45,205 : INFO : extropies processed\n",
      "2021-01-14 16:47:45,215 : INFO : token count processed\n",
      "2021-01-14 16:47:45,220 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:47:45,225 : INFO : alphabet_target #32008\n",
      "2021-01-14 16:47:45,225 : INFO : vocab #32006\n",
      "2021-01-14 16:47:45,232 : INFO : diff #set()\n",
      "2021-01-14 16:48:05,228 : INFO : alphabet #32006\n",
      "2021-01-14 16:48:15,339 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/fireException.py')[[1.1403663059628102, 0.46720974686160827], [0.7837904989719391, 0.2162095], [2.5032583347756456, 1.2991301890771523], [5.476474452049073, 5.176618657501385, 6.214543473550221, 4.4385496360002366, 0.7380690215011478, 1.037924816048836]]\n",
      "2021-01-14 16:48:15,343 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:48:15,344 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:48:15,348 : INFO : built Dictionary(190 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 577 corpus positions)\n",
      "2021-01-14 16:48:15,554 : INFO : token count processed\n",
      "2021-01-14 16:48:15,583 : INFO : frequencies processed\n",
      "2021-01-14 16:48:25,725 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:48:25,726 : INFO : entropies processed\n",
      "2021-01-14 16:48:25,727 : INFO : extropies processed\n",
      "2021-01-14 16:48:25,738 : INFO : token count processed\n",
      "2021-01-14 16:48:25,743 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:48:25,747 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:48:25,748 : INFO : vocab #32006\n",
      "2021-01-14 16:48:25,754 : INFO : diff #set()\n",
      "2021-01-14 16:48:45,479 : INFO : alphabet #32006\n",
      "2021-01-14 16:48:55,538 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.1379572749312963, 0.4677361946029231], [0.7418752312660217, 0.25812477], [3.3027227528624894, 1.342089957691207], [5.476474452049073, 6.468846789852156, 6.894125376185001, 5.051195865716228, 1.4176509241359279, 0.425278586332845]]\n",
      "2021-01-14 16:48:55,545 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:48:55,547 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:48:55,552 : INFO : built Dictionary(394 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 2626 corpus positions)\n",
      "2021-01-14 16:48:56,193 : INFO : token count processed\n",
      "2021-01-14 16:48:56,226 : INFO : frequencies processed\n",
      "2021-01-14 16:49:06,123 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:49:06,125 : INFO : entropies processed\n",
      "2021-01-14 16:49:06,128 : INFO : extropies processed\n",
      "2021-01-14 16:49:06,136 : INFO : token count processed\n",
      "2021-01-14 16:49:06,146 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:49:06,152 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:49:06,152 : INFO : vocab #32006\n",
      "2021-01-14 16:49:06,159 : INFO : diff #set()\n",
      "2021-01-14 16:49:26,225 : INFO : alphabet #32006\n",
      "2021-01-14 16:49:36,057 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.1182321644936422, 0.4720917833098089], [0.739731103181839, 0.2602689], [4.357075175138721, 1.3970768472004824], [5.476474452049073, 6.957796704012729, 7.171355058787564, 5.262916097274238, 1.694880606738491, 0.2135583547748352]]\n",
      "2021-01-14 16:49:36,064 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:49:36,065 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:49:36,070 : INFO : built Dictionary(308 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 3123 corpus positions)\n",
      "2021-01-14 16:49:36,484 : INFO : token count processed\n",
      "2021-01-14 16:49:36,516 : INFO : frequencies processed\n",
      "2021-01-14 16:49:46,363 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:49:46,365 : INFO : entropies processed\n",
      "2021-01-14 16:49:46,365 : INFO : extropies processed\n",
      "2021-01-14 16:49:46,377 : INFO : token count processed\n",
      "2021-01-14 16:49:46,382 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:49:46,386 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:49:46,387 : INFO : vocab #32006\n",
      "2021-01-14 16:49:46,394 : INFO : diff #set()\n",
      "2021-01-14 16:50:06,549 : INFO : alphabet #32006\n",
      "2021-01-14 16:50:16,793 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.1105139768874945, 0.47381823145978963], [0.7441062927246094, 0.2558937], [3.734885540886845, 1.347917761963232], [5.476474452049073, 6.441859572014148, 6.666692885553007, 5.251641138510213, 1.1902184335039339, 0.22483331353885916]]\n",
      "2021-01-14 16:50:16,798 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:50:16,799 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:50:16,804 : INFO : built Dictionary(328 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1599 corpus positions)\n",
      "2021-01-14 16:50:17,259 : INFO : token count processed\n",
      "2021-01-14 16:50:17,290 : INFO : frequencies processed\n",
      "2021-01-14 16:50:27,284 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:50:27,285 : INFO : entropies processed\n",
      "2021-01-14 16:50:27,286 : INFO : extropies processed\n",
      "2021-01-14 16:50:27,297 : INFO : token count processed\n",
      "2021-01-14 16:50:27,302 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:50:27,307 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:50:27,308 : INFO : vocab #32006\n",
      "2021-01-14 16:50:27,314 : INFO : diff #set()\n",
      "2021-01-14 16:50:46,734 : INFO : alphabet #32006\n",
      "2021-01-14 16:50:56,360 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.0756590058583486, 0.4817747024812823], [0.659171462059021, 0.34082854], [4.455748576083888, 1.40287820686196], [5.476474452049073, 6.998955278238291, 7.23796904794761, 5.237460682339753, 1.7614945958985366, 0.2390137697093193]]\n",
      "2021-01-14 16:50:56,364 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:50:56,365 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:50:56,367 : INFO : built Dictionary(245 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1418 corpus positions)\n",
      "2021-01-14 16:50:56,671 : INFO : token count processed\n",
      "2021-01-14 16:50:56,701 : INFO : frequencies processed\n",
      "2021-01-14 16:51:06,619 : INFO : scalar_distribution processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 16:51:06,620 : INFO : entropies processed\n",
      "2021-01-14 16:51:06,620 : INFO : extropies processed\n",
      "2021-01-14 16:51:06,628 : INFO : token count processed\n",
      "2021-01-14 16:51:06,632 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:51:06,636 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:51:06,637 : INFO : vocab #32006\n",
      "2021-01-14 16:51:06,644 : INFO : diff #set()\n",
      "2021-01-14 16:51:26,713 : INFO : alphabet #32006\n",
      "2021-01-14 16:51:36,777 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.0922062661528165, 0.4779643461439472], [0.719043493270874, 0.2809565], [4.393061650825727, 1.4002762364403958], [5.476474452049073, 6.492983191376071, 6.737719073016896, 5.231738570408249, 1.2612446209678225, 0.24473588164082472]]\n",
      "2021-01-14 16:51:36,784 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:51:36,785 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:51:36,787 : INFO : built Dictionary(458 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 3357 corpus positions)\n",
      "2021-01-14 16:51:37,562 : INFO : token count processed\n",
      "2021-01-14 16:51:37,595 : INFO : frequencies processed\n",
      "2021-01-14 16:51:47,684 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:51:47,685 : INFO : entropies processed\n",
      "2021-01-14 16:51:47,685 : INFO : extropies processed\n",
      "2021-01-14 16:51:47,693 : INFO : token count processed\n",
      "2021-01-14 16:51:47,698 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:51:47,702 : INFO : alphabet_target #32008\n",
      "2021-01-14 16:51:47,703 : INFO : vocab #32006\n",
      "2021-01-14 16:51:47,709 : INFO : diff #set()\n",
      "2021-01-14 16:52:07,541 : INFO : alphabet #32006\n",
      "2021-01-14 16:52:17,563 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.0677288592561978, 0.48362240316156324], [0.6712221503257751, 0.32877785], [4.063650847340264, 1.388505024400294], [5.476474452049073, 6.560342487747443, 6.823067458406742, 5.213749481389774, 1.3465930063576685, 0.2627249706592991]]\n",
      "2021-01-14 16:52:17,571 : INFO : Removed 1 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:52:17,572 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:52:17,577 : INFO : built Dictionary(462 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 3562 corpus positions)\n",
      "2021-01-14 16:52:18,355 : INFO : token count processed\n",
      "2021-01-14 16:52:18,387 : INFO : frequencies processed\n",
      "2021-01-14 16:52:28,500 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:52:28,501 : INFO : entropies processed\n",
      "2021-01-14 16:52:28,502 : INFO : extropies processed\n",
      "2021-01-14 16:52:28,510 : INFO : token count processed\n",
      "2021-01-14 16:52:28,515 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:52:28,519 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:52:28,520 : INFO : vocab #32006\n",
      "2021-01-14 16:52:28,527 : INFO : diff #set()\n",
      "2021-01-14 16:52:48,175 : INFO : alphabet #32006\n",
      "2021-01-14 16:52:58,158 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.0793415575514138, 0.48092147072632824], [0.6928995549678802, 0.30710045], [4.64649641251965, 1.4002779438652788], [5.476474452049073, 7.046173750105238, 7.273180539775135, 5.2494676623791765, 1.7967060877260614, 0.22700678966989685]]\n",
      "2021-01-14 16:52:58,169 : INFO : Removed 1 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:52:58,170 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:52:58,173 : INFO : built Dictionary(513 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 5671 corpus positions)\n",
      "2021-01-14 16:52:59,069 : INFO : token count processed\n",
      "2021-01-14 16:52:59,102 : INFO : frequencies processed\n",
      "2021-01-14 16:53:08,971 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:53:08,972 : INFO : entropies processed\n",
      "2021-01-14 16:53:08,973 : INFO : extropies processed\n",
      "2021-01-14 16:53:08,982 : INFO : token count processed\n",
      "2021-01-14 16:53:08,987 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:53:08,991 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:53:08,991 : INFO : vocab #32006\n",
      "2021-01-14 16:53:08,998 : INFO : diff #set()\n",
      "2021-01-14 16:53:29,023 : INFO : alphabet #32006\n",
      "2021-01-14 16:53:39,170 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.0394155382106212, 0.49033656028599154], [0.6787488758563995, 0.32125112], [4.673807955786001, 1.3946707067716213], [5.476474452049073, 7.009229588004272, 7.144527496590077, 5.341176543463269, 1.6680530445410033, 0.13529790858580437]]\n",
      "2021-01-14 16:53:39,184 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:53:39,184 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:53:39,188 : INFO : built Dictionary(594 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 6612 corpus positions)\n",
      "2021-01-14 16:53:40,317 : INFO : token count processed\n",
      "2021-01-14 16:53:40,350 : INFO : frequencies processed\n",
      "2021-01-14 16:53:50,325 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:53:50,326 : INFO : entropies processed\n",
      "2021-01-14 16:53:50,326 : INFO : extropies processed\n",
      "2021-01-14 16:53:50,336 : INFO : token count processed\n",
      "2021-01-14 16:53:50,340 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:53:50,344 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:53:50,345 : INFO : vocab #32006\n",
      "2021-01-14 16:53:50,352 : INFO : diff #set()\n",
      "2021-01-14 16:54:10,401 : INFO : alphabet #32006\n",
      "2021-01-14 16:54:20,162 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.0647095070720327, 0.48432963405980595], [0.6833531260490417, 0.31664687], [4.60999475703297, 1.3945547445390425], [5.476474452049073, 7.376088004590871, 7.562267780345765, 5.290294676294179, 2.085793328296692, 0.1861797757548942]]\n",
      "2021-01-14 16:54:20,166 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:54:20,167 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:54:20,172 : INFO : built Dictionary(163 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 398 corpus positions)\n",
      "2021-01-14 16:54:20,333 : INFO : token count processed\n",
      "2021-01-14 16:54:20,364 : INFO : frequencies processed\n",
      "2021-01-14 16:54:30,247 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:54:30,249 : INFO : entropies processed\n",
      "2021-01-14 16:54:30,249 : INFO : extropies processed\n",
      "2021-01-14 16:54:30,260 : INFO : token count processed\n",
      "2021-01-14 16:54:30,265 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:54:30,270 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:54:30,271 : INFO : vocab #32006\n",
      "2021-01-14 16:54:30,277 : INFO : diff #set()\n",
      "2021-01-14 16:54:50,402 : INFO : alphabet #32006\n",
      "2021-01-14 16:55:00,422 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.0736534629990127, 0.4822406529554626], [0.6664542853832245, 0.3335457], [3.516150648454355, 1.3448695928569514], [5.476474452049073, 6.2993628166120885, 6.7211651237363945, 5.054672144924767, 1.2446906716873212, 0.421802307124306]]\n",
      "2021-01-14 16:55:00,425 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:55:00,426 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:55:00,426 : INFO : built Dictionary(73 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 114 corpus positions)\n",
      "2021-01-14 16:55:00,453 : INFO : token count processed\n",
      "2021-01-14 16:55:00,486 : INFO : frequencies processed\n",
      "2021-01-14 16:55:10,452 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:55:10,453 : INFO : entropies processed\n",
      "2021-01-14 16:55:10,454 : INFO : extropies processed\n",
      "2021-01-14 16:55:10,461 : INFO : token count processed\n",
      "2021-01-14 16:55:10,466 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:55:10,470 : INFO : alphabet_target #32008\n",
      "2021-01-14 16:55:10,471 : INFO : vocab #32006\n",
      "2021-01-14 16:55:10,478 : INFO : diff #set()\n",
      "2021-01-14 16:55:30,262 : INFO : alphabet #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 16:55:40,276 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2346421416947062, 0.4474989446147385], [0.8493224531412125, 0.15067755], [0.9182958340544896, 0.9182958340544896], [5.476474452049073, 3.8936606896881862, 5.77235924565494, 3.59777589608232, 0.2958847936058664, 1.8786985559667535]]\n",
      "2021-01-14 16:55:40,300 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:55:40,301 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:55:40,309 : INFO : built Dictionary(755 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 12556 corpus positions)\n",
      "2021-01-14 16:55:42,049 : INFO : token count processed\n",
      "2021-01-14 16:55:42,083 : INFO : frequencies processed\n",
      "2021-01-14 16:55:52,083 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:55:52,084 : INFO : entropies processed\n",
      "2021-01-14 16:55:52,085 : INFO : extropies processed\n",
      "2021-01-14 16:55:52,101 : INFO : token count processed\n",
      "2021-01-14 16:55:52,105 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:55:52,109 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:55:52,110 : INFO : vocab #32006\n",
      "2021-01-14 16:55:52,117 : INFO : diff #set()\n",
      "2021-01-14 16:56:12,198 : INFO : alphabet #32006\n",
      "2021-01-14 16:56:21,555 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.0889203707961002, 0.4787161894634087], [0.671013593673706, 0.3289864], [4.461210672320922, 1.3858265942208434], [5.476474452049073, 7.434393313070278, 7.6541366329695535, 5.256731132149799, 2.17766218092048, 0.2197433198992753]]\n",
      "2021-01-14 16:56:21,564 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:56:21,565 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:56:21,567 : INFO : built Dictionary(506 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 4183 corpus positions)\n",
      "2021-01-14 16:56:22,453 : INFO : token count processed\n",
      "2021-01-14 16:56:22,528 : INFO : frequencies processed\n",
      "2021-01-14 16:56:32,506 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:56:32,508 : INFO : entropies processed\n",
      "2021-01-14 16:56:32,508 : INFO : extropies processed\n",
      "2021-01-14 16:56:32,519 : INFO : token count processed\n",
      "2021-01-14 16:56:32,524 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:56:32,528 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:56:32,529 : INFO : vocab #32006\n",
      "2021-01-14 16:56:32,535 : INFO : diff #set()\n",
      "2021-01-14 16:56:52,364 : INFO : alphabet #32006\n",
      "2021-01-14 16:57:02,494 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.0851023985487265, 0.47959275318853417], [0.6873736083507538, 0.3126264], [4.504574504151559, 1.390501030660833], [5.476474452049073, 7.2991514951718255, 7.521588228699818, 5.254037718521081, 2.0451137766507443, 0.2224367335279922]]\n",
      "2021-01-14 16:57:02,502 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:57:02,504 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:57:02,506 : INFO : built Dictionary(475 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 3600 corpus positions)\n",
      "2021-01-14 16:57:03,317 : INFO : token count processed\n",
      "2021-01-14 16:57:03,406 : INFO : frequencies processed\n",
      "2021-01-14 16:57:13,526 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:57:13,527 : INFO : entropies processed\n",
      "2021-01-14 16:57:13,528 : INFO : extropies processed\n",
      "2021-01-14 16:57:13,540 : INFO : token count processed\n",
      "2021-01-14 16:57:13,544 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:57:13,549 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:57:13,549 : INFO : vocab #32006\n",
      "2021-01-14 16:57:13,556 : INFO : diff #set()\n",
      "2021-01-14 16:57:33,889 : INFO : alphabet #32006\n",
      "2021-01-14 16:57:43,958 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.0843770283107923, 0.4797596530846503], [0.6854768097400665, 0.3145232], [4.465096011575605, 1.395967249843375], [5.476474452049073, 7.170319527000998, 7.395500053710612, 5.251293925339458, 1.9190256016615388, 0.22518052670961453]]\n",
      "2021-01-14 16:57:43,962 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:57:43,963 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:57:43,964 : INFO : built Dictionary(195 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 636 corpus positions)\n",
      "2021-01-14 16:57:44,189 : INFO : token count processed\n",
      "2021-01-14 16:57:44,268 : INFO : frequencies processed\n",
      "2021-01-14 16:57:54,102 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:57:54,103 : INFO : entropies processed\n",
      "2021-01-14 16:57:54,104 : INFO : extropies processed\n",
      "2021-01-14 16:57:54,111 : INFO : token count processed\n",
      "2021-01-14 16:57:54,115 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:57:54,119 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:57:54,120 : INFO : vocab #32006\n",
      "2021-01-14 16:57:54,127 : INFO : diff #set()\n",
      "2021-01-14 16:58:13,801 : INFO : alphabet #32006\n",
      "2021-01-14 16:58:23,912 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.101015085274566, 0.4759604093320051], [0.7044676542282104, 0.29553235], [3.8512274809529554, 1.3769777429328072], [5.476474452049073, 6.353654804387375, 6.755329254203405, 5.074800002233044, 1.2788548021543313, 0.4016744498160296]]\n",
      "2021-01-14 16:58:23,916 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:58:23,917 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:58:23,921 : INFO : built Dictionary(195 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 764 corpus positions)\n",
      "2021-01-14 16:58:24,146 : INFO : token count processed\n",
      "2021-01-14 16:58:24,214 : INFO : frequencies processed\n",
      "2021-01-14 16:58:34,220 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:58:34,221 : INFO : entropies processed\n",
      "2021-01-14 16:58:34,222 : INFO : extropies processed\n",
      "2021-01-14 16:58:34,235 : INFO : token count processed\n",
      "2021-01-14 16:58:34,239 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:58:34,246 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:58:34,247 : INFO : vocab #32006\n",
      "2021-01-14 16:58:34,256 : INFO : diff #set()\n",
      "2021-01-14 16:58:54,349 : INFO : alphabet #32006\n",
      "2021-01-14 16:59:04,327 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.1013031599575682, 0.4758951583265087], [0.7204705774784088, 0.27952942], [3.392147223664534, 1.3581797203736243], [5.476474452049073, 6.245180322479091, 6.636357373875608, 5.085297400652557, 1.1598829218265347, 0.39117705139651715]]\n",
      "2021-01-14 16:59:04,333 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 16:59:04,334 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:59:04,336 : INFO : built Dictionary(405 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1836 corpus positions)\n",
      "2021-01-14 16:59:05,000 : INFO : token count processed\n",
      "2021-01-14 16:59:05,081 : INFO : frequencies processed\n",
      "2021-01-14 16:59:14,737 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:59:14,738 : INFO : entropies processed\n",
      "2021-01-14 16:59:14,739 : INFO : extropies processed\n",
      "2021-01-14 16:59:14,749 : INFO : token count processed\n",
      "2021-01-14 16:59:14,754 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:59:14,758 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:59:14,759 : INFO : vocab #32006\n",
      "2021-01-14 16:59:14,765 : INFO : diff #set()\n",
      "2021-01-14 16:59:34,954 : INFO : alphabet #32006\n",
      "2021-01-14 16:59:44,999 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.1027976970564035, 0.4755569218093817], [0.7095070779323578, 0.29049292], [4.7173740358571745, 1.4094710901384733], [5.476474452049073, 7.2691387000368, 7.531690399177537, 5.213922752908337, 2.0552159471284632, 0.2625516991407366]]\n",
      "2021-01-14 16:59:45,004 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 16:59:45,005 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 16:59:45,010 : INFO : built Dictionary(342 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1673 corpus positions)\n",
      "2021-01-14 16:59:45,552 : INFO : token count processed\n",
      "2021-01-14 16:59:45,632 : INFO : frequencies processed\n",
      "2021-01-14 16:59:55,696 : INFO : scalar_distribution processed\n",
      "2021-01-14 16:59:55,698 : INFO : entropies processed\n",
      "2021-01-14 16:59:55,698 : INFO : extropies processed\n",
      "2021-01-14 16:59:55,710 : INFO : token count processed\n",
      "2021-01-14 16:59:55,715 : INFO : alphabet_source #32006\n",
      "2021-01-14 16:59:55,719 : INFO : alphabet_target #32009\n",
      "2021-01-14 16:59:55,720 : INFO : vocab #32006\n",
      "2021-01-14 16:59:55,727 : INFO : diff #set()\n",
      "2021-01-14 17:00:15,810 : INFO : alphabet #32006\n",
      "2021-01-14 17:00:25,734 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.1018232169648654, 0.4757774069334188], [0.6995746493339539, 0.30042535], [3.71067003032422, 1.347805995872648], [5.476474452049073, 7.08857858466988, 7.34267467089283, 5.222378365826124, 1.8662002188437565, 0.2540960862229502]]\n",
      "2021-01-14 17:00:25,738 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:00:25,739 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:00:25,743 : INFO : built Dictionary(172 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 596 corpus positions)\n",
      "2021-01-14 17:00:25,926 : INFO : token count processed\n",
      "2021-01-14 17:00:25,989 : INFO : frequencies processed\n",
      "2021-01-14 17:00:35,771 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:00:35,772 : INFO : entropies processed\n",
      "2021-01-14 17:00:35,772 : INFO : extropies processed\n",
      "2021-01-14 17:00:35,780 : INFO : token count processed\n",
      "2021-01-14 17:00:35,784 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:00:35,788 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:00:35,789 : INFO : vocab #32006\n",
      "2021-01-14 17:00:35,795 : INFO : diff #set()\n",
      "2021-01-14 17:00:55,827 : INFO : alphabet #32006\n",
      "2021-01-14 17:01:05,848 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.104457559015314, 0.47518183282722265], [0.712443619966507, 0.28755638], [3.553763114472245, 1.3611985321354638], [5.476474452049073, 6.0479231618016716, 6.501673931284957, 5.022723682565788, 1.0251994792358836, 0.4537507694832854]]\n",
      "2021-01-14 17:01:05,851 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:01:05,852 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:01:05,854 : INFO : built Dictionary(176 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 715 corpus positions)\n",
      "2021-01-14 17:01:06,032 : INFO : token count processed\n",
      "2021-01-14 17:01:06,071 : INFO : frequencies processed\n",
      "2021-01-14 17:01:15,910 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:01:15,912 : INFO : entropies processed\n",
      "2021-01-14 17:01:15,912 : INFO : extropies processed\n",
      "2021-01-14 17:01:15,923 : INFO : token count processed\n",
      "2021-01-14 17:01:15,927 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:01:15,932 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:01:15,935 : INFO : vocab #32006\n",
      "2021-01-14 17:01:15,941 : INFO : diff #set()\n",
      "2021-01-14 17:01:35,875 : INFO : alphabet #32006\n",
      "2021-01-14 17:01:45,692 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.114177689055104, 0.4729971398226858], [0.7299725711345673, 0.27002743], [3.392147223664534, 1.3581797203736246], [5.476474452049073, 6.036583168403119, 6.46209492268169, 5.050962697770503, 0.9856204706326164, 0.42551175427857046]]\n",
      "2021-01-14 17:01:45,706 : INFO : Removed 1 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:01:45,707 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:01:45,713 : INFO : built Dictionary(580 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 7073 corpus positions)\n",
      "2021-01-14 17:01:46,838 : INFO : token count processed\n",
      "2021-01-14 17:01:46,887 : INFO : frequencies processed\n",
      "2021-01-14 17:01:56,782 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:01:56,783 : INFO : entropies processed\n",
      "2021-01-14 17:01:56,784 : INFO : extropies processed\n",
      "2021-01-14 17:01:56,797 : INFO : token count processed\n",
      "2021-01-14 17:01:56,802 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:01:56,807 : INFO : alphabet_target #32010\n",
      "2021-01-14 17:01:56,808 : INFO : vocab #32006\n",
      "2021-01-14 17:01:56,814 : INFO : diff #set()\n",
      "2021-01-14 17:02:16,856 : INFO : alphabet #32006\n",
      "2021-01-14 17:02:26,953 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.0889424040610147, 0.4787111401711924], [0.6974588632583618, 0.30254114], [3.966733735016133, 1.3646598962335454], [5.476474452049073, 7.29352035514053, 7.47931262722453, 5.290682179965074, 2.002838175175457, 0.18579227208399995]]\n",
      "2021-01-14 17:02:26,960 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:02:26,961 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:02:26,963 : INFO : built Dictionary(387 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 3316 corpus positions)\n",
      "2021-01-14 17:02:27,502 : INFO : token count processed\n",
      "2021-01-14 17:02:27,545 : INFO : frequencies processed\n",
      "2021-01-14 17:02:37,456 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:02:37,457 : INFO : entropies processed\n",
      "2021-01-14 17:02:37,458 : INFO : extropies processed\n",
      "2021-01-14 17:02:37,470 : INFO : token count processed\n",
      "2021-01-14 17:02:37,474 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:02:37,479 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:02:37,480 : INFO : vocab #32006\n",
      "2021-01-14 17:02:37,486 : INFO : diff #set()\n",
      "2021-01-14 17:02:57,083 : INFO : alphabet #32006\n",
      "2021-01-14 17:03:07,141 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.0861038756947008, 0.47936251480621334], [0.6805269420146942, 0.31947306], [4.349712458420389, 1.3979927264749952], [5.476474452049073, 6.8153433747477745, 7.011899864291994, 5.279917962504854, 1.5354254122429207, 0.19655648954421956]]\n",
      "2021-01-14 17:03:07,144 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:03:07,145 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:03:07,149 : INFO : built Dictionary(154 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 337 corpus positions)\n",
      "2021-01-14 17:03:07,307 : INFO : token count processed\n",
      "2021-01-14 17:03:07,398 : INFO : frequencies processed\n",
      "2021-01-14 17:03:17,814 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:03:17,816 : INFO : entropies processed\n",
      "2021-01-14 17:03:17,817 : INFO : extropies processed\n",
      "2021-01-14 17:03:17,826 : INFO : token count processed\n",
      "2021-01-14 17:03:17,832 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:03:17,837 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:03:17,837 : INFO : vocab #32006\n",
      "2021-01-14 17:03:17,844 : INFO : diff #set()\n",
      "2021-01-14 17:03:38,310 : INFO : alphabet #32006\n",
      "2021-01-14 17:03:48,291 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.1201186906909957, 0.47167170611286713], [0.7548652589321136, 0.24513474], [3.0849625007211556, 1.3480058660457088], [5.476474452049073, 6.150121915859574, 6.68635517638992, 4.9402411915187265, 1.209880724340847, 0.5362332605303459]]\n",
      "2021-01-14 17:03:48,296 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:03:48,297 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:03:48,298 : INFO : built Dictionary(306 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1048 corpus positions)\n",
      "2021-01-14 17:03:48,724 : INFO : token count processed\n",
      "2021-01-14 17:03:48,768 : INFO : frequencies processed\n",
      "2021-01-14 17:03:58,827 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:03:58,828 : INFO : entropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 17:03:58,829 : INFO : extropies processed\n",
      "2021-01-14 17:03:58,838 : INFO : token count processed\n",
      "2021-01-14 17:03:58,842 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:03:58,847 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:03:58,847 : INFO : vocab #32006\n",
      "2021-01-14 17:03:58,855 : INFO : diff #set()\n",
      "2021-01-14 17:04:18,609 : INFO : alphabet #32006\n",
      "2021-01-14 17:04:28,649 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.0966882505165423, 0.47694262595006154], [0.7192074060440063, 0.2807926], [4.037401197654111, 1.3774650538577515], [5.476474452049073, 7.0391145208191315, 7.3803011530517475, 5.135287819816457, 1.9038267010026741, 0.341186632232616]]\n",
      "2021-01-14 17:04:28,659 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:04:28,660 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:04:28,666 : INFO : built Dictionary(606 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 4423 corpus positions)\n",
      "2021-01-14 17:04:29,936 : INFO : token count processed\n",
      "2021-01-14 17:04:29,972 : INFO : frequencies processed\n",
      "2021-01-14 17:04:39,995 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:04:39,996 : INFO : entropies processed\n",
      "2021-01-14 17:04:39,997 : INFO : extropies processed\n",
      "2021-01-14 17:04:40,006 : INFO : token count processed\n",
      "2021-01-14 17:04:40,010 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:04:40,014 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:04:40,015 : INFO : vocab #32006\n",
      "2021-01-14 17:04:40,022 : INFO : diff #set()\n",
      "2021-01-14 17:05:00,223 : INFO : alphabet #32006\n",
      "2021-01-14 17:05:10,479 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.0950263626717196, 0.4773209625509114], [0.7067854702472687, 0.29321453], [4.194306165790041, 1.3751192381160628], [5.476474452049073, 7.482466367279176, 7.785943980567433, 5.172996838760815, 2.3094695285183597, 0.3034776132882575]]\n",
      "2021-01-14 17:05:10,483 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:05:10,484 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:05:10,485 : INFO : built Dictionary(196 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 645 corpus positions)\n",
      "2021-01-14 17:05:10,697 : INFO : token count processed\n",
      "2021-01-14 17:05:10,729 : INFO : frequencies processed\n",
      "2021-01-14 17:05:20,367 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:05:20,369 : INFO : entropies processed\n",
      "2021-01-14 17:05:20,370 : INFO : extropies processed\n",
      "2021-01-14 17:05:20,377 : INFO : token count processed\n",
      "2021-01-14 17:05:20,381 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:05:20,386 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:05:20,386 : INFO : vocab #32006\n",
      "2021-01-14 17:05:20,393 : INFO : diff #set()\n",
      "2021-01-14 17:05:40,580 : INFO : alphabet #32006\n",
      "2021-01-14 17:05:50,570 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.121450101654109, 0.4713756874226234], [0.7117635011672974, 0.2882365], [3.749902506162897, 1.3702629834071027], [5.476474452049073, 6.372162341197667, 6.780317202397555, 5.068319590849185, 1.3038427503484815, 0.408154861199888]]\n",
      "2021-01-14 17:05:50,575 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:05:50,576 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:05:50,578 : INFO : built Dictionary(337 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 2066 corpus positions)\n",
      "2021-01-14 17:05:51,056 : INFO : token count processed\n",
      "2021-01-14 17:05:51,111 : INFO : frequencies processed\n",
      "2021-01-14 17:06:01,279 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:06:01,280 : INFO : entropies processed\n",
      "2021-01-14 17:06:01,281 : INFO : extropies processed\n",
      "2021-01-14 17:06:01,289 : INFO : token count processed\n",
      "2021-01-14 17:06:01,293 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:06:01,298 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:06:01,298 : INFO : vocab #32006\n",
      "2021-01-14 17:06:01,305 : INFO : diff #set()\n",
      "2021-01-14 17:06:20,674 : INFO : alphabet #32006\n",
      "2021-01-14 17:06:30,541 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.0993795719339763, 0.4763312044037785], [0.6845568120479584, 0.3154432], [4.261590980759191, 1.393751925275433], [5.476474452049073, 6.798155919669889, 7.059134492613584, 5.215495879105379, 1.5826600405645106, 0.26097857294369486]]\n",
      "2021-01-14 17:06:30,545 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:06:30,546 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:06:30,551 : INFO : built Dictionary(201 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 782 corpus positions)\n",
      "2021-01-14 17:06:30,772 : INFO : token count processed\n",
      "2021-01-14 17:06:30,804 : INFO : frequencies processed\n",
      "2021-01-14 17:06:40,788 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:06:40,789 : INFO : entropies processed\n",
      "2021-01-14 17:06:40,790 : INFO : extropies processed\n",
      "2021-01-14 17:06:40,797 : INFO : token count processed\n",
      "2021-01-14 17:06:40,802 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:06:40,807 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:06:40,808 : INFO : vocab #32006\n",
      "2021-01-14 17:06:40,814 : INFO : diff #set()\n",
      "2021-01-14 17:07:00,740 : INFO : alphabet #32006\n",
      "2021-01-14 17:07:10,888 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.072669558588395, 0.4824695744945743], [0.6771453022956848, 0.3228547], [3.547636167541365, 1.3474296065099571], [5.476474452049073, 6.271631856729336, 6.639644085921808, 5.108462222856601, 1.1631696338727346, 0.36801222919247234]]\n",
      "2021-01-14 17:07:10,896 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:07:10,897 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:07:10,899 : INFO : built Dictionary(371 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 3297 corpus positions)\n",
      "2021-01-14 17:07:11,446 : INFO : token count processed\n",
      "2021-01-14 17:07:11,481 : INFO : frequencies processed\n",
      "2021-01-14 17:07:21,480 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:07:21,481 : INFO : entropies processed\n",
      "2021-01-14 17:07:21,482 : INFO : extropies processed\n",
      "2021-01-14 17:07:21,491 : INFO : token count processed\n",
      "2021-01-14 17:07:21,495 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:07:21,499 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:07:21,500 : INFO : vocab #32006\n",
      "2021-01-14 17:07:21,507 : INFO : diff #set()\n",
      "2021-01-14 17:07:41,458 : INFO : alphabet #32006\n",
      "2021-01-14 17:07:51,261 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.1069336119667217, 0.47462340261710856], [0.7108849883079529, 0.289115], [4.3430812668472125, 1.3998775258392024], [5.476474452049073, 6.873598627629562, 7.085330183289488, 5.264742896389147, 1.608855731240415, 0.2117315556599264]]\n",
      "2021-01-14 17:07:51,264 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:07:51,265 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:07:51,269 : INFO : built Dictionary(132 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 245 corpus positions)\n",
      "2021-01-14 17:07:51,380 : INFO : token count processed\n",
      "2021-01-14 17:07:51,414 : INFO : frequencies processed\n",
      "2021-01-14 17:08:01,315 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:08:01,316 : INFO : entropies processed\n",
      "2021-01-14 17:08:01,317 : INFO : extropies processed\n",
      "2021-01-14 17:08:01,324 : INFO : token count processed\n",
      "2021-01-14 17:08:01,329 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:08:01,333 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:08:01,334 : INFO : vocab #32006\n",
      "2021-01-14 17:08:01,341 : INFO : diff #set()\n",
      "2021-01-14 17:08:21,430 : INFO : alphabet #32006\n",
      "2021-01-14 17:08:31,763 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.0712487577335017, 0.4828005309677369], [0.6721226572990417, 0.32787734], [3.419381945646371, 1.36352700040423], [5.476474452049073, 6.049830202851529, 6.601982316467624, 4.924322338432979, 1.1255078644185508, 0.5521521136160947]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 17:08:31,768 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:08:31,769 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:08:31,773 : INFO : built Dictionary(278 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1325 corpus positions)\n",
      "2021-01-14 17:08:32,156 : INFO : token count processed\n",
      "2021-01-14 17:08:32,241 : INFO : frequencies processed\n",
      "2021-01-14 17:08:42,206 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:08:42,207 : INFO : entropies processed\n",
      "2021-01-14 17:08:42,208 : INFO : extropies processed\n",
      "2021-01-14 17:08:42,219 : INFO : token count processed\n",
      "2021-01-14 17:08:42,224 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:08:42,230 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:08:42,231 : INFO : vocab #32006\n",
      "2021-01-14 17:08:42,239 : INFO : diff #set()\n",
      "2021-01-14 17:09:02,004 : INFO : alphabet #32006\n",
      "2021-01-14 17:09:12,151 : INFO : Computed distances or similarities ('278', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.1059480989985702, 0.4748455104261708], [0.7246606647968292, 0.27533934], [3.653756708287001, 1.3764967152699399], [5.476474452049073, 6.778844940588858, 7.1081257931620865, 5.147193599475845, 1.6316513411130131, 0.3292808525732287]]\n",
      "2021-01-14 17:09:12,154 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:09:12,155 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:09:12,158 : INFO : built Dictionary(173 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 358 corpus positions)\n",
      "2021-01-14 17:09:12,354 : INFO : token count processed\n",
      "2021-01-14 17:09:12,421 : INFO : frequencies processed\n",
      "2021-01-14 17:09:22,531 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:09:22,532 : INFO : entropies processed\n",
      "2021-01-14 17:09:22,533 : INFO : extropies processed\n",
      "2021-01-14 17:09:22,541 : INFO : token count processed\n",
      "2021-01-14 17:09:22,545 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:09:22,550 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:09:22,550 : INFO : vocab #32006\n",
      "2021-01-14 17:09:22,557 : INFO : diff #set()\n",
      "2021-01-14 17:09:42,656 : INFO : alphabet #32006\n",
      "2021-01-14 17:09:52,681 : INFO : Computed distances or similarities ('278', 'sacp-python-common/setup.py')[[1.0408846827021507, 0.4899835882329179], [0.6658536195755005, 0.33414638], [3.9102329406731844, 1.3813510649520628], [5.476474452049073, 6.469677430851302, 6.898473473186276, 5.047678409714099, 1.4219990211372027, 0.4287960423349739]]\n",
      "2021-01-14 17:09:52,686 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:09:52,687 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:09:52,691 : INFO : built Dictionary(241 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1260 corpus positions)\n",
      "2021-01-14 17:09:52,998 : INFO : token count processed\n",
      "2021-01-14 17:09:53,064 : INFO : frequencies processed\n",
      "2021-01-14 17:10:02,852 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:10:02,853 : INFO : entropies processed\n",
      "2021-01-14 17:10:02,854 : INFO : extropies processed\n",
      "2021-01-14 17:10:02,861 : INFO : token count processed\n",
      "2021-01-14 17:10:02,865 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:10:02,869 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:10:02,870 : INFO : vocab #32006\n",
      "2021-01-14 17:10:02,876 : INFO : diff #set()\n",
      "2021-01-14 17:10:22,771 : INFO : alphabet #32006\n",
      "2021-01-14 17:10:32,720 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.0483136545596754, 0.488206480376644], [0.6140566468238831, 0.38594335], [3.719658289928746, 1.3503477609863974], [5.476474452049073, 6.459180448028249, 6.734586280269518, 5.201068619807804, 1.2581118282204447, 0.2754058322412689]]\n",
      "2021-01-14 17:10:32,723 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:10:32,724 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:10:32,726 : INFO : built Dictionary(155 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 410 corpus positions)\n",
      "2021-01-14 17:10:32,888 : INFO : token count processed\n",
      "2021-01-14 17:10:32,958 : INFO : frequencies processed\n",
      "2021-01-14 17:10:42,990 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:10:42,991 : INFO : entropies processed\n",
      "2021-01-14 17:10:42,992 : INFO : extropies processed\n",
      "2021-01-14 17:10:43,003 : INFO : token count processed\n",
      "2021-01-14 17:10:43,007 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:10:43,014 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:10:43,014 : INFO : vocab #32006\n",
      "2021-01-14 17:10:43,023 : INFO : diff #set()\n",
      "2021-01-14 17:11:03,405 : INFO : alphabet #32006\n",
      "2021-01-14 17:11:13,416 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.0366890206842192, 0.49099297430495953], [0.6197656989097595, 0.3802343], [3.6369210852794294, 1.3555390408583659], [5.476474452049073, 6.097125733496388, 6.542270824565382, 5.031329360980081, 1.0657963725163082, 0.44514509106899336]]\n",
      "2021-01-14 17:11:13,419 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:11:13,420 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:11:13,422 : INFO : built Dictionary(146 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 406 corpus positions)\n",
      "2021-01-14 17:11:13,551 : INFO : token count processed\n",
      "2021-01-14 17:11:13,577 : INFO : frequencies processed\n",
      "2021-01-14 17:11:23,086 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:11:23,087 : INFO : entropies processed\n",
      "2021-01-14 17:11:23,088 : INFO : extropies processed\n",
      "2021-01-14 17:11:23,097 : INFO : token count processed\n",
      "2021-01-14 17:11:23,101 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:11:23,106 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:11:23,106 : INFO : vocab #32006\n",
      "2021-01-14 17:11:23,113 : INFO : diff #set()\n",
      "2021-01-14 17:11:42,217 : INFO : alphabet #32006\n",
      "2021-01-14 17:11:52,026 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.0258685617275702, 0.49361543926978396], [0.6158289909362793, 0.384171], [3.748373334400346, 1.3641995608057327], [5.476474452049073, 6.0695858597523715, 6.503106221645436, 5.042954090156008, 1.0266317695963627, 0.43352036189306453]]\n",
      "2021-01-14 17:11:52,029 : INFO : Removed 1 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:11:52,030 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:11:52,035 : INFO : built Dictionary(148 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 502 corpus positions)\n",
      "2021-01-14 17:11:52,185 : INFO : token count processed\n",
      "2021-01-14 17:11:52,250 : INFO : frequencies processed\n",
      "2021-01-14 17:12:02,280 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:12:02,282 : INFO : entropies processed\n",
      "2021-01-14 17:12:02,283 : INFO : extropies processed\n",
      "2021-01-14 17:12:02,293 : INFO : token count processed\n",
      "2021-01-14 17:12:02,298 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:12:02,304 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:12:02,305 : INFO : vocab #32006\n",
      "2021-01-14 17:12:02,311 : INFO : diff #set()\n",
      "2021-01-14 17:12:22,256 : INFO : alphabet #32006\n",
      "2021-01-14 17:12:32,234 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[0.9829623167312868, 0.5042960179134413], [0.5414629280567169, 0.45853707], [3.4627184877075803, 1.3270934925234743], [5.476474452049073, 6.104787343210121, 6.4614272453599195, 5.1198345498992754, 0.9849527933108462, 0.3566399021497988]]\n",
      "2021-01-14 17:12:32,251 : INFO : Removed 1 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:12:32,252 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:12:32,255 : INFO : built Dictionary(421 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 9187 corpus positions)\n",
      "2021-01-14 17:12:32,922 : INFO : token count processed\n",
      "2021-01-14 17:12:32,969 : INFO : frequencies processed\n",
      "2021-01-14 17:12:43,166 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:12:43,168 : INFO : entropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 17:12:43,170 : INFO : extropies processed\n",
      "2021-01-14 17:12:43,186 : INFO : token count processed\n",
      "2021-01-14 17:12:43,191 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:12:43,195 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:12:43,196 : INFO : vocab #32006\n",
      "2021-01-14 17:12:43,203 : INFO : diff #set()\n",
      "2021-01-14 17:13:03,319 : INFO : alphabet #32006\n",
      "2021-01-14 17:13:13,452 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.0231934784028909, 0.4942681017286593], [0.5980076193809509, 0.40199238], [4.475274610914669, 1.3871864967153362], [5.476474452049073, 6.89087415148015, 7.065372907685031, 5.301975695844192, 1.5888984556359578, 0.17449875620488076]]\n",
      "2021-01-14 17:13:13,459 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:13:13,460 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:13:13,465 : INFO : built Dictionary(289 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 2347 corpus positions)\n",
      "2021-01-14 17:13:13,828 : INFO : token count processed\n",
      "2021-01-14 17:13:13,897 : INFO : frequencies processed\n",
      "2021-01-14 17:13:23,863 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:13:23,863 : INFO : entropies processed\n",
      "2021-01-14 17:13:23,864 : INFO : extropies processed\n",
      "2021-01-14 17:13:23,873 : INFO : token count processed\n",
      "2021-01-14 17:13:23,878 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:13:23,882 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:13:23,883 : INFO : vocab #32006\n",
      "2021-01-14 17:13:23,890 : INFO : diff #set()\n",
      "2021-01-14 17:13:43,768 : INFO : alphabet #32006\n",
      "2021-01-14 17:13:53,519 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.0122802528773616, 0.4969486723184304], [0.550160676240921, 0.44983932], [3.9197698362566356, 1.3629140767367767], [5.476474452049073, 6.655493573668506, 6.856956806453535, 5.275011219264044, 1.380482354404462, 0.20146323278502898]]\n",
      "2021-01-14 17:13:53,524 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:13:53,525 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:13:53,527 : INFO : built Dictionary(268 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1354 corpus positions)\n",
      "2021-01-14 17:13:53,863 : INFO : token count processed\n",
      "2021-01-14 17:13:53,894 : INFO : frequencies processed\n",
      "2021-01-14 17:14:03,779 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:14:03,780 : INFO : entropies processed\n",
      "2021-01-14 17:14:03,781 : INFO : extropies processed\n",
      "2021-01-14 17:14:03,788 : INFO : token count processed\n",
      "2021-01-14 17:14:03,793 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:14:03,797 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:14:03,797 : INFO : vocab #32006\n",
      "2021-01-14 17:14:03,804 : INFO : diff #set()\n",
      "2021-01-14 17:14:23,807 : INFO : alphabet #32006\n",
      "2021-01-14 17:14:33,871 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.0507787385530551, 0.4876196447723847], [0.6704026460647583, 0.32959735], [4.0746987351738495, 1.3886686623995295], [5.476474452049073, 6.6236746347295465, 6.864117407014331, 5.236031679764289, 1.3876429549652576, 0.2404427722847844]]\n",
      "2021-01-14 17:14:33,876 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:14:33,877 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:14:33,882 : INFO : built Dictionary(272 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1293 corpus positions)\n",
      "2021-01-14 17:14:34,220 : INFO : token count processed\n",
      "2021-01-14 17:14:34,273 : INFO : frequencies processed\n",
      "2021-01-14 17:14:44,330 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:14:44,331 : INFO : entropies processed\n",
      "2021-01-14 17:14:44,332 : INFO : extropies processed\n",
      "2021-01-14 17:14:44,342 : INFO : token count processed\n",
      "2021-01-14 17:14:44,347 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:14:44,352 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:14:44,353 : INFO : vocab #32006\n",
      "2021-01-14 17:14:44,359 : INFO : diff #set()\n",
      "2021-01-14 17:15:04,407 : INFO : alphabet #32006\n",
      "2021-01-14 17:15:14,437 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.079591121455566, 0.48086375715052626], [0.6061827540397644, 0.39381725], [3.4214419452073273, 1.3189036505737253], [5.476474452049073, 6.75472436518627, 6.988929384313362, 5.242269432921981, 1.5124549322642888, 0.23420501912709213]]\n",
      "2021-01-14 17:15:14,441 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:15:14,442 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:15:14,446 : INFO : built Dictionary(225 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1088 corpus positions)\n",
      "2021-01-14 17:15:14,714 : INFO : token count processed\n",
      "2021-01-14 17:15:14,747 : INFO : frequencies processed\n",
      "2021-01-14 17:15:24,651 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:15:24,652 : INFO : entropies processed\n",
      "2021-01-14 17:15:24,653 : INFO : extropies processed\n",
      "2021-01-14 17:15:24,660 : INFO : token count processed\n",
      "2021-01-14 17:15:24,664 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:15:24,668 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:15:24,669 : INFO : vocab #32006\n",
      "2021-01-14 17:15:24,675 : INFO : diff #set()\n",
      "2021-01-14 17:15:44,689 : INFO : alphabet #32006\n",
      "2021-01-14 17:15:54,759 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.0915215535915956, 0.47812081988004534], [0.6693695187568665, 0.33063048], [3.503352977363976, 1.3365701600722482], [5.476474452049073, 6.597313085495733, 6.863877671850041, 5.209909865694765, 1.3874032198009676, 0.266564586354308]]\n",
      "2021-01-14 17:15:54,764 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:15:54,765 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:15:54,768 : INFO : built Dictionary(247 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1054 corpus positions)\n",
      "2021-01-14 17:15:55,091 : INFO : token count processed\n",
      "2021-01-14 17:15:55,124 : INFO : frequencies processed\n",
      "2021-01-14 17:16:05,354 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:16:05,355 : INFO : entropies processed\n",
      "2021-01-14 17:16:05,356 : INFO : extropies processed\n",
      "2021-01-14 17:16:05,366 : INFO : token count processed\n",
      "2021-01-14 17:16:05,370 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:16:05,375 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:16:05,376 : INFO : vocab #32006\n",
      "2021-01-14 17:16:05,383 : INFO : diff #set()\n",
      "2021-01-14 17:16:25,731 : INFO : alphabet #32006\n",
      "2021-01-14 17:16:35,544 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.0713750374658024, 0.48277109741721963], [0.6162214875221252, 0.3837785], [3.6775671571169286, 1.3442859118726953], [5.476474452049073, 6.659481538516613, 6.912352583782554, 5.223603406783133, 1.4358781317334808, 0.25287104526594106]]\n",
      "2021-01-14 17:16:35,548 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:16:35,550 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:16:35,551 : INFO : built Dictionary(273 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1216 corpus positions)\n",
      "2021-01-14 17:16:35,898 : INFO : token count processed\n",
      "2021-01-14 17:16:35,932 : INFO : frequencies processed\n",
      "2021-01-14 17:16:45,910 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:16:45,911 : INFO : entropies processed\n",
      "2021-01-14 17:16:45,912 : INFO : extropies processed\n",
      "2021-01-14 17:16:45,923 : INFO : token count processed\n",
      "2021-01-14 17:16:45,927 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:16:45,932 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:16:45,933 : INFO : vocab #32006\n",
      "2021-01-14 17:16:45,939 : INFO : diff #set()\n",
      "2021-01-14 17:17:05,969 : INFO : alphabet #32006\n",
      "2021-01-14 17:17:16,005 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.0511960425019282, 0.4875204413812435], [0.5948666930198669, 0.4051333], [3.5696528421062865, 1.335115231542813], [5.476474452049073, 6.774682571479102, 6.9863107323074605, 5.2648462912207155, 1.5098362802583871, 0.21162816082835878]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 17:17:16,020 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:17:16,021 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:17:16,024 : INFO : built Dictionary(436 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 7953 corpus positions)\n",
      "2021-01-14 17:17:16,739 : INFO : token count processed\n",
      "2021-01-14 17:17:16,769 : INFO : frequencies processed\n",
      "2021-01-14 17:17:26,811 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:17:26,812 : INFO : entropies processed\n",
      "2021-01-14 17:17:26,813 : INFO : extropies processed\n",
      "2021-01-14 17:17:26,823 : INFO : token count processed\n",
      "2021-01-14 17:17:26,827 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:17:26,832 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:17:26,833 : INFO : vocab #32006\n",
      "2021-01-14 17:17:26,839 : INFO : diff #set()\n",
      "2021-01-14 17:17:46,750 : INFO : alphabet #32006\n",
      "2021-01-14 17:17:56,438 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.047180792190214, 0.4884766425197511], [0.6435566842556, 0.35644332], [4.5237727540025, 1.3889218271055592], [5.476474452049073, 6.839453716525233, 7.0084839391640354, 5.307444229410271, 1.532009487114962, 0.16903022263880274]]\n",
      "2021-01-14 17:17:56,444 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:17:56,445 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:17:56,447 : INFO : built Dictionary(349 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 2395 corpus positions)\n",
      "2021-01-14 17:17:56,939 : INFO : token count processed\n",
      "2021-01-14 17:17:56,970 : INFO : frequencies processed\n",
      "2021-01-14 17:18:06,833 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:18:06,834 : INFO : entropies processed\n",
      "2021-01-14 17:18:06,835 : INFO : extropies processed\n",
      "2021-01-14 17:18:06,842 : INFO : token count processed\n",
      "2021-01-14 17:18:06,847 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:18:06,851 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:18:06,852 : INFO : vocab #32006\n",
      "2021-01-14 17:18:06,859 : INFO : diff #set()\n",
      "2021-01-14 17:18:27,076 : INFO : alphabet #32006\n",
      "2021-01-14 17:18:37,309 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.0144741656392677, 0.4964074581133498], [0.5414240956306458, 0.4585759], [3.7196582899287454, 1.3503477609863974], [5.476474452049073, 6.86432793886027, 7.038135527488536, 5.302666863420806, 1.5616610754394626, 0.17380758862826617]]\n",
      "2021-01-14 17:18:37,313 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:18:37,314 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:18:37,315 : INFO : built Dictionary(185 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 597 corpus positions)\n",
      "2021-01-14 17:18:37,502 : INFO : token count processed\n",
      "2021-01-14 17:18:37,535 : INFO : frequencies processed\n",
      "2021-01-14 17:18:47,519 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:18:47,520 : INFO : entropies processed\n",
      "2021-01-14 17:18:47,520 : INFO : extropies processed\n",
      "2021-01-14 17:18:47,527 : INFO : token count processed\n",
      "2021-01-14 17:18:47,531 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:18:47,535 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:18:47,536 : INFO : vocab #32006\n",
      "2021-01-14 17:18:47,542 : INFO : diff #set()\n",
      "2021-01-14 17:19:07,721 : INFO : alphabet #32006\n",
      "2021-01-14 17:19:17,711 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.0560873077542055, 0.48636066971896547], [0.6430545151233673, 0.35694548], [3.6595365985012456, 1.35296350698535], [5.476474452049073, 6.431978396403875, 6.769571391632439, 5.13888145682051, 1.293096939583366, 0.33759299522856434]]\n",
      "2021-01-14 17:19:17,715 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:19:17,716 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:19:17,717 : INFO : built Dictionary(240 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 751 corpus positions)\n",
      "2021-01-14 17:19:18,007 : INFO : token count processed\n",
      "2021-01-14 17:19:18,037 : INFO : frequencies processed\n",
      "2021-01-14 17:19:27,684 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:19:27,685 : INFO : entropies processed\n",
      "2021-01-14 17:19:27,685 : INFO : extropies processed\n",
      "2021-01-14 17:19:27,693 : INFO : token count processed\n",
      "2021-01-14 17:19:27,697 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:19:27,702 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:19:27,702 : INFO : vocab #32006\n",
      "2021-01-14 17:19:27,709 : INFO : diff #set()\n",
      "2021-01-14 17:19:47,809 : INFO : alphabet #32006\n",
      "2021-01-14 17:19:57,835 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/test_auth_utility.py')[[1.0785425622045377, 0.4811063377693757], [0.6750340163707733, 0.32496598], [4.195816471537619, 1.3955728918158388], [5.476474452049073, 6.911818353685893, 7.172983231576032, 5.215309574158935, 1.6965087795269582, 0.2611648778901383]]\n",
      "2021-01-14 17:19:57,848 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:19:57,849 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:19:57,852 : INFO : built Dictionary(332 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 7281 corpus positions)\n",
      "2021-01-14 17:19:58,310 : INFO : token count processed\n",
      "2021-01-14 17:19:58,344 : INFO : frequencies processed\n",
      "2021-01-14 17:20:08,298 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:20:08,299 : INFO : entropies processed\n",
      "2021-01-14 17:20:08,299 : INFO : extropies processed\n",
      "2021-01-14 17:20:08,309 : INFO : token count processed\n",
      "2021-01-14 17:20:08,313 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:20:08,318 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:20:08,319 : INFO : vocab #32006\n",
      "2021-01-14 17:20:08,325 : INFO : diff #set()\n",
      "2021-01-14 17:20:28,341 : INFO : alphabet #32006\n",
      "2021-01-14 17:20:38,006 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.0640009641671964, 0.48449589770588597], [0.6613142490386963, 0.33868575], [3.7527243339694953, 1.3443638017099313], [5.476474452049073, 6.363791471162389, 6.439171129291269, 5.401094793920192, 0.9626966772421959, 0.07537965812888014]]\n",
      "2021-01-14 17:20:38,011 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:20:38,012 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:20:38,016 : INFO : built Dictionary(233 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1351 corpus positions)\n",
      "2021-01-14 17:20:38,322 : INFO : token count processed\n",
      "2021-01-14 17:20:38,401 : INFO : frequencies processed\n",
      "2021-01-14 17:20:48,353 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:20:48,354 : INFO : entropies processed\n",
      "2021-01-14 17:20:48,355 : INFO : extropies processed\n",
      "2021-01-14 17:20:48,366 : INFO : token count processed\n",
      "2021-01-14 17:20:48,372 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:20:48,378 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:20:48,378 : INFO : vocab #32006\n",
      "2021-01-14 17:20:48,385 : INFO : diff #set()\n",
      "2021-01-14 17:21:08,255 : INFO : alphabet #32006\n",
      "2021-01-14 17:21:18,410 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.0612412319732953, 0.4851445742925812], [0.7066351473331451, 0.29336485], [4.103727665980415, 1.39063483057316], [5.476474452049073, 6.29000629755059, 6.640235626413041, 5.126245123186623, 1.1637611743639678, 0.35022932886245073]]\n",
      "2021-01-14 17:21:18,415 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:21:18,416 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:21:18,421 : INFO : built Dictionary(241 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1304 corpus positions)\n",
      "2021-01-14 17:21:18,743 : INFO : token count processed\n",
      "2021-01-14 17:21:18,824 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 17:21:28,951 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:21:28,952 : INFO : entropies processed\n",
      "2021-01-14 17:21:28,953 : INFO : extropies processed\n",
      "2021-01-14 17:21:28,961 : INFO : token count processed\n",
      "2021-01-14 17:21:28,965 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:21:28,970 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:21:28,970 : INFO : vocab #32006\n",
      "2021-01-14 17:21:28,977 : INFO : diff #set()\n",
      "2021-01-14 17:21:49,238 : INFO : alphabet #32006\n",
      "2021-01-14 17:21:59,277 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.0502117606255905, 0.48775449405034405], [0.6344352960586548, 0.3655647], [3.9019469592513696, 1.3590390474816045], [5.476474452049073, 6.361621244785958, 6.671444575445853, 5.166651121389179, 1.1949701233967795, 0.30982333065989476]]\n",
      "2021-01-14 17:21:59,282 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:21:59,283 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:21:59,284 : INFO : built Dictionary(253 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1327 corpus positions)\n",
      "2021-01-14 17:21:59,614 : INFO : token count processed\n",
      "2021-01-14 17:21:59,685 : INFO : frequencies processed\n",
      "2021-01-14 17:22:09,788 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:22:09,789 : INFO : entropies processed\n",
      "2021-01-14 17:22:09,792 : INFO : extropies processed\n",
      "2021-01-14 17:22:09,800 : INFO : token count processed\n",
      "2021-01-14 17:22:09,806 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:22:09,811 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:22:09,813 : INFO : vocab #32006\n",
      "2021-01-14 17:22:09,822 : INFO : diff #set()\n",
      "2021-01-14 17:22:29,596 : INFO : alphabet #32006\n",
      "2021-01-14 17:22:39,452 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.0667994372285026, 0.48383988401940003], [0.6951169967651367, 0.304883], [3.5280643115840484, 1.3195869225508257], [5.476474452049073, 6.620594433343389, 6.950789767248733, 5.14627911814373, 1.47431531519966, 0.33019533390534406]]\n",
      "2021-01-14 17:22:39,457 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:22:39,458 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:22:39,460 : INFO : built Dictionary(225 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1420 corpus positions)\n",
      "2021-01-14 17:22:39,730 : INFO : token count processed\n",
      "2021-01-14 17:22:39,762 : INFO : frequencies processed\n",
      "2021-01-14 17:22:49,690 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:22:49,691 : INFO : entropies processed\n",
      "2021-01-14 17:22:49,692 : INFO : extropies processed\n",
      "2021-01-14 17:22:49,701 : INFO : token count processed\n",
      "2021-01-14 17:22:49,706 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:22:49,710 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:22:49,710 : INFO : vocab #32006\n",
      "2021-01-14 17:22:49,717 : INFO : diff #set()\n",
      "2021-01-14 17:23:09,855 : INFO : alphabet #32006\n",
      "2021-01-14 17:23:19,870 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.0276832300723657, 0.4931736797785279], [0.5341978073120117, 0.4658022], [3.5001530166977126, 1.3213654520518203], [5.476474452049073, 6.207411496248084, 6.460607388940032, 5.223278559357125, 0.9841329368909584, 0.25319589269194775]]\n",
      "2021-01-14 17:23:19,873 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:23:19,874 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:23:19,875 : INFO : built Dictionary(171 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 324 corpus positions)\n",
      "2021-01-14 17:23:20,037 : INFO : token count processed\n",
      "2021-01-14 17:23:20,070 : INFO : frequencies processed\n",
      "2021-01-14 17:23:30,005 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:23:30,006 : INFO : entropies processed\n",
      "2021-01-14 17:23:30,007 : INFO : extropies processed\n",
      "2021-01-14 17:23:30,014 : INFO : token count processed\n",
      "2021-01-14 17:23:30,019 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:23:30,023 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:23:30,024 : INFO : vocab #32006\n",
      "2021-01-14 17:23:30,031 : INFO : diff #set()\n",
      "2021-01-14 17:23:50,142 : INFO : alphabet #32006\n",
      "2021-01-14 17:23:59,767 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.1173377113394054, 0.4722912148801292], [0.7394686341285706, 0.26053137], [3.521640636343319, 1.3740281872300928], [5.476474452049073, 6.5805228788529595, 7.0124520325864825, 5.044545298315549, 1.5359775805374092, 0.43192915373352303]]\n",
      "2021-01-14 17:23:59,771 : INFO : Removed 1 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:23:59,772 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:23:59,777 : INFO : built Dictionary(246 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1285 corpus positions)\n",
      "2021-01-14 17:24:00,099 : INFO : token count processed\n",
      "2021-01-14 17:24:00,182 : INFO : frequencies processed\n",
      "2021-01-14 17:24:10,057 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:24:10,058 : INFO : entropies processed\n",
      "2021-01-14 17:24:10,059 : INFO : extropies processed\n",
      "2021-01-14 17:24:10,067 : INFO : token count processed\n",
      "2021-01-14 17:24:10,072 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:24:10,076 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:24:10,076 : INFO : vocab #32006\n",
      "2021-01-14 17:24:10,083 : INFO : diff #set()\n",
      "2021-01-14 17:24:30,370 : INFO : alphabet #32006\n",
      "2021-01-14 17:24:40,379 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.0821416342275825, 0.48027472462072573], [0.691061943769455, 0.30893806], [4.220167681284828, 1.3971779875729124], [5.476474452049073, 6.422089779976135, 6.660570764285764, 5.237993467739444, 1.184096312236691, 0.23848098430962938]]\n",
      "2021-01-14 17:24:40,384 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:24:40,385 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:24:40,388 : INFO : built Dictionary(259 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1567 corpus positions)\n",
      "2021-01-14 17:24:40,713 : INFO : token count processed\n",
      "2021-01-14 17:24:40,781 : INFO : frequencies processed\n",
      "2021-01-14 17:24:50,783 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:24:50,784 : INFO : entropies processed\n",
      "2021-01-14 17:24:50,785 : INFO : extropies processed\n",
      "2021-01-14 17:24:50,792 : INFO : token count processed\n",
      "2021-01-14 17:24:50,797 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:24:50,801 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:24:50,802 : INFO : vocab #32006\n",
      "2021-01-14 17:24:50,809 : INFO : diff #set()\n",
      "2021-01-14 17:25:10,981 : INFO : alphabet #32006\n",
      "2021-01-14 17:25:20,974 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.0794809903270886, 0.48088922411486273], [0.7165100574493408, 0.28348994], [3.5868878409958134, 1.3208729438700497], [5.476474452049073, 6.485445644653597, 6.840240120493811, 5.121679976208859, 1.3637656684447377, 0.3547944758402144]]\n",
      "2021-01-14 17:25:20,979 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:25:20,980 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:25:20,981 : INFO : built Dictionary(235 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1453 corpus positions)\n",
      "2021-01-14 17:25:21,270 : INFO : token count processed\n",
      "2021-01-14 17:25:21,339 : INFO : frequencies processed\n",
      "2021-01-14 17:25:31,304 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:25:31,305 : INFO : entropies processed\n",
      "2021-01-14 17:25:31,306 : INFO : extropies processed\n",
      "2021-01-14 17:25:31,314 : INFO : token count processed\n",
      "2021-01-14 17:25:31,318 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:25:31,323 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:25:31,324 : INFO : vocab #32006\n",
      "2021-01-14 17:25:31,330 : INFO : diff #set()\n",
      "2021-01-14 17:25:51,412 : INFO : alphabet #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 17:26:01,475 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.0304442681389787, 0.4925030525051341], [0.5397606194019318, 0.46023938], [3.5001530166977126, 1.3213654520518203], [5.476474452049073, 6.2276600107346916, 6.475964202859533, 5.228170259924232, 0.9994897508104597, 0.24830419212484145]]\n",
      "2021-01-14 17:26:01,480 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:26:01,481 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:26:01,482 : INFO : built Dictionary(225 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1651 corpus positions)\n",
      "2021-01-14 17:26:01,746 : INFO : token count processed\n",
      "2021-01-14 17:26:01,817 : INFO : frequencies processed\n",
      "2021-01-14 17:26:12,003 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:26:12,004 : INFO : entropies processed\n",
      "2021-01-14 17:26:12,005 : INFO : extropies processed\n",
      "2021-01-14 17:26:12,012 : INFO : token count processed\n",
      "2021-01-14 17:26:12,016 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:26:12,020 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:26:12,021 : INFO : vocab #32006\n",
      "2021-01-14 17:26:12,028 : INFO : diff #set()\n",
      "2021-01-14 17:26:32,092 : INFO : alphabet #32006\n",
      "2021-01-14 17:26:42,173 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.0466015267641968, 0.4886148998340003], [0.6361749768257141, 0.36382502], [3.5735256625754146, 1.3327666510207348], [5.476474452049073, 6.253918170574241, 6.541351706919432, 5.189040915703881, 1.0648772548703587, 0.2874335363451914]]\n",
      "2021-01-14 17:26:42,177 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:26:42,178 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:26:42,179 : INFO : built Dictionary(202 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 748 corpus positions)\n",
      "2021-01-14 17:26:42,410 : INFO : token count processed\n",
      "2021-01-14 17:26:42,481 : INFO : frequencies processed\n",
      "2021-01-14 17:26:52,604 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:26:52,605 : INFO : entropies processed\n",
      "2021-01-14 17:26:52,606 : INFO : extropies processed\n",
      "2021-01-14 17:26:52,613 : INFO : token count processed\n",
      "2021-01-14 17:26:52,618 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:26:52,622 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:26:52,622 : INFO : vocab #32006\n",
      "2021-01-14 17:26:52,628 : INFO : diff #set()\n",
      "2021-01-14 17:27:12,206 : INFO : alphabet #32006\n",
      "2021-01-14 17:27:22,197 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.0651563684621506, 0.48422483414399503], [0.6564624905586243, 0.3435375], [3.003690046478698, 1.2719205212181601], [5.476474452049073, 6.374522245625576, 6.758440378887081, 5.092556318787568, 1.2819659268380077, 0.38391813326150537]]\n",
      "2021-01-14 17:27:22,203 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:27:22,204 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:27:22,205 : INFO : built Dictionary(317 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 2030 corpus positions)\n",
      "2021-01-14 17:27:22,632 : INFO : token count processed\n",
      "2021-01-14 17:27:22,665 : INFO : frequencies processed\n",
      "2021-01-14 17:27:32,728 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:27:32,729 : INFO : entropies processed\n",
      "2021-01-14 17:27:32,730 : INFO : extropies processed\n",
      "2021-01-14 17:27:32,737 : INFO : token count processed\n",
      "2021-01-14 17:27:32,742 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:27:32,746 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:27:32,747 : INFO : vocab #32006\n",
      "2021-01-14 17:27:32,754 : INFO : diff #set()\n",
      "2021-01-14 17:27:52,744 : INFO : alphabet #32006\n",
      "2021-01-14 17:28:02,910 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.0972655009606527, 0.47681135246917944], [0.7425828278064728, 0.25741717], [3.6971597234241487, 1.332562212601185], [5.476474452049073, 6.731238669067808, 7.088261964356458, 5.119451156760424, 1.6117875123073846, 0.3570232952886503]]\n",
      "2021-01-14 17:28:02,915 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:28:02,916 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:28:02,921 : INFO : built Dictionary(243 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1422 corpus positions)\n",
      "2021-01-14 17:28:03,239 : INFO : token count processed\n",
      "2021-01-14 17:28:03,321 : INFO : frequencies processed\n",
      "2021-01-14 17:28:13,422 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:28:13,424 : INFO : entropies processed\n",
      "2021-01-14 17:28:13,424 : INFO : extropies processed\n",
      "2021-01-14 17:28:13,432 : INFO : token count processed\n",
      "2021-01-14 17:28:13,437 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:28:13,441 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:28:13,442 : INFO : vocab #32006\n",
      "2021-01-14 17:28:13,448 : INFO : diff #set()\n",
      "2021-01-14 17:28:33,410 : INFO : alphabet #32006\n",
      "2021-01-14 17:28:43,427 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.0701105532011455, 0.48306598816842683], [0.6664920449256897, 0.33350796], [3.6820861928883817, 1.3423903004683018], [5.476474452049073, 6.503741451859337, 6.79905089887224, 5.181165005036171, 1.3225764468231667, 0.29530944701290274]]\n",
      "2021-01-14 17:28:43,432 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:28:43,433 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:28:43,435 : INFO : built Dictionary(262 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 1744 corpus positions)\n",
      "2021-01-14 17:28:43,777 : INFO : token count processed\n",
      "2021-01-14 17:28:43,846 : INFO : frequencies processed\n",
      "2021-01-14 17:28:53,941 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:28:53,942 : INFO : entropies processed\n",
      "2021-01-14 17:28:53,944 : INFO : extropies processed\n",
      "2021-01-14 17:28:53,954 : INFO : token count processed\n",
      "2021-01-14 17:28:53,960 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:28:53,965 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:28:53,966 : INFO : vocab #32006\n",
      "2021-01-14 17:28:53,972 : INFO : diff #set()\n",
      "2021-01-14 17:29:14,567 : INFO : alphabet #32006\n",
      "2021-01-14 17:29:24,536 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.058498298838715, 0.48579102570264054], [0.6163038909435272, 0.3836961], [3.90194695925137, 1.3590390474816048], [5.476474452049073, 6.334729224484471, 6.559320328045084, 5.251883348488461, 1.0828458759960107, 0.22459110356061274]]\n",
      "2021-01-14 17:29:24,542 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:29:24,543 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:29:24,547 : INFO : built Dictionary(259 unique tokens: ['#', '(', ')', '-', '.']...) from 2 documents (total 2093 corpus positions)\n",
      "2021-01-14 17:29:24,866 : INFO : token count processed\n",
      "2021-01-14 17:29:24,898 : INFO : frequencies processed\n",
      "2021-01-14 17:29:34,898 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:29:34,899 : INFO : entropies processed\n",
      "2021-01-14 17:29:34,900 : INFO : extropies processed\n",
      "2021-01-14 17:29:34,908 : INFO : token count processed\n",
      "2021-01-14 17:29:34,912 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:29:34,917 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:29:34,918 : INFO : vocab #32006\n",
      "2021-01-14 17:29:34,924 : INFO : diff #set()\n",
      "2021-01-14 17:29:54,911 : INFO : alphabet #32006\n",
      "2021-01-14 17:30:04,846 : INFO : Computed distances or similarities ('278', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.0408785973491823, 0.4899850492326496], [0.6693634986877441, 0.3306365], [3.896626673055183, 1.356938883773511], [5.476474452049073, 6.21319712067992, 6.497311024941682, 5.192360547787311, 1.020836572892609, 0.284113904261762]]\n",
      "2021-01-14 17:30:04,851 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 17:30:04,852 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:30:04,853 : INFO : built Dictionary(296 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1457 corpus positions)\n",
      "2021-01-14 17:30:05,196 : INFO : token count processed\n",
      "2021-01-14 17:30:05,229 : INFO : frequencies processed\n",
      "2021-01-14 17:30:15,143 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:30:15,144 : INFO : entropies processed\n",
      "2021-01-14 17:30:15,145 : INFO : extropies processed\n",
      "2021-01-14 17:30:15,153 : INFO : token count processed\n",
      "2021-01-14 17:30:15,157 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:30:15,162 : INFO : alphabet_target #32010\n",
      "2021-01-14 17:30:15,162 : INFO : vocab #32006\n",
      "2021-01-14 17:30:15,169 : INFO : diff #set()\n",
      "2021-01-14 17:30:35,154 : INFO : alphabet #32006\n",
      "2021-01-14 17:30:45,092 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.1381845129581087, 0.4676864853990232], [0.7428025901317596, 0.2571974], [3.6163485660751635, 1.3692552630709707], [5.463113392440269, 6.905617163738059, 7.144218936535156, 5.224511619643172, 1.681105544094887, 0.2386017727970966]]\n",
      "2021-01-14 17:30:45,098 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:30:45,100 : INFO : built Dictionary(380 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 2347 corpus positions)\n",
      "2021-01-14 17:30:45,644 : INFO : token count processed\n",
      "2021-01-14 17:30:45,678 : INFO : frequencies processed\n",
      "2021-01-14 17:30:55,842 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:30:55,843 : INFO : entropies processed\n",
      "2021-01-14 17:30:55,844 : INFO : extropies processed\n",
      "2021-01-14 17:30:55,855 : INFO : token count processed\n",
      "2021-01-14 17:30:55,860 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:30:55,867 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:30:55,867 : INFO : vocab #32006\n",
      "2021-01-14 17:30:55,876 : INFO : diff #set()\n",
      "2021-01-14 17:31:15,850 : INFO : alphabet #32006\n",
      "2021-01-14 17:31:25,765 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.1418242973281767, 0.4668917059384619], [0.76193006336689, 0.23806994], [4.208966082694623, 1.3952508126504846], [5.463113392440269, 7.1219284286457345, 7.395662606172797, 5.189379214913206, 1.9325492137325284, 0.2737341775270625]]\n",
      "2021-01-14 17:31:25,771 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:31:25,774 : INFO : built Dictionary(306 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 2330 corpus positions)\n",
      "2021-01-14 17:31:26,137 : INFO : token count processed\n",
      "2021-01-14 17:31:26,180 : INFO : frequencies processed\n",
      "2021-01-14 17:31:36,098 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:31:36,099 : INFO : entropies processed\n",
      "2021-01-14 17:31:36,100 : INFO : extropies processed\n",
      "2021-01-14 17:31:36,111 : INFO : token count processed\n",
      "2021-01-14 17:31:36,116 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:31:36,121 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:31:36,121 : INFO : vocab #32006\n",
      "2021-01-14 17:31:36,128 : INFO : diff #set()\n",
      "2021-01-14 17:31:55,635 : INFO : alphabet #32006\n",
      "2021-01-14 17:32:05,660 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.214459406797374, 0.4515774806846578], [0.8499571532011032, 0.15004285], [3.6143694458867563, 1.3676145452014115], [5.463113392440269, 6.41099024988467, 6.574908481596854, 5.299195160728086, 1.111795089156585, 0.16391823171218345]]\n",
      "2021-01-14 17:32:05,664 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:32:05,666 : INFO : built Dictionary(187 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 729 corpus positions)\n",
      "2021-01-14 17:32:05,856 : INFO : token count processed\n",
      "2021-01-14 17:32:05,892 : INFO : frequencies processed\n",
      "2021-01-14 17:32:15,983 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:32:15,984 : INFO : entropies processed\n",
      "2021-01-14 17:32:15,987 : INFO : extropies processed\n",
      "2021-01-14 17:32:15,995 : INFO : token count processed\n",
      "2021-01-14 17:32:16,004 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:32:16,011 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:32:16,012 : INFO : vocab #32006\n",
      "2021-01-14 17:32:16,020 : INFO : diff #set()\n",
      "2021-01-14 17:32:36,107 : INFO : alphabet #32006\n",
      "2021-01-14 17:32:46,292 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.1639745832107602, 0.46211263651547474], [0.8103443384170532, 0.18965566], [3.2402239289418517, 1.3431603347450127], [5.463113392440269, 6.077866832717642, 6.4419696071624, 5.09901061799551, 0.9788562147221311, 0.364102774444758]]\n",
      "2021-01-14 17:32:46,295 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:32:46,297 : INFO : built Dictionary(163 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 494 corpus positions)\n",
      "2021-01-14 17:32:46,443 : INFO : token count processed\n",
      "2021-01-14 17:32:46,475 : INFO : frequencies processed\n",
      "2021-01-14 17:32:56,469 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:32:56,470 : INFO : entropies processed\n",
      "2021-01-14 17:32:56,471 : INFO : extropies processed\n",
      "2021-01-14 17:32:56,478 : INFO : token count processed\n",
      "2021-01-14 17:32:56,482 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:32:56,487 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:32:56,487 : INFO : vocab #32006\n",
      "2021-01-14 17:32:56,494 : INFO : diff #set()\n",
      "2021-01-14 17:33:16,582 : INFO : alphabet #32006\n",
      "2021-01-14 17:33:26,756 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.159774833327404, 0.463011229026766], [0.8043037056922913, 0.1956963], [3.2402239289418517, 1.3431603347450127], [5.463113392440269, 5.977547459003844, 6.394247627083693, 5.046413224360419, 0.9311342346434248, 0.4167001680798492]]\n",
      "2021-01-14 17:33:26,762 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:33:26,764 : INFO : built Dictionary(264 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 2205 corpus positions)\n",
      "2021-01-14 17:33:27,063 : INFO : token count processed\n",
      "2021-01-14 17:33:27,096 : INFO : frequencies processed\n",
      "2021-01-14 17:33:37,099 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:33:37,100 : INFO : entropies processed\n",
      "2021-01-14 17:33:37,101 : INFO : extropies processed\n",
      "2021-01-14 17:33:37,112 : INFO : token count processed\n",
      "2021-01-14 17:33:37,117 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:33:37,122 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:33:37,122 : INFO : vocab #32006\n",
      "2021-01-14 17:33:37,129 : INFO : diff #set()\n",
      "2021-01-14 17:33:57,058 : INFO : alphabet #32006\n",
      "2021-01-14 17:34:07,213 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.1930016982433944, 0.4559959989091687], [0.8294805437326431, 0.17051946], [3.721928094887362, 1.3747546725700366], [5.463113392440269, 6.4614394051846435, 6.6581557553385675, 5.266397042286345, 1.195042362898299, 0.19671635015392397]]\n",
      "2021-01-14 17:34:07,217 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:34:07,218 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:34:07,220 : INFO : built Dictionary(234 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1429 corpus positions)\n",
      "2021-01-14 17:34:07,455 : INFO : token count processed\n",
      "2021-01-14 17:34:07,485 : INFO : frequencies processed\n",
      "2021-01-14 17:34:17,462 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:34:17,463 : INFO : entropies processed\n",
      "2021-01-14 17:34:17,464 : INFO : extropies processed\n",
      "2021-01-14 17:34:17,472 : INFO : token count processed\n",
      "2021-01-14 17:34:17,476 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:34:17,481 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:34:17,481 : INFO : vocab #32006\n",
      "2021-01-14 17:34:17,488 : INFO : diff #set()\n",
      "2021-01-14 17:34:37,533 : INFO : alphabet #32006\n",
      "2021-01-14 17:34:47,597 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.162531701620712, 0.46242096670793253], [0.769097164273262, 0.23090284], [2.584962500721156, 1.2886455778445765], [5.463113392440269, 6.327195724598159, 6.581584634457288, 5.20872448258114, 1.1184712420170193, 0.254388909859129]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 17:34:47,609 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:34:47,610 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:34:47,612 : INFO : built Dictionary(432 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 6321 corpus positions)\n",
      "2021-01-14 17:34:48,216 : INFO : token count processed\n",
      "2021-01-14 17:34:48,250 : INFO : frequencies processed\n",
      "2021-01-14 17:34:58,232 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:34:58,233 : INFO : entropies processed\n",
      "2021-01-14 17:34:58,233 : INFO : extropies processed\n",
      "2021-01-14 17:34:58,243 : INFO : token count processed\n",
      "2021-01-14 17:34:58,247 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:34:58,252 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:34:58,253 : INFO : vocab #32006\n",
      "2021-01-14 17:34:58,259 : INFO : diff #set()\n",
      "2021-01-14 17:35:18,459 : INFO : alphabet #32006\n",
      "2021-01-14 17:35:28,415 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.1551789684478888, 0.4639985888133353], [0.7910432666540146, 0.20895673], [4.426733681937771, 1.4015494230942118], [5.463113392440269, 6.9079058562486315, 7.0432514011515, 5.327767847537399, 1.5801380087112316, 0.13534554490286865]]\n",
      "2021-01-14 17:35:28,422 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:35:28,424 : INFO : built Dictionary(340 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 2715 corpus positions)\n",
      "2021-01-14 17:35:28,853 : INFO : token count processed\n",
      "2021-01-14 17:35:28,886 : INFO : frequencies processed\n",
      "2021-01-14 17:35:38,882 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:35:38,883 : INFO : entropies processed\n",
      "2021-01-14 17:35:38,884 : INFO : extropies processed\n",
      "2021-01-14 17:35:38,892 : INFO : token count processed\n",
      "2021-01-14 17:35:38,897 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:35:38,901 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:35:38,902 : INFO : vocab #32006\n",
      "2021-01-14 17:35:38,908 : INFO : diff #set()\n",
      "2021-01-14 17:35:59,007 : INFO : alphabet #32006\n",
      "2021-01-14 17:36:09,090 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.092413766461035, 0.47791694741682544], [0.7352588176727295, 0.26474118], [4.0536606896881855, 1.388863945760654], [5.463113392440269, 6.61034830706307, 6.826811905222003, 5.2466497942813355, 1.3636985127817347, 0.21646359815893312]]\n",
      "2021-01-14 17:36:09,094 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:36:09,095 : INFO : built Dictionary(233 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 762 corpus positions)\n",
      "2021-01-14 17:36:09,342 : INFO : token count processed\n",
      "2021-01-14 17:36:09,375 : INFO : frequencies processed\n",
      "2021-01-14 17:36:19,413 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:36:19,414 : INFO : entropies processed\n",
      "2021-01-14 17:36:19,415 : INFO : extropies processed\n",
      "2021-01-14 17:36:19,423 : INFO : token count processed\n",
      "2021-01-14 17:36:19,428 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:36:19,433 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:36:19,434 : INFO : vocab #32006\n",
      "2021-01-14 17:36:19,440 : INFO : diff #set()\n",
      "2021-01-14 17:36:39,532 : INFO : alphabet #32006\n",
      "2021-01-14 17:36:49,516 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.1373470264169494, 0.4678697411512069], [0.7527952939271927, 0.2472047], [3.350209029099897, 1.3531670961856495], [5.463113392440269, 6.616715366949855, 6.911679162220482, 5.168149597169641, 1.4485657697802132, 0.2949637952706272]]\n",
      "2021-01-14 17:36:49,522 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:36:49,525 : INFO : built Dictionary(442 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 2796 corpus positions)\n",
      "2021-01-14 17:36:50,221 : INFO : token count processed\n",
      "2021-01-14 17:36:50,267 : INFO : frequencies processed\n",
      "2021-01-14 17:37:00,331 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:37:00,332 : INFO : entropies processed\n",
      "2021-01-14 17:37:00,333 : INFO : extropies processed\n",
      "2021-01-14 17:37:00,341 : INFO : token count processed\n",
      "2021-01-14 17:37:00,346 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:37:00,350 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:37:00,351 : INFO : vocab #32006\n",
      "2021-01-14 17:37:00,358 : INFO : diff #set()\n",
      "2021-01-14 17:37:20,631 : INFO : alphabet #32006\n",
      "2021-01-14 17:37:30,556 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.1042038514534875, 0.4752391263371398], [0.7044226229190826, 0.29557738], [4.294246013232288, 1.3969801158968538], [5.463113392440269, 7.32185870753746, 7.559871172710254, 5.225100927267475, 2.0967577802699857, 0.23801246517279395]]\n",
      "2021-01-14 17:37:30,559 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:37:30,561 : INFO : built Dictionary(96 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 161 corpus positions)\n",
      "2021-01-14 17:37:30,615 : INFO : token count processed\n",
      "2021-01-14 17:37:30,648 : INFO : frequencies processed\n",
      "2021-01-14 17:37:40,759 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:37:40,760 : INFO : entropies processed\n",
      "2021-01-14 17:37:40,760 : INFO : extropies processed\n",
      "2021-01-14 17:37:40,767 : INFO : token count processed\n",
      "2021-01-14 17:37:40,771 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:37:40,775 : INFO : alphabet_target #32008\n",
      "2021-01-14 17:37:40,776 : INFO : vocab #32006\n",
      "2021-01-14 17:37:40,782 : INFO : diff #set()\n",
      "2021-01-14 17:38:01,039 : INFO : alphabet #32006\n",
      "2021-01-14 17:38:11,180 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/fireException.py')[[1.1614066361228192, 0.46266166823371235], [0.7886741757392883, 0.21132582], [0.7219280948873623, 0.7219280948873623], [5.463113392440269, 5.176618657501385, 6.245123661756737, 4.394608388184916, 0.7820102693164683, 1.0685050042553517]]\n",
      "2021-01-14 17:38:11,184 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:38:11,185 : INFO : built Dictionary(186 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 554 corpus positions)\n",
      "2021-01-14 17:38:11,356 : INFO : token count processed\n",
      "2021-01-14 17:38:11,388 : INFO : frequencies processed\n",
      "2021-01-14 17:38:21,279 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:38:21,280 : INFO : entropies processed\n",
      "2021-01-14 17:38:21,280 : INFO : extropies processed\n",
      "2021-01-14 17:38:21,288 : INFO : token count processed\n",
      "2021-01-14 17:38:21,292 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:38:21,297 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:38:21,298 : INFO : vocab #32006\n",
      "2021-01-14 17:38:21,304 : INFO : diff #set()\n",
      "2021-01-14 17:38:41,311 : INFO : alphabet #32006\n",
      "2021-01-14 17:38:51,389 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.1730109966675564, 0.4601909523392015], [0.8012743592262268, 0.19872564], [2.7321588913645702, 1.2899803216866634], [5.463113392440269, 6.468846789852156, 6.885903765888662, 5.046056416403763, 1.422790373448393, 0.4170569760365055]]\n",
      "2021-01-14 17:38:51,396 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:38:51,398 : INFO : built Dictionary(391 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 2603 corpus positions)\n",
      "2021-01-14 17:38:52,000 : INFO : token count processed\n",
      "2021-01-14 17:38:52,033 : INFO : frequencies processed\n",
      "2021-01-14 17:39:01,946 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:39:01,948 : INFO : entropies processed\n",
      "2021-01-14 17:39:01,948 : INFO : extropies processed\n",
      "2021-01-14 17:39:01,956 : INFO : token count processed\n",
      "2021-01-14 17:39:01,961 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:39:01,965 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:39:01,966 : INFO : vocab #32006\n",
      "2021-01-14 17:39:01,973 : INFO : diff #set()\n",
      "2021-01-14 17:39:22,203 : INFO : alphabet #32006\n",
      "2021-01-14 17:39:32,289 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.1860757630909926, 0.4574406875020901], [0.8113445788621902, 0.18865542], [4.134336113194451, 1.3926122728818413], [5.463113392440269, 6.957796704012729, 7.159432354702195, 5.261477741750802, 1.6963189622619268, 0.20163565068946632]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 17:39:32,296 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:39:32,299 : INFO : built Dictionary(307 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 3100 corpus positions)\n",
      "2021-01-14 17:39:32,665 : INFO : token count processed\n",
      "2021-01-14 17:39:32,698 : INFO : frequencies processed\n",
      "2021-01-14 17:39:42,599 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:39:42,600 : INFO : entropies processed\n",
      "2021-01-14 17:39:42,600 : INFO : extropies processed\n",
      "2021-01-14 17:39:42,609 : INFO : token count processed\n",
      "2021-01-14 17:39:42,613 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:39:42,618 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:39:42,619 : INFO : vocab #32006\n",
      "2021-01-14 17:39:42,625 : INFO : diff #set()\n",
      "2021-01-14 17:40:02,810 : INFO : alphabet #32006\n",
      "2021-01-14 17:40:12,794 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.1961907764107467, 0.4553338492907745], [0.8356447368860245, 0.16435526], [3.499227547132693, 1.3607849614768985], [5.463113392440269, 6.441859572014148, 6.662900334504805, 5.242072629949611, 1.1997869420645362, 0.22104076249065674]]\n",
      "2021-01-14 17:40:12,799 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:40:12,801 : INFO : built Dictionary(326 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1576 corpus positions)\n",
      "2021-01-14 17:40:13,221 : INFO : token count processed\n",
      "2021-01-14 17:40:13,250 : INFO : frequencies processed\n",
      "2021-01-14 17:40:23,219 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:40:23,220 : INFO : entropies processed\n",
      "2021-01-14 17:40:23,221 : INFO : extropies processed\n",
      "2021-01-14 17:40:23,229 : INFO : token count processed\n",
      "2021-01-14 17:40:23,234 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:40:23,238 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:40:23,238 : INFO : vocab #32006\n",
      "2021-01-14 17:40:23,245 : INFO : diff #set()\n",
      "2021-01-14 17:40:42,400 : INFO : alphabet #32006\n",
      "2021-01-14 17:40:51,828 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.124789140389828, 0.470634935481896], [0.7416602671146393, 0.25833973], [4.134336113194451, 1.3926122728818413], [5.463113392440269, 6.998955278238291, 7.237214494044018, 5.22485417663454, 1.7741011016037493, 0.2382592158057273]]\n",
      "2021-01-14 17:40:51,833 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:40:51,835 : INFO : built Dictionary(245 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1395 corpus positions)\n",
      "2021-01-14 17:40:52,114 : INFO : token count processed\n",
      "2021-01-14 17:40:52,145 : INFO : frequencies processed\n",
      "2021-01-14 17:41:01,573 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:41:01,574 : INFO : entropies processed\n",
      "2021-01-14 17:41:01,575 : INFO : extropies processed\n",
      "2021-01-14 17:41:01,582 : INFO : token count processed\n",
      "2021-01-14 17:41:01,589 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:41:01,594 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:41:01,595 : INFO : vocab #32006\n",
      "2021-01-14 17:41:01,602 : INFO : diff #set()\n",
      "2021-01-14 17:41:20,637 : INFO : alphabet #32006\n",
      "2021-01-14 17:41:30,076 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.166003813605434, 0.4616797042178076], [0.8056232929229736, 0.1943767], [3.8268748818646374, 1.377696421466828], [5.463113392440269, 6.492983191376071, 6.728914440774185, 5.227182143042155, 1.2658010483339162, 0.2359312493981136]]\n",
      "2021-01-14 17:41:30,083 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:41:30,086 : INFO : built Dictionary(460 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 3334 corpus positions)\n",
      "2021-01-14 17:41:30,729 : INFO : token count processed\n",
      "2021-01-14 17:41:30,795 : INFO : frequencies processed\n",
      "2021-01-14 17:41:40,127 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:41:40,128 : INFO : entropies processed\n",
      "2021-01-14 17:41:40,129 : INFO : extropies processed\n",
      "2021-01-14 17:41:40,143 : INFO : token count processed\n",
      "2021-01-14 17:41:40,147 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:41:40,152 : INFO : alphabet_target #32008\n",
      "2021-01-14 17:41:40,152 : INFO : vocab #32006\n",
      "2021-01-14 17:41:40,159 : INFO : diff #set()\n",
      "2021-01-14 17:41:59,136 : INFO : alphabet #32006\n",
      "2021-01-14 17:42:08,560 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.1976269192747573, 0.45503628993132816], [0.8466300517320633, 0.15336995], [3.2776134368191157, 1.3618978811135465], [5.463113392440269, 6.560342487747443, 6.823297910603003, 5.200157969584708, 1.3601845181627343, 0.2629554228555602]]\n",
      "2021-01-14 17:42:08,567 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:42:08,568 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:42:08,571 : INFO : built Dictionary(465 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 3539 corpus positions)\n",
      "2021-01-14 17:42:09,244 : INFO : token count processed\n",
      "2021-01-14 17:42:09,276 : INFO : frequencies processed\n",
      "2021-01-14 17:42:18,796 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:42:18,798 : INFO : entropies processed\n",
      "2021-01-14 17:42:18,799 : INFO : extropies processed\n",
      "2021-01-14 17:42:18,806 : INFO : token count processed\n",
      "2021-01-14 17:42:18,811 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:42:18,815 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:42:18,816 : INFO : vocab #32006\n",
      "2021-01-14 17:42:18,824 : INFO : diff #set()\n",
      "2021-01-14 17:42:37,737 : INFO : alphabet #32006\n",
      "2021-01-14 17:42:47,192 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.1060576655872232, 0.47482080682780076], [0.7342119216918945, 0.26578808], [4.2817276788697365, 1.3981745317517609], [5.463113392440269, 7.046173750105238, 7.274359264930478, 5.2349278776150285, 1.8112458724902094, 0.22818551482524008]]\n",
      "2021-01-14 17:42:47,203 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:42:47,204 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:42:47,207 : INFO : built Dictionary(515 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 5648 corpus positions)\n",
      "2021-01-14 17:42:48,074 : INFO : token count processed\n",
      "2021-01-14 17:42:48,145 : INFO : frequencies processed\n",
      "2021-01-14 17:42:57,861 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:42:57,862 : INFO : entropies processed\n",
      "2021-01-14 17:42:57,863 : INFO : extropies processed\n",
      "2021-01-14 17:42:57,878 : INFO : token count processed\n",
      "2021-01-14 17:42:57,883 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:42:57,887 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:42:57,888 : INFO : vocab #32006\n",
      "2021-01-14 17:42:57,895 : INFO : diff #set()\n",
      "2021-01-14 17:43:16,770 : INFO : alphabet #32006\n",
      "2021-01-14 17:43:26,359 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.1387138743086873, 0.4675707265064793], [0.7616177946329117, 0.2383822], [4.448860987743579, 1.4005289188368233], [5.463113392440269, 7.009229588004272, 7.14382557688708, 5.328517403557461, 1.6807121844468114, 0.13459598888280766]]\n",
      "2021-01-14 17:43:26,372 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:43:26,375 : INFO : built Dictionary(596 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 6589 corpus positions)\n",
      "2021-01-14 17:43:27,479 : INFO : token count processed\n",
      "2021-01-14 17:43:27,509 : INFO : frequencies processed\n",
      "2021-01-14 17:43:37,291 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:43:37,292 : INFO : entropies processed\n",
      "2021-01-14 17:43:37,293 : INFO : extropies processed\n",
      "2021-01-14 17:43:37,302 : INFO : token count processed\n",
      "2021-01-14 17:43:37,306 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:43:37,312 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:43:37,314 : INFO : vocab #32006\n",
      "2021-01-14 17:43:37,322 : INFO : diff #set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 17:43:56,240 : INFO : alphabet #32006\n",
      "2021-01-14 17:44:05,815 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.149594958110748, 0.46520391956952084], [0.7458382546901703, 0.25416175], [4.318867931899243, 1.3958576521449182], [5.463113392440269, 7.376088004590871, 7.562695304449055, 5.276506092582085, 2.0995819120087864, 0.18660729985818403]]\n",
      "2021-01-14 17:44:05,818 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:44:05,820 : INFO : built Dictionary(164 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 375 corpus positions)\n",
      "2021-01-14 17:44:05,956 : INFO : token count processed\n",
      "2021-01-14 17:44:05,987 : INFO : frequencies processed\n",
      "2021-01-14 17:44:15,415 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:44:15,416 : INFO : entropies processed\n",
      "2021-01-14 17:44:15,417 : INFO : extropies processed\n",
      "2021-01-14 17:44:15,423 : INFO : token count processed\n",
      "2021-01-14 17:44:15,430 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:44:15,435 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:44:15,436 : INFO : vocab #32006\n",
      "2021-01-14 17:44:15,443 : INFO : diff #set()\n",
      "2021-01-14 17:44:34,642 : INFO : alphabet #32006\n",
      "2021-01-14 17:44:44,079 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.161204706881092, 0.4627048964015695], [0.782554492354393, 0.21744551], [2.2810361125534233, 1.2263316211630273], [5.463113392440269, 6.2993628166120885, 6.785500295119488, 4.976975913932869, 1.3223869026792192, 0.4861374785073993]]\n",
      "2021-01-14 17:44:44,082 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:44:44,083 : INFO : built Dictionary(65 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 91 corpus positions)\n",
      "2021-01-14 17:44:44,108 : INFO : token count processed\n",
      "2021-01-14 17:44:44,140 : INFO : frequencies processed\n",
      "2021-01-14 17:44:53,590 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:44:53,591 : INFO : entropies processed\n",
      "2021-01-14 17:44:53,592 : INFO : extropies processed\n",
      "2021-01-14 17:44:53,599 : INFO : token count processed\n",
      "2021-01-14 17:44:53,603 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:44:53,607 : INFO : alphabet_target #32008\n",
      "2021-01-14 17:44:53,608 : INFO : vocab #32006\n",
      "2021-01-14 17:44:53,615 : INFO : diff #set()\n",
      "2021-01-14 17:45:12,666 : INFO : alphabet #32006\n",
      "2021-01-14 17:45:22,166 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.1790797442680088, 0.45890931831680976], [0.7457115352153778, 0.25428846], [0.9182958340544896, 0.9182958340544896], [5.463113392440269, 3.8936606896881862, 5.6991107851375915, 3.6576632969908633, 0.2359973926973229, 1.8054500954494053]]\n",
      "2021-01-14 17:45:22,188 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:45:22,193 : INFO : built Dictionary(753 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 12533 corpus positions)\n",
      "2021-01-14 17:45:23,795 : INFO : token count processed\n",
      "2021-01-14 17:45:23,828 : INFO : frequencies processed\n",
      "2021-01-14 17:45:33,267 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:45:33,268 : INFO : entropies processed\n",
      "2021-01-14 17:45:33,269 : INFO : extropies processed\n",
      "2021-01-14 17:45:33,285 : INFO : token count processed\n",
      "2021-01-14 17:45:33,290 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:45:33,295 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:45:33,296 : INFO : vocab #32006\n",
      "2021-01-14 17:45:33,302 : INFO : diff #set()\n",
      "2021-01-14 17:45:52,349 : INFO : alphabet #32006\n",
      "2021-01-14 17:46:01,768 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.133160653441434, 0.46878794543050345], [0.7341833412647247, 0.26581666], [4.442844501396511, 1.3997975240243834], [5.463113392440269, 7.434393313070278, 7.652780159402152, 5.244726546108396, 2.1896667669618832, 0.21838684633187366]]\n",
      "2021-01-14 17:46:01,776 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:46:01,779 : INFO : built Dictionary(503 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 4160 corpus positions)\n",
      "2021-01-14 17:46:02,558 : INFO : token count processed\n",
      "2021-01-14 17:46:02,588 : INFO : frequencies processed\n",
      "2021-01-14 17:46:11,999 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:46:12,000 : INFO : entropies processed\n",
      "2021-01-14 17:46:12,001 : INFO : extropies processed\n",
      "2021-01-14 17:46:12,016 : INFO : token count processed\n",
      "2021-01-14 17:46:12,021 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:46:12,025 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:46:12,026 : INFO : vocab #32006\n",
      "2021-01-14 17:46:12,033 : INFO : diff #set()\n",
      "2021-01-14 17:46:31,204 : INFO : alphabet #32006\n",
      "2021-01-14 17:46:40,642 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.1045557901664271, 0.4751596534872191], [0.7077058255672455, 0.29229417], [4.512814895472356, 1.4028631438402488], [5.463113392440269, 7.2991514951718255, 7.5135486521849595, 5.248716235427135, 2.050435259744691, 0.214397157013134]]\n",
      "2021-01-14 17:46:40,650 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:46:40,652 : INFO : built Dictionary(474 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 3577 corpus positions)\n",
      "2021-01-14 17:46:41,377 : INFO : token count processed\n",
      "2021-01-14 17:46:41,409 : INFO : frequencies processed\n",
      "2021-01-14 17:46:50,932 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:46:50,934 : INFO : entropies processed\n",
      "2021-01-14 17:46:50,935 : INFO : extropies processed\n",
      "2021-01-14 17:46:50,948 : INFO : token count processed\n",
      "2021-01-14 17:46:50,953 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:46:50,957 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:46:50,958 : INFO : vocab #32006\n",
      "2021-01-14 17:46:50,965 : INFO : diff #set()\n",
      "2021-01-14 17:47:09,846 : INFO : alphabet #32006\n",
      "2021-01-14 17:47:19,278 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.116073408508016, 0.4725733974914755], [0.7312063276767731, 0.26879367], [4.184862968552385, 1.3912378462682995], [5.463113392440269, 7.170319527000998, 7.390449020806647, 5.242983898634618, 1.9273356283663787, 0.22012949380564972]]\n",
      "2021-01-14 17:47:19,282 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:47:19,284 : INFO : built Dictionary(196 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 613 corpus positions)\n",
      "2021-01-14 17:47:19,476 : INFO : token count processed\n",
      "2021-01-14 17:47:19,509 : INFO : frequencies processed\n",
      "2021-01-14 17:47:29,053 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:47:29,054 : INFO : entropies processed\n",
      "2021-01-14 17:47:29,055 : INFO : extropies processed\n",
      "2021-01-14 17:47:29,062 : INFO : token count processed\n",
      "2021-01-14 17:47:29,066 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:47:29,071 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:47:29,072 : INFO : vocab #32006\n",
      "2021-01-14 17:47:29,078 : INFO : diff #set()\n",
      "2021-01-14 17:47:47,963 : INFO : alphabet #32006\n",
      "2021-01-14 17:47:57,388 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.147911579499242, 0.4655685129427614], [0.7724991589784622, 0.22750084], [2.7321588913645702, 1.2899803216866634], [5.463113392440269, 6.353654804387375, 6.767009975649995, 5.049758221177648, 1.3038965832097267, 0.4133551712626202]]\n",
      "2021-01-14 17:47:57,392 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:47:57,394 : INFO : built Dictionary(194 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 741 corpus positions)\n",
      "2021-01-14 17:47:57,588 : INFO : token count processed\n",
      "2021-01-14 17:47:57,623 : INFO : frequencies processed\n",
      "2021-01-14 17:48:07,164 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:48:07,165 : INFO : entropies processed\n",
      "2021-01-14 17:48:07,166 : INFO : extropies processed\n",
      "2021-01-14 17:48:07,173 : INFO : token count processed\n",
      "2021-01-14 17:48:07,181 : INFO : alphabet_source #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 17:48:07,185 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:48:07,186 : INFO : vocab #32006\n",
      "2021-01-14 17:48:07,193 : INFO : diff #set()\n",
      "2021-01-14 17:48:26,091 : INFO : alphabet #32006\n",
      "2021-01-14 17:48:35,643 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.1614403627094607, 0.4626544489742276], [0.7674433588981628, 0.23255664], [2.0, 1.1742577727983858], [5.463113392440269, 6.245180322479091, 6.624496831433783, 5.083796883485578, 1.1613834389935143, 0.37931650895469193]]\n",
      "2021-01-14 17:48:35,648 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:48:35,652 : INFO : built Dictionary(405 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1813 corpus positions)\n",
      "2021-01-14 17:48:36,247 : INFO : token count processed\n",
      "2021-01-14 17:48:36,279 : INFO : frequencies processed\n",
      "2021-01-14 17:48:45,693 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:48:45,694 : INFO : entropies processed\n",
      "2021-01-14 17:48:45,695 : INFO : extropies processed\n",
      "2021-01-14 17:48:45,709 : INFO : token count processed\n",
      "2021-01-14 17:48:45,714 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:48:45,718 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:48:45,719 : INFO : vocab #32006\n",
      "2021-01-14 17:48:45,726 : INFO : diff #set()\n",
      "2021-01-14 17:49:04,629 : INFO : alphabet #32006\n",
      "2021-01-14 17:49:14,173 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.1251509596122913, 0.47055480716646975], [0.7217040956020355, 0.2782959], [4.324848729602135, 1.3986760366017497], [5.463113392440269, 7.2691387000368, 7.528049527387228, 5.20420256508984, 2.0649361349469597, 0.25891082735042836]]\n",
      "2021-01-14 17:49:14,178 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:49:14,180 : INFO : built Dictionary(337 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1650 corpus positions)\n",
      "2021-01-14 17:49:14,637 : INFO : token count processed\n",
      "2021-01-14 17:49:14,697 : INFO : frequencies processed\n",
      "2021-01-14 17:49:24,124 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:49:24,125 : INFO : entropies processed\n",
      "2021-01-14 17:49:24,126 : INFO : extropies processed\n",
      "2021-01-14 17:49:24,134 : INFO : token count processed\n",
      "2021-01-14 17:49:24,139 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:49:24,144 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:49:24,145 : INFO : vocab #32006\n",
      "2021-01-14 17:49:24,152 : INFO : diff #set()\n",
      "2021-01-14 17:49:43,151 : INFO : alphabet #32006\n",
      "2021-01-14 17:49:52,759 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.1355998778733705, 0.4682525085156868], [0.7090984582901001, 0.29090154], [3.7943360657504512, 1.3753985459471023], [5.463113392440269, 7.08857858466988, 7.328486817218778, 5.223205159891371, 1.8653734247785092, 0.23990823254889815]]\n",
      "2021-01-14 17:49:52,762 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:49:52,764 : INFO : built Dictionary(171 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 573 corpus positions)\n",
      "2021-01-14 17:49:52,916 : INFO : token count processed\n",
      "2021-01-14 17:49:52,949 : INFO : frequencies processed\n",
      "2021-01-14 17:50:02,453 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:50:02,454 : INFO : entropies processed\n",
      "2021-01-14 17:50:02,455 : INFO : extropies processed\n",
      "2021-01-14 17:50:02,462 : INFO : token count processed\n",
      "2021-01-14 17:50:02,470 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:50:02,474 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:50:02,475 : INFO : vocab #32006\n",
      "2021-01-14 17:50:02,482 : INFO : diff #set()\n",
      "2021-01-14 17:50:21,392 : INFO : alphabet #32006\n",
      "2021-01-14 17:50:30,833 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.1517875644385558, 0.4647298908714159], [0.7808737605810165, 0.21912624], [2.5219280948873624, 1.2629960611029938], [5.463113392440269, 6.0479231618016716, 6.492340067255487, 5.018696486986453, 1.0292266748152183, 0.4444169054538154]]\n",
      "2021-01-14 17:50:30,837 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:50:30,838 : INFO : built Dictionary(175 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 692 corpus positions)\n",
      "2021-01-14 17:50:31,013 : INFO : token count processed\n",
      "2021-01-14 17:50:31,078 : INFO : frequencies processed\n",
      "2021-01-14 17:50:40,529 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:50:40,530 : INFO : entropies processed\n",
      "2021-01-14 17:50:40,531 : INFO : extropies processed\n",
      "2021-01-14 17:50:40,538 : INFO : token count processed\n",
      "2021-01-14 17:50:40,544 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:50:40,548 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:50:40,549 : INFO : vocab #32006\n",
      "2021-01-14 17:50:40,556 : INFO : diff #set()\n",
      "2021-01-14 17:50:59,564 : INFO : alphabet #32006\n",
      "2021-01-14 17:51:09,012 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.164995261370533, 0.4618947754033226], [0.7746618539094925, 0.22533815], [2.0, 1.1742577727983858], [5.463113392440269, 6.036583168403119, 6.442134620264325, 5.057561940579063, 0.979021227824056, 0.40555145186120534]]\n",
      "2021-01-14 17:51:09,026 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:51:09,027 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:51:09,030 : INFO : built Dictionary(575 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 7050 corpus positions)\n",
      "2021-01-14 17:51:10,069 : INFO : token count processed\n",
      "2021-01-14 17:51:10,110 : INFO : frequencies processed\n",
      "2021-01-14 17:51:19,656 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:51:19,658 : INFO : entropies processed\n",
      "2021-01-14 17:51:19,659 : INFO : extropies processed\n",
      "2021-01-14 17:51:19,668 : INFO : token count processed\n",
      "2021-01-14 17:51:19,673 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:51:19,678 : INFO : alphabet_target #32010\n",
      "2021-01-14 17:51:19,679 : INFO : vocab #32006\n",
      "2021-01-14 17:51:19,686 : INFO : diff #set()\n",
      "2021-01-14 17:51:38,587 : INFO : alphabet #32006\n",
      "2021-01-14 17:51:48,024 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.1773952388042772, 0.4592643458470832], [0.7936456948518753, 0.2063543], [4.162294909570876, 1.3947501691870232], [5.463113392440269, 7.29352035514053, 7.479719651393557, 5.276914096187243, 2.0166062589532885, 0.18619929625302678]]\n",
      "2021-01-14 17:51:48,031 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:51:48,033 : INFO : built Dictionary(384 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 3293 corpus positions)\n",
      "2021-01-14 17:51:48,530 : INFO : token count processed\n",
      "2021-01-14 17:51:48,562 : INFO : frequencies processed\n",
      "2021-01-14 17:51:58,115 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:51:58,116 : INFO : entropies processed\n",
      "2021-01-14 17:51:58,117 : INFO : extropies processed\n",
      "2021-01-14 17:51:58,125 : INFO : token count processed\n",
      "2021-01-14 17:51:58,129 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:51:58,134 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:51:58,135 : INFO : vocab #32006\n",
      "2021-01-14 17:51:58,143 : INFO : diff #set()\n",
      "2021-01-14 17:52:17,047 : INFO : alphabet #32006\n",
      "2021-01-14 17:52:26,494 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.0831305186688267, 0.4800467330482131], [0.7088834345340729, 0.29111657], [4.142295219190901, 1.3940934903110445], [5.463113392440269, 6.8153433747477745, 6.999760291911727, 5.278696475276316, 1.5366468994714584, 0.18441691716395248]]\n",
      "2021-01-14 17:52:26,497 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:52:26,498 : INFO : built Dictionary(149 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 314 corpus positions)\n",
      "2021-01-14 17:52:26,626 : INFO : token count processed\n",
      "2021-01-14 17:52:26,685 : INFO : frequencies processed\n",
      "2021-01-14 17:52:36,252 : INFO : scalar_distribution processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 17:52:36,253 : INFO : entropies processed\n",
      "2021-01-14 17:52:36,253 : INFO : extropies processed\n",
      "2021-01-14 17:52:36,261 : INFO : token count processed\n",
      "2021-01-14 17:52:36,265 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:52:36,269 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:52:36,269 : INFO : vocab #32006\n",
      "2021-01-14 17:52:36,276 : INFO : diff #set()\n",
      "2021-01-14 17:52:55,319 : INFO : alphabet #32006\n",
      "2021-01-14 17:53:04,898 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.1494586951595553, 0.46523341074287056], [0.779887780547142, 0.22011222], [2.2810361125534233, 1.2263316211630275], [5.463113392440269, 6.150121915859574, 6.670100801223791, 4.943134507076051, 1.2069874087835224, 0.5199788853642167]]\n",
      "2021-01-14 17:53:04,902 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:53:04,905 : INFO : built Dictionary(301 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1025 corpus positions)\n",
      "2021-01-14 17:53:05,271 : INFO : token count processed\n",
      "2021-01-14 17:53:05,338 : INFO : frequencies processed\n",
      "2021-01-14 17:53:14,774 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:53:14,775 : INFO : entropies processed\n",
      "2021-01-14 17:53:14,776 : INFO : extropies processed\n",
      "2021-01-14 17:53:14,790 : INFO : token count processed\n",
      "2021-01-14 17:53:14,794 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:53:14,798 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:53:14,799 : INFO : vocab #32006\n",
      "2021-01-14 17:53:14,805 : INFO : diff #set()\n",
      "2021-01-14 17:53:33,831 : INFO : alphabet #32006\n",
      "2021-01-14 17:53:43,270 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.1254305611904427, 0.4704929054186109], [0.707182765007019, 0.29281723], [3.9614063297218425, 1.382632066570991], [5.463113392440269, 7.0391145208191315, 7.3642965214403375, 5.1379313918190626, 1.901183129000069, 0.32518200062120606]]\n",
      "2021-01-14 17:53:43,279 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 17:53:43,280 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:53:43,283 : INFO : built Dictionary(596 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 4400 corpus positions)\n",
      "2021-01-14 17:53:44,419 : INFO : token count processed\n",
      "2021-01-14 17:53:44,484 : INFO : frequencies processed\n",
      "2021-01-14 17:53:53,925 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:53:53,926 : INFO : entropies processed\n",
      "2021-01-14 17:53:53,927 : INFO : extropies processed\n",
      "2021-01-14 17:53:53,941 : INFO : token count processed\n",
      "2021-01-14 17:53:53,946 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:53:53,951 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:53:53,952 : INFO : vocab #32006\n",
      "2021-01-14 17:53:53,958 : INFO : diff #set()\n",
      "2021-01-14 17:54:12,949 : INFO : alphabet #32006\n",
      "2021-01-14 17:54:22,384 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.122858212323196, 0.47106301975091786], [0.7075578272342682, 0.29244217], [4.627089685478835, 1.4063158367854376], [5.463113392440269, 7.482466367279176, 7.771373533820803, 5.17420622589864, 2.3082601413805346, 0.2889071665416276]]\n",
      "2021-01-14 17:54:22,388 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:54:22,389 : INFO : built Dictionary(197 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 622 corpus positions)\n",
      "2021-01-14 17:54:22,594 : INFO : token count processed\n",
      "2021-01-14 17:54:22,622 : INFO : frequencies processed\n",
      "2021-01-14 17:54:31,972 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:54:31,973 : INFO : entropies processed\n",
      "2021-01-14 17:54:31,974 : INFO : extropies processed\n",
      "2021-01-14 17:54:31,987 : INFO : token count processed\n",
      "2021-01-14 17:54:31,991 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:54:31,995 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:54:31,996 : INFO : vocab #32006\n",
      "2021-01-14 17:54:32,003 : INFO : diff #set()\n",
      "2021-01-14 17:54:51,011 : INFO : alphabet #32006\n",
      "2021-01-14 17:55:00,441 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.1663922742045272, 0.4615969194070302], [0.7764524519443512, 0.22354755], [2.5219280948873624, 1.2629960611029938], [5.463113392440269, 6.372162341197667, 6.7867358781417995, 5.048539855496136, 1.3236224857015308, 0.41457353694413257]]\n",
      "2021-01-14 17:55:00,447 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:55:00,449 : INFO : built Dictionary(331 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 2043 corpus positions)\n",
      "2021-01-14 17:55:00,865 : INFO : token count processed\n",
      "2021-01-14 17:55:00,911 : INFO : frequencies processed\n",
      "2021-01-14 17:55:11,425 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:55:11,426 : INFO : entropies processed\n",
      "2021-01-14 17:55:11,426 : INFO : extropies processed\n",
      "2021-01-14 17:55:11,434 : INFO : token count processed\n",
      "2021-01-14 17:55:11,439 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:55:11,444 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:55:11,445 : INFO : vocab #32006\n",
      "2021-01-14 17:55:11,452 : INFO : diff #set()\n",
      "2021-01-14 17:55:30,598 : INFO : alphabet #32006\n",
      "2021-01-14 17:55:40,021 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.1212542777277972, 0.4714192025442419], [0.7486757934093475, 0.2513242], [4.280226253673659, 1.3976476821653485], [5.463113392440269, 6.798155919669889, 7.049267813216937, 5.212001498893222, 1.5861544207766682, 0.25111189354704777]]\n",
      "2021-01-14 17:55:40,025 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:55:40,026 : INFO : built Dictionary(199 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 759 corpus positions)\n",
      "2021-01-14 17:55:40,231 : INFO : token count processed\n",
      "2021-01-14 17:55:40,264 : INFO : frequencies processed\n",
      "2021-01-14 17:55:49,705 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:55:49,707 : INFO : entropies processed\n",
      "2021-01-14 17:55:49,708 : INFO : extropies processed\n",
      "2021-01-14 17:55:49,721 : INFO : token count processed\n",
      "2021-01-14 17:55:49,727 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:55:49,732 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:55:49,733 : INFO : vocab #32006\n",
      "2021-01-14 17:55:49,740 : INFO : diff #set()\n",
      "2021-01-14 17:56:09,040 : INFO : alphabet #32006\n",
      "2021-01-14 17:56:18,475 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.1216851589169405, 0.47132346465131114], [0.7507289201021194, 0.24927108], [3.1068905956085184, 1.3362845045884086], [5.463113392440269, 6.271631856729336, 6.637293966412305, 5.097451282757299, 1.1741805739720368, 0.36566210968296975]]\n",
      "2021-01-14 17:56:18,482 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:56:18,485 : INFO : built Dictionary(367 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 3274 corpus positions)\n",
      "2021-01-14 17:56:19,019 : INFO : token count processed\n",
      "2021-01-14 17:56:19,051 : INFO : frequencies processed\n",
      "2021-01-14 17:56:28,588 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:56:28,589 : INFO : entropies processed\n",
      "2021-01-14 17:56:28,590 : INFO : extropies processed\n",
      "2021-01-14 17:56:28,603 : INFO : token count processed\n",
      "2021-01-14 17:56:28,608 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:56:28,612 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:56:28,613 : INFO : vocab #32006\n",
      "2021-01-14 17:56:28,620 : INFO : diff #set()\n",
      "2021-01-14 17:56:47,494 : INFO : alphabet #32006\n",
      "2021-01-14 17:56:56,917 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.1071828055528985, 0.4745672740707527], [0.7106006741523743, 0.28939933], [4.039211536948156, 1.3857621944819998], [5.463113392440269, 6.873598627629562, 7.069288091034422, 5.267423929035409, 1.6061746985941534, 0.19568946340486004]]\n",
      "2021-01-14 17:56:56,920 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:56:56,922 : INFO : built Dictionary(131 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 222 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 17:56:57,022 : INFO : token count processed\n",
      "2021-01-14 17:56:57,055 : INFO : frequencies processed\n",
      "2021-01-14 17:57:06,607 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:57:06,608 : INFO : entropies processed\n",
      "2021-01-14 17:57:06,609 : INFO : extropies processed\n",
      "2021-01-14 17:57:06,616 : INFO : token count processed\n",
      "2021-01-14 17:57:06,623 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:57:06,628 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:57:06,629 : INFO : vocab #32006\n",
      "2021-01-14 17:57:06,637 : INFO : diff #set()\n",
      "2021-01-14 17:57:25,528 : INFO : alphabet #32006\n",
      "2021-01-14 17:57:34,964 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.1483967138467344, 0.4654633818581327], [0.7641106098890305, 0.23588939], [2.058813890331201, 1.2062416803425784], [5.463113392440269, 6.049830202851529, 6.682250139804415, 4.830693455487383, 1.2191367473641463, 0.6324199369528856]]\n",
      "2021-01-14 17:57:34,969 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:57:34,970 : INFO : built Dictionary(272 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1302 corpus positions)\n",
      "2021-01-14 17:57:35,298 : INFO : token count processed\n",
      "2021-01-14 17:57:35,346 : INFO : frequencies processed\n",
      "2021-01-14 17:57:44,896 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:57:44,897 : INFO : entropies processed\n",
      "2021-01-14 17:57:44,898 : INFO : extropies processed\n",
      "2021-01-14 17:57:44,905 : INFO : token count processed\n",
      "2021-01-14 17:57:44,911 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:57:44,916 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:57:44,917 : INFO : vocab #32006\n",
      "2021-01-14 17:57:44,923 : INFO : diff #set()\n",
      "2021-01-14 17:58:03,828 : INFO : alphabet #32006\n",
      "2021-01-14 17:58:13,380 : INFO : Computed distances or similarities ('277', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.1219596085649126, 0.4712625046978641], [0.705763429403305, 0.29423657], [3.290321609248305, 1.348849529410241], [5.463113392440269, 6.778844940588858, 7.066080097652686, 5.17587823537644, 1.6029667052124177, 0.2872351570638285]]\n",
      "2021-01-14 17:58:13,384 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:58:13,385 : INFO : built Dictionary(175 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 335 corpus positions)\n",
      "2021-01-14 17:58:13,556 : INFO : token count processed\n",
      "2021-01-14 17:58:13,589 : INFO : frequencies processed\n",
      "2021-01-14 17:58:23,011 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:58:23,012 : INFO : entropies processed\n",
      "2021-01-14 17:58:23,013 : INFO : extropies processed\n",
      "2021-01-14 17:58:23,020 : INFO : token count processed\n",
      "2021-01-14 17:58:23,027 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:58:23,031 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:58:23,032 : INFO : vocab #32006\n",
      "2021-01-14 17:58:23,038 : INFO : diff #set()\n",
      "2021-01-14 17:58:41,818 : INFO : alphabet #32006\n",
      "2021-01-14 17:58:51,384 : INFO : Computed distances or similarities ('277', 'sacp-python-common/setup.py')[[1.1327623147012904, 0.46887550155351354], [0.7484712302684784, 0.25152877], [2.751629167387823, 1.2995901901368234], [5.463113392440269, 6.469677430851302, 6.976759652526333, 4.956031170765238, 1.5136462600860643, 0.5070822216750308]]\n",
      "2021-01-14 17:58:51,389 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:58:51,390 : INFO : built Dictionary(236 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1237 corpus positions)\n",
      "2021-01-14 17:58:51,630 : INFO : token count processed\n",
      "2021-01-14 17:58:51,662 : INFO : frequencies processed\n",
      "2021-01-14 17:59:01,472 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:59:01,473 : INFO : entropies processed\n",
      "2021-01-14 17:59:01,474 : INFO : extropies processed\n",
      "2021-01-14 17:59:01,481 : INFO : token count processed\n",
      "2021-01-14 17:59:01,487 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:59:01,492 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:59:01,493 : INFO : vocab #32006\n",
      "2021-01-14 17:59:01,500 : INFO : diff #set()\n",
      "2021-01-14 17:59:20,538 : INFO : alphabet #32006\n",
      "2021-01-14 17:59:29,971 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.1018168208628865, 0.4757788547859546], [0.7366836965084076, 0.2633163], [3.893660689688186, 1.3840535698355407], [5.463113392440269, 6.459180448028249, 6.736194592562683, 5.186099247905835, 1.273081200122414, 0.2770141445344336]]\n",
      "2021-01-14 17:59:29,974 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 17:59:29,975 : INFO : built Dictionary(152 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 387 corpus positions)\n",
      "2021-01-14 17:59:30,115 : INFO : token count processed\n",
      "2021-01-14 17:59:30,149 : INFO : frequencies processed\n",
      "2021-01-14 17:59:39,561 : INFO : scalar_distribution processed\n",
      "2021-01-14 17:59:39,562 : INFO : entropies processed\n",
      "2021-01-14 17:59:39,563 : INFO : extropies processed\n",
      "2021-01-14 17:59:39,569 : INFO : token count processed\n",
      "2021-01-14 17:59:39,576 : INFO : alphabet_source #32006\n",
      "2021-01-14 17:59:39,580 : INFO : alphabet_target #32009\n",
      "2021-01-14 17:59:39,582 : INFO : vocab #32006\n",
      "2021-01-14 17:59:39,589 : INFO : diff #set()\n",
      "2021-01-14 17:59:58,616 : INFO : alphabet #32006\n",
      "2021-01-14 18:00:08,042 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.1253920807519202, 0.47050142374023546], [0.7619051337242126, 0.23809487], [3.2359263506290334, 1.3395174843463549], [5.463113392440269, 6.097125733496388, 6.574646018614603, 4.985593107322055, 1.1115326261743341, 0.47752028511821454]]\n",
      "2021-01-14 18:00:08,045 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:00:08,047 : INFO : built Dictionary(143 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 383 corpus positions)\n",
      "2021-01-14 18:00:08,176 : INFO : token count processed\n",
      "2021-01-14 18:00:08,211 : INFO : frequencies processed\n",
      "2021-01-14 18:00:17,651 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:00:17,652 : INFO : entropies processed\n",
      "2021-01-14 18:00:17,652 : INFO : extropies processed\n",
      "2021-01-14 18:00:17,659 : INFO : token count processed\n",
      "2021-01-14 18:00:17,664 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:00:17,668 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:00:17,669 : INFO : vocab #32006\n",
      "2021-01-14 18:00:17,677 : INFO : diff #set()\n",
      "2021-01-14 18:00:37,061 : INFO : alphabet #32006\n",
      "2021-01-14 18:00:46,489 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.1074566449262804, 0.4745056095970033], [0.7492185235023499, 0.25078148], [3.3735572622751846, 1.3500361649016168], [5.463113392440269, 6.0695858597523715, 6.534578153822448, 4.998121098370191, 1.0714647613821793, 0.46499229407007636]]\n",
      "2021-01-14 18:00:46,493 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 18:00:46,494 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:00:46,495 : INFO : built Dictionary(146 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 479 corpus positions)\n",
      "2021-01-14 18:00:46,627 : INFO : token count processed\n",
      "2021-01-14 18:00:46,661 : INFO : frequencies processed\n",
      "2021-01-14 18:00:56,088 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:00:56,089 : INFO : entropies processed\n",
      "2021-01-14 18:00:56,089 : INFO : extropies processed\n",
      "2021-01-14 18:00:56,096 : INFO : token count processed\n",
      "2021-01-14 18:00:56,100 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:00:56,107 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:00:56,108 : INFO : vocab #32006\n",
      "2021-01-14 18:00:56,115 : INFO : diff #set()\n",
      "2021-01-14 18:01:15,128 : INFO : alphabet #32006\n",
      "2021-01-14 18:01:24,559 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.094865003647919, 0.4773577286644427], [0.7332015037536621, 0.2667985], [3.25, 1.346727543794437], [5.463113392440269, 6.104787343210121, 6.50213480678307, 5.06576592886732, 1.0390214143428018, 0.3973474635729497]]\n",
      "2021-01-14 18:01:24,576 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 18:01:24,577 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:01:24,580 : INFO : built Dictionary(422 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 9164 corpus positions)\n",
      "2021-01-14 18:01:25,183 : INFO : token count processed\n",
      "2021-01-14 18:01:25,215 : INFO : frequencies processed\n",
      "2021-01-14 18:01:34,639 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:01:34,640 : INFO : entropies processed\n",
      "2021-01-14 18:01:34,641 : INFO : extropies processed\n",
      "2021-01-14 18:01:34,651 : INFO : token count processed\n",
      "2021-01-14 18:01:34,658 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:01:34,663 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:01:34,664 : INFO : vocab #32006\n",
      "2021-01-14 18:01:34,672 : INFO : diff #set()\n",
      "2021-01-14 18:01:53,697 : INFO : alphabet #32006\n",
      "2021-01-14 18:02:03,151 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.1300667352363976, 0.4694688591008013], [0.7710284292697906, 0.22897157], [4.35937791471612, 1.4017739024285514], [5.463113392440269, 6.89087415148015, 7.067608715527074, 5.286378828393344, 1.6044953230868053, 0.17673456404692356]]\n",
      "2021-01-14 18:02:03,157 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:02:03,159 : INFO : built Dictionary(286 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 2324 corpus positions)\n",
      "2021-01-14 18:02:03,480 : INFO : token count processed\n",
      "2021-01-14 18:02:03,547 : INFO : frequencies processed\n",
      "2021-01-14 18:02:13,085 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:02:13,086 : INFO : entropies processed\n",
      "2021-01-14 18:02:13,087 : INFO : extropies processed\n",
      "2021-01-14 18:02:13,095 : INFO : token count processed\n",
      "2021-01-14 18:02:13,100 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:02:13,106 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:02:13,107 : INFO : vocab #32006\n",
      "2021-01-14 18:02:13,114 : INFO : diff #set()\n",
      "2021-01-14 18:02:32,137 : INFO : alphabet #32006\n",
      "2021-01-14 18:02:41,573 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.0971689371981614, 0.4768333071612294], [0.6995655596256256, 0.30043444], [3.893660689688186, 1.3840535698355407], [5.463113392440269, 6.655493573668506, 6.860615661550694, 5.257991304558081, 1.3975022691104257, 0.205122087882188]]\n",
      "2021-01-14 18:02:41,578 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:02:41,580 : INFO : built Dictionary(264 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1331 corpus positions)\n",
      "2021-01-14 18:02:41,881 : INFO : token count processed\n",
      "2021-01-14 18:02:41,941 : INFO : frequencies processed\n",
      "2021-01-14 18:02:51,375 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:02:51,376 : INFO : entropies processed\n",
      "2021-01-14 18:02:51,377 : INFO : extropies processed\n",
      "2021-01-14 18:02:51,384 : INFO : token count processed\n",
      "2021-01-14 18:02:51,389 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:02:51,393 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:02:51,394 : INFO : vocab #32006\n",
      "2021-01-14 18:02:51,401 : INFO : diff #set()\n",
      "2021-01-14 18:03:10,495 : INFO : alphabet #32006\n",
      "2021-01-14 18:03:20,044 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.1416624190085856, 0.46692699611497046], [0.7582792490720749, 0.24172075], [3.823067982273661, 1.3806412677662334], [5.463113392440269, 6.6236746347295465, 6.857322783128157, 5.229465244041658, 1.3942093906878883, 0.23364814839861037]]\n",
      "2021-01-14 18:03:20,049 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:03:20,050 : INFO : built Dictionary(262 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1270 corpus positions)\n",
      "2021-01-14 18:03:20,341 : INFO : token count processed\n",
      "2021-01-14 18:03:20,387 : INFO : frequencies processed\n",
      "2021-01-14 18:03:29,816 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:03:29,817 : INFO : entropies processed\n",
      "2021-01-14 18:03:29,818 : INFO : extropies processed\n",
      "2021-01-14 18:03:29,825 : INFO : token count processed\n",
      "2021-01-14 18:03:29,831 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:03:29,836 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:03:29,837 : INFO : vocab #32006\n",
      "2021-01-14 18:03:29,844 : INFO : diff #set()\n",
      "2021-01-14 18:03:48,715 : INFO : alphabet #32006\n",
      "2021-01-14 18:03:58,242 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.0657325505015525, 0.4840897722975821], [0.6422508955001831, 0.3577491], [4.085055102756477, 1.39200869190869], [5.463113392440269, 6.75472436518627, 6.961861339039245, 5.255976418587293, 1.4987479465989768, 0.20713697385297536]]\n",
      "2021-01-14 18:03:58,246 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:03:58,248 : INFO : built Dictionary(219 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1065 corpus positions)\n",
      "2021-01-14 18:03:58,487 : INFO : token count processed\n",
      "2021-01-14 18:03:58,519 : INFO : frequencies processed\n",
      "2021-01-14 18:04:07,924 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:04:07,925 : INFO : entropies processed\n",
      "2021-01-14 18:04:07,926 : INFO : extropies processed\n",
      "2021-01-14 18:04:07,933 : INFO : token count processed\n",
      "2021-01-14 18:04:07,937 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:04:07,941 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:04:07,942 : INFO : vocab #32006\n",
      "2021-01-14 18:04:07,949 : INFO : diff #set()\n",
      "2021-01-14 18:04:26,941 : INFO : alphabet #32006\n",
      "2021-01-14 18:04:36,361 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.1154974208455175, 0.472702065313945], [0.6853380799293518, 0.31466192], [3.621928094887362, 1.3709563519274055], [5.463113392440269, 6.597313085495733, 6.839294619258208, 5.2211318586777935, 1.3761812268179394, 0.2419815337624751]]\n",
      "2021-01-14 18:04:36,365 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:04:36,366 : INFO : built Dictionary(240 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1031 corpus positions)\n",
      "2021-01-14 18:04:36,629 : INFO : token count processed\n",
      "2021-01-14 18:04:36,698 : INFO : frequencies processed\n",
      "2021-01-14 18:04:46,408 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:04:46,410 : INFO : entropies processed\n",
      "2021-01-14 18:04:46,411 : INFO : extropies processed\n",
      "2021-01-14 18:04:46,418 : INFO : token count processed\n",
      "2021-01-14 18:04:46,422 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:04:46,426 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:04:46,427 : INFO : vocab #32006\n",
      "2021-01-14 18:04:46,434 : INFO : diff #set()\n",
      "2021-01-14 18:05:05,470 : INFO : alphabet #32006\n",
      "2021-01-14 18:05:14,901 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.1023304079675393, 0.4756626247758865], [0.7057040333747864, 0.29429597], [3.9736606896881854, 1.3864587577980974], [5.463113392440269, 6.659481538516613, 6.904789512575242, 5.217805418381641, 1.4416761201349733, 0.2453079740586288]]\n",
      "2021-01-14 18:05:14,906 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:05:14,908 : INFO : built Dictionary(268 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1193 corpus positions)\n",
      "2021-01-14 18:05:15,202 : INFO : token count processed\n",
      "2021-01-14 18:05:15,250 : INFO : frequencies processed\n",
      "2021-01-14 18:05:24,686 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:05:24,687 : INFO : entropies processed\n",
      "2021-01-14 18:05:24,687 : INFO : extropies processed\n",
      "2021-01-14 18:05:24,695 : INFO : token count processed\n",
      "2021-01-14 18:05:24,700 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:05:24,705 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:05:24,706 : INFO : vocab #32006\n",
      "2021-01-14 18:05:24,713 : INFO : diff #set()\n",
      "2021-01-14 18:05:43,688 : INFO : alphabet #32006\n",
      "2021-01-14 18:05:53,107 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.1071993449459618, 0.4745635491955055], [0.7174125611782074, 0.28258744], [3.725650756112093, 1.3761145168664612], [5.463113392440269, 6.774682571479102, 6.987561107599314, 5.250234856320057, 1.5244477151590452, 0.21287853612021213]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 18:05:53,122 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:05:53,125 : INFO : built Dictionary(437 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 7930 corpus positions)\n",
      "2021-01-14 18:05:53,836 : INFO : token count processed\n",
      "2021-01-14 18:05:53,868 : INFO : frequencies processed\n",
      "2021-01-14 18:06:03,287 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:06:03,288 : INFO : entropies processed\n",
      "2021-01-14 18:06:03,289 : INFO : extropies processed\n",
      "2021-01-14 18:06:03,298 : INFO : token count processed\n",
      "2021-01-14 18:06:03,302 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:06:03,307 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:06:03,308 : INFO : vocab #32006\n",
      "2021-01-14 18:06:03,314 : INFO : diff #set()\n",
      "2021-01-14 18:06:22,854 : INFO : alphabet #32006\n",
      "2021-01-14 18:06:32,275 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.1466060575834263, 0.46585166219355817], [0.7923751622438431, 0.20762484], [4.4057645846554525, 1.4027976893010505], [5.463113392440269, 6.839453716525233, 7.010882884745437, 5.291684224220064, 1.5477694923051688, 0.1714291682202047]]\n",
      "2021-01-14 18:06:32,281 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:06:32,283 : INFO : built Dictionary(342 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 2372 corpus positions)\n",
      "2021-01-14 18:06:32,707 : INFO : token count processed\n",
      "2021-01-14 18:06:32,739 : INFO : frequencies processed\n",
      "2021-01-14 18:06:42,265 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:06:42,266 : INFO : entropies processed\n",
      "2021-01-14 18:06:42,267 : INFO : extropies processed\n",
      "2021-01-14 18:06:42,274 : INFO : token count processed\n",
      "2021-01-14 18:06:42,278 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:06:42,283 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:06:42,284 : INFO : vocab #32006\n",
      "2021-01-14 18:06:42,293 : INFO : diff #set()\n",
      "2021-01-14 18:07:00,886 : INFO : alphabet #32006\n",
      "2021-01-14 18:07:10,298 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.0771790194917599, 0.48142215505559943], [0.6498824059963226, 0.3501176], [3.974695407915898, 1.3842106220890444], [5.463113392440269, 6.86432793886027, 7.032123148938801, 5.295318182361736, 1.5690097564985326, 0.16779521007853138]]\n",
      "2021-01-14 18:07:10,302 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:07:10,303 : INFO : built Dictionary(182 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 574 corpus positions)\n",
      "2021-01-14 18:07:10,479 : INFO : token count processed\n",
      "2021-01-14 18:07:10,511 : INFO : frequencies processed\n",
      "2021-01-14 18:07:20,046 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:07:20,048 : INFO : entropies processed\n",
      "2021-01-14 18:07:20,049 : INFO : extropies processed\n",
      "2021-01-14 18:07:20,056 : INFO : token count processed\n",
      "2021-01-14 18:07:20,061 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:07:20,066 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:07:20,067 : INFO : vocab #32006\n",
      "2021-01-14 18:07:20,075 : INFO : diff #set()\n",
      "2021-01-14 18:07:38,968 : INFO : alphabet #32006\n",
      "2021-01-14 18:07:48,496 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.0994774573795334, 0.4763089960718853], [0.6609808802604675, 0.33901912], [3.3921472236645345, 1.3581797203736243], [5.463113392440269, 6.431978396403875, 6.769325686254957, 5.125766102589187, 1.3062122938146885, 0.3373472898510821]]\n",
      "2021-01-14 18:07:48,499 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:07:48,501 : INFO : built Dictionary(238 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 728 corpus positions)\n",
      "2021-01-14 18:07:48,756 : INFO : token count processed\n",
      "2021-01-14 18:07:48,802 : INFO : frequencies processed\n",
      "2021-01-14 18:07:58,218 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:07:58,220 : INFO : entropies processed\n",
      "2021-01-14 18:07:58,220 : INFO : extropies processed\n",
      "2021-01-14 18:07:58,227 : INFO : token count processed\n",
      "2021-01-14 18:07:58,233 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:07:58,239 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:07:58,240 : INFO : vocab #32006\n",
      "2021-01-14 18:07:58,247 : INFO : diff #set()\n",
      "2021-01-14 18:08:17,409 : INFO : alphabet #32006\n",
      "2021-01-14 18:08:26,956 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/test_auth_utility.py')[[1.1374048459954014, 0.46785708466675363], [0.7164356708526611, 0.28356433], [3.7256507561120933, 1.376114516866461], [5.463113392440269, 6.911818353685893, 7.173342728013386, 5.2015890181127755, 1.7102293355731177, 0.2615243743274931]]\n",
      "2021-01-14 18:08:26,969 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:08:26,972 : INFO : built Dictionary(332 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 7258 corpus positions)\n",
      "2021-01-14 18:08:27,397 : INFO : token count processed\n",
      "2021-01-14 18:08:27,442 : INFO : frequencies processed\n",
      "2021-01-14 18:08:36,852 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:08:36,853 : INFO : entropies processed\n",
      "2021-01-14 18:08:36,853 : INFO : extropies processed\n",
      "2021-01-14 18:08:36,863 : INFO : token count processed\n",
      "2021-01-14 18:08:36,867 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:08:36,871 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:08:36,872 : INFO : vocab #32006\n",
      "2021-01-14 18:08:36,879 : INFO : diff #set()\n",
      "2021-01-14 18:08:55,892 : INFO : alphabet #32006\n",
      "2021-01-14 18:09:05,422 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.1033430259176533, 0.47543362527075994], [0.7232728898525238, 0.2767271], [3.392147223664534, 1.3581797203736246], [5.463113392440269, 6.363791471162389, 6.432922106610377, 5.39398275699228, 0.9698087141701084, 0.06913063544798792]]\n",
      "2021-01-14 18:09:05,427 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:09:05,429 : INFO : built Dictionary(229 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1328 corpus positions)\n",
      "2021-01-14 18:09:05,673 : INFO : token count processed\n",
      "2021-01-14 18:09:05,718 : INFO : frequencies processed\n",
      "2021-01-14 18:09:15,144 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:09:15,146 : INFO : entropies processed\n",
      "2021-01-14 18:09:15,147 : INFO : extropies processed\n",
      "2021-01-14 18:09:15,158 : INFO : token count processed\n",
      "2021-01-14 18:09:15,163 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:09:15,168 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:09:15,169 : INFO : vocab #32006\n",
      "2021-01-14 18:09:15,175 : INFO : diff #set()\n",
      "2021-01-14 18:09:34,161 : INFO : alphabet #32006\n",
      "2021-01-14 18:09:43,571 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.1029130651707368, 0.4755308322357155], [0.7516774237155914, 0.24832258], [3.795088586397732, 1.378800483023463], [5.463113392440269, 6.29000629755059, 6.616363045252482, 5.136756644738377, 1.1532496528122138, 0.32635674770189205]]\n",
      "2021-01-14 18:09:43,575 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:09:43,577 : INFO : built Dictionary(240 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1281 corpus positions)\n",
      "2021-01-14 18:09:43,845 : INFO : token count processed\n",
      "2021-01-14 18:09:43,877 : INFO : frequencies processed\n",
      "2021-01-14 18:09:53,301 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:09:53,302 : INFO : entropies processed\n",
      "2021-01-14 18:09:53,303 : INFO : extropies processed\n",
      "2021-01-14 18:09:53,314 : INFO : token count processed\n",
      "2021-01-14 18:09:53,319 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:09:53,324 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:09:53,326 : INFO : vocab #32006\n",
      "2021-01-14 18:09:53,332 : INFO : diff #set()\n",
      "2021-01-14 18:10:12,315 : INFO : alphabet #32006\n",
      "2021-01-14 18:10:21,739 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.118955711194472, 0.4719305810484789], [0.7507733404636383, 0.24922666], [3.697845823084412, 1.374237150068622], [5.463113392440269, 6.361621244785958, 6.678414205871665, 5.146320431354562, 1.215300813431396, 0.31679296108570654]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 18:10:21,743 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:10:21,745 : INFO : built Dictionary(248 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1304 corpus positions)\n",
      "2021-01-14 18:10:22,016 : INFO : token count processed\n",
      "2021-01-14 18:10:22,048 : INFO : frequencies processed\n",
      "2021-01-14 18:10:31,625 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:10:31,626 : INFO : entropies processed\n",
      "2021-01-14 18:10:31,627 : INFO : extropies processed\n",
      "2021-01-14 18:10:31,634 : INFO : token count processed\n",
      "2021-01-14 18:10:31,639 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:10:31,643 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:10:31,644 : INFO : vocab #32006\n",
      "2021-01-14 18:10:31,652 : INFO : diff #set()\n",
      "2021-01-14 18:10:50,682 : INFO : alphabet #32006\n",
      "2021-01-14 18:11:00,115 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.0905438942006447, 0.4783444168639986], [0.6853747367858887, 0.31462526], [3.6978458230844113, 1.3742371500686217], [5.463113392440269, 6.620594433343389, 6.9263622008356975, 5.157345624947961, 1.463248808395429, 0.3057677674923083]]\n",
      "2021-01-14 18:11:00,120 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:11:00,121 : INFO : built Dictionary(222 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1397 corpus positions)\n",
      "2021-01-14 18:11:00,344 : INFO : token count processed\n",
      "2021-01-14 18:11:00,389 : INFO : frequencies processed\n",
      "2021-01-14 18:11:10,203 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:11:10,204 : INFO : entropies processed\n",
      "2021-01-14 18:11:10,204 : INFO : extropies processed\n",
      "2021-01-14 18:11:10,212 : INFO : token count processed\n",
      "2021-01-14 18:11:10,219 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:11:10,223 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:11:10,224 : INFO : vocab #32006\n",
      "2021-01-14 18:11:10,231 : INFO : diff #set()\n",
      "2021-01-14 18:11:29,113 : INFO : alphabet #32006\n",
      "2021-01-14 18:11:38,530 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.0931485697013492, 0.4777491738881584], [0.6905438303947449, 0.30945617], [3.484183719779189, 1.363148481046742], [5.463113392440269, 6.207411496248084, 6.449389341523743, 5.221135547164609, 0.986275949083474, 0.2419778452756587]]\n",
      "2021-01-14 18:11:38,533 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:11:38,535 : INFO : built Dictionary(166 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 301 corpus positions)\n",
      "2021-01-14 18:11:38,686 : INFO : token count processed\n",
      "2021-01-14 18:11:38,716 : INFO : frequencies processed\n",
      "2021-01-14 18:11:48,224 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:11:48,225 : INFO : entropies processed\n",
      "2021-01-14 18:11:48,226 : INFO : extropies processed\n",
      "2021-01-14 18:11:48,233 : INFO : token count processed\n",
      "2021-01-14 18:11:48,237 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:11:48,242 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:11:48,243 : INFO : vocab #32006\n",
      "2021-01-14 18:11:48,250 : INFO : diff #set()\n",
      "2021-01-14 18:12:07,290 : INFO : alphabet #32006\n",
      "2021-01-14 18:12:16,735 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.133610264067872, 0.4686891588595157], [0.6748134791851044, 0.32518652], [2.950212064914747, 1.3236480206617924], [5.463113392440269, 6.5805228788529595, 7.001206255854337, 5.042430015438891, 1.538092863414068, 0.42068337700137715]]\n",
      "2021-01-14 18:12:16,740 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 18:12:16,741 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:12:16,742 : INFO : built Dictionary(248 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1262 corpus positions)\n",
      "2021-01-14 18:12:17,005 : INFO : token count processed\n",
      "2021-01-14 18:12:17,036 : INFO : frequencies processed\n",
      "2021-01-14 18:12:26,574 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:12:26,575 : INFO : entropies processed\n",
      "2021-01-14 18:12:26,576 : INFO : extropies processed\n",
      "2021-01-14 18:12:26,583 : INFO : token count processed\n",
      "2021-01-14 18:12:26,590 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:12:26,595 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:12:26,596 : INFO : vocab #32006\n",
      "2021-01-14 18:12:26,603 : INFO : diff #set()\n",
      "2021-01-14 18:12:45,518 : INFO : alphabet #32006\n",
      "2021-01-14 18:12:54,960 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.1441037532577945, 0.46639534046828657], [0.7643266916275024, 0.23567331], [3.077819531114783, 1.3342657629881893], [5.463113392440269, 6.422089779976135, 6.651459064440931, 5.233744107975473, 1.1883456720006622, 0.22936928446479588]]\n",
      "2021-01-14 18:12:54,965 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:12:54,967 : INFO : built Dictionary(254 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1544 corpus positions)\n",
      "2021-01-14 18:12:55,247 : INFO : token count processed\n",
      "2021-01-14 18:12:55,293 : INFO : frequencies processed\n",
      "2021-01-14 18:13:04,827 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:13:04,828 : INFO : entropies processed\n",
      "2021-01-14 18:13:04,829 : INFO : extropies processed\n",
      "2021-01-14 18:13:04,836 : INFO : token count processed\n",
      "2021-01-14 18:13:04,840 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:13:04,845 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:13:04,846 : INFO : vocab #32006\n",
      "2021-01-14 18:13:04,852 : INFO : diff #set()\n",
      "2021-01-14 18:13:23,760 : INFO : alphabet #32006\n",
      "2021-01-14 18:13:33,307 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.1113697592553933, 0.473626183010533], [0.7244580090045929, 0.275542], [3.795088586397732, 1.378800483023463], [5.463113392440269, 6.485445644653597, 6.813404417068828, 5.1351546200250375, 1.350291024628559, 0.3279587724152311]]\n",
      "2021-01-14 18:13:33,312 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:13:33,313 : INFO : built Dictionary(232 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1430 corpus positions)\n",
      "2021-01-14 18:13:33,555 : INFO : token count processed\n",
      "2021-01-14 18:13:33,621 : INFO : frequencies processed\n",
      "2021-01-14 18:13:43,062 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:13:43,064 : INFO : entropies processed\n",
      "2021-01-14 18:13:43,065 : INFO : extropies processed\n",
      "2021-01-14 18:13:43,072 : INFO : token count processed\n",
      "2021-01-14 18:13:43,076 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:13:43,080 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:13:43,082 : INFO : vocab #32006\n",
      "2021-01-14 18:13:43,088 : INFO : diff #set()\n",
      "2021-01-14 18:14:02,277 : INFO : alphabet #32006\n",
      "2021-01-14 18:14:11,704 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.097115939519805, 0.4768453575480328], [0.6933372914791107, 0.3066627], [3.484183719779189, 1.363148481046742], [5.463113392440269, 6.2276600107346916, 6.464664965514842, 5.226108437660118, 1.0015515730745737, 0.2370049547801507]]\n",
      "2021-01-14 18:14:11,709 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:14:11,711 : INFO : built Dictionary(220 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1628 corpus positions)\n",
      "2021-01-14 18:14:11,936 : INFO : token count processed\n",
      "2021-01-14 18:14:11,982 : INFO : frequencies processed\n",
      "2021-01-14 18:14:21,417 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:14:21,418 : INFO : entropies processed\n",
      "2021-01-14 18:14:21,419 : INFO : extropies processed\n",
      "2021-01-14 18:14:21,426 : INFO : token count processed\n",
      "2021-01-14 18:14:21,432 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:14:21,437 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:14:21,439 : INFO : vocab #32006\n",
      "2021-01-14 18:14:21,446 : INFO : diff #set()\n",
      "2021-01-14 18:14:40,463 : INFO : alphabet #32006\n",
      "2021-01-14 18:14:49,895 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.1009316578106005, 0.4759793095992988], [0.7406159937381744, 0.259384], [3.697845823084412, 1.374237150068622], [5.463113392440269, 6.253918170574241, 6.52853214330427, 5.188499419710238, 1.0654187508640014, 0.2746139727300294]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 18:14:49,899 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:14:49,900 : INFO : built Dictionary(194 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 725 corpus positions)\n",
      "2021-01-14 18:14:50,083 : INFO : token count processed\n",
      "2021-01-14 18:14:50,127 : INFO : frequencies processed\n",
      "2021-01-14 18:14:59,553 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:14:59,555 : INFO : entropies processed\n",
      "2021-01-14 18:14:59,555 : INFO : extropies processed\n",
      "2021-01-14 18:14:59,562 : INFO : token count processed\n",
      "2021-01-14 18:14:59,570 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:14:59,574 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:14:59,575 : INFO : vocab #32006\n",
      "2021-01-14 18:14:59,582 : INFO : diff #set()\n",
      "2021-01-14 18:15:18,765 : INFO : alphabet #32006\n",
      "2021-01-14 18:15:28,083 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.0401245312635894, 0.4901661563672446], [0.6274760961532593, 0.3725239], [3.484183719779189, 1.363148481046742], [5.463113392440269, 6.374522245625576, 6.71776829057063, 5.119867347495214, 1.2546548981303616, 0.3432460449450545]]\n",
      "2021-01-14 18:15:28,088 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:15:28,091 : INFO : built Dictionary(313 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 2007 corpus positions)\n",
      "2021-01-14 18:15:28,487 : INFO : token count processed\n",
      "2021-01-14 18:15:28,533 : INFO : frequencies processed\n",
      "2021-01-14 18:15:37,960 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:15:37,961 : INFO : entropies processed\n",
      "2021-01-14 18:15:37,962 : INFO : extropies processed\n",
      "2021-01-14 18:15:37,969 : INFO : token count processed\n",
      "2021-01-14 18:15:37,977 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:15:37,982 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:15:37,983 : INFO : vocab #32006\n",
      "2021-01-14 18:15:37,990 : INFO : diff #set()\n",
      "2021-01-14 18:15:57,285 : INFO : alphabet #32006\n",
      "2021-01-14 18:16:06,722 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.1515390484644448, 0.46478357002802295], [0.7755921930074692, 0.2244078], [3.8035088547976783, 1.3802337821262136], [5.463113392440269, 6.731238669067808, 7.067120289089245, 5.127231772418832, 1.6040068966489764, 0.3358816200214374]]\n",
      "2021-01-14 18:16:06,727 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:16:06,729 : INFO : built Dictionary(239 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1399 corpus positions)\n",
      "2021-01-14 18:16:06,984 : INFO : token count processed\n",
      "2021-01-14 18:16:07,016 : INFO : frequencies processed\n",
      "2021-01-14 18:16:16,550 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:16:16,551 : INFO : entropies processed\n",
      "2021-01-14 18:16:16,552 : INFO : extropies processed\n",
      "2021-01-14 18:16:16,559 : INFO : token count processed\n",
      "2021-01-14 18:16:16,563 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:16:16,568 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:16:16,569 : INFO : vocab #32006\n",
      "2021-01-14 18:16:16,575 : INFO : diff #set()\n",
      "2021-01-14 18:16:35,475 : INFO : alphabet #32006\n",
      "2021-01-14 18:16:44,899 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.1004292646836868, 0.47609315715309014], [0.735037237405777, 0.26496276], [3.697845823084412, 1.374237150068622], [5.463113392440269, 6.503741451859337, 6.788034171130345, 5.178820673169261, 1.3249207786900765, 0.28429271927100785]]\n",
      "2021-01-14 18:16:44,904 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:16:44,906 : INFO : built Dictionary(260 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 1721 corpus positions)\n",
      "2021-01-14 18:16:45,195 : INFO : token count processed\n",
      "2021-01-14 18:16:45,261 : INFO : frequencies processed\n",
      "2021-01-14 18:16:54,782 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:16:54,783 : INFO : entropies processed\n",
      "2021-01-14 18:16:54,784 : INFO : extropies processed\n",
      "2021-01-14 18:16:54,791 : INFO : token count processed\n",
      "2021-01-14 18:16:54,795 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:16:54,800 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:16:54,801 : INFO : vocab #32006\n",
      "2021-01-14 18:16:54,808 : INFO : diff #set()\n",
      "2021-01-14 18:17:13,664 : INFO : alphabet #32006\n",
      "2021-01-14 18:17:23,090 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.1035541356729937, 0.47538591141609404], [0.7192373871803284, 0.2807626], [3.795088586397732, 1.378800483023463], [5.463113392440269, 6.334729224484471, 6.550987509263935, 5.246855107660805, 1.0878741168236665, 0.21625828477946385]]\n",
      "2021-01-14 18:17:23,096 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:17:23,098 : INFO : built Dictionary(257 unique tokens: ['#', '-', '.', '/', '3']...) from 2 documents (total 2070 corpus positions)\n",
      "2021-01-14 18:17:23,380 : INFO : token count processed\n",
      "2021-01-14 18:17:23,447 : INFO : frequencies processed\n",
      "2021-01-14 18:17:32,997 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:17:32,998 : INFO : entropies processed\n",
      "2021-01-14 18:17:32,999 : INFO : extropies processed\n",
      "2021-01-14 18:17:33,006 : INFO : token count processed\n",
      "2021-01-14 18:17:33,012 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:17:33,016 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:17:33,017 : INFO : vocab #32006\n",
      "2021-01-14 18:17:33,024 : INFO : diff #set()\n",
      "2021-01-14 18:17:51,931 : INFO : alphabet #32006\n",
      "2021-01-14 18:18:01,468 : INFO : Computed distances or similarities ('277', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.1038505874607196, 0.4753189251937173], [0.7274452447891235, 0.27255476], [3.795088586397732, 1.378800483023463], [5.463113392440269, 6.21319712067992, 6.483947258649976, 5.192363254470213, 1.0208338662097072, 0.2707501379700554]]\n",
      "2021-01-14 18:18:01,472 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 18:18:01,473 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:18:01,475 : INFO : built Dictionary(300 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1467 corpus positions)\n",
      "2021-01-14 18:18:01,883 : INFO : token count processed\n",
      "2021-01-14 18:18:01,948 : INFO : frequencies processed\n",
      "2021-01-14 18:18:11,376 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:18:11,377 : INFO : entropies processed\n",
      "2021-01-14 18:18:11,378 : INFO : extropies processed\n",
      "2021-01-14 18:18:11,385 : INFO : token count processed\n",
      "2021-01-14 18:18:11,393 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:18:11,398 : INFO : alphabet_target #32010\n",
      "2021-01-14 18:18:11,399 : INFO : vocab #32006\n",
      "2021-01-14 18:18:11,406 : INFO : diff #set()\n",
      "2021-01-14 18:18:30,298 : INFO : alphabet #32006\n",
      "2021-01-14 18:18:39,860 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.1491206344552662, 0.4653065928304524], [0.8233533054590225, 0.1766467], [4.1219280948873624, 1.3984144061464991], [5.681880802803404, 6.905617163738059, 7.168988152457624, 5.418509814083839, 1.4871073496542202, 0.263370988719565]]\n",
      "2021-01-14 18:18:39,866 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:18:39,868 : INFO : built Dictionary(385 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 2357 corpus positions)\n",
      "2021-01-14 18:18:40,481 : INFO : token count processed\n",
      "2021-01-14 18:18:40,541 : INFO : frequencies processed\n",
      "2021-01-14 18:18:49,967 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:18:49,969 : INFO : entropies processed\n",
      "2021-01-14 18:18:49,969 : INFO : extropies processed\n",
      "2021-01-14 18:18:49,977 : INFO : token count processed\n",
      "2021-01-14 18:18:49,984 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:18:49,991 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:18:49,992 : INFO : vocab #32006\n",
      "2021-01-14 18:18:49,999 : INFO : diff #set()\n",
      "2021-01-14 18:19:09,013 : INFO : alphabet #32006\n",
      "2021-01-14 18:19:18,432 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.143597865945932, 0.46650540938037255], [0.8274849951267242, 0.172515], [4.438067278128811, 1.403437800508852], [5.681880802803404, 7.1219284286457345, 7.412676258493963, 5.391132972955175, 1.7307954556905596, 0.2907478298482289]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 18:19:18,438 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:19:18,442 : INFO : built Dictionary(306 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 2340 corpus positions)\n",
      "2021-01-14 18:19:18,844 : INFO : token count processed\n",
      "2021-01-14 18:19:18,876 : INFO : frequencies processed\n",
      "2021-01-14 18:19:28,280 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:19:28,281 : INFO : entropies processed\n",
      "2021-01-14 18:19:28,281 : INFO : extropies processed\n",
      "2021-01-14 18:19:28,289 : INFO : token count processed\n",
      "2021-01-14 18:19:28,293 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:19:28,298 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:19:28,299 : INFO : vocab #32006\n",
      "2021-01-14 18:19:28,305 : INFO : diff #set()\n",
      "2021-01-14 18:19:47,183 : INFO : alphabet #32006\n",
      "2021-01-14 18:19:56,592 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.1819736969133476, 0.45830066669209385], [0.8732703030109406, 0.1267297], [4.3637132757501895, 1.4033950360813567], [5.681880802803404, 6.41099024988467, 6.588575313570991, 5.504295739117084, 0.9066945107675872, 0.17758506368632077]]\n",
      "2021-01-14 18:19:56,596 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:19:56,598 : INFO : built Dictionary(194 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 739 corpus positions)\n",
      "2021-01-14 18:19:56,798 : INFO : token count processed\n",
      "2021-01-14 18:19:56,832 : INFO : frequencies processed\n",
      "2021-01-14 18:20:06,257 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:20:06,258 : INFO : entropies processed\n",
      "2021-01-14 18:20:06,259 : INFO : extropies processed\n",
      "2021-01-14 18:20:06,266 : INFO : token count processed\n",
      "2021-01-14 18:20:06,272 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:20:06,276 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:20:06,277 : INFO : vocab #32006\n",
      "2021-01-14 18:20:06,284 : INFO : diff #set()\n",
      "2021-01-14 18:20:25,309 : INFO : alphabet #32006\n",
      "2021-01-14 18:20:34,739 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.1486986450350742, 0.4653979757983589], [0.8577844351530075, 0.14221556], [3.521640636343319, 1.3740281872300928], [5.681880802803404, 6.077866832717642, 6.497062576756655, 5.262685058764389, 0.8151817739532516, 0.4191957440390137]]\n",
      "2021-01-14 18:20:34,743 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:20:34,744 : INFO : built Dictionary(170 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 504 corpus positions)\n",
      "2021-01-14 18:20:34,914 : INFO : token count processed\n",
      "2021-01-14 18:20:34,953 : INFO : frequencies processed\n",
      "2021-01-14 18:20:44,374 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:20:44,375 : INFO : entropies processed\n",
      "2021-01-14 18:20:44,376 : INFO : extropies processed\n",
      "2021-01-14 18:20:44,382 : INFO : token count processed\n",
      "2021-01-14 18:20:44,387 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:20:44,391 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:20:44,392 : INFO : vocab #32006\n",
      "2021-01-14 18:20:44,399 : INFO : diff #set()\n",
      "2021-01-14 18:21:03,391 : INFO : alphabet #32006\n",
      "2021-01-14 18:21:12,812 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.1406882055426983, 0.4671394915947062], [0.8462711423635483, 0.15372886], [3.521640636343319, 1.3740281872300928], [5.681880802803404, 5.977547459003844, 6.461189980674891, 5.198238281132357, 0.7793091778714869, 0.4836425216710465]]\n",
      "2021-01-14 18:21:12,818 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:21:12,820 : INFO : built Dictionary(270 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 2215 corpus positions)\n",
      "2021-01-14 18:21:13,160 : INFO : token count processed\n",
      "2021-01-14 18:21:13,205 : INFO : frequencies processed\n",
      "2021-01-14 18:21:22,791 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:21:22,792 : INFO : entropies processed\n",
      "2021-01-14 18:21:22,793 : INFO : extropies processed\n",
      "2021-01-14 18:21:22,800 : INFO : token count processed\n",
      "2021-01-14 18:21:22,805 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:21:22,809 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:21:22,810 : INFO : vocab #32006\n",
      "2021-01-14 18:21:22,817 : INFO : diff #set()\n",
      "2021-01-14 18:21:42,085 : INFO : alphabet #32006\n",
      "2021-01-14 18:21:51,510 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.1745279002731226, 0.45986993308956814], [0.8533371835947037, 0.14666282], [3.9705730958116847, 1.3904984042298727], [5.681880802803404, 6.4614394051846435, 6.678447132038124, 5.4648730759499236, 0.99656632923472, 0.21700772685348024]]\n",
      "2021-01-14 18:21:51,515 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 18:21:51,516 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:21:51,519 : INFO : built Dictionary(239 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1439 corpus positions)\n",
      "2021-01-14 18:21:51,805 : INFO : token count processed\n",
      "2021-01-14 18:21:51,853 : INFO : frequencies processed\n",
      "2021-01-14 18:22:01,404 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:22:01,405 : INFO : entropies processed\n",
      "2021-01-14 18:22:01,406 : INFO : extropies processed\n",
      "2021-01-14 18:22:01,413 : INFO : token count processed\n",
      "2021-01-14 18:22:01,418 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:22:01,422 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:22:01,423 : INFO : vocab #32006\n",
      "2021-01-14 18:22:01,429 : INFO : diff #set()\n",
      "2021-01-14 18:22:20,327 : INFO : alphabet #32006\n",
      "2021-01-14 18:22:29,761 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.1818655179204234, 0.45832338968036795], [0.8540558516979218, 0.14594415], [3.2776134368191157, 1.3618978811135465], [5.681880802803404, 6.327195724598159, 6.618213314284851, 5.390863213116711, 0.9363325114814476, 0.29101758968669245]]\n",
      "2021-01-14 18:22:29,773 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 18:22:29,774 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:22:29,777 : INFO : built Dictionary(438 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 6331 corpus positions)\n",
      "2021-01-14 18:22:30,462 : INFO : token count processed\n",
      "2021-01-14 18:22:30,494 : INFO : frequencies processed\n",
      "2021-01-14 18:22:40,046 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:22:40,047 : INFO : entropies processed\n",
      "2021-01-14 18:22:40,047 : INFO : extropies processed\n",
      "2021-01-14 18:22:40,056 : INFO : token count processed\n",
      "2021-01-14 18:22:40,061 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:22:40,065 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:22:40,066 : INFO : vocab #32006\n",
      "2021-01-14 18:22:40,074 : INFO : diff #set()\n",
      "2021-01-14 18:22:58,957 : INFO : alphabet #32006\n",
      "2021-01-14 18:23:08,535 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.1382113798609608, 0.4676806088577763], [0.8368340134620667, 0.16316599], [4.5541241456383155, 1.404749498034752], [5.681880802803404, 6.9079058562486315, 7.052096056495127, 5.537690602556907, 1.3702152536917236, 0.14419020024649587]]\n",
      "2021-01-14 18:23:08,542 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:23:08,544 : INFO : built Dictionary(344 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 2725 corpus positions)\n",
      "2021-01-14 18:23:09,058 : INFO : token count processed\n",
      "2021-01-14 18:23:09,128 : INFO : frequencies processed\n",
      "2021-01-14 18:23:18,855 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:23:18,856 : INFO : entropies processed\n",
      "2021-01-14 18:23:18,857 : INFO : extropies processed\n",
      "2021-01-14 18:23:18,865 : INFO : token count processed\n",
      "2021-01-14 18:23:18,870 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:23:18,874 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:23:18,875 : INFO : vocab #32006\n",
      "2021-01-14 18:23:18,882 : INFO : diff #set()\n",
      "2021-01-14 18:23:38,271 : INFO : alphabet #32006\n",
      "2021-01-14 18:23:47,744 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.1453010497345353, 0.4661350443676622], [0.8315683752298355, 0.16843162], [4.31971753049182, 1.3955204622412976], [5.681880802803404, 6.61034830706307, 6.847831843489959, 5.444397266376515, 1.1659510406865552, 0.2374835364268888]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 18:23:47,748 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:23:47,749 : INFO : built Dictionary(237 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 772 corpus positions)\n",
      "2021-01-14 18:23:48,028 : INFO : token count processed\n",
      "2021-01-14 18:23:48,054 : INFO : frequencies processed\n",
      "2021-01-14 18:23:57,360 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:23:57,361 : INFO : entropies processed\n",
      "2021-01-14 18:23:57,362 : INFO : extropies processed\n",
      "2021-01-14 18:23:57,369 : INFO : token count processed\n",
      "2021-01-14 18:23:57,373 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:23:57,378 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:23:57,379 : INFO : vocab #32006\n",
      "2021-01-14 18:23:57,386 : INFO : diff #set()\n",
      "2021-01-14 18:24:16,363 : INFO : alphabet #32006\n",
      "2021-01-14 18:24:25,931 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.1502307033295351, 0.46506637564589937], [0.8098448663949966, 0.19015513], [3.8924071185928764, 1.3869143508791244], [5.681880802803404, 6.616715366949855, 6.955995962880598, 5.34260020687266, 1.2741151600771943, 0.33928059593074344]]\n",
      "2021-01-14 18:24:25,938 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:24:25,943 : INFO : built Dictionary(447 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 2806 corpus positions)\n",
      "2021-01-14 18:24:26,766 : INFO : token count processed\n",
      "2021-01-14 18:24:26,799 : INFO : frequencies processed\n",
      "2021-01-14 18:24:36,413 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:24:36,414 : INFO : entropies processed\n",
      "2021-01-14 18:24:36,415 : INFO : extropies processed\n",
      "2021-01-14 18:24:36,423 : INFO : token count processed\n",
      "2021-01-14 18:24:36,430 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:24:36,435 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:24:36,436 : INFO : vocab #32006\n",
      "2021-01-14 18:24:36,443 : INFO : diff #set()\n",
      "2021-01-14 18:24:55,451 : INFO : alphabet #32006\n",
      "2021-01-14 18:25:05,151 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.109802887748469, 0.47397792742011846], [0.7590376734733582, 0.24096233], [4.45148902643068, 1.3993162074095904], [5.681880802803404, 7.32185870753746, 7.571268541465448, 5.432470968875416, 1.8893877386620446, 0.249409833927988]]\n",
      "2021-01-14 18:25:05,154 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:25:05,156 : INFO : built Dictionary(102 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 171 corpus positions)\n",
      "2021-01-14 18:25:05,225 : INFO : token count processed\n",
      "2021-01-14 18:25:05,289 : INFO : frequencies processed\n",
      "2021-01-14 18:25:14,741 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:25:14,742 : INFO : entropies processed\n",
      "2021-01-14 18:25:14,743 : INFO : extropies processed\n",
      "2021-01-14 18:25:14,756 : INFO : token count processed\n",
      "2021-01-14 18:25:14,762 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:25:14,768 : INFO : alphabet_target #32008\n",
      "2021-01-14 18:25:14,769 : INFO : vocab #32006\n",
      "2021-01-14 18:25:14,775 : INFO : diff #set()\n",
      "2021-01-14 18:25:33,833 : INFO : alphabet #32006\n",
      "2021-01-14 18:25:43,304 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/fireException.py')[[1.1742546086537051, 0.45992773616296867], [0.8627023696899414, 0.13729763], [1.9219280948873623, 1.2148067842293933], [5.681880802803404, 5.176618657501385, 6.36272784549212, 4.495771614812668, 0.6808470426887165, 1.186109187990735]]\n",
      "2021-01-14 18:25:43,308 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:25:43,309 : INFO : built Dictionary(193 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 564 corpus positions)\n",
      "2021-01-14 18:25:43,506 : INFO : token count processed\n",
      "2021-01-14 18:25:43,550 : INFO : frequencies processed\n",
      "2021-01-14 18:25:53,003 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:25:53,004 : INFO : entropies processed\n",
      "2021-01-14 18:25:53,005 : INFO : extropies processed\n",
      "2021-01-14 18:25:53,012 : INFO : token count processed\n",
      "2021-01-14 18:25:53,016 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:25:53,020 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:25:53,021 : INFO : vocab #32006\n",
      "2021-01-14 18:25:53,028 : INFO : diff #set()\n",
      "2021-01-14 18:26:12,036 : INFO : alphabet #32006\n",
      "2021-01-14 18:26:21,475 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.1918793544889599, 0.4562294899817383], [0.8712930530309677, 0.12870695], [3.121928094887362, 1.3519647487142497], [5.681880802803404, 6.468846789852156, 6.95024043194573, 5.20048716070983, 1.268359629142326, 0.4813936420935736]]\n",
      "2021-01-14 18:26:21,481 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:26:21,483 : INFO : built Dictionary(396 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 2613 corpus positions)\n",
      "2021-01-14 18:26:22,162 : INFO : token count processed\n",
      "2021-01-14 18:26:22,208 : INFO : frequencies processed\n",
      "2021-01-14 18:26:31,622 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:26:31,623 : INFO : entropies processed\n",
      "2021-01-14 18:26:31,624 : INFO : extropies processed\n",
      "2021-01-14 18:26:31,632 : INFO : token count processed\n",
      "2021-01-14 18:26:31,639 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:26:31,643 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:26:31,644 : INFO : vocab #32006\n",
      "2021-01-14 18:26:31,651 : INFO : diff #set()\n",
      "2021-01-14 18:26:50,685 : INFO : alphabet #32006\n",
      "2021-01-14 18:27:00,127 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.1686815722033892, 0.46110964966792983], [0.8458013832569122, 0.15419862], [4.243010159554855, 1.391940975135472], [5.681880802803404, 6.957796704012729, 7.173823982103242, 5.465853524712891, 1.491943179299838, 0.2160272780905128]]\n",
      "2021-01-14 18:27:00,134 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:27:00,137 : INFO : built Dictionary(312 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 3110 corpus positions)\n",
      "2021-01-14 18:27:00,551 : INFO : token count processed\n",
      "2021-01-14 18:27:00,620 : INFO : frequencies processed\n",
      "2021-01-14 18:27:10,172 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:27:10,173 : INFO : entropies processed\n",
      "2021-01-14 18:27:10,174 : INFO : extropies processed\n",
      "2021-01-14 18:27:10,185 : INFO : token count processed\n",
      "2021-01-14 18:27:10,190 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:27:10,194 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:27:10,195 : INFO : vocab #32006\n",
      "2021-01-14 18:27:10,201 : INFO : diff #set()\n",
      "2021-01-14 18:27:29,136 : INFO : alphabet #32006\n",
      "2021-01-14 18:27:38,690 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.1562889416620432, 0.46375974048691787], [0.8469285219907761, 0.15307148], [3.8841837197791893, 1.3868082146232044], [5.681880802803404, 6.441859572014148, 6.676493769665855, 5.447246605151696, 0.9946129668624515, 0.23463419765170723]]\n",
      "2021-01-14 18:27:38,695 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:27:38,697 : INFO : built Dictionary(333 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1586 corpus positions)\n",
      "2021-01-14 18:27:39,174 : INFO : token count processed\n",
      "2021-01-14 18:27:39,206 : INFO : frequencies processed\n",
      "2021-01-14 18:27:48,649 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:27:48,650 : INFO : entropies processed\n",
      "2021-01-14 18:27:48,650 : INFO : extropies processed\n",
      "2021-01-14 18:27:48,658 : INFO : token count processed\n",
      "2021-01-14 18:27:48,662 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:27:48,666 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:27:48,668 : INFO : vocab #32006\n",
      "2021-01-14 18:27:48,674 : INFO : diff #set()\n",
      "2021-01-14 18:28:07,588 : INFO : alphabet #32006\n",
      "2021-01-14 18:28:17,030 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.1385962154908411, 0.4675964507729593], [0.8170371502637863, 0.18296285], [4.293660689688186, 1.4013179594871905], [5.681880802803404, 6.998955278238291, 7.268017528401183, 5.412818552640511, 1.586136725597779, 0.2690622501628921]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 18:28:17,035 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:28:17,038 : INFO : built Dictionary(247 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1405 corpus positions)\n",
      "2021-01-14 18:28:17,355 : INFO : token count processed\n",
      "2021-01-14 18:28:17,385 : INFO : frequencies processed\n",
      "2021-01-14 18:28:26,779 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:28:26,780 : INFO : entropies processed\n",
      "2021-01-14 18:28:26,781 : INFO : extropies processed\n",
      "2021-01-14 18:28:26,796 : INFO : token count processed\n",
      "2021-01-14 18:28:26,800 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:28:26,805 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:28:26,806 : INFO : vocab #32006\n",
      "2021-01-14 18:28:26,813 : INFO : diff #set()\n",
      "2021-01-14 18:28:45,705 : INFO : alphabet #32006\n",
      "2021-01-14 18:28:55,247 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.1327429810688856, 0.46887975197968823], [0.8336948305368423, 0.16630517], [4.3637132757501895, 1.4033950360813567], [5.681880802803404, 6.492983191376071, 6.746199075949109, 5.428664918230366, 1.0643182731457053, 0.2532158845730379]]\n",
      "2021-01-14 18:28:55,254 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:28:55,257 : INFO : built Dictionary(470 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 3344 corpus positions)\n",
      "2021-01-14 18:28:56,002 : INFO : token count processed\n",
      "2021-01-14 18:28:56,034 : INFO : frequencies processed\n",
      "2021-01-14 18:29:05,436 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:29:05,437 : INFO : entropies processed\n",
      "2021-01-14 18:29:05,438 : INFO : extropies processed\n",
      "2021-01-14 18:29:05,446 : INFO : token count processed\n",
      "2021-01-14 18:29:05,451 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:29:05,455 : INFO : alphabet_target #32008\n",
      "2021-01-14 18:29:05,456 : INFO : vocab #32006\n",
      "2021-01-14 18:29:05,463 : INFO : diff #set()\n",
      "2021-01-14 18:29:24,717 : INFO : alphabet #32006\n",
      "2021-01-14 18:29:34,131 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.209084194856909, 0.4526762729678458], [0.877989761531353, 0.12201024], [3.0, 1.3485155455967714], [5.681880802803404, 6.560342487747443, 6.845662131948167, 5.39656115860268, 1.1637813291447632, 0.2853196442007242]]\n",
      "2021-01-14 18:29:34,138 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 18:29:34,139 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:29:34,142 : INFO : built Dictionary(469 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 3549 corpus positions)\n",
      "2021-01-14 18:29:34,942 : INFO : token count processed\n",
      "2021-01-14 18:29:34,990 : INFO : frequencies processed\n",
      "2021-01-14 18:29:44,589 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:29:44,590 : INFO : entropies processed\n",
      "2021-01-14 18:29:44,591 : INFO : extropies processed\n",
      "2021-01-14 18:29:44,599 : INFO : token count processed\n",
      "2021-01-14 18:29:44,603 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:29:44,607 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:29:44,608 : INFO : vocab #32006\n",
      "2021-01-14 18:29:44,616 : INFO : diff #set()\n",
      "2021-01-14 18:30:03,633 : INFO : alphabet #32006\n",
      "2021-01-14 18:30:13,066 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.146317686877901, 0.465914252169552], [0.8284483999013901, 0.1715516], [4.574400937409154, 1.408225935401017], [5.681880802803404, 7.046173750105238, 7.290400200108447, 5.437654352800195, 1.6085193973050433, 0.2442264500032092]]\n",
      "2021-01-14 18:30:13,077 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 18:30:13,078 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:30:13,081 : INFO : built Dictionary(516 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 5658 corpus positions)\n",
      "2021-01-14 18:30:14,085 : INFO : token count processed\n",
      "2021-01-14 18:30:14,152 : INFO : frequencies processed\n",
      "2021-01-14 18:30:23,546 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:30:23,547 : INFO : entropies processed\n",
      "2021-01-14 18:30:23,548 : INFO : extropies processed\n",
      "2021-01-14 18:30:23,557 : INFO : token count processed\n",
      "2021-01-14 18:30:23,564 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:30:23,568 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:30:23,570 : INFO : vocab #32006\n",
      "2021-01-14 18:30:23,577 : INFO : diff #set()\n",
      "2021-01-14 18:30:42,986 : INFO : alphabet #32006\n",
      "2021-01-14 18:30:52,421 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.121571927491445, 0.47134861988035637], [0.8111659586429596, 0.18883404], [4.861530252405224, 1.41250924329691], [5.681880802803404, 7.009229588004272, 7.150638739850364, 5.540471650957312, 1.4687579370469601, 0.1414091518460916]]\n",
      "2021-01-14 18:30:52,433 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:30:52,437 : INFO : built Dictionary(600 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 6599 corpus positions)\n",
      "2021-01-14 18:30:53,755 : INFO : token count processed\n",
      "2021-01-14 18:30:53,787 : INFO : frequencies processed\n",
      "2021-01-14 18:31:03,575 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:31:03,576 : INFO : entropies processed\n",
      "2021-01-14 18:31:03,577 : INFO : extropies processed\n",
      "2021-01-14 18:31:03,594 : INFO : token count processed\n",
      "2021-01-14 18:31:03,599 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:31:03,603 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:31:03,604 : INFO : vocab #32006\n",
      "2021-01-14 18:31:03,611 : INFO : diff #set()\n",
      "2021-01-14 18:31:22,751 : INFO : alphabet #32006\n",
      "2021-01-14 18:31:32,166 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.1463903764297694, 0.46589847354019776], [0.8073654174804688, 0.19263458], [4.558518613048906, 1.4031195605627262], [5.681880802803404, 7.376088004590871, 7.56826117540899, 5.489707631985285, 1.8863803726055863, 0.19217317081811913]]\n",
      "2021-01-14 18:31:32,169 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:31:32,170 : INFO : built Dictionary(170 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 385 corpus positions)\n",
      "2021-01-14 18:31:32,342 : INFO : token count processed\n",
      "2021-01-14 18:31:32,375 : INFO : frequencies processed\n",
      "2021-01-14 18:31:41,791 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:31:41,792 : INFO : entropies processed\n",
      "2021-01-14 18:31:41,793 : INFO : extropies processed\n",
      "2021-01-14 18:31:41,800 : INFO : token count processed\n",
      "2021-01-14 18:31:41,807 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:31:41,811 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:31:41,812 : INFO : vocab #32006\n",
      "2021-01-14 18:31:41,819 : INFO : diff #set()\n",
      "2021-01-14 18:32:00,837 : INFO : alphabet #32006\n",
      "2021-01-14 18:32:10,271 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.1885846057838998, 0.4569163089958879], [0.8674719780683517, 0.13252802], [2.94770277922009, 1.3393100707180505], [5.681880802803404, 6.2993628166120885, 6.865047836368009, 5.116195783047483, 1.1831670335646054, 0.5656850197559207]]\n",
      "2021-01-14 18:32:10,274 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:32:10,276 : INFO : built Dictionary(74 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 101 corpus positions)\n",
      "2021-01-14 18:32:10,303 : INFO : token count processed\n",
      "2021-01-14 18:32:10,335 : INFO : frequencies processed\n",
      "2021-01-14 18:32:19,889 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:32:19,890 : INFO : entropies processed\n",
      "2021-01-14 18:32:19,892 : INFO : extropies processed\n",
      "2021-01-14 18:32:19,903 : INFO : token count processed\n",
      "2021-01-14 18:32:19,906 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:32:19,910 : INFO : alphabet_target #32008\n",
      "2021-01-14 18:32:19,911 : INFO : vocab #32006\n",
      "2021-01-14 18:32:19,918 : INFO : diff #set()\n",
      "2021-01-14 18:32:38,840 : INFO : alphabet #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 18:32:48,252 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2766741579750247, 0.43923720770364627], [0.9254608601331711, 0.07453914], [0.0, 0.0], [5.681880802803404, 3.8936606896881862, 5.968911387905326, 3.606630104586264, 0.28703058510192214, 2.0752506982171397]]\n",
      "2021-01-14 18:32:48,274 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:32:48,279 : INFO : built Dictionary(747 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 12543 corpus positions)\n",
      "2021-01-14 18:32:50,068 : INFO : token count processed\n",
      "2021-01-14 18:32:50,141 : INFO : frequencies processed\n",
      "2021-01-14 18:32:59,675 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:32:59,676 : INFO : entropies processed\n",
      "2021-01-14 18:32:59,677 : INFO : extropies processed\n",
      "2021-01-14 18:32:59,688 : INFO : token count processed\n",
      "2021-01-14 18:32:59,692 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:32:59,697 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:32:59,698 : INFO : vocab #32006\n",
      "2021-01-14 18:32:59,704 : INFO : diff #set()\n",
      "2021-01-14 18:33:18,576 : INFO : alphabet #32006\n",
      "2021-01-14 18:33:28,119 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.0995904762612854, 0.4762833568290362], [0.7762429118156433, 0.22375709], [5.113248785942744, 1.4166717796775559], [5.681880802803404, 7.434393313070278, 7.650305896761136, 5.465968219112547, 1.9684250939577321, 0.21591258369085775]]\n",
      "2021-01-14 18:33:28,127 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:33:28,130 : INFO : built Dictionary(505 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 4170 corpus positions)\n",
      "2021-01-14 18:33:29,035 : INFO : token count processed\n",
      "2021-01-14 18:33:29,072 : INFO : frequencies processed\n",
      "2021-01-14 18:33:38,484 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:33:38,485 : INFO : entropies processed\n",
      "2021-01-14 18:33:38,486 : INFO : extropies processed\n",
      "2021-01-14 18:33:38,494 : INFO : token count processed\n",
      "2021-01-14 18:33:38,500 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:33:38,505 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:33:38,506 : INFO : vocab #32006\n",
      "2021-01-14 18:33:38,513 : INFO : diff #set()\n",
      "2021-01-14 18:33:57,378 : INFO : alphabet #32006\n",
      "2021-01-14 18:34:06,905 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.1124294877879295, 0.47338858209519163], [0.7793739587068558, 0.22062604], [4.825553479199119, 1.4102315889998804], [5.681880802803404, 7.2991514951718255, 7.521935555903653, 5.459096742071576, 1.8400547531002491, 0.22278406073182744]]\n",
      "2021-01-14 18:34:06,912 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:34:06,915 : INFO : built Dictionary(478 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 3587 corpus positions)\n",
      "2021-01-14 18:34:07,744 : INFO : token count processed\n",
      "2021-01-14 18:34:07,805 : INFO : frequencies processed\n",
      "2021-01-14 18:34:17,206 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:34:17,207 : INFO : entropies processed\n",
      "2021-01-14 18:34:17,208 : INFO : extropies processed\n",
      "2021-01-14 18:34:17,216 : INFO : token count processed\n",
      "2021-01-14 18:34:17,220 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:34:17,224 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:34:17,225 : INFO : vocab #32006\n",
      "2021-01-14 18:34:17,231 : INFO : diff #set()\n",
      "2021-01-14 18:34:36,412 : INFO : alphabet #32006\n",
      "2021-01-14 18:34:45,840 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.1462470200049197, 0.46592959276314233], [0.8193070441484451, 0.18069296], [4.445543568156237, 1.3985112857165842], [5.681880802803404, 7.170319527000998, 7.405099435687617, 5.447100894116783, 1.7232186328842136, 0.2347799086866198]]\n",
      "2021-01-14 18:34:45,843 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:34:45,845 : INFO : built Dictionary(201 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 623 corpus positions)\n",
      "2021-01-14 18:34:46,056 : INFO : token count processed\n",
      "2021-01-14 18:34:46,098 : INFO : frequencies processed\n",
      "2021-01-14 18:34:55,530 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:34:55,532 : INFO : entropies processed\n",
      "2021-01-14 18:34:55,533 : INFO : extropies processed\n",
      "2021-01-14 18:34:55,540 : INFO : token count processed\n",
      "2021-01-14 18:34:55,544 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:34:55,548 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:34:55,549 : INFO : vocab #32006\n",
      "2021-01-14 18:34:55,557 : INFO : diff #set()\n",
      "2021-01-14 18:35:14,554 : INFO : alphabet #32006\n",
      "2021-01-14 18:35:23,983 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.172056191454112, 0.4603932457799522], [0.8607189506292343, 0.13928105], [3.4182958340544896, 1.369895090630202], [5.681880802803404, 6.353654804387375, 6.824978500003601, 5.210557107187178, 1.1430976972001972, 0.4713236956162259]]\n",
      "2021-01-14 18:35:23,987 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:35:23,989 : INFO : built Dictionary(199 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 751 corpus positions)\n",
      "2021-01-14 18:35:24,197 : INFO : token count processed\n",
      "2021-01-14 18:35:24,244 : INFO : frequencies processed\n",
      "2021-01-14 18:35:33,669 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:35:33,670 : INFO : entropies processed\n",
      "2021-01-14 18:35:33,671 : INFO : extropies processed\n",
      "2021-01-14 18:35:33,678 : INFO : token count processed\n",
      "2021-01-14 18:35:33,682 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:35:33,688 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:35:33,689 : INFO : vocab #32006\n",
      "2021-01-14 18:35:33,696 : INFO : diff #set()\n",
      "2021-01-14 18:35:52,673 : INFO : alphabet #32006\n",
      "2021-01-14 18:36:02,105 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.1875447368554024, 0.4571335082442707], [0.8644044101238251, 0.13559559], [2.94770277922009, 1.3393100707180505], [5.681880802803404, 6.245180322479091, 6.676537534593267, 5.2505235906892285, 0.9946567317898634, 0.4313572121141762]]\n",
      "2021-01-14 18:36:02,110 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:36:02,112 : INFO : built Dictionary(405 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1823 corpus positions)\n",
      "2021-01-14 18:36:02,768 : INFO : token count processed\n",
      "2021-01-14 18:36:02,837 : INFO : frequencies processed\n",
      "2021-01-14 18:36:12,268 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:36:12,269 : INFO : entropies processed\n",
      "2021-01-14 18:36:12,270 : INFO : extropies processed\n",
      "2021-01-14 18:36:12,277 : INFO : token count processed\n",
      "2021-01-14 18:36:12,281 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:36:12,286 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:36:12,287 : INFO : vocab #32006\n",
      "2021-01-14 18:36:12,296 : INFO : diff #set()\n",
      "2021-01-14 18:36:31,229 : INFO : alphabet #32006\n",
      "2021-01-14 18:36:40,596 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.1320110581418232, 0.46904071917504997], [0.7833098620176315, 0.21669014], [4.722435394086042, 1.4069886542847272], [5.681880802803404, 7.2691387000368, 7.536024385735953, 5.414995117104251, 1.854143582932549, 0.26688568569915283]]\n",
      "2021-01-14 18:36:40,601 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:36:40,605 : INFO : built Dictionary(335 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1660 corpus positions)\n",
      "2021-01-14 18:36:41,081 : INFO : token count processed\n",
      "2021-01-14 18:36:41,153 : INFO : frequencies processed\n",
      "2021-01-14 18:36:50,867 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:36:50,869 : INFO : entropies processed\n",
      "2021-01-14 18:36:50,870 : INFO : extropies processed\n",
      "2021-01-14 18:36:50,877 : INFO : token count processed\n",
      "2021-01-14 18:36:50,881 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:36:50,886 : INFO : alphabet_target #32009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 18:36:50,887 : INFO : vocab #32006\n",
      "2021-01-14 18:36:50,893 : INFO : diff #set()\n",
      "2021-01-14 18:37:09,948 : INFO : alphabet #32006\n",
      "2021-01-14 18:37:19,377 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.0885362346694, 0.4788042378198398], [0.7420090138912201, 0.257991], [4.5041206053780485, 1.400342431257938], [5.681880802803404, 7.08857858466988, 7.331198855203137, 5.439260532270147, 1.6493180523997335, 0.24262027053325763]]\n",
      "2021-01-14 18:37:19,381 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:37:19,382 : INFO : built Dictionary(176 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 583 corpus positions)\n",
      "2021-01-14 18:37:19,550 : INFO : token count processed\n",
      "2021-01-14 18:37:19,582 : INFO : frequencies processed\n",
      "2021-01-14 18:37:29,114 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:37:29,115 : INFO : entropies processed\n",
      "2021-01-14 18:37:29,115 : INFO : extropies processed\n",
      "2021-01-14 18:37:29,123 : INFO : token count processed\n",
      "2021-01-14 18:37:29,130 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:37:29,134 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:37:29,135 : INFO : vocab #32006\n",
      "2021-01-14 18:37:29,142 : INFO : diff #set()\n",
      "2021-01-14 18:37:48,007 : INFO : alphabet #32006\n",
      "2021-01-14 18:37:57,416 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.176899305911086, 0.45936897369787866], [0.8675167560577393, 0.13248324], [3.2776134368191157, 1.3618978811135465], [5.681880802803404, 6.0479231618016716, 6.55832398686276, 5.171479977742315, 0.8764431840593563, 0.5104008250610885]]\n",
      "2021-01-14 18:37:57,419 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:37:57,421 : INFO : built Dictionary(179 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 702 corpus positions)\n",
      "2021-01-14 18:37:57,611 : INFO : token count processed\n",
      "2021-01-14 18:37:57,680 : INFO : frequencies processed\n",
      "2021-01-14 18:38:07,223 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:38:07,224 : INFO : entropies processed\n",
      "2021-01-14 18:38:07,224 : INFO : extropies processed\n",
      "2021-01-14 18:38:07,231 : INFO : token count processed\n",
      "2021-01-14 18:38:07,236 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:38:07,240 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:38:07,241 : INFO : vocab #32006\n",
      "2021-01-14 18:38:07,248 : INFO : diff #set()\n",
      "2021-01-14 18:38:26,088 : INFO : alphabet #32006\n",
      "2021-01-14 18:38:35,607 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.1885084460348465, 0.4569322096114394], [0.8688243627548218, 0.13117564], [3.121928094887362, 1.3519647487142497], [5.681880802803404, 6.036583168403119, 6.494333874663145, 5.224130096543378, 0.8124530718597409, 0.45775070626002545]]\n",
      "2021-01-14 18:38:35,621 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 18:38:35,622 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:38:35,625 : INFO : built Dictionary(577 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 7060 corpus positions)\n",
      "2021-01-14 18:38:36,725 : INFO : token count processed\n",
      "2021-01-14 18:38:36,757 : INFO : frequencies processed\n",
      "2021-01-14 18:38:46,174 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:38:46,175 : INFO : entropies processed\n",
      "2021-01-14 18:38:46,175 : INFO : extropies processed\n",
      "2021-01-14 18:38:46,184 : INFO : token count processed\n",
      "2021-01-14 18:38:46,189 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:38:46,193 : INFO : alphabet_target #32010\n",
      "2021-01-14 18:38:46,194 : INFO : vocab #32006\n",
      "2021-01-14 18:38:46,201 : INFO : diff #set()\n",
      "2021-01-14 18:39:05,180 : INFO : alphabet #32006\n",
      "2021-01-14 18:39:14,599 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.149349663204069, 0.46525701104830214], [0.8309258222579956, 0.16907418], [4.571860873964196, 1.4084747052851883], [5.681880802803404, 7.29352035514053, 7.483554949703263, 5.491846208240672, 1.8016741468998596, 0.19003459456273308]]\n",
      "2021-01-14 18:39:14,606 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:39:14,608 : INFO : built Dictionary(386 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 3303 corpus positions)\n",
      "2021-01-14 18:39:15,206 : INFO : token count processed\n",
      "2021-01-14 18:39:15,239 : INFO : frequencies processed\n",
      "2021-01-14 18:39:24,655 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:39:24,656 : INFO : entropies processed\n",
      "2021-01-14 18:39:24,657 : INFO : extropies processed\n",
      "2021-01-14 18:39:24,665 : INFO : token count processed\n",
      "2021-01-14 18:39:24,669 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:39:24,673 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:39:24,674 : INFO : vocab #32006\n",
      "2021-01-14 18:39:24,681 : INFO : diff #set()\n",
      "2021-01-14 18:39:43,918 : INFO : alphabet #32006\n",
      "2021-01-14 18:39:53,328 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.1283923219019334, 0.46983819181719233], [0.8134327530860901, 0.18656725], [4.436006945330954, 1.3968278238638363], [5.681880802803404, 6.8153433747477745, 7.01632483619505, 5.480899341356128, 1.3344440333916463, 0.20098146144727558]]\n",
      "2021-01-14 18:39:53,331 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:39:53,332 : INFO : built Dictionary(155 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 324 corpus positions)\n",
      "2021-01-14 18:39:53,473 : INFO : token count processed\n",
      "2021-01-14 18:39:53,516 : INFO : frequencies processed\n",
      "2021-01-14 18:40:02,935 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:40:02,936 : INFO : entropies processed\n",
      "2021-01-14 18:40:02,937 : INFO : extropies processed\n",
      "2021-01-14 18:40:02,948 : INFO : token count processed\n",
      "2021-01-14 18:40:02,953 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:40:02,958 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:40:02,959 : INFO : vocab #32006\n",
      "2021-01-14 18:40:02,968 : INFO : diff #set()\n",
      "2021-01-14 18:40:21,949 : INFO : alphabet #32006\n",
      "2021-01-14 18:40:31,369 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.169597355536146, 0.46091501607351576], [0.8626519590616226, 0.13734804], [2.94770277922009, 1.3393100707180505], [5.681880802803404, 6.150121915859574, 6.7539660722115755, 5.078036646451402, 1.0720852694081717, 0.6038441563520012]]\n",
      "2021-01-14 18:40:31,374 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:40:31,375 : INFO : built Dictionary(306 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1035 corpus positions)\n",
      "2021-01-14 18:40:31,792 : INFO : token count processed\n",
      "2021-01-14 18:40:31,861 : INFO : frequencies processed\n",
      "2021-01-14 18:40:41,387 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:40:41,388 : INFO : entropies processed\n",
      "2021-01-14 18:40:41,389 : INFO : extropies processed\n",
      "2021-01-14 18:40:41,396 : INFO : token count processed\n",
      "2021-01-14 18:40:41,403 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:40:41,408 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:40:41,409 : INFO : vocab #32006\n",
      "2021-01-14 18:40:41,414 : INFO : diff #set()\n",
      "2021-01-14 18:41:00,150 : INFO : alphabet #32006\n",
      "2021-01-14 18:41:09,522 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.1039313951106935, 0.4753006691776598], [0.7416510283946991, 0.25834897], [4.25075201325044, 1.3937433663141456], [5.681880802803404, 7.0391145208191315, 7.392121065415956, 5.32887425820658, 1.710240262612552, 0.35300654459682423]]\n",
      "2021-01-14 18:41:09,532 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 18:41:09,532 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:41:09,535 : INFO : built Dictionary(595 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 4410 corpus positions)\n",
      "2021-01-14 18:41:10,776 : INFO : token count processed\n",
      "2021-01-14 18:41:10,807 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 18:41:20,205 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:41:20,206 : INFO : entropies processed\n",
      "2021-01-14 18:41:20,207 : INFO : extropies processed\n",
      "2021-01-14 18:41:20,215 : INFO : token count processed\n",
      "2021-01-14 18:41:20,222 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:41:20,227 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:41:20,228 : INFO : vocab #32006\n",
      "2021-01-14 18:41:20,235 : INFO : diff #set()\n",
      "2021-01-14 18:41:39,253 : INFO : alphabet #32006\n",
      "2021-01-14 18:41:48,709 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.0849744705299602, 0.4796221796163381], [0.7204280495643616, 0.27957195], [5.029438378928221, 1.414631448906934], [5.681880802803404, 7.482466367279176, 7.772065181653003, 5.392281988429575, 2.0901843788495995, 0.28959881437382773]]\n",
      "2021-01-14 18:41:48,713 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:41:48,714 : INFO : built Dictionary(201 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 632 corpus positions)\n",
      "2021-01-14 18:41:48,930 : INFO : token count processed\n",
      "2021-01-14 18:41:48,974 : INFO : frequencies processed\n",
      "2021-01-14 18:41:58,522 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:41:58,523 : INFO : entropies processed\n",
      "2021-01-14 18:41:58,524 : INFO : extropies processed\n",
      "2021-01-14 18:41:58,531 : INFO : token count processed\n",
      "2021-01-14 18:41:58,536 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:41:58,542 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:41:58,543 : INFO : vocab #32006\n",
      "2021-01-14 18:41:58,550 : INFO : diff #set()\n",
      "2021-01-14 18:42:17,422 : INFO : alphabet #32006\n",
      "2021-01-14 18:42:26,841 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.1733090444658922, 0.46012784171050003], [0.8600066304206848, 0.13999337], [3.4182958340544896, 1.369895090630202], [5.681880802803404, 6.372162341197667, 6.836260461922485, 5.217782682078585, 1.1543796591190816, 0.4640981207248185]]\n",
      "2021-01-14 18:42:26,847 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:42:26,849 : INFO : built Dictionary(339 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 2053 corpus positions)\n",
      "2021-01-14 18:42:27,336 : INFO : token count processed\n",
      "2021-01-14 18:42:27,382 : INFO : frequencies processed\n",
      "2021-01-14 18:42:36,914 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:42:36,916 : INFO : entropies processed\n",
      "2021-01-14 18:42:36,917 : INFO : extropies processed\n",
      "2021-01-14 18:42:36,924 : INFO : token count processed\n",
      "2021-01-14 18:42:36,929 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:42:36,933 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:42:36,934 : INFO : vocab #32006\n",
      "2021-01-14 18:42:36,941 : INFO : diff #set()\n",
      "2021-01-14 18:42:56,332 : INFO : alphabet #32006\n",
      "2021-01-14 18:43:07,200 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.1501863125331504, 0.4650759769844748], [0.8312273323535919, 0.16877267], [4.1852301329094015, 1.3859226686864623], [5.681880802803404, 6.798155919669889, 7.077112549812229, 5.402924172661065, 1.3952317470088254, 0.27895663014234007]]\n",
      "2021-01-14 18:43:07,204 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:43:07,205 : INFO : built Dictionary(204 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 769 corpus positions)\n",
      "2021-01-14 18:43:07,439 : INFO : token count processed\n",
      "2021-01-14 18:43:07,478 : INFO : frequencies processed\n",
      "2021-01-14 18:43:18,411 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:43:18,412 : INFO : entropies processed\n",
      "2021-01-14 18:43:18,413 : INFO : extropies processed\n",
      "2021-01-14 18:43:18,420 : INFO : token count processed\n",
      "2021-01-14 18:43:18,424 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:43:18,428 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:43:18,429 : INFO : vocab #32006\n",
      "2021-01-14 18:43:18,435 : INFO : diff #set()\n",
      "2021-01-14 18:43:37,361 : INFO : alphabet #32006\n",
      "2021-01-14 18:43:46,933 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.141218844749482, 0.46702372457262664], [0.8340169340372086, 0.16598307], [3.625, 1.3785939957689282], [5.681880802803404, 6.271631856729336, 6.687166837018935, 5.266345822513804, 1.0052860342155316, 0.4155349802895998]]\n",
      "2021-01-14 18:43:46,940 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:43:46,943 : INFO : built Dictionary(370 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 3284 corpus positions)\n",
      "2021-01-14 18:43:47,516 : INFO : token count processed\n",
      "2021-01-14 18:43:47,568 : INFO : frequencies processed\n",
      "2021-01-14 18:43:57,003 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:43:57,004 : INFO : entropies processed\n",
      "2021-01-14 18:43:57,005 : INFO : extropies processed\n",
      "2021-01-14 18:43:57,016 : INFO : token count processed\n",
      "2021-01-14 18:43:57,020 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:43:57,024 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:43:57,025 : INFO : vocab #32006\n",
      "2021-01-14 18:43:57,031 : INFO : diff #set()\n",
      "2021-01-14 18:44:16,290 : INFO : alphabet #32006\n",
      "2021-01-14 18:44:26,205 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.1233375343218346, 0.4709566820328387], [0.7956049144268036, 0.20439509], [4.391489514102183, 1.3974280420004093], [5.681880802803404, 6.873598627629562, 7.082012270479628, 5.473467159953338, 1.400131467676224, 0.2084136428500658]]\n",
      "2021-01-14 18:44:26,208 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:44:26,209 : INFO : built Dictionary(136 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 232 corpus positions)\n",
      "2021-01-14 18:44:26,319 : INFO : token count processed\n",
      "2021-01-14 18:44:26,351 : INFO : frequencies processed\n",
      "2021-01-14 18:44:35,784 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:44:35,786 : INFO : entropies processed\n",
      "2021-01-14 18:44:35,786 : INFO : extropies processed\n",
      "2021-01-14 18:44:35,793 : INFO : token count processed\n",
      "2021-01-14 18:44:35,798 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:44:35,802 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:44:35,803 : INFO : vocab #32006\n",
      "2021-01-14 18:44:35,810 : INFO : diff #set()\n",
      "2021-01-14 18:44:55,065 : INFO : alphabet #32006\n",
      "2021-01-14 18:45:04,485 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.1290050808303154, 0.46970296548564294], [0.8120479136705399, 0.18795209], [2.9219280948873623, 1.3359016564230495], [5.681880802803404, 6.049830202851529, 6.758769790439071, 4.9729412152158625, 1.0768889876356669, 0.7089395875875413]]\n",
      "2021-01-14 18:45:04,490 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:45:04,492 : INFO : built Dictionary(277 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1312 corpus positions)\n",
      "2021-01-14 18:45:04,858 : INFO : token count processed\n",
      "2021-01-14 18:45:04,890 : INFO : frequencies processed\n",
      "2021-01-14 18:45:14,297 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:45:14,298 : INFO : entropies processed\n",
      "2021-01-14 18:45:14,299 : INFO : extropies processed\n",
      "2021-01-14 18:45:14,306 : INFO : token count processed\n",
      "2021-01-14 18:45:14,310 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:45:14,315 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:45:14,316 : INFO : vocab #32006\n",
      "2021-01-14 18:45:14,323 : INFO : diff #set()\n",
      "2021-01-14 18:45:33,275 : INFO : alphabet #32006\n",
      "2021-01-14 18:45:42,700 : INFO : Computed distances or similarities ('279', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.1368846205340986, 0.4679709846711599], [0.7717409431934357, 0.22825906], [3.593269689515108, 1.3605909165877443], [5.681880802803404, 6.778844940588858, 7.099290634089366, 5.361435109302896, 1.417409831285962, 0.320445693500508]]\n",
      "2021-01-14 18:45:42,703 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:45:42,704 : INFO : built Dictionary(180 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 345 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 18:45:42,883 : INFO : token count processed\n",
      "2021-01-14 18:45:42,915 : INFO : frequencies processed\n",
      "2021-01-14 18:45:52,279 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:45:52,280 : INFO : entropies processed\n",
      "2021-01-14 18:45:52,281 : INFO : extropies processed\n",
      "2021-01-14 18:45:52,294 : INFO : token count processed\n",
      "2021-01-14 18:45:52,298 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:45:52,302 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:45:52,303 : INFO : vocab #32006\n",
      "2021-01-14 18:45:52,310 : INFO : diff #set()\n",
      "2021-01-14 18:46:11,245 : INFO : alphabet #32006\n",
      "2021-01-14 18:46:20,688 : INFO : Computed distances or similarities ('279', 'sacp-python-common/setup.py')[[1.1326206411287536, 0.4689066497409121], [0.8116039484739304, 0.18839605], [3.3927474104487847, 1.3672090515720436], [5.681880802803404, 6.469677430851302, 7.043604089132102, 5.107954144522604, 1.361723286328698, 0.5739266582807998]]\n",
      "2021-01-14 18:46:20,693 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:46:20,695 : INFO : built Dictionary(243 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1247 corpus positions)\n",
      "2021-01-14 18:46:20,982 : INFO : token count processed\n",
      "2021-01-14 18:46:21,014 : INFO : frequencies processed\n",
      "2021-01-14 18:46:30,423 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:46:30,424 : INFO : entropies processed\n",
      "2021-01-14 18:46:30,425 : INFO : extropies processed\n",
      "2021-01-14 18:46:30,432 : INFO : token count processed\n",
      "2021-01-14 18:46:30,439 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:46:30,443 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:46:30,444 : INFO : vocab #32006\n",
      "2021-01-14 18:46:30,451 : INFO : diff #set()\n",
      "2021-01-14 18:46:49,450 : INFO : alphabet #32006\n",
      "2021-01-14 18:46:58,870 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.1479343890867757, 0.4655635689250099], [0.8320800364017487, 0.16791996], [4.001822825622231, 1.3874928763412118], [5.681880802803404, 6.459180448028249, 6.782550612956028, 5.358510637875625, 1.100669810152624, 0.3233701649277787]]\n",
      "2021-01-14 18:46:58,874 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:46:58,876 : INFO : built Dictionary(157 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 397 corpus positions)\n",
      "2021-01-14 18:46:59,034 : INFO : token count processed\n",
      "2021-01-14 18:46:59,096 : INFO : frequencies processed\n",
      "2021-01-14 18:47:08,538 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:47:08,539 : INFO : entropies processed\n",
      "2021-01-14 18:47:08,540 : INFO : extropies processed\n",
      "2021-01-14 18:47:08,553 : INFO : token count processed\n",
      "2021-01-14 18:47:08,557 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:47:08,562 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:47:08,563 : INFO : vocab #32006\n",
      "2021-01-14 18:47:08,569 : INFO : diff #set()\n",
      "2021-01-14 18:47:27,566 : INFO : alphabet #32006\n",
      "2021-01-14 18:47:36,982 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.1491524836109983, 0.46529969726475784], [0.8400259763002396, 0.15997402], [3.8073549220576055, 1.389897650914655], [5.681880802803404, 6.097125733496388, 6.6553588977202445, 5.123647638579548, 0.9734780949168407, 0.5582331642238563]]\n",
      "2021-01-14 18:47:36,986 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:47:36,987 : INFO : built Dictionary(148 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 393 corpus positions)\n",
      "2021-01-14 18:47:37,115 : INFO : token count processed\n",
      "2021-01-14 18:47:37,148 : INFO : frequencies processed\n",
      "2021-01-14 18:47:46,677 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:47:46,678 : INFO : entropies processed\n",
      "2021-01-14 18:47:46,679 : INFO : extropies processed\n",
      "2021-01-14 18:47:46,686 : INFO : token count processed\n",
      "2021-01-14 18:47:46,690 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:47:46,694 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:47:46,695 : INFO : vocab #32006\n",
      "2021-01-14 18:47:46,702 : INFO : diff #set()\n",
      "2021-01-14 18:48:05,558 : INFO : alphabet #32006\n",
      "2021-01-14 18:48:15,080 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.1301086483527347, 0.46945962158940785], [0.832837849855423, 0.16716215], [3.906890595608518, 1.3934994297128018], [5.681880802803404, 6.0695858597523715, 6.615087706267662, 5.136378956288112, 0.9332069034642583, 0.5455018465152905]]\n",
      "2021-01-14 18:48:15,084 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 18:48:15,085 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:48:15,086 : INFO : built Dictionary(151 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 489 corpus positions)\n",
      "2021-01-14 18:48:15,228 : INFO : token count processed\n",
      "2021-01-14 18:48:15,262 : INFO : frequencies processed\n",
      "2021-01-14 18:48:24,700 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:48:24,701 : INFO : entropies processed\n",
      "2021-01-14 18:48:24,702 : INFO : extropies processed\n",
      "2021-01-14 18:48:24,709 : INFO : token count processed\n",
      "2021-01-14 18:48:24,713 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:48:24,717 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:48:24,718 : INFO : vocab #32006\n",
      "2021-01-14 18:48:24,725 : INFO : diff #set()\n",
      "2021-01-14 18:48:43,599 : INFO : alphabet #32006\n",
      "2021-01-14 18:48:53,141 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.1174747488491361, 0.4722606494096365], [0.8077954053878784, 0.1922046], [3.75, 1.3846096858033596], [5.681880802803404, 6.104787343210121, 6.572777803189899, 5.2138903428236265, 0.8908970003864951, 0.4679904599797782]]\n",
      "2021-01-14 18:48:53,158 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 18:48:53,159 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:48:53,163 : INFO : built Dictionary(428 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 9174 corpus positions)\n",
      "2021-01-14 18:48:53,822 : INFO : token count processed\n",
      "2021-01-14 18:48:53,853 : INFO : frequencies processed\n",
      "2021-01-14 18:49:03,720 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:49:03,722 : INFO : entropies processed\n",
      "2021-01-14 18:49:03,722 : INFO : extropies processed\n",
      "2021-01-14 18:49:03,732 : INFO : token count processed\n",
      "2021-01-14 18:49:03,737 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:49:03,741 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:49:03,742 : INFO : vocab #32006\n",
      "2021-01-14 18:49:03,751 : INFO : diff #set()\n",
      "2021-01-14 18:49:23,829 : INFO : alphabet #32006\n",
      "2021-01-14 18:49:35,481 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.116527329171232, 0.47247204712049223], [0.8144966065883636, 0.1855034], [4.400432302535625, 1.396864578091662], [5.681880802803404, 6.89087415148015, 7.074981502405659, 5.497773451877895, 1.3931006996022548, 0.18410735092550823]]\n",
      "2021-01-14 18:49:35,487 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:49:35,489 : INFO : built Dictionary(289 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 2334 corpus positions)\n",
      "2021-01-14 18:49:35,870 : INFO : token count processed\n",
      "2021-01-14 18:49:35,907 : INFO : frequencies processed\n",
      "2021-01-14 18:49:45,414 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:49:45,415 : INFO : entropies processed\n",
      "2021-01-14 18:49:45,416 : INFO : extropies processed\n",
      "2021-01-14 18:49:45,423 : INFO : token count processed\n",
      "2021-01-14 18:49:45,429 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:49:45,435 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:49:45,436 : INFO : vocab #32006\n",
      "2021-01-14 18:49:45,443 : INFO : diff #set()\n",
      "2021-01-14 18:50:04,428 : INFO : alphabet #32006\n",
      "2021-01-14 18:50:13,722 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.1013855238321655, 0.47587650560015393], [0.7524506747722626, 0.24754933], [4.202819531114783, 1.389800251098866], [5.681880802803404, 6.655493573668506, 6.875043594378939, 5.462330782092971, 1.193162791575535, 0.2195500207104324]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 18:50:13,726 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:50:13,728 : INFO : built Dictionary(269 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1341 corpus positions)\n",
      "2021-01-14 18:50:14,067 : INFO : token count processed\n",
      "2021-01-14 18:50:14,100 : INFO : frequencies processed\n",
      "2021-01-14 18:50:23,538 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:50:23,539 : INFO : entropies processed\n",
      "2021-01-14 18:50:23,540 : INFO : extropies processed\n",
      "2021-01-14 18:50:23,547 : INFO : token count processed\n",
      "2021-01-14 18:50:23,554 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:50:23,559 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:50:23,560 : INFO : vocab #32006\n",
      "2021-01-14 18:50:23,567 : INFO : diff #set()\n",
      "2021-01-14 18:50:42,737 : INFO : alphabet #32006\n",
      "2021-01-14 18:50:52,154 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.1588230267370239, 0.4632153667137137], [0.8357589393854141, 0.16424106], [4.20184123230257, 1.4009137160862843], [5.681880802803404, 6.6236746347295465, 6.893227387920479, 5.412328049612471, 1.2113465851170755, 0.26955275319093275]]\n",
      "2021-01-14 18:50:52,159 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:50:52,160 : INFO : built Dictionary(270 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1280 corpus positions)\n",
      "2021-01-14 18:50:52,496 : INFO : token count processed\n",
      "2021-01-14 18:50:52,528 : INFO : frequencies processed\n",
      "2021-01-14 18:51:01,945 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:51:01,946 : INFO : entropies processed\n",
      "2021-01-14 18:51:01,947 : INFO : extropies processed\n",
      "2021-01-14 18:51:01,954 : INFO : token count processed\n",
      "2021-01-14 18:51:01,960 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:51:01,964 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:51:01,965 : INFO : vocab #32006\n",
      "2021-01-14 18:51:01,972 : INFO : diff #set()\n",
      "2021-01-14 18:51:20,927 : INFO : alphabet #32006\n",
      "2021-01-14 18:51:30,350 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.1104766201319471, 0.4738266183386955], [0.7359248995780945, 0.2640751], [4.1336606896881865, 1.396507583562077], [5.681880802803404, 6.75472436518627, 7.005016566772922, 5.431588601216752, 1.3231357639695185, 0.2502922015866522]]\n",
      "2021-01-14 18:51:30,354 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:51:30,356 : INFO : built Dictionary(227 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1075 corpus positions)\n",
      "2021-01-14 18:51:30,625 : INFO : token count processed\n",
      "2021-01-14 18:51:30,685 : INFO : frequencies processed\n",
      "2021-01-14 18:51:40,102 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:51:40,103 : INFO : entropies processed\n",
      "2021-01-14 18:51:40,104 : INFO : extropies processed\n",
      "2021-01-14 18:51:40,111 : INFO : token count processed\n",
      "2021-01-14 18:51:40,115 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:51:40,119 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:51:40,120 : INFO : vocab #32006\n",
      "2021-01-14 18:51:40,127 : INFO : diff #set()\n",
      "2021-01-14 18:51:59,107 : INFO : alphabet #32006\n",
      "2021-01-14 18:52:08,516 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.127222931001934, 0.4700964743403713], [0.762413278222084, 0.23758672], [3.75, 1.3846096858033596], [5.681880802803404, 6.597313085495733, 6.886917225364556, 5.392276662934581, 1.205036422561152, 0.28960413986882294]]\n",
      "2021-01-14 18:52:08,520 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:52:08,521 : INFO : built Dictionary(246 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1041 corpus positions)\n",
      "2021-01-14 18:52:08,839 : INFO : token count processed\n",
      "2021-01-14 18:52:08,913 : INFO : frequencies processed\n",
      "2021-01-14 18:52:18,445 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:52:18,446 : INFO : entropies processed\n",
      "2021-01-14 18:52:18,447 : INFO : extropies processed\n",
      "2021-01-14 18:52:18,454 : INFO : token count processed\n",
      "2021-01-14 18:52:18,458 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:52:18,463 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:52:18,464 : INFO : vocab #32006\n",
      "2021-01-14 18:52:18,470 : INFO : diff #set()\n",
      "2021-01-14 18:52:37,332 : INFO : alphabet #32006\n",
      "2021-01-14 18:52:46,730 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.1264019712701951, 0.47027796884643364], [0.7977407872676849, 0.20225921], [4.16829583405449, 1.393424708894837], [5.681880802803404, 6.659481538516613, 6.9467925828310815, 5.394569758488936, 1.2649117800276777, 0.2873110443144684]]\n",
      "2021-01-14 18:52:46,734 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:52:46,736 : INFO : built Dictionary(274 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1203 corpus positions)\n",
      "2021-01-14 18:52:47,088 : INFO : token count processed\n",
      "2021-01-14 18:52:47,120 : INFO : frequencies processed\n",
      "2021-01-14 18:52:56,634 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:52:56,635 : INFO : entropies processed\n",
      "2021-01-14 18:52:56,635 : INFO : extropies processed\n",
      "2021-01-14 18:52:56,643 : INFO : token count processed\n",
      "2021-01-14 18:52:56,647 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:52:56,652 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:52:56,653 : INFO : vocab #32006\n",
      "2021-01-14 18:52:56,661 : INFO : diff #set()\n",
      "2021-01-14 18:53:15,928 : INFO : alphabet #32006\n",
      "2021-01-14 18:53:25,458 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.1440471364297775, 0.4664076563471357], [0.8090120851993561, 0.19098791], [4.058813890331201, 1.3971393108496475], [5.681880802803404, 6.774682571479102, 7.032875481528828, 5.423687892753678, 1.3509946787254243, 0.25819291004972644]]\n",
      "2021-01-14 18:53:25,473 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:53:25,476 : INFO : built Dictionary(444 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 7940 corpus positions)\n",
      "2021-01-14 18:53:26,244 : INFO : token count processed\n",
      "2021-01-14 18:53:26,286 : INFO : frequencies processed\n",
      "2021-01-14 18:53:35,724 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:53:35,725 : INFO : entropies processed\n",
      "2021-01-14 18:53:35,726 : INFO : extropies processed\n",
      "2021-01-14 18:53:35,735 : INFO : token count processed\n",
      "2021-01-14 18:53:35,742 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:53:35,747 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:53:35,748 : INFO : vocab #32006\n",
      "2021-01-14 18:53:35,756 : INFO : diff #set()\n",
      "2021-01-14 18:53:54,618 : INFO : alphabet #32006\n",
      "2021-01-14 18:54:04,321 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.122776151690049, 0.4710812297395793], [0.8224084675312042, 0.17759153], [4.395998870534841, 1.3959888784142582], [5.681880802803404, 6.839453716525233, 7.019168118705842, 5.502166400622794, 1.3372873159024383, 0.1797144021806094]]\n",
      "2021-01-14 18:54:04,326 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:54:04,328 : INFO : built Dictionary(347 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 2382 corpus positions)\n",
      "2021-01-14 18:54:04,805 : INFO : token count processed\n",
      "2021-01-14 18:54:04,837 : INFO : frequencies processed\n",
      "2021-01-14 18:54:14,254 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:54:14,255 : INFO : entropies processed\n",
      "2021-01-14 18:54:14,255 : INFO : extropies processed\n",
      "2021-01-14 18:54:14,263 : INFO : token count processed\n",
      "2021-01-14 18:54:14,268 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:54:14,273 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:54:14,274 : INFO : vocab #32006\n",
      "2021-01-14 18:54:14,281 : INFO : diff #set()\n",
      "2021-01-14 18:54:33,395 : INFO : alphabet #32006\n",
      "2021-01-14 18:54:42,910 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.1187268663814545, 0.4719815545209406], [0.7483351826667786, 0.25166482], [4.17512313511346, 1.3894783697023454], [5.681880802803404, 6.86432793886027, 7.053697972280032, 5.492510769383641, 1.3718171694766284, 0.18937003341976233]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 18:54:42,914 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:54:42,916 : INFO : built Dictionary(190 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 584 corpus positions)\n",
      "2021-01-14 18:54:43,114 : INFO : token count processed\n",
      "2021-01-14 18:54:43,160 : INFO : frequencies processed\n",
      "2021-01-14 18:54:52,565 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:54:52,566 : INFO : entropies processed\n",
      "2021-01-14 18:54:52,567 : INFO : extropies processed\n",
      "2021-01-14 18:54:52,574 : INFO : token count processed\n",
      "2021-01-14 18:54:52,582 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:54:52,586 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:54:52,587 : INFO : vocab #32006\n",
      "2021-01-14 18:54:52,594 : INFO : diff #set()\n",
      "2021-01-14 18:55:11,563 : INFO : alphabet #32006\n",
      "2021-01-14 18:55:21,129 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.1247041557439652, 0.4706537600995325], [0.7343055009841919, 0.2656945], [3.521640636343319, 1.3740281872300928], [5.681880802803404, 6.431978396403875, 6.84043007895513, 5.27342912025215, 1.1585492761517262, 0.40845168255125497]]\n",
      "2021-01-14 18:55:21,133 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:55:21,135 : INFO : built Dictionary(241 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 738 corpus positions)\n",
      "2021-01-14 18:55:21,437 : INFO : token count processed\n",
      "2021-01-14 18:55:21,470 : INFO : frequencies processed\n",
      "2021-01-14 18:55:30,862 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:55:30,863 : INFO : entropies processed\n",
      "2021-01-14 18:55:30,864 : INFO : extropies processed\n",
      "2021-01-14 18:55:30,871 : INFO : token count processed\n",
      "2021-01-14 18:55:30,877 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:55:30,883 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:55:30,884 : INFO : vocab #32006\n",
      "2021-01-14 18:55:30,892 : INFO : diff #set()\n",
      "2021-01-14 18:55:50,049 : INFO : alphabet #32006\n",
      "2021-01-14 18:55:59,634 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/test_auth_utility.py')[[1.132309180281868, 0.46897514171364724], [0.7861236035823822, 0.2138764], [4.251629167387822, 1.4017549191576797], [5.681880802803404, 6.911818353685893, 7.208823605270526, 5.384875551218771, 1.526942802467122, 0.29700525158463265]]\n",
      "2021-01-14 18:55:59,648 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:55:59,651 : INFO : built Dictionary(339 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 7268 corpus positions)\n",
      "2021-01-14 18:56:00,153 : INFO : token count processed\n",
      "2021-01-14 18:56:00,187 : INFO : frequencies processed\n",
      "2021-01-14 18:56:09,597 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:56:09,598 : INFO : entropies processed\n",
      "2021-01-14 18:56:09,599 : INFO : extropies processed\n",
      "2021-01-14 18:56:09,608 : INFO : token count processed\n",
      "2021-01-14 18:56:09,612 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:56:09,616 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:56:09,617 : INFO : vocab #32006\n",
      "2021-01-14 18:56:09,624 : INFO : diff #set()\n",
      "2021-01-14 18:56:28,594 : INFO : alphabet #32006\n",
      "2021-01-14 18:56:37,993 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.1547500582529926, 0.46409094928196465], [0.8329466879367828, 0.16705331], [3.6644977792004623, 1.381962919072374], [5.681880802803404, 6.363791471162389, 6.447502898986242, 5.59816937497955, 0.7656220961828382, 0.08371142782385288]]\n",
      "2021-01-14 18:56:37,998 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:56:37,999 : INFO : built Dictionary(233 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1338 corpus positions)\n",
      "2021-01-14 18:56:38,271 : INFO : token count processed\n",
      "2021-01-14 18:56:38,305 : INFO : frequencies processed\n",
      "2021-01-14 18:56:47,702 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:56:47,703 : INFO : entropies processed\n",
      "2021-01-14 18:56:47,704 : INFO : extropies processed\n",
      "2021-01-14 18:56:47,711 : INFO : token count processed\n",
      "2021-01-14 18:56:47,715 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:56:47,721 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:56:47,722 : INFO : vocab #32006\n",
      "2021-01-14 18:56:47,730 : INFO : diff #set()\n",
      "2021-01-14 18:57:06,719 : INFO : alphabet #32006\n",
      "2021-01-14 18:57:16,144 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.132013206449054, 0.46904024654966214], [0.8237317651510239, 0.17626823], [4.163856189774724, 1.3937789596714385], [5.681880802803404, 6.29000629755059, 6.650901005659447, 5.320986094694547, 0.9690202028560435, 0.36089470810885693]]\n",
      "2021-01-14 18:57:16,149 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:57:16,150 : INFO : built Dictionary(246 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1291 corpus positions)\n",
      "2021-01-14 18:57:16,454 : INFO : token count processed\n",
      "2021-01-14 18:57:16,524 : INFO : frequencies processed\n",
      "2021-01-14 18:57:26,054 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:57:26,055 : INFO : entropies processed\n",
      "2021-01-14 18:57:26,056 : INFO : extropies processed\n",
      "2021-01-14 18:57:26,067 : INFO : token count processed\n",
      "2021-01-14 18:57:26,072 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:57:26,076 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:57:26,077 : INFO : vocab #32006\n",
      "2021-01-14 18:57:26,084 : INFO : diff #set()\n",
      "2021-01-14 18:57:44,918 : INFO : alphabet #32006\n",
      "2021-01-14 18:57:54,483 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.1359952686198085, 0.4681658310255334], [0.817906990647316, 0.18209301], [3.9139770731827515, 1.3837651655416812], [5.681880802803404, 6.361621244785958, 6.714889618515823, 5.328612429073539, 1.033008815712419, 0.35326837372986475]]\n",
      "2021-01-14 18:57:54,488 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:57:54,489 : INFO : built Dictionary(253 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1314 corpus positions)\n",
      "2021-01-14 18:57:54,788 : INFO : token count processed\n",
      "2021-01-14 18:57:54,821 : INFO : frequencies processed\n",
      "2021-01-14 18:58:04,330 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:58:04,331 : INFO : entropies processed\n",
      "2021-01-14 18:58:04,332 : INFO : extropies processed\n",
      "2021-01-14 18:58:04,339 : INFO : token count processed\n",
      "2021-01-14 18:58:04,344 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:58:04,349 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:58:04,350 : INFO : vocab #32006\n",
      "2021-01-14 18:58:04,357 : INFO : diff #set()\n",
      "2021-01-14 18:58:23,218 : INFO : alphabet #32006\n",
      "2021-01-14 18:58:32,439 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.0815316543499571, 0.4804154661353399], [0.7111593782901764, 0.28884062], [3.869331261111518, 1.3749011168088656], [5.681880802803404, 6.620594433343389, 6.9480357430678605, 5.354439493078933, 1.2661549402644567, 0.3274413097244713]]\n",
      "2021-01-14 18:58:32,443 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:58:32,445 : INFO : built Dictionary(226 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1407 corpus positions)\n",
      "2021-01-14 18:58:32,706 : INFO : token count processed\n",
      "2021-01-14 18:58:32,737 : INFO : frequencies processed\n",
      "2021-01-14 18:58:42,191 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:58:42,192 : INFO : entropies processed\n",
      "2021-01-14 18:58:42,193 : INFO : extropies processed\n",
      "2021-01-14 18:58:42,200 : INFO : token count processed\n",
      "2021-01-14 18:58:42,207 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:58:42,211 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:58:42,212 : INFO : vocab #32006\n",
      "2021-01-14 18:58:42,219 : INFO : diff #set()\n",
      "2021-01-14 18:59:01,119 : INFO : alphabet #32006\n",
      "2021-01-14 18:59:10,667 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.108787100426946, 0.4742062391208384], [0.7390939891338348, 0.260906], [3.7834651896016473, 1.3679111184610857], [5.681880802803404, 6.207411496248084, 6.477464093190061, 5.4118282058614255, 0.7955832903866575, 0.27005259694197736]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 18:59:10,670 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:59:10,671 : INFO : built Dictionary(169 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 311 corpus positions)\n",
      "2021-01-14 18:59:10,847 : INFO : token count processed\n",
      "2021-01-14 18:59:10,882 : INFO : frequencies processed\n",
      "2021-01-14 18:59:20,284 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:59:20,285 : INFO : entropies processed\n",
      "2021-01-14 18:59:20,286 : INFO : extropies processed\n",
      "2021-01-14 18:59:20,300 : INFO : token count processed\n",
      "2021-01-14 18:59:20,304 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:59:20,308 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:59:20,309 : INFO : vocab #32006\n",
      "2021-01-14 18:59:20,316 : INFO : diff #set()\n",
      "2021-01-14 18:59:39,326 : INFO : alphabet #32006\n",
      "2021-01-14 18:59:48,762 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.1156029201822741, 0.4726784929536034], [0.7114348411560059, 0.28856516], [3.75, 1.3846096858033596], [5.681880802803404, 6.5805228788529595, 7.031265906392237, 5.231137775264125, 1.3493851035888333, 0.45074302753927764]]\n",
      "2021-01-14 18:59:48,767 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 18:59:48,768 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 18:59:48,769 : INFO : built Dictionary(253 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1272 corpus positions)\n",
      "2021-01-14 18:59:49,084 : INFO : token count processed\n",
      "2021-01-14 18:59:49,145 : INFO : frequencies processed\n",
      "2021-01-14 18:59:58,574 : INFO : scalar_distribution processed\n",
      "2021-01-14 18:59:58,575 : INFO : entropies processed\n",
      "2021-01-14 18:59:58,576 : INFO : extropies processed\n",
      "2021-01-14 18:59:58,584 : INFO : token count processed\n",
      "2021-01-14 18:59:58,588 : INFO : alphabet_source #32006\n",
      "2021-01-14 18:59:58,592 : INFO : alphabet_target #32009\n",
      "2021-01-14 18:59:58,593 : INFO : vocab #32006\n",
      "2021-01-14 18:59:58,600 : INFO : diff #set()\n",
      "2021-01-14 19:00:17,584 : INFO : alphabet #32006\n",
      "2021-01-14 19:00:27,006 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.1770959549432343, 0.45932748059608336], [0.8503193855285645, 0.14968061], [3.625, 1.3785939957689282], [5.681880802803404, 6.422089779976135, 6.694182181350869, 5.40978840142867, 1.0123013785474653, 0.2720924013747341]]\n",
      "2021-01-14 19:00:27,011 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:00:27,013 : INFO : built Dictionary(259 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1554 corpus positions)\n",
      "2021-01-14 19:00:27,330 : INFO : token count processed\n",
      "2021-01-14 19:00:27,399 : INFO : frequencies processed\n",
      "2021-01-14 19:00:36,824 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:00:36,825 : INFO : entropies processed\n",
      "2021-01-14 19:00:36,826 : INFO : extropies processed\n",
      "2021-01-14 19:00:36,833 : INFO : token count processed\n",
      "2021-01-14 19:00:36,837 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:00:36,842 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:00:36,843 : INFO : vocab #32006\n",
      "2021-01-14 19:00:36,849 : INFO : diff #set()\n",
      "2021-01-14 19:00:55,835 : INFO : alphabet #32006\n",
      "2021-01-14 19:01:05,247 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.098040066459393, 0.4766353207389306], [0.7541572749614716, 0.24584273], [3.9582291686698787, 1.3768872542578305], [5.681880802803404, 6.485445644653597, 6.835377285625851, 5.331949161831149, 1.1534964828224474, 0.3499316409722546]]\n",
      "2021-01-14 19:01:05,252 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:01:05,254 : INFO : built Dictionary(237 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1440 corpus positions)\n",
      "2021-01-14 19:01:05,535 : INFO : token count processed\n",
      "2021-01-14 19:01:05,567 : INFO : frequencies processed\n",
      "2021-01-14 19:01:14,966 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:01:14,967 : INFO : entropies processed\n",
      "2021-01-14 19:01:14,968 : INFO : extropies processed\n",
      "2021-01-14 19:01:14,975 : INFO : token count processed\n",
      "2021-01-14 19:01:14,979 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:01:14,983 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:01:14,984 : INFO : vocab #32006\n",
      "2021-01-14 19:01:14,991 : INFO : diff #set()\n",
      "2021-01-14 19:01:33,958 : INFO : alphabet #32006\n",
      "2021-01-14 19:01:43,364 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.1124786028407072, 0.47337757582740614], [0.7446832954883575, 0.2553167], [3.6753108689123635, 1.359444863106031], [5.681880802803404, 6.2276600107346916, 6.495414867192551, 5.4141259463455444, 0.8135340643891471, 0.26775485645785935]]\n",
      "2021-01-14 19:01:43,369 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:01:43,371 : INFO : built Dictionary(226 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1638 corpus positions)\n",
      "2021-01-14 19:01:43,632 : INFO : token count processed\n",
      "2021-01-14 19:01:43,664 : INFO : frequencies processed\n",
      "2021-01-14 19:01:53,176 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:01:53,177 : INFO : entropies processed\n",
      "2021-01-14 19:01:53,178 : INFO : extropies processed\n",
      "2021-01-14 19:01:53,185 : INFO : token count processed\n",
      "2021-01-14 19:01:53,190 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:01:53,194 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:01:53,195 : INFO : vocab #32006\n",
      "2021-01-14 19:01:53,206 : INFO : diff #set()\n",
      "2021-01-14 19:02:12,096 : INFO : alphabet #32006\n",
      "2021-01-14 19:02:21,517 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.131705069103971, 0.4691080461802976], [0.8132868409156799, 0.18671316], [3.8408115970719874, 1.3733868811825505], [5.681880802803404, 6.253918170574241, 6.5641774531803545, 5.371621520197289, 0.8822966503769507, 0.3102592826061139]]\n",
      "2021-01-14 19:02:21,521 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:02:21,522 : INFO : built Dictionary(199 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 735 corpus positions)\n",
      "2021-01-14 19:02:21,746 : INFO : token count processed\n",
      "2021-01-14 19:02:21,778 : INFO : frequencies processed\n",
      "2021-01-14 19:02:31,286 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:02:31,287 : INFO : entropies processed\n",
      "2021-01-14 19:02:31,288 : INFO : extropies processed\n",
      "2021-01-14 19:02:31,295 : INFO : token count processed\n",
      "2021-01-14 19:02:31,299 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:02:31,304 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:02:31,305 : INFO : vocab #32006\n",
      "2021-01-14 19:02:31,312 : INFO : diff #set()\n",
      "2021-01-14 19:02:50,166 : INFO : alphabet #32006\n",
      "2021-01-14 19:02:59,467 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.0582003720566613, 0.4858613444913275], [0.6808898746967316, 0.31911013], [3.675310868912364, 1.359444863106031], [5.681880802803404, 6.374522245625576, 6.760059866894602, 5.296343181534377, 1.0781790640911986, 0.3855376212690267]]\n",
      "2021-01-14 19:02:59,472 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:02:59,474 : INFO : built Dictionary(316 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 2017 corpus positions)\n",
      "2021-01-14 19:02:59,912 : INFO : token count processed\n",
      "2021-01-14 19:02:59,957 : INFO : frequencies processed\n",
      "2021-01-14 19:03:09,753 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:03:09,754 : INFO : entropies processed\n",
      "2021-01-14 19:03:09,755 : INFO : extropies processed\n",
      "2021-01-14 19:03:09,762 : INFO : token count processed\n",
      "2021-01-14 19:03:09,768 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:03:09,773 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:03:09,774 : INFO : vocab #32006\n",
      "2021-01-14 19:03:09,780 : INFO : diff #set()\n",
      "2021-01-14 19:03:28,660 : INFO : alphabet #32006\n",
      "2021-01-14 19:03:38,217 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.1182497135851102, 0.47208787216475673], [0.7945997565984726, 0.20540024], [4.1162646156680225, 1.3841455727593104], [5.681880802803404, 6.731238669067808, 7.080804980860985, 5.332314491010227, 1.3989241780575812, 0.3495663117931773]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 19:03:38,221 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:03:38,223 : INFO : built Dictionary(243 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1409 corpus positions)\n",
      "2021-01-14 19:03:38,521 : INFO : token count processed\n",
      "2021-01-14 19:03:38,567 : INFO : frequencies processed\n",
      "2021-01-14 19:03:47,985 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:03:47,986 : INFO : entropies processed\n",
      "2021-01-14 19:03:47,987 : INFO : extropies processed\n",
      "2021-01-14 19:03:47,994 : INFO : token count processed\n",
      "2021-01-14 19:03:47,999 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:03:48,003 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:03:48,004 : INFO : vocab #32006\n",
      "2021-01-14 19:03:48,011 : INFO : diff #set()\n",
      "2021-01-14 19:04:07,041 : INFO : alphabet #32006\n",
      "2021-01-14 19:04:16,578 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.1165636403021872, 0.47246394153176857], [0.801891565322876, 0.19810843], [4.105388542207534, 1.3936738798262605], [5.681880802803404, 6.503741451859337, 6.8208409639990055, 5.3647812906637355, 1.1389601611956017, 0.31709951213966825]]\n",
      "2021-01-14 19:04:16,583 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:04:16,585 : INFO : built Dictionary(264 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 1731 corpus positions)\n",
      "2021-01-14 19:04:16,918 : INFO : token count processed\n",
      "2021-01-14 19:04:16,950 : INFO : frequencies processed\n",
      "2021-01-14 19:04:26,367 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:04:26,368 : INFO : entropies processed\n",
      "2021-01-14 19:04:26,369 : INFO : extropies processed\n",
      "2021-01-14 19:04:26,376 : INFO : token count processed\n",
      "2021-01-14 19:04:26,381 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:04:26,385 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:04:26,386 : INFO : vocab #32006\n",
      "2021-01-14 19:04:26,392 : INFO : diff #set()\n",
      "2021-01-14 19:04:45,368 : INFO : alphabet #32006\n",
      "2021-01-14 19:04:54,813 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.1140829392570433, 0.47301833879395105], [0.781458243727684, 0.21854176], [4.012188403968661, 1.3787186434127232], [5.681880802803404, 6.334729224484471, 6.578147929633978, 5.438462097653897, 0.8962671268305744, 0.24341870514950692]]\n",
      "2021-01-14 19:04:54,819 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:04:54,821 : INFO : built Dictionary(261 unique tokens: ['#', \"'\", ',', '-', '.']...) from 2 documents (total 2080 corpus positions)\n",
      "2021-01-14 19:04:55,148 : INFO : token count processed\n",
      "2021-01-14 19:04:55,180 : INFO : frequencies processed\n",
      "2021-01-14 19:05:04,580 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:05:04,581 : INFO : entropies processed\n",
      "2021-01-14 19:05:04,582 : INFO : extropies processed\n",
      "2021-01-14 19:05:04,589 : INFO : token count processed\n",
      "2021-01-14 19:05:04,595 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:05:04,599 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:05:04,600 : INFO : vocab #32006\n",
      "2021-01-14 19:05:04,606 : INFO : diff #set()\n",
      "2021-01-14 19:05:23,610 : INFO : alphabet #32006\n",
      "2021-01-14 19:05:33,033 : INFO : Computed distances or similarities ('279', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.0857514935978771, 0.4794435018119158], [0.75047168135643, 0.24952832], [4.031401845392171, 1.3822343529267183], [5.681880802803404, 6.21319712067992, 6.499775775210612, 5.395302148272712, 0.817894972407208, 0.2865786545306914]]\n",
      "2021-01-14 19:05:33,038 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 19:05:33,039 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:05:33,041 : INFO : built Dictionary(269 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1403 corpus positions)\n",
      "2021-01-14 19:05:33,124 : INFO : token count processed\n",
      "2021-01-14 19:05:33,155 : INFO : frequencies processed\n",
      "2021-01-14 19:05:42,566 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:05:42,567 : INFO : entropies processed\n",
      "2021-01-14 19:05:42,568 : INFO : extropies processed\n",
      "2021-01-14 19:05:42,575 : INFO : token count processed\n",
      "2021-01-14 19:05:42,580 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:05:42,584 : INFO : alphabet_target #32010\n",
      "2021-01-14 19:05:42,585 : INFO : vocab #32006\n",
      "2021-01-14 19:05:42,591 : INFO : diff #set()\n",
      "2021-01-14 19:06:01,580 : INFO : alphabet #32006\n",
      "2021-01-14 19:06:11,006 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.295504648026328, 0.4356340558316006], [0.9720767177641392, 0.027923282], [1.0, 1.0], [3.4182958340544896, 6.905617163738059, 7.024450253543922, 3.2994627442486273, 3.606154419489432, 0.11883308980586271]]\n",
      "2021-01-14 19:06:11,012 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:06:11,014 : INFO : built Dictionary(359 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 2293 corpus positions)\n",
      "2021-01-14 19:06:11,144 : INFO : token count processed\n",
      "2021-01-14 19:06:11,176 : INFO : frequencies processed\n",
      "2021-01-14 19:06:20,576 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:06:20,577 : INFO : entropies processed\n",
      "2021-01-14 19:06:20,578 : INFO : extropies processed\n",
      "2021-01-14 19:06:20,585 : INFO : token count processed\n",
      "2021-01-14 19:06:20,589 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:06:20,595 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:06:20,596 : INFO : vocab #32006\n",
      "2021-01-14 19:06:20,604 : INFO : diff #set()\n",
      "2021-01-14 19:06:39,591 : INFO : alphabet #32006\n",
      "2021-01-14 19:06:49,009 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.2994190580877953, 0.4348924553280877], [0.980039095506072, 0.019960904], [1.584962500721156, 1.1699250014423124], [3.4182958340544896, 7.1219284286457345, 7.326775272431516, 3.2134489902687084, 3.9084794383770265, 0.20484684378578155]]\n",
      "2021-01-14 19:06:49,014 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:06:49,016 : INFO : built Dictionary(280 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 2276 corpus positions)\n",
      "2021-01-14 19:06:49,101 : INFO : token count processed\n",
      "2021-01-14 19:06:49,133 : INFO : frequencies processed\n",
      "2021-01-14 19:06:58,573 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:06:58,574 : INFO : entropies processed\n",
      "2021-01-14 19:06:58,575 : INFO : extropies processed\n",
      "2021-01-14 19:06:58,583 : INFO : token count processed\n",
      "2021-01-14 19:06:58,587 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:06:58,592 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:06:58,593 : INFO : vocab #32006\n",
      "2021-01-14 19:06:58,600 : INFO : diff #set()\n",
      "2021-01-14 19:07:17,839 : INFO : alphabet #32006\n",
      "2021-01-14 19:07:27,252 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.3010418389703475, 0.4345857528811698], [0.9796568043529987, 0.020343196], [0.0, 0.0], [3.4182958340544896, 6.41099024988467, 6.469609060359069, 3.3596770235800903, 3.0513132263045795, 0.058618810474398764]]\n",
      "2021-01-14 19:07:27,256 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:07:27,257 : INFO : built Dictionary(159 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 675 corpus positions)\n",
      "2021-01-14 19:07:27,307 : INFO : token count processed\n",
      "2021-01-14 19:07:27,335 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:07:27,337 : INFO : frequencies processed\n",
      "2021-01-14 19:07:27,338 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:07:27,344 : INFO : token count processed\n",
      "2021-01-14 19:07:27,350 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:07:27,355 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:07:27,356 : INFO : vocab #32006\n",
      "2021-01-14 19:07:27,364 : INFO : diff #set()\n",
      "2021-01-14 19:07:46,330 : INFO : alphabet #32006\n",
      "2021-01-14 19:07:55,747 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.2996799752571764, 0.4348431132849989], [0.9812427256256342, 0.018757274], [nan, nan], [3.4182958340544896, 6.077866832717642, 6.197898639493227, 3.298264027278904, 2.779602805438737, 0.120031806775585]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 19:07:55,750 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:07:55,751 : INFO : built Dictionary(134 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 440 corpus positions)\n",
      "2021-01-14 19:07:55,796 : INFO : token count processed\n",
      "2021-01-14 19:07:55,832 : INFO : frequencies processed\n",
      "2021-01-14 19:08:05,247 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:08:05,248 : INFO : entropies processed\n",
      "2021-01-14 19:08:05,249 : INFO : extropies processed\n",
      "2021-01-14 19:08:05,256 : INFO : token count processed\n",
      "2021-01-14 19:08:05,261 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:08:05,267 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:08:05,268 : INFO : vocab #32006\n",
      "2021-01-14 19:08:05,276 : INFO : diff #set()\n",
      "2021-01-14 19:08:24,287 : INFO : alphabet #32006\n",
      "2021-01-14 19:08:33,725 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.298603530177433, 0.4350467520263524], [0.9722774624824524, 0.027722538], [0.0, 0.0], [3.4182958340544896, 5.977547459003844, 6.07641066090237, 3.319432632155964, 2.6581148268478807, 0.09886320189852604]]\n",
      "2021-01-14 19:08:33,731 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:08:33,733 : INFO : built Dictionary(236 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 2151 corpus positions)\n",
      "2021-01-14 19:08:33,806 : INFO : token count processed\n",
      "2021-01-14 19:08:33,838 : INFO : frequencies processed\n",
      "2021-01-14 19:08:43,390 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:08:43,391 : INFO : entropies processed\n",
      "2021-01-14 19:08:43,392 : INFO : extropies processed\n",
      "2021-01-14 19:08:43,406 : INFO : token count processed\n",
      "2021-01-14 19:08:43,410 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:08:43,415 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:08:43,416 : INFO : vocab #32006\n",
      "2021-01-14 19:08:43,422 : INFO : diff #set()\n",
      "2021-01-14 19:09:02,563 : INFO : alphabet #32006\n",
      "2021-01-14 19:09:11,982 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.2883730610266089, 0.43699168506702335], [0.9626518748700619, 0.037348125], [1.9219280948873623, 1.2148067842293933], [3.4182958340544896, 6.4614394051846435, 6.546156637948007, 3.3335786012911264, 3.1278608038935176, 0.08471723276336363]]\n",
      "2021-01-14 19:09:11,986 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 19:09:11,987 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:09:11,989 : INFO : built Dictionary(201 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1375 corpus positions)\n",
      "2021-01-14 19:09:12,060 : INFO : token count processed\n",
      "2021-01-14 19:09:12,128 : INFO : frequencies processed\n",
      "2021-01-14 19:09:21,826 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:09:21,827 : INFO : entropies processed\n",
      "2021-01-14 19:09:21,828 : INFO : extropies processed\n",
      "2021-01-14 19:09:21,835 : INFO : token count processed\n",
      "2021-01-14 19:09:21,842 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:09:21,847 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:09:21,848 : INFO : vocab #32006\n",
      "2021-01-14 19:09:21,855 : INFO : diff #set()\n",
      "2021-01-14 19:09:40,703 : INFO : alphabet #32006\n",
      "2021-01-14 19:09:50,122 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.3021495009358282, 0.4343766552057102], [0.9755252115428448, 0.024474788], [0.0, 0.0], [3.4182958340544896, 6.327195724598159, 6.419593021453985, 3.325898537198664, 3.0012971873994956, 0.0923972968558262]]\n",
      "2021-01-14 19:09:50,134 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 19:09:50,135 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:09:50,138 : INFO : built Dictionary(413 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 6267 corpus positions)\n",
      "2021-01-14 19:09:50,302 : INFO : token count processed\n",
      "2021-01-14 19:09:50,337 : INFO : frequencies processed\n",
      "2021-01-14 19:10:00,042 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:10:00,043 : INFO : entropies processed\n",
      "2021-01-14 19:10:00,044 : INFO : extropies processed\n",
      "2021-01-14 19:10:00,053 : INFO : token count processed\n",
      "2021-01-14 19:10:00,059 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:10:00,064 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:10:00,065 : INFO : vocab #32006\n",
      "2021-01-14 19:10:00,072 : INFO : diff #set()\n",
      "2021-01-14 19:10:18,931 : INFO : alphabet #32006\n",
      "2021-01-14 19:10:28,465 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.2969820696845662, 0.4353538554775594], [0.9706434886902571, 0.029356511], [2.2516291673878226, 1.2667563532600834], [3.4182958340544896, 6.9079058562486315, 7.007742382999327, 3.318459307303794, 3.5894465489448373, 0.09983652675069532]]\n",
      "2021-01-14 19:10:28,472 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:10:28,474 : INFO : built Dictionary(319 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 2661 corpus positions)\n",
      "2021-01-14 19:10:28,577 : INFO : token count processed\n",
      "2021-01-14 19:10:28,609 : INFO : frequencies processed\n",
      "2021-01-14 19:10:38,028 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:10:38,029 : INFO : entropies processed\n",
      "2021-01-14 19:10:38,030 : INFO : extropies processed\n",
      "2021-01-14 19:10:38,037 : INFO : token count processed\n",
      "2021-01-14 19:10:38,041 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:10:38,047 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:10:38,048 : INFO : vocab #32006\n",
      "2021-01-14 19:10:38,057 : INFO : diff #set()\n",
      "2021-01-14 19:10:57,058 : INFO : alphabet #32006\n",
      "2021-01-14 19:11:06,508 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.3069250361002827, 0.4334774578069687], [0.9846416087821126, 0.015358391], [0.0, 0.0], [3.4182958340544896, 6.61034830706307, 6.75599482512073, 3.27264931599683, 3.3376989910662407, 0.14564651805766005]]\n",
      "2021-01-14 19:11:06,512 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:11:06,514 : INFO : built Dictionary(206 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 708 corpus positions)\n",
      "2021-01-14 19:11:06,578 : INFO : token count processed\n",
      "2021-01-14 19:11:06,607 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:11:06,608 : INFO : frequencies processed\n",
      "2021-01-14 19:11:06,608 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:11:06,614 : INFO : token count processed\n",
      "2021-01-14 19:11:06,618 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:11:06,622 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:11:06,623 : INFO : vocab #32006\n",
      "2021-01-14 19:11:06,629 : INFO : diff #set()\n",
      "2021-01-14 19:11:25,516 : INFO : alphabet #32006\n",
      "2021-01-14 19:11:35,321 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.3086738915539942, 0.43314909206466096], [0.9882898731157184, 0.011710127], [nan, nan], [3.4182958340544896, 6.616715366949855, 6.735793288889422, 3.2992179121149228, 3.3174974548349323, 0.11907792193956723]]\n",
      "2021-01-14 19:11:35,328 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:11:35,330 : INFO : built Dictionary(421 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 2742 corpus positions)\n",
      "2021-01-14 19:11:35,486 : INFO : token count processed\n",
      "2021-01-14 19:11:35,518 : INFO : frequencies processed\n",
      "2021-01-14 19:11:44,926 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:11:44,927 : INFO : entropies processed\n",
      "2021-01-14 19:11:44,928 : INFO : extropies processed\n",
      "2021-01-14 19:11:44,935 : INFO : token count processed\n",
      "2021-01-14 19:11:44,939 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:11:44,944 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:11:44,946 : INFO : vocab #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 19:11:44,953 : INFO : diff #set()\n",
      "2021-01-14 19:12:04,413 : INFO : alphabet #32006\n",
      "2021-01-14 19:12:14,342 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.2839566921002092, 0.4378366732866775], [0.9551351107656956, 0.04486489], [2.2516291673878226, 1.2667563532600834], [3.4182958340544896, 7.32185870753746, 7.508224707137783, 3.231929834454167, 4.089928873083293, 0.18636599960032285]]\n",
      "2021-01-14 19:12:14,345 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:12:14,346 : INFO : built Dictionary(59 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 107 corpus positions)\n",
      "2021-01-14 19:12:14,363 : INFO : token count processed\n",
      "2021-01-14 19:12:14,390 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:12:14,391 : INFO : frequencies processed\n",
      "2021-01-14 19:12:14,392 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:12:14,398 : INFO : token count processed\n",
      "2021-01-14 19:12:14,405 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:12:14,410 : INFO : alphabet_target #32008\n",
      "2021-01-14 19:12:14,411 : INFO : vocab #32006\n",
      "2021-01-14 19:12:14,417 : INFO : diff #set()\n",
      "2021-01-14 19:12:33,276 : INFO : alphabet #32006\n",
      "2021-01-14 19:12:42,817 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/fireException.py')[[1.308313870329164, 0.43321664911080776], [0.9947278634645045, 0.0052721365], [nan, nan], [3.4182958340544896, 5.176618657501385, 5.556131792290967, 3.0387826992649076, 2.137835958236477, 0.3795131347895815]]\n",
      "2021-01-14 19:12:42,821 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:12:42,822 : INFO : built Dictionary(155 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 500 corpus positions)\n",
      "2021-01-14 19:12:42,865 : INFO : token count processed\n",
      "2021-01-14 19:12:42,893 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:12:42,893 : INFO : frequencies processed\n",
      "2021-01-14 19:12:42,894 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:12:42,899 : INFO : token count processed\n",
      "2021-01-14 19:12:42,905 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:12:42,909 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:12:42,912 : INFO : vocab #32006\n",
      "2021-01-14 19:12:42,918 : INFO : diff #set()\n",
      "2021-01-14 19:13:01,755 : INFO : alphabet #32006\n",
      "2021-01-14 19:13:11,555 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.3019125973716932, 0.43442135949983185], [0.9825953487306833, 0.017404651], [nan, nan], [3.4182958340544896, 6.468846789852156, 6.6281776468888065, 3.2589649770178397, 3.209881812834317, 0.1593308570366503]]\n",
      "2021-01-14 19:13:11,561 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:13:11,563 : INFO : built Dictionary(368 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 2549 corpus positions)\n",
      "2021-01-14 19:13:11,697 : INFO : token count processed\n",
      "2021-01-14 19:13:11,729 : INFO : frequencies processed\n",
      "2021-01-14 19:13:21,136 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:13:21,137 : INFO : entropies processed\n",
      "2021-01-14 19:13:21,138 : INFO : extropies processed\n",
      "2021-01-14 19:13:21,146 : INFO : token count processed\n",
      "2021-01-14 19:13:21,150 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:13:21,154 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:13:21,155 : INFO : vocab #32006\n",
      "2021-01-14 19:13:21,162 : INFO : diff #set()\n",
      "2021-01-14 19:13:40,033 : INFO : alphabet #32006\n",
      "2021-01-14 19:13:49,569 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.2956881734410979, 0.43559922970769166], [0.9697747472673655, 0.030225253], [1.9219280948873623, 1.2148067842293933], [3.4182958340544896, 6.957796704012729, 7.083138556778922, 3.2929539812882975, 3.664842722724432, 0.12534185276619247]]\n",
      "2021-01-14 19:13:49,576 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:13:49,578 : INFO : built Dictionary(279 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 3046 corpus positions)\n",
      "2021-01-14 19:13:49,671 : INFO : token count processed\n",
      "2021-01-14 19:13:49,704 : INFO : frequencies processed\n",
      "2021-01-14 19:13:59,113 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:13:59,114 : INFO : entropies processed\n",
      "2021-01-14 19:13:59,115 : INFO : extropies processed\n",
      "2021-01-14 19:13:59,123 : INFO : token count processed\n",
      "2021-01-14 19:13:59,127 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:13:59,131 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:13:59,132 : INFO : vocab #32006\n",
      "2021-01-14 19:13:59,139 : INFO : diff #set()\n",
      "2021-01-14 19:14:18,015 : INFO : alphabet #32006\n",
      "2021-01-14 19:14:27,542 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.2878780747980156, 0.4370862289452573], [0.969770111143589, 0.030229889], [0.9182958340544896, 0.9182958340544896], [3.4182958340544896, 6.441859572014148, 6.575361223873275, 3.2847941821953626, 3.157065389818785, 0.13350165185912655]]\n",
      "2021-01-14 19:14:27,547 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:14:27,550 : INFO : built Dictionary(306 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1522 corpus positions)\n",
      "2021-01-14 19:14:27,651 : INFO : token count processed\n",
      "2021-01-14 19:14:27,685 : INFO : frequencies processed\n",
      "2021-01-14 19:14:37,086 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:14:37,087 : INFO : entropies processed\n",
      "2021-01-14 19:14:37,088 : INFO : extropies processed\n",
      "2021-01-14 19:14:37,096 : INFO : token count processed\n",
      "2021-01-14 19:14:37,103 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:14:37,107 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:14:37,108 : INFO : vocab #32006\n",
      "2021-01-14 19:14:37,115 : INFO : diff #set()\n",
      "2021-01-14 19:14:56,084 : INFO : alphabet #32006\n",
      "2021-01-14 19:15:05,494 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.3078324071855265, 0.43330702735885884], [0.9866266120225191, 0.013373388], [0.0, 0.0], [3.4182958340544896, 6.998955278238291, 7.148618648426364, 3.2686324638664157, 3.7303228143718745, 0.1496633701880734]]\n",
      "2021-01-14 19:15:05,499 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:15:05,501 : INFO : built Dictionary(220 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1341 corpus positions)\n",
      "2021-01-14 19:15:05,570 : INFO : token count processed\n",
      "2021-01-14 19:15:05,602 : INFO : frequencies processed\n",
      "2021-01-14 19:15:15,009 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:15:15,010 : INFO : entropies processed\n",
      "2021-01-14 19:15:15,011 : INFO : extropies processed\n",
      "2021-01-14 19:15:15,018 : INFO : token count processed\n",
      "2021-01-14 19:15:15,024 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:15:15,030 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:15:15,031 : INFO : vocab #32006\n",
      "2021-01-14 19:15:15,038 : INFO : diff #set()\n",
      "2021-01-14 19:15:33,860 : INFO : alphabet #32006\n",
      "2021-01-14 19:15:43,289 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.2977291822870725, 0.4352122990424128], [0.9756100308150053, 0.02438997], [1.0, 1.0], [3.4182958340544896, 6.492983191376071, 6.589638547523105, 3.321640477907456, 3.1713427134686154, 0.0966553561470338]]\n",
      "2021-01-14 19:15:43,296 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:15:43,299 : INFO : built Dictionary(431 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 3280 corpus positions)\n",
      "2021-01-14 19:15:43,467 : INFO : token count processed\n",
      "2021-01-14 19:15:43,526 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:15:43,529 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 19:15:43,529 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:15:43,540 : INFO : token count processed\n",
      "2021-01-14 19:15:43,544 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:15:43,551 : INFO : alphabet_target #32008\n",
      "2021-01-14 19:15:43,552 : INFO : vocab #32006\n",
      "2021-01-14 19:15:43,558 : INFO : diff #set()\n",
      "2021-01-14 19:16:02,572 : INFO : alphabet #32006\n",
      "2021-01-14 19:16:11,999 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.316247171254353, 0.4317328532163752], [0.9905496658757329, 0.009450334], [nan, nan], [3.4182958340544896, 6.560342487747443, 6.74013235810141, 3.238505963700523, 3.3218365240469203, 0.1797898703539671]]\n",
      "2021-01-14 19:16:12,006 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 19:16:12,007 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:16:12,009 : INFO : built Dictionary(446 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 3485 corpus positions)\n",
      "2021-01-14 19:16:12,191 : INFO : token count processed\n",
      "2021-01-14 19:16:12,257 : INFO : frequencies processed\n",
      "2021-01-14 19:16:21,671 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:16:21,672 : INFO : entropies processed\n",
      "2021-01-14 19:16:21,673 : INFO : extropies processed\n",
      "2021-01-14 19:16:21,681 : INFO : token count processed\n",
      "2021-01-14 19:16:21,685 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:16:21,690 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:16:21,691 : INFO : vocab #32006\n",
      "2021-01-14 19:16:21,698 : INFO : diff #set()\n",
      "2021-01-14 19:16:40,666 : INFO : alphabet #32006\n",
      "2021-01-14 19:16:50,072 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.3071825905827552, 0.43342906802509157], [0.9858991988003254, 0.014100801], [1.0, 1.0], [3.4182958340544896, 7.046173750105238, 7.2252323986997755, 3.2392371854599524, 3.806936564645286, 0.17905864859453757]]\n",
      "2021-01-14 19:16:50,082 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 19:16:50,083 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:16:50,086 : INFO : built Dictionary(497 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 5594 corpus positions)\n",
      "2021-01-14 19:16:50,318 : INFO : token count processed\n",
      "2021-01-14 19:16:50,351 : INFO : frequencies processed\n",
      "2021-01-14 19:16:59,767 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:16:59,768 : INFO : entropies processed\n",
      "2021-01-14 19:16:59,769 : INFO : extropies processed\n",
      "2021-01-14 19:16:59,778 : INFO : token count processed\n",
      "2021-01-14 19:16:59,782 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:16:59,787 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:16:59,788 : INFO : vocab #32006\n",
      "2021-01-14 19:16:59,794 : INFO : diff #set()\n",
      "2021-01-14 19:17:18,767 : INFO : alphabet #32006\n",
      "2021-01-14 19:17:28,184 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.2986292540867612, 0.4350418834277375], [0.9762676935642958, 0.023732306], [2.321928094887362, 1.2877123795494492], [3.4182958340544896, 7.009229588004272, 7.109509595958439, 3.318015826100323, 3.6912137619039496, 0.1002800079541668]]\n",
      "2021-01-14 19:17:28,197 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:17:28,200 : INFO : built Dictionary(577 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 6535 corpus positions)\n",
      "2021-01-14 19:17:28,447 : INFO : token count processed\n",
      "2021-01-14 19:17:28,479 : INFO : frequencies processed\n",
      "2021-01-14 19:17:37,881 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:17:37,882 : INFO : entropies processed\n",
      "2021-01-14 19:17:37,882 : INFO : extropies processed\n",
      "2021-01-14 19:17:37,891 : INFO : token count processed\n",
      "2021-01-14 19:17:37,896 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:17:37,900 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:17:37,901 : INFO : vocab #32006\n",
      "2021-01-14 19:17:37,908 : INFO : diff #set()\n",
      "2021-01-14 19:17:56,860 : INFO : alphabet #32006\n",
      "2021-01-14 19:18:06,272 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.2961142717022465, 0.4355183939772476], [0.9693169351667166, 0.030683065], [1.9219280948873623, 1.2148067842293933], [3.4182958340544896, 7.376088004590871, 7.534257485733431, 3.2601263529119304, 4.115961651678941, 0.15816948114255958]]\n",
      "2021-01-14 19:18:06,275 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:18:06,276 : INFO : built Dictionary(131 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 321 corpus positions)\n",
      "2021-01-14 19:18:06,312 : INFO : token count processed\n",
      "2021-01-14 19:18:06,340 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:18:06,342 : INFO : frequencies processed\n",
      "2021-01-14 19:18:06,343 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:18:06,349 : INFO : token count processed\n",
      "2021-01-14 19:18:06,354 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:18:06,360 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:18:06,360 : INFO : vocab #32006\n",
      "2021-01-14 19:18:06,369 : INFO : diff #set()\n",
      "2021-01-14 19:18:25,362 : INFO : alphabet #32006\n",
      "2021-01-14 19:18:34,776 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.3061687049627466, 0.43361962108325197], [0.9852214520797133, 0.014778548], [nan, nan], [3.4182958340544896, 6.2993628166120885, 6.4531513787951305, 3.264507271871448, 3.034855544740641, 0.15378856218304193]]\n",
      "2021-01-14 19:18:34,779 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:18:34,780 : INFO : built Dictionary(28 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 37 corpus positions)\n",
      "2021-01-14 19:18:34,789 : INFO : token count processed\n",
      "2021-01-14 19:18:34,817 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:18:34,818 : INFO : frequencies processed\n",
      "2021-01-14 19:18:34,819 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:18:34,825 : INFO : token count processed\n",
      "2021-01-14 19:18:34,829 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:18:34,835 : INFO : alphabet_target #32008\n",
      "2021-01-14 19:18:34,836 : INFO : vocab #32006\n",
      "2021-01-14 19:18:34,843 : INFO : diff #set()\n",
      "2021-01-14 19:18:53,831 : INFO : alphabet #32006\n",
      "2021-01-14 19:19:03,245 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.325052567112851, 0.4300978025807633], [0.9979687742888927, 0.0020312257], [nan, nan], [3.4182958340544896, 3.8936606896881862, 4.536286231168867, 2.7756702925738095, 1.1179903971143772, 0.6426255414806805]]\n",
      "2021-01-14 19:19:03,267 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:19:03,279 : INFO : built Dictionary(735 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 12479 corpus positions)\n",
      "2021-01-14 19:19:03,704 : INFO : token count processed\n",
      "2021-01-14 19:19:03,746 : INFO : frequencies processed\n",
      "2021-01-14 19:19:13,151 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:19:13,152 : INFO : entropies processed\n",
      "2021-01-14 19:19:13,153 : INFO : extropies processed\n",
      "2021-01-14 19:19:13,164 : INFO : token count processed\n",
      "2021-01-14 19:19:13,170 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:19:13,176 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:19:13,177 : INFO : vocab #32006\n",
      "2021-01-14 19:19:13,185 : INFO : diff #set()\n",
      "2021-01-14 19:19:32,185 : INFO : alphabet #32006\n",
      "2021-01-14 19:19:41,602 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.2870568609667832, 0.43724317355943687], [0.9651900716125965, 0.03480993], [2.321928094887362, 1.2877123795494492], [3.4182958340544896, 7.434393313070278, 7.635555887267662, 3.217133259857105, 4.217260053213172, 0.20116257419738393]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 19:19:41,610 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:19:41,613 : INFO : built Dictionary(485 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 4106 corpus positions)\n",
      "2021-01-14 19:19:41,813 : INFO : token count processed\n",
      "2021-01-14 19:19:41,881 : INFO : frequencies processed\n",
      "2021-01-14 19:19:51,210 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:19:51,211 : INFO : entropies processed\n",
      "2021-01-14 19:19:51,212 : INFO : extropies processed\n",
      "2021-01-14 19:19:51,226 : INFO : token count processed\n",
      "2021-01-14 19:19:51,231 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:19:51,236 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:19:51,237 : INFO : vocab #32006\n",
      "2021-01-14 19:19:51,247 : INFO : diff #set()\n",
      "2021-01-14 19:20:10,211 : INFO : alphabet #32006\n",
      "2021-01-14 19:20:19,624 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.2926341062782298, 0.43617950080283846], [0.9689399246126413, 0.031060075], [2.5216406363433186, 1.2998438251349493], [3.4182958340544896, 7.2991514951718255, 7.478089960063539, 3.239357369162777, 4.059794126009049, 0.17893846489171317]]\n",
      "2021-01-14 19:20:19,632 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:20:19,634 : INFO : built Dictionary(453 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 3523 corpus positions)\n",
      "2021-01-14 19:20:19,811 : INFO : token count processed\n",
      "2021-01-14 19:20:19,846 : INFO : frequencies processed\n",
      "2021-01-14 19:20:29,428 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:20:29,429 : INFO : entropies processed\n",
      "2021-01-14 19:20:29,430 : INFO : extropies processed\n",
      "2021-01-14 19:20:29,438 : INFO : token count processed\n",
      "2021-01-14 19:20:29,446 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:20:29,451 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:20:29,452 : INFO : vocab #32006\n",
      "2021-01-14 19:20:29,459 : INFO : diff #set()\n",
      "2021-01-14 19:20:48,463 : INFO : alphabet #32006\n",
      "2021-01-14 19:20:57,889 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.2983747785317241, 0.4350900511703457], [0.9760758709162474, 0.02392413], [1.9219280948873623, 1.2148067842293933], [3.4182958340544896, 7.170319527000998, 7.343586057448268, 3.245029303607219, 3.925290223393778, 0.17326653044727003]]\n",
      "2021-01-14 19:20:57,893 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:20:57,894 : INFO : built Dictionary(165 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 559 corpus positions)\n",
      "2021-01-14 19:20:57,943 : INFO : token count processed\n",
      "2021-01-14 19:20:57,970 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:20:57,970 : INFO : frequencies processed\n",
      "2021-01-14 19:20:57,971 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:20:57,976 : INFO : token count processed\n",
      "2021-01-14 19:20:57,980 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:20:57,984 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:20:57,984 : INFO : vocab #32006\n",
      "2021-01-14 19:20:57,992 : INFO : diff #set()\n",
      "2021-01-14 19:21:17,004 : INFO : alphabet #32006\n",
      "2021-01-14 19:21:26,439 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.3093291307766892, 0.4330261921840796], [0.9905005414038897, 0.009499459], [nan, nan], [3.4182958340544896, 6.353654804387375, 6.515005191557689, 3.256945446884176, 3.0967093575031996, 0.16135038717031414]]\n",
      "2021-01-14 19:21:26,443 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:21:26,444 : INFO : built Dictionary(158 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 687 corpus positions)\n",
      "2021-01-14 19:21:26,491 : INFO : token count processed\n",
      "2021-01-14 19:21:26,522 : INFO : frequencies processed\n",
      "2021-01-14 19:21:35,952 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:21:35,954 : INFO : entropies processed\n",
      "2021-01-14 19:21:35,955 : INFO : extropies processed\n",
      "2021-01-14 19:21:35,961 : INFO : token count processed\n",
      "2021-01-14 19:21:35,966 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:21:35,970 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:21:35,971 : INFO : vocab #32006\n",
      "2021-01-14 19:21:35,979 : INFO : diff #set()\n",
      "2021-01-14 19:21:54,979 : INFO : alphabet #32006\n",
      "2021-01-14 19:22:04,400 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.3026467997012001, 0.4342828436083917], [0.9824567101895809, 0.01754329], [1.0, 1.0], [3.4182958340544896, 6.245180322479091, 6.364875378809623, 3.2986007777239568, 2.9465795447551337, 0.11969505633053235]]\n",
      "2021-01-14 19:22:04,405 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:22:04,407 : INFO : built Dictionary(386 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1759 corpus positions)\n",
      "2021-01-14 19:22:04,550 : INFO : token count processed\n",
      "2021-01-14 19:22:04,582 : INFO : frequencies processed\n",
      "2021-01-14 19:22:14,116 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:22:14,117 : INFO : entropies processed\n",
      "2021-01-14 19:22:14,118 : INFO : extropies processed\n",
      "2021-01-14 19:22:14,126 : INFO : token count processed\n",
      "2021-01-14 19:22:14,130 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:22:14,135 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:22:14,136 : INFO : vocab #32006\n",
      "2021-01-14 19:22:14,143 : INFO : diff #set()\n",
      "2021-01-14 19:22:33,203 : INFO : alphabet #32006\n",
      "2021-01-14 19:22:42,622 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.3003078768013345, 0.4347244167117916], [0.979819979518652, 0.02018002], [1.584962500721156, 1.1699250014423124], [3.4182958340544896, 7.2691387000368, 7.459647115065028, 3.227787419026262, 4.041351281010538, 0.19050841502822813]]\n",
      "2021-01-14 19:22:42,627 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:22:42,629 : INFO : built Dictionary(312 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1596 corpus positions)\n",
      "2021-01-14 19:22:42,739 : INFO : token count processed\n",
      "2021-01-14 19:22:42,771 : INFO : frequencies processed\n",
      "2021-01-14 19:22:52,307 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:22:52,308 : INFO : entropies processed\n",
      "2021-01-14 19:22:52,309 : INFO : extropies processed\n",
      "2021-01-14 19:22:52,317 : INFO : token count processed\n",
      "2021-01-14 19:22:52,321 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:22:52,325 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:22:52,326 : INFO : vocab #32006\n",
      "2021-01-14 19:22:52,333 : INFO : diff #set()\n",
      "2021-01-14 19:23:11,241 : INFO : alphabet #32006\n",
      "2021-01-14 19:23:20,692 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.2688178633978504, 0.44075816579757077], [0.9440695196390152, 0.05593048], [1.584962500721156, 1.1699250014423124], [3.4182958340544896, 7.08857858466988, 7.235684841110395, 3.271189577613974, 3.817389007055905, 0.147106256440515]]\n",
      "2021-01-14 19:23:20,696 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:23:20,697 : INFO : built Dictionary(139 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 519 corpus positions)\n",
      "2021-01-14 19:23:20,745 : INFO : token count processed\n",
      "2021-01-14 19:23:20,774 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:23:20,777 : INFO : frequencies processed\n",
      "2021-01-14 19:23:20,777 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:23:20,784 : INFO : token count processed\n",
      "2021-01-14 19:23:20,788 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:23:20,792 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:23:20,793 : INFO : vocab #32006\n",
      "2021-01-14 19:23:20,800 : INFO : diff #set()\n",
      "2021-01-14 19:23:40,178 : INFO : alphabet #32006\n",
      "2021-01-14 19:23:49,627 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.3119377319156447, 0.4325376009030362], [0.9917517798021436, 0.00824822], [nan, nan], [3.4182958340544896, 6.0479231618016716, 6.196835375932512, 3.2693836199236497, 2.7785395418780223, 0.14891221413084033]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 19:23:49,631 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:23:49,632 : INFO : built Dictionary(138 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 638 corpus positions)\n",
      "2021-01-14 19:23:49,680 : INFO : token count processed\n",
      "2021-01-14 19:23:49,713 : INFO : frequencies processed\n",
      "2021-01-14 19:23:59,125 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:23:59,126 : INFO : entropies processed\n",
      "2021-01-14 19:23:59,127 : INFO : extropies processed\n",
      "2021-01-14 19:23:59,134 : INFO : token count processed\n",
      "2021-01-14 19:23:59,138 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:23:59,142 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:23:59,143 : INFO : vocab #32006\n",
      "2021-01-14 19:23:59,150 : INFO : diff #set()\n",
      "2021-01-14 19:24:18,036 : INFO : alphabet #32006\n",
      "2021-01-14 19:24:27,586 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.3014229708840528, 0.43451378240822325], [0.9806266985833645, 0.019373301], [1.584962500721156, 1.1699250014423124], [3.4182958340544896, 6.036583168403119, 6.1504776385247215, 3.3044013639328877, 2.732181804470232, 0.11389447012160225]]\n",
      "2021-01-14 19:24:27,600 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 19:24:27,601 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:24:27,607 : INFO : built Dictionary(552 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 6996 corpus positions)\n",
      "2021-01-14 19:24:27,867 : INFO : token count processed\n",
      "2021-01-14 19:24:27,929 : INFO : frequencies processed\n",
      "2021-01-14 19:24:37,350 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:24:37,351 : INFO : entropies processed\n",
      "2021-01-14 19:24:37,352 : INFO : extropies processed\n",
      "2021-01-14 19:24:37,361 : INFO : token count processed\n",
      "2021-01-14 19:24:37,365 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:24:37,370 : INFO : alphabet_target #32010\n",
      "2021-01-14 19:24:37,371 : INFO : vocab #32006\n",
      "2021-01-14 19:24:37,378 : INFO : diff #set()\n",
      "2021-01-14 19:24:56,248 : INFO : alphabet #32006\n",
      "2021-01-14 19:25:05,784 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.2906275279902795, 0.4365615918697034], [0.9692720174789429, 0.030727983], [1.9219280948873623, 1.2148067842293933], [3.4182958340544896, 7.29352035514053, 7.44360347522291, 3.268212713972109, 4.025307641168421, 0.15008312008238]]\n",
      "2021-01-14 19:25:05,791 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:25:05,794 : INFO : built Dictionary(362 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 3239 corpus positions)\n",
      "2021-01-14 19:25:05,918 : INFO : token count processed\n",
      "2021-01-14 19:25:05,951 : INFO : frequencies processed\n",
      "2021-01-14 19:25:15,642 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:25:15,643 : INFO : entropies processed\n",
      "2021-01-14 19:25:15,644 : INFO : extropies processed\n",
      "2021-01-14 19:25:15,652 : INFO : token count processed\n",
      "2021-01-14 19:25:15,656 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:25:15,661 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:25:15,662 : INFO : vocab #32006\n",
      "2021-01-14 19:25:15,668 : INFO : diff #set()\n",
      "2021-01-14 19:25:34,542 : INFO : alphabet #32006\n",
      "2021-01-14 19:25:44,081 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.3072871779003792, 0.4334094210630492], [0.9865828920155764, 0.013417108], [1.584962500721156, 1.1699250014423124], [3.4182958340544896, 6.8153433747477745, 6.9456930704120605, 3.287946138390204, 3.527397236357571, 0.130349695664286]]\n",
      "2021-01-14 19:25:44,084 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:25:44,086 : INFO : built Dictionary(116 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 260 corpus positions)\n",
      "2021-01-14 19:25:44,120 : INFO : token count processed\n",
      "2021-01-14 19:25:44,147 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:25:44,148 : INFO : frequencies processed\n",
      "2021-01-14 19:25:44,149 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:25:44,155 : INFO : token count processed\n",
      "2021-01-14 19:25:44,159 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:25:44,167 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:25:44,167 : INFO : vocab #32006\n",
      "2021-01-14 19:25:44,175 : INFO : diff #set()\n",
      "2021-01-14 19:26:03,184 : INFO : alphabet #32006\n",
      "2021-01-14 19:26:12,722 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.307398398443764, 0.4333885299887764], [0.9881913848221302, 0.011808615], [nan, nan], [3.4182958340544896, 6.150121915859574, 6.295476364859713, 3.2729413850543505, 2.8771805308052234, 0.1453544490001386]]\n",
      "2021-01-14 19:26:12,726 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:26:12,728 : INFO : built Dictionary(278 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 971 corpus positions)\n",
      "2021-01-14 19:26:12,813 : INFO : token count processed\n",
      "2021-01-14 19:26:12,845 : INFO : frequencies processed\n",
      "2021-01-14 19:26:22,264 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:26:22,265 : INFO : entropies processed\n",
      "2021-01-14 19:26:22,266 : INFO : extropies processed\n",
      "2021-01-14 19:26:22,273 : INFO : token count processed\n",
      "2021-01-14 19:26:22,277 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:26:22,284 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:26:22,285 : INFO : vocab #32006\n",
      "2021-01-14 19:26:22,292 : INFO : diff #set()\n",
      "2021-01-14 19:26:41,184 : INFO : alphabet #32006\n",
      "2021-01-14 19:26:50,724 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.2873189848547468, 0.43719306603993563], [0.9653415717184544, 0.03465843], [1.584962500721156, 1.1699250014423124], [3.4182958340544896, 7.0391145208191315, 7.255638922962346, 3.2017714319112756, 3.8373430889078564, 0.21652440214321445]]\n",
      "2021-01-14 19:26:50,733 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 19:26:50,734 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:26:50,737 : INFO : built Dictionary(584 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 4346 corpus positions)\n",
      "2021-01-14 19:26:51,021 : INFO : token count processed\n",
      "2021-01-14 19:26:51,053 : INFO : frequencies processed\n",
      "2021-01-14 19:27:00,726 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:27:00,728 : INFO : entropies processed\n",
      "2021-01-14 19:27:00,728 : INFO : extropies processed\n",
      "2021-01-14 19:27:00,737 : INFO : token count processed\n",
      "2021-01-14 19:27:00,741 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:27:00,745 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:27:00,746 : INFO : vocab #32006\n",
      "2021-01-14 19:27:00,753 : INFO : diff #set()\n",
      "2021-01-14 19:27:19,730 : INFO : alphabet #32006\n",
      "2021-01-14 19:27:29,431 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.2880067639855053, 0.43706164498311556], [0.9595080800354481, 0.04049192], [1.0, 1.0], [3.4182958340544896, 7.482466367279176, 7.7445881787658815, 3.1561740225677832, 4.326292344711392, 0.2621218114867059]]\n",
      "2021-01-14 19:27:29,435 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:27:29,437 : INFO : built Dictionary(165 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 568 corpus positions)\n",
      "2021-01-14 19:27:29,483 : INFO : token count processed\n",
      "2021-01-14 19:27:29,510 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:27:29,510 : INFO : frequencies processed\n",
      "2021-01-14 19:27:29,511 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:27:29,516 : INFO : token count processed\n",
      "2021-01-14 19:27:29,520 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:27:29,524 : INFO : alphabet_target #32009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 19:27:29,525 : INFO : vocab #32006\n",
      "2021-01-14 19:27:29,531 : INFO : diff #set()\n",
      "2021-01-14 19:27:48,390 : INFO : alphabet #32006\n",
      "2021-01-14 19:27:57,934 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.3085547377371376, 0.4331714486355248], [0.9904680894687772, 0.0095319105], [nan, nan], [3.4182958340544896, 6.372162341197667, 6.530218713084439, 3.2602394621677178, 3.1119228790299496, 0.15805637188677224]]\n",
      "2021-01-14 19:27:57,940 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:27:57,942 : INFO : built Dictionary(311 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1989 corpus positions)\n",
      "2021-01-14 19:27:58,048 : INFO : token count processed\n",
      "2021-01-14 19:27:58,083 : INFO : frequencies processed\n",
      "2021-01-14 19:28:07,443 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:28:07,444 : INFO : entropies processed\n",
      "2021-01-14 19:28:07,445 : INFO : extropies processed\n",
      "2021-01-14 19:28:07,457 : INFO : token count processed\n",
      "2021-01-14 19:28:07,461 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:28:07,465 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:28:07,466 : INFO : vocab #32006\n",
      "2021-01-14 19:28:07,472 : INFO : diff #set()\n",
      "2021-01-14 19:28:26,310 : INFO : alphabet #32006\n",
      "2021-01-14 19:28:35,846 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.3039241640710626, 0.4340420642287928], [0.9823539853096008, 0.017646015], [1.584962500721156, 1.1699250014423124], [3.4182958340544896, 6.798155919669889, 6.967256823780508, 3.2491949299438705, 3.548960989726018, 0.16910090411061862]]\n",
      "2021-01-14 19:28:35,850 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:28:35,851 : INFO : built Dictionary(170 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 705 corpus positions)\n",
      "2021-01-14 19:28:35,897 : INFO : token count processed\n",
      "2021-01-14 19:28:35,924 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:28:35,927 : INFO : frequencies processed\n",
      "2021-01-14 19:28:35,927 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:28:35,934 : INFO : token count processed\n",
      "2021-01-14 19:28:35,938 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:28:35,945 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:28:35,945 : INFO : vocab #32006\n",
      "2021-01-14 19:28:35,954 : INFO : diff #set()\n",
      "2021-01-14 19:28:54,791 : INFO : alphabet #32006\n",
      "2021-01-14 19:29:04,476 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.3097131820856545, 0.4329541900509946], [0.9892909619957209, 0.010709038], [nan, nan], [3.4182958340544896, 6.271631856729336, 6.424646760430073, 3.265280930353753, 3.006350926375583, 0.15301490370073711]]\n",
      "2021-01-14 19:29:04,483 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:29:04,485 : INFO : built Dictionary(345 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 3220 corpus positions)\n",
      "2021-01-14 19:29:04,616 : INFO : token count processed\n",
      "2021-01-14 19:29:04,649 : INFO : frequencies processed\n",
      "2021-01-14 19:29:14,050 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:29:14,051 : INFO : entropies processed\n",
      "2021-01-14 19:29:14,052 : INFO : extropies processed\n",
      "2021-01-14 19:29:14,060 : INFO : token count processed\n",
      "2021-01-14 19:29:14,066 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:29:14,071 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:29:14,072 : INFO : vocab #32006\n",
      "2021-01-14 19:29:14,079 : INFO : diff #set()\n",
      "2021-01-14 19:29:32,931 : INFO : alphabet #32006\n",
      "2021-01-14 19:29:42,484 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.2935449021335594, 0.436006288374714], [0.9726222865283489, 0.027377713], [1.584962500721156, 1.1699250014423124], [3.4182958340544896, 6.873598627629562, 7.01428230142149, 3.277612160262562, 3.5959864673670006, 0.14068367379192814]]\n",
      "2021-01-14 19:29:42,488 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:29:42,489 : INFO : built Dictionary(97 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 168 corpus positions)\n",
      "2021-01-14 19:29:42,522 : INFO : token count processed\n",
      "2021-01-14 19:29:42,550 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:29:42,551 : INFO : frequencies processed\n",
      "2021-01-14 19:29:42,551 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:29:42,557 : INFO : token count processed\n",
      "2021-01-14 19:29:42,561 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:29:42,565 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:29:42,566 : INFO : vocab #32006\n",
      "2021-01-14 19:29:42,573 : INFO : diff #set()\n",
      "2021-01-14 19:30:01,408 : INFO : alphabet #32006\n",
      "2021-01-14 19:30:10,936 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.2980855746632882, 0.43514480532193345], [0.980157632380724, 0.019842368], [nan, nan], [3.4182958340544896, 6.049830202851529, 6.281528849207768, 3.1865971876982515, 2.8632330151532783, 0.2316986463562385]]\n",
      "2021-01-14 19:30:10,945 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:30:10,948 : INFO : built Dictionary(245 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1248 corpus positions)\n",
      "2021-01-14 19:30:11,039 : INFO : token count processed\n",
      "2021-01-14 19:30:11,096 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:30:11,104 : INFO : frequencies processed\n",
      "2021-01-14 19:30:11,105 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:30:11,111 : INFO : token count processed\n",
      "2021-01-14 19:30:11,120 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:30:11,124 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:30:11,125 : INFO : vocab #32006\n",
      "2021-01-14 19:30:11,131 : INFO : diff #set()\n",
      "2021-01-14 19:30:30,174 : INFO : alphabet #32006\n",
      "2021-01-14 19:30:39,612 : INFO : Computed distances or similarities ('276', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.3031525960913672, 0.4341874705553941], [0.9865060755982995, 0.013493924], [nan, nan], [3.4182958340544896, 6.778844940588858, 6.955082465722112, 3.2420583089212354, 3.536786631667623, 0.17623752513325464]]\n",
      "2021-01-14 19:30:39,615 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:30:39,616 : INFO : built Dictionary(144 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 281 corpus positions)\n",
      "2021-01-14 19:30:39,665 : INFO : token count processed\n",
      "2021-01-14 19:30:39,693 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:30:39,696 : INFO : frequencies processed\n",
      "2021-01-14 19:30:39,696 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:30:39,703 : INFO : token count processed\n",
      "2021-01-14 19:30:39,709 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:30:39,715 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:30:39,717 : INFO : vocab #32006\n",
      "2021-01-14 19:30:39,724 : INFO : diff #set()\n",
      "2021-01-14 19:30:58,876 : INFO : alphabet #32006\n",
      "2021-01-14 19:31:08,293 : INFO : Computed distances or similarities ('276', 'sacp-python-common/setup.py')[[1.303747323383826, 0.4340753822477217], [0.9856477044522762, 0.014352296], [nan, nan], [3.4182958340544896, 6.469677430851302, 6.712766048436508, 3.1752072164692837, 3.294470214382019, 0.24308861758520628]]\n",
      "2021-01-14 19:31:08,298 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:31:08,299 : INFO : built Dictionary(214 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1183 corpus positions)\n",
      "2021-01-14 19:31:08,362 : INFO : token count processed\n",
      "2021-01-14 19:31:08,389 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:31:08,390 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 19:31:08,390 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:31:08,396 : INFO : token count processed\n",
      "2021-01-14 19:31:08,400 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:31:08,403 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:31:08,404 : INFO : vocab #32006\n",
      "2021-01-14 19:31:08,412 : INFO : diff #set()\n",
      "2021-01-14 19:31:27,378 : INFO : alphabet #32006\n",
      "2021-01-14 19:31:36,808 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.3028029362286349, 0.43425339800796336], [0.982360664755106, 0.017639335], [nan, nan], [3.4182958340544896, 6.459180448028249, 6.609728586039154, 3.2677476960435854, 3.191432751984664, 0.1505481380109046]]\n",
      "2021-01-14 19:31:36,811 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:31:36,812 : INFO : built Dictionary(124 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 333 corpus positions)\n",
      "2021-01-14 19:31:36,854 : INFO : token count processed\n",
      "2021-01-14 19:31:36,883 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:31:36,883 : INFO : frequencies processed\n",
      "2021-01-14 19:31:36,884 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:31:36,889 : INFO : token count processed\n",
      "2021-01-14 19:31:36,893 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:31:36,896 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:31:36,898 : INFO : vocab #32006\n",
      "2021-01-14 19:31:36,905 : INFO : diff #set()\n",
      "2021-01-14 19:31:55,890 : INFO : alphabet #32006\n",
      "2021-01-14 19:32:05,307 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.2991732550107125, 0.43493894939002353], [0.9731952119618654, 0.026804788], [nan, nan], [3.4182958340544896, 6.097125733496388, 6.261919627370555, 3.253501940180322, 2.8436237933160657, 0.1647938938741671]]\n",
      "2021-01-14 19:32:05,310 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:32:05,311 : INFO : built Dictionary(116 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 329 corpus positions)\n",
      "2021-01-14 19:32:05,343 : INFO : token count processed\n",
      "2021-01-14 19:32:05,370 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:32:05,371 : INFO : frequencies processed\n",
      "2021-01-14 19:32:05,371 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:32:05,377 : INFO : token count processed\n",
      "2021-01-14 19:32:05,381 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:32:05,384 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:32:05,385 : INFO : vocab #32006\n",
      "2021-01-14 19:32:05,392 : INFO : diff #set()\n",
      "2021-01-14 19:32:24,034 : INFO : alphabet #32006\n",
      "2021-01-14 19:32:33,522 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.3048171446894368, 0.43387389854510355], [0.9783410858362913, 0.021658914], [nan, nan], [3.4182958340544896, 6.0695858597523715, 6.2282520254524245, 3.259629668354436, 2.809956191397935, 0.15866616570005299]]\n",
      "2021-01-14 19:32:33,526 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 19:32:33,526 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:32:33,528 : INFO : built Dictionary(118 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 425 corpus positions)\n",
      "2021-01-14 19:32:33,560 : INFO : token count processed\n",
      "2021-01-14 19:32:33,586 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:32:33,587 : INFO : frequencies processed\n",
      "2021-01-14 19:32:33,587 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:32:33,592 : INFO : token count processed\n",
      "2021-01-14 19:32:33,598 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:32:33,602 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:32:33,603 : INFO : vocab #32006\n",
      "2021-01-14 19:32:33,612 : INFO : diff #set()\n",
      "2021-01-14 19:32:52,807 : INFO : alphabet #32006\n",
      "2021-01-14 19:33:02,351 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.300850481149891, 0.43462189663894724], [0.9738239496946335, 0.02617605], [nan, nan], [3.4182958340544896, 6.104787343210121, 6.2299818203654125, 3.2931013568991974, 2.811685986310923, 0.12519447715529175]]\n",
      "2021-01-14 19:33:02,368 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 19:33:02,369 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:33:02,372 : INFO : built Dictionary(406 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 9110 corpus positions)\n",
      "2021-01-14 19:33:02,525 : INFO : token count processed\n",
      "2021-01-14 19:33:02,553 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:33:02,556 : INFO : frequencies processed\n",
      "2021-01-14 19:33:02,556 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:33:02,564 : INFO : token count processed\n",
      "2021-01-14 19:33:02,568 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:33:02,572 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:33:02,573 : INFO : vocab #32006\n",
      "2021-01-14 19:33:02,579 : INFO : diff #set()\n",
      "2021-01-14 19:33:21,440 : INFO : alphabet #32006\n",
      "2021-01-14 19:33:30,988 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.3037304228389102, 0.43407856669605027], [0.9803500697016716, 0.01964993], [nan, nan], [3.4182958340544896, 6.89087415148015, 7.044139732180463, 3.265030253354176, 3.6258438981259737, 0.15326558070031293]]\n",
      "2021-01-14 19:33:30,993 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:33:30,995 : INFO : built Dictionary(262 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 2270 corpus positions)\n",
      "2021-01-14 19:33:31,074 : INFO : token count processed\n",
      "2021-01-14 19:33:31,106 : INFO : frequencies processed\n",
      "2021-01-14 19:33:40,701 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:33:40,702 : INFO : entropies processed\n",
      "2021-01-14 19:33:40,703 : INFO : extropies processed\n",
      "2021-01-14 19:33:40,716 : INFO : token count processed\n",
      "2021-01-14 19:33:40,720 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:33:40,724 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:33:40,726 : INFO : vocab #32006\n",
      "2021-01-14 19:33:40,732 : INFO : diff #set()\n",
      "2021-01-14 19:33:59,618 : INFO : alphabet #32006\n",
      "2021-01-14 19:34:09,162 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.2964339431046108, 0.43545776833801414], [0.972343685105443, 0.027656315], [1.0, 1.0], [3.4182958340544896, 6.655493573668506, 6.78022599620196, 3.2935634115210366, 3.36193016214747, 0.1247324225334534]]\n",
      "2021-01-14 19:34:09,167 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:34:09,169 : INFO : built Dictionary(240 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1277 corpus positions)\n",
      "2021-01-14 19:34:09,247 : INFO : token count processed\n",
      "2021-01-14 19:34:09,280 : INFO : frequencies processed\n",
      "2021-01-14 19:34:18,720 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:34:18,721 : INFO : entropies processed\n",
      "2021-01-14 19:34:18,722 : INFO : extropies processed\n",
      "2021-01-14 19:34:18,729 : INFO : token count processed\n",
      "2021-01-14 19:34:18,737 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:34:18,741 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:34:18,742 : INFO : vocab #32006\n",
      "2021-01-14 19:34:18,749 : INFO : diff #set()\n",
      "2021-01-14 19:34:37,628 : INFO : alphabet #32006\n",
      "2021-01-14 19:34:47,182 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.3054832348449208, 0.433748545591686], [0.9825595710426569, 0.017440429], [0.0, 0.0], [3.4182958340544896, 6.6236746347295465, 6.729507345517454, 3.312463123266583, 3.311211511462964, 0.10583271078790712]]\n",
      "2021-01-14 19:34:47,186 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 19:34:47,188 : INFO : built Dictionary(241 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1216 corpus positions)\n",
      "2021-01-14 19:34:47,263 : INFO : token count processed\n",
      "2021-01-14 19:34:47,296 : INFO : frequencies processed\n",
      "2021-01-14 19:34:56,899 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:34:56,900 : INFO : entropies processed\n",
      "2021-01-14 19:34:56,901 : INFO : extropies processed\n",
      "2021-01-14 19:34:56,908 : INFO : token count processed\n",
      "2021-01-14 19:34:56,912 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:34:56,918 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:34:56,919 : INFO : vocab #32006\n",
      "2021-01-14 19:34:56,926 : INFO : diff #set()\n",
      "2021-01-14 19:35:15,901 : INFO : alphabet #32006\n",
      "2021-01-14 19:35:25,324 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.2989143825516265, 0.4349879262967911], [0.9728752262890339, 0.027124774], [0.0, 0.0], [3.4182958340544896, 6.75472436518627, 6.8572394865289255, 3.3157807127118346, 3.438943652474436, 0.10251512134265539]]\n",
      "2021-01-14 19:35:25,333 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:35:25,336 : INFO : built Dictionary(194 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1011 corpus positions)\n",
      "2021-01-14 19:35:25,391 : INFO : token count processed\n",
      "2021-01-14 19:35:25,419 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:35:25,421 : INFO : frequencies processed\n",
      "2021-01-14 19:35:25,422 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:35:25,427 : INFO : token count processed\n",
      "2021-01-14 19:35:25,431 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:35:25,435 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:35:25,435 : INFO : vocab #32006\n",
      "2021-01-14 19:35:25,441 : INFO : diff #set()\n",
      "2021-01-14 19:35:44,436 : INFO : alphabet #32006\n",
      "2021-01-14 19:35:53,864 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.3029700175165673, 0.43422189277060624], [0.9780545365065336, 0.021945463], [nan, nan], [3.4182958340544896, 6.597313085495733, 6.6929061996274655, 3.3227027199227575, 3.274610365572976, 0.09559311413173255]]\n",
      "2021-01-14 19:35:53,868 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:35:53,870 : INFO : built Dictionary(218 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 977 corpus positions)\n",
      "2021-01-14 19:35:53,933 : INFO : token count processed\n",
      "2021-01-14 19:35:53,965 : INFO : frequencies processed\n",
      "2021-01-14 19:36:03,383 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:36:03,384 : INFO : entropies processed\n",
      "2021-01-14 19:36:03,385 : INFO : extropies processed\n",
      "2021-01-14 19:36:03,400 : INFO : token count processed\n",
      "2021-01-14 19:36:03,404 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:36:03,409 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:36:03,410 : INFO : vocab #32006\n",
      "2021-01-14 19:36:03,419 : INFO : diff #set()\n",
      "2021-01-14 19:36:22,382 : INFO : alphabet #32006\n",
      "2021-01-14 19:36:31,790 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.300801629816649, 0.4346311246657497], [0.9737811107188463, 0.02621889], [0.0, 0.0], [3.4182958340544896, 6.659481538516613, 6.7721514335139545, 3.3056259390571476, 3.353855599459465, 0.11266989499734148]]\n",
      "2021-01-14 19:36:31,794 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:36:31,796 : INFO : built Dictionary(243 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1139 corpus positions)\n",
      "2021-01-14 19:36:31,878 : INFO : token count processed\n",
      "2021-01-14 19:36:31,941 : INFO : frequencies processed\n",
      "2021-01-14 19:36:41,362 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:36:41,363 : INFO : entropies processed\n",
      "2021-01-14 19:36:41,364 : INFO : extropies processed\n",
      "2021-01-14 19:36:41,371 : INFO : token count processed\n",
      "2021-01-14 19:36:41,375 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:36:41,379 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:36:41,380 : INFO : vocab #32006\n",
      "2021-01-14 19:36:41,387 : INFO : diff #set()\n",
      "2021-01-14 19:37:00,374 : INFO : alphabet #32006\n",
      "2021-01-14 19:37:09,675 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.3072962318361339, 0.43340772034469344], [0.982771860435605, 0.01722814], [0.0, 0.0], [3.4182958340544896, 6.774682571479102, 6.862075493952457, 3.330902911581134, 3.443779659897967, 0.08739292247335495]]\n",
      "2021-01-14 19:37:09,690 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:37:09,693 : INFO : built Dictionary(420 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 7876 corpus positions)\n",
      "2021-01-14 19:37:09,877 : INFO : token count processed\n",
      "2021-01-14 19:37:09,943 : INFO : frequencies processed\n",
      "2021-01-14 19:37:19,357 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:37:19,358 : INFO : entropies processed\n",
      "2021-01-14 19:37:19,359 : INFO : extropies processed\n",
      "2021-01-14 19:37:19,368 : INFO : token count processed\n",
      "2021-01-14 19:37:19,373 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:37:19,377 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:37:19,378 : INFO : vocab #32006\n",
      "2021-01-14 19:37:19,385 : INFO : diff #set()\n",
      "2021-01-14 19:37:38,352 : INFO : alphabet #32006\n",
      "2021-01-14 19:37:47,768 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.3035752166896528, 0.4341078132612694], [0.9834939073771238, 0.016506093], [1.0, 1.0], [3.4182958340544896, 6.839453716525233, 6.982868966940689, 3.2748805836390336, 3.5645731328861996, 0.14341525041545644]]\n",
      "2021-01-14 19:37:47,774 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:37:47,776 : INFO : built Dictionary(320 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 2318 corpus positions)\n",
      "2021-01-14 19:37:47,885 : INFO : token count processed\n",
      "2021-01-14 19:37:47,917 : INFO : frequencies processed\n",
      "2021-01-14 19:37:57,326 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:37:57,327 : INFO : entropies processed\n",
      "2021-01-14 19:37:57,328 : INFO : extropies processed\n",
      "2021-01-14 19:37:57,336 : INFO : token count processed\n",
      "2021-01-14 19:37:57,341 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:37:57,345 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:37:57,346 : INFO : vocab #32006\n",
      "2021-01-14 19:37:57,353 : INFO : diff #set()\n",
      "2021-01-14 19:38:16,331 : INFO : alphabet #32006\n",
      "2021-01-14 19:38:26,110 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.2926040412774, 0.436185220821131], [0.9629163183271885, 0.03708368], [1.0, 1.0], [3.4182958340544896, 6.86432793886027, 6.964802275918544, 3.317821496996215, 3.546506441864054, 0.10047433705827391]]\n",
      "2021-01-14 19:38:26,114 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:38:26,115 : INFO : built Dictionary(155 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 520 corpus positions)\n",
      "2021-01-14 19:38:26,158 : INFO : token count processed\n",
      "2021-01-14 19:38:26,185 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:38:26,186 : INFO : frequencies processed\n",
      "2021-01-14 19:38:26,188 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:38:26,194 : INFO : token count processed\n",
      "2021-01-14 19:38:26,198 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:38:26,202 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:38:26,203 : INFO : vocab #32006\n",
      "2021-01-14 19:38:26,210 : INFO : diff #set()\n",
      "2021-01-14 19:38:45,158 : INFO : alphabet #32006\n",
      "2021-01-14 19:38:54,579 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.3047332574695947, 0.4338896906004284], [0.9810541085898876, 0.018945891], [nan, nan], [3.4182958340544896, 6.431978396403875, 6.549110410570716, 3.3011638198876483, 3.1308145765162263, 0.11713201416684083]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 19:38:54,583 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:38:54,584 : INFO : built Dictionary(213 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 674 corpus positions)\n",
      "2021-01-14 19:38:54,653 : INFO : token count processed\n",
      "2021-01-14 19:38:54,687 : INFO : frequencies processed\n",
      "2021-01-14 19:39:04,098 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:39:04,099 : INFO : entropies processed\n",
      "2021-01-14 19:39:04,100 : INFO : extropies processed\n",
      "2021-01-14 19:39:04,107 : INFO : token count processed\n",
      "2021-01-14 19:39:04,111 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:39:04,116 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:39:04,117 : INFO : vocab #32006\n",
      "2021-01-14 19:39:04,127 : INFO : diff #set()\n",
      "2021-01-14 19:39:23,087 : INFO : alphabet #32006\n",
      "2021-01-14 19:39:32,486 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/test_auth_utility.py')[[1.3015720710677057, 0.4344856337851272], [0.9737536087632179, 0.026246391], [0.0, 0.0], [3.4182958340544896, 6.911818353685893, 7.015861583973284, 3.314252603767099, 3.5975657499187945, 0.10404323028739082]]\n",
      "2021-01-14 19:39:32,500 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:39:32,503 : INFO : built Dictionary(304 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 7204 corpus positions)\n",
      "2021-01-14 19:39:32,614 : INFO : token count processed\n",
      "2021-01-14 19:39:32,646 : INFO : frequencies processed\n",
      "2021-01-14 19:39:42,057 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:39:42,059 : INFO : entropies processed\n",
      "2021-01-14 19:39:42,059 : INFO : extropies processed\n",
      "2021-01-14 19:39:42,069 : INFO : token count processed\n",
      "2021-01-14 19:39:42,073 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:39:42,077 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:39:42,078 : INFO : vocab #32006\n",
      "2021-01-14 19:39:42,086 : INFO : diff #set()\n",
      "2021-01-14 19:40:01,065 : INFO : alphabet #32006\n",
      "2021-01-14 19:40:10,470 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.303563624114057, 0.43410999788842247], [0.98310536891222, 0.016894631], [0.0, 0.0], [3.4182958340544896, 6.363791471162389, 6.391721486568159, 3.3903658186487196, 2.973425652513669, 0.02793001540576956]]\n",
      "2021-01-14 19:40:10,474 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:40:10,476 : INFO : built Dictionary(205 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1274 corpus positions)\n",
      "2021-01-14 19:40:10,537 : INFO : token count processed\n",
      "2021-01-14 19:40:10,569 : INFO : frequencies processed\n",
      "2021-01-14 19:40:20,089 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:40:20,090 : INFO : entropies processed\n",
      "2021-01-14 19:40:20,091 : INFO : extropies processed\n",
      "2021-01-14 19:40:20,098 : INFO : token count processed\n",
      "2021-01-14 19:40:20,102 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:40:20,106 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:40:20,107 : INFO : vocab #32006\n",
      "2021-01-14 19:40:20,114 : INFO : diff #set()\n",
      "2021-01-14 19:40:38,979 : INFO : alphabet #32006\n",
      "2021-01-14 19:40:48,542 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.3078638950176877, 0.43330111544222405], [0.9876011703163385, 0.01239883], [0.0, 0.0], [3.4182958340544896, 6.29000629755059, 6.483894239882493, 3.2244078917225876, 3.0655984058280032, 0.19388794233190243]]\n",
      "2021-01-14 19:40:48,547 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:40:48,548 : INFO : built Dictionary(215 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1227 corpus positions)\n",
      "2021-01-14 19:40:48,613 : INFO : token count processed\n",
      "2021-01-14 19:40:48,644 : INFO : frequencies processed\n",
      "2021-01-14 19:40:58,170 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:40:58,172 : INFO : entropies processed\n",
      "2021-01-14 19:40:58,173 : INFO : extropies processed\n",
      "2021-01-14 19:40:58,180 : INFO : token count processed\n",
      "2021-01-14 19:40:58,185 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:40:58,189 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:40:58,190 : INFO : vocab #32006\n",
      "2021-01-14 19:40:58,196 : INFO : diff #set()\n",
      "2021-01-14 19:41:17,497 : INFO : alphabet #32006\n",
      "2021-01-14 19:41:27,009 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.3033519197448238, 0.4341498975591992], [0.9808988254517317, 0.019101175], [0.0, 0.0], [3.4182958340544896, 6.361621244785958, 6.541103786452129, 3.2388132923883193, 3.1228079523976393, 0.17948254166617073]]\n",
      "2021-01-14 19:41:27,014 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:41:27,015 : INFO : built Dictionary(224 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1250 corpus positions)\n",
      "2021-01-14 19:41:27,087 : INFO : token count processed\n",
      "2021-01-14 19:41:27,115 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:41:27,116 : INFO : frequencies processed\n",
      "2021-01-14 19:41:27,116 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:41:27,122 : INFO : token count processed\n",
      "2021-01-14 19:41:27,126 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:41:27,130 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:41:27,131 : INFO : vocab #32006\n",
      "2021-01-14 19:41:27,138 : INFO : diff #set()\n",
      "2021-01-14 19:41:45,991 : INFO : alphabet #32006\n",
      "2021-01-14 19:41:55,404 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.2985948453795821, 0.43504839576670307], [0.975712563842535, 0.024287436], [nan, nan], [3.4182958340544896, 6.620594433343389, 6.8091876574567465, 3.229702609941132, 3.390891823402257, 0.18859322411335722]]\n",
      "2021-01-14 19:41:55,409 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:41:55,411 : INFO : built Dictionary(196 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1343 corpus positions)\n",
      "2021-01-14 19:41:55,471 : INFO : token count processed\n",
      "2021-01-14 19:41:55,498 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:41:55,501 : INFO : frequencies processed\n",
      "2021-01-14 19:41:55,502 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:41:55,508 : INFO : token count processed\n",
      "2021-01-14 19:41:55,514 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:41:55,518 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:41:55,519 : INFO : vocab #32006\n",
      "2021-01-14 19:41:55,528 : INFO : diff #set()\n",
      "2021-01-14 19:42:15,067 : INFO : alphabet #32006\n",
      "2021-01-14 19:42:24,475 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.2978885924792873, 0.4351821072931384], [0.9725698344409466, 0.027430166], [nan, nan], [3.4182958340544896, 6.207411496248084, 6.313999479039154, 3.311707851263419, 2.8957036449846645, 0.10658798279107007]]\n",
      "2021-01-14 19:42:24,478 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:42:24,479 : INFO : built Dictionary(135 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 247 corpus positions)\n",
      "2021-01-14 19:42:24,518 : INFO : token count processed\n",
      "2021-01-14 19:42:24,550 : INFO : frequencies processed\n",
      "2021-01-14 19:42:34,063 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:42:34,064 : INFO : entropies processed\n",
      "2021-01-14 19:42:34,065 : INFO : extropies processed\n",
      "2021-01-14 19:42:34,072 : INFO : token count processed\n",
      "2021-01-14 19:42:34,079 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:42:34,083 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:42:34,084 : INFO : vocab #32006\n",
      "2021-01-14 19:42:34,091 : INFO : diff #set()\n",
      "2021-01-14 19:42:53,202 : INFO : alphabet #32006\n",
      "2021-01-14 19:43:02,613 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.2926960940015069, 0.4361677077988439], [0.965730007737875, 0.034269992], [0.0, 0.0], [3.4182958340544896, 6.5805228788529595, 6.723312714281768, 3.2755059986256807, 3.3050168802272784, 0.1427898354288084]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 19:43:02,617 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 19:43:02,618 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:43:02,620 : INFO : built Dictionary(219 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1208 corpus positions)\n",
      "2021-01-14 19:43:02,695 : INFO : token count processed\n",
      "2021-01-14 19:43:02,722 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:43:02,723 : INFO : frequencies processed\n",
      "2021-01-14 19:43:02,724 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:43:02,730 : INFO : token count processed\n",
      "2021-01-14 19:43:02,734 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:43:02,738 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:43:02,739 : INFO : vocab #32006\n",
      "2021-01-14 19:43:02,745 : INFO : diff #set()\n",
      "2021-01-14 19:43:21,678 : INFO : alphabet #32006\n",
      "2021-01-14 19:43:31,075 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.305072935061375, 0.4338257522308612], [0.9835742209106684, 0.01642578], [nan, nan], [3.4182958340544896, 6.422089779976135, 6.500004396020566, 3.3403812180100587, 3.0817085619660767, 0.07791461604443128]]\n",
      "2021-01-14 19:43:31,080 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:43:31,083 : INFO : built Dictionary(231 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1490 corpus positions)\n",
      "2021-01-14 19:43:31,162 : INFO : token count processed\n",
      "2021-01-14 19:43:31,220 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:43:31,225 : INFO : frequencies processed\n",
      "2021-01-14 19:43:31,226 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:43:31,235 : INFO : token count processed\n",
      "2021-01-14 19:43:31,244 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:43:31,247 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:43:31,248 : INFO : vocab #32006\n",
      "2021-01-14 19:43:31,254 : INFO : diff #set()\n",
      "2021-01-14 19:43:50,206 : INFO : alphabet #32006\n",
      "2021-01-14 19:43:59,633 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.302219054047572, 0.43436353210693934], [0.9818648472428322, 0.018135153], [nan, nan], [3.4182958340544896, 6.485445644653597, 6.703310102897749, 3.2004313758103375, 3.2850142688432595, 0.21786445824415246]]\n",
      "2021-01-14 19:43:59,637 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:43:59,639 : INFO : built Dictionary(206 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1376 corpus positions)\n",
      "2021-01-14 19:43:59,705 : INFO : token count processed\n",
      "2021-01-14 19:43:59,732 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:43:59,733 : INFO : frequencies processed\n",
      "2021-01-14 19:43:59,734 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:43:59,742 : INFO : token count processed\n",
      "2021-01-14 19:43:59,747 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:43:59,755 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:43:59,757 : INFO : vocab #32006\n",
      "2021-01-14 19:43:59,765 : INFO : diff #set()\n",
      "2021-01-14 19:44:18,718 : INFO : alphabet #32006\n",
      "2021-01-14 19:44:28,291 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.298605717397627, 0.4350463380610368], [0.9739731047302485, 0.026026895], [nan, nan], [3.4182958340544896, 6.2276600107346916, 6.331911813845855, 3.314044030943326, 2.913615979791366, 0.10425180311116389]]\n",
      "2021-01-14 19:44:28,296 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:44:28,299 : INFO : built Dictionary(194 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1574 corpus positions)\n",
      "2021-01-14 19:44:28,365 : INFO : token count processed\n",
      "2021-01-14 19:44:28,401 : INFO : frequencies processed\n",
      "2021-01-14 19:44:37,807 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:44:37,808 : INFO : entropies processed\n",
      "2021-01-14 19:44:37,808 : INFO : extropies processed\n",
      "2021-01-14 19:44:37,816 : INFO : token count processed\n",
      "2021-01-14 19:44:37,821 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:44:37,827 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:44:37,828 : INFO : vocab #32006\n",
      "2021-01-14 19:44:37,836 : INFO : diff #set()\n",
      "2021-01-14 19:44:56,830 : INFO : alphabet #32006\n",
      "2021-01-14 19:45:06,262 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.3037787757099875, 0.4340694560361232], [0.9811398591846228, 0.01886014], [1.0, 1.0], [3.4182958340544896, 6.253918170574241, 6.406796747189963, 3.2654172574387665, 2.9885009131354736, 0.15287857661572257]]\n",
      "2021-01-14 19:45:06,266 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:45:06,267 : INFO : built Dictionary(168 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 671 corpus positions)\n",
      "2021-01-14 19:45:06,322 : INFO : token count processed\n",
      "2021-01-14 19:45:06,350 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:45:06,353 : INFO : frequencies processed\n",
      "2021-01-14 19:45:06,353 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:45:06,358 : INFO : token count processed\n",
      "2021-01-14 19:45:06,362 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:45:06,366 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:45:06,367 : INFO : vocab #32006\n",
      "2021-01-14 19:45:06,374 : INFO : diff #set()\n",
      "2021-01-14 19:45:25,281 : INFO : alphabet #32006\n",
      "2021-01-14 19:45:34,625 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.2979232814584793, 0.4351755378731815], [0.9729588311165571, 0.027041169], [nan, nan], [3.4182958340544896, 6.374522245625576, 6.544001906694147, 3.2488161729859186, 3.1257060726396575, 0.16947966106857137]]\n",
      "2021-01-14 19:45:34,630 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:45:34,634 : INFO : built Dictionary(290 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1953 corpus positions)\n",
      "2021-01-14 19:45:34,734 : INFO : token count processed\n",
      "2021-01-14 19:45:34,765 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:45:34,766 : INFO : frequencies processed\n",
      "2021-01-14 19:45:34,766 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:45:34,773 : INFO : token count processed\n",
      "2021-01-14 19:45:34,777 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:45:34,781 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:45:34,782 : INFO : vocab #32006\n",
      "2021-01-14 19:45:34,788 : INFO : diff #set()\n",
      "2021-01-14 19:45:53,632 : INFO : alphabet #32006\n",
      "2021-01-14 19:46:03,156 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.3033475878087237, 0.43415071407062106], [0.9846390187740326, 0.015360981], [nan, nan], [3.4182958340544896, 6.731238669067808, 6.978248639158284, 3.1712858639640125, 3.5599528051037947, 0.24700997009047665]]\n",
      "2021-01-14 19:46:03,161 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:46:03,163 : INFO : built Dictionary(213 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1345 corpus positions)\n",
      "2021-01-14 19:46:03,228 : INFO : token count processed\n",
      "2021-01-14 19:46:03,260 : INFO : frequencies processed\n",
      "2021-01-14 19:46:12,666 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:46:12,667 : INFO : entropies processed\n",
      "2021-01-14 19:46:12,668 : INFO : extropies processed\n",
      "2021-01-14 19:46:12,676 : INFO : token count processed\n",
      "2021-01-14 19:46:12,680 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:46:12,685 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:46:12,686 : INFO : vocab #32006\n",
      "2021-01-14 19:46:12,694 : INFO : diff #set()\n",
      "2021-01-14 19:46:31,712 : INFO : alphabet #32006\n",
      "2021-01-14 19:46:41,159 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.2965110145050993, 0.4354431542822368], [0.9749828074127436, 0.025017193], [1.0, 1.0], [3.4182958340544896, 6.503741451859337, 6.662417740149241, 3.2596195457645862, 3.2441219060947515, 0.15867628828990377]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 19:46:41,164 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:46:41,165 : INFO : built Dictionary(235 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 1667 corpus positions)\n",
      "2021-01-14 19:46:41,246 : INFO : token count processed\n",
      "2021-01-14 19:46:41,280 : INFO : frequencies processed\n",
      "2021-01-14 19:46:50,687 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:46:50,688 : INFO : entropies processed\n",
      "2021-01-14 19:46:50,689 : INFO : extropies processed\n",
      "2021-01-14 19:46:50,703 : INFO : token count processed\n",
      "2021-01-14 19:46:50,707 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:46:50,713 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:46:50,714 : INFO : vocab #32006\n",
      "2021-01-14 19:46:50,721 : INFO : diff #set()\n",
      "2021-01-14 19:47:09,705 : INFO : alphabet #32006\n",
      "2021-01-14 19:47:19,125 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.2961884241419668, 0.4355043294731691], [0.9720451813191175, 0.027954819], [1.0, 1.0], [3.4182958340544896, 6.334729224484471, 6.441045153526965, 3.311979905011996, 3.0227493194724757, 0.10631592904249398]]\n",
      "2021-01-14 19:47:19,130 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:47:19,132 : INFO : built Dictionary(234 unique tokens: ['▁*', '▁fix', '▁function', '▁functionality', '▁image']...) from 2 documents (total 2016 corpus positions)\n",
      "2021-01-14 19:47:19,207 : INFO : token count processed\n",
      "2021-01-14 19:47:19,234 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 19:47:19,237 : INFO : frequencies processed\n",
      "2021-01-14 19:47:19,237 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 19:47:19,244 : INFO : token count processed\n",
      "2021-01-14 19:47:19,250 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:47:19,255 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:47:19,256 : INFO : vocab #32006\n",
      "2021-01-14 19:47:19,264 : INFO : diff #set()\n",
      "2021-01-14 19:47:38,217 : INFO : alphabet #32006\n",
      "2021-01-14 19:47:47,623 : INFO : Computed distances or similarities ('276', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.3063886371729954, 0.43357827205814187], [0.9862018711864948, 0.013798129], [nan, nan], [3.4182958340544896, 6.21319712067992, 6.386951073128144, 3.244541881606266, 2.9686552390736547, 0.1737539524482239]]\n",
      "2021-01-14 19:47:47,628 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 19:47:47,629 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:47:47,631 : INFO : built Dictionary(288 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1455 corpus positions)\n",
      "2021-01-14 19:47:47,932 : INFO : token count processed\n",
      "2021-01-14 19:47:47,967 : INFO : frequencies processed\n",
      "2021-01-14 19:47:57,368 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:47:57,369 : INFO : entropies processed\n",
      "2021-01-14 19:47:57,369 : INFO : extropies processed\n",
      "2021-01-14 19:47:57,377 : INFO : token count processed\n",
      "2021-01-14 19:47:57,383 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:47:57,388 : INFO : alphabet_target #32010\n",
      "2021-01-14 19:47:57,389 : INFO : vocab #32006\n",
      "2021-01-14 19:47:57,395 : INFO : diff #set()\n",
      "2021-01-14 19:48:16,413 : INFO : alphabet #32006\n",
      "2021-01-14 19:48:25,846 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.0916032973218155, 0.4781021340329908], [0.741656631231308, 0.25834337], [3.7841837197791888, 1.3830098939805733], [5.234069531114783, 6.905617163738059, 7.116251565095951, 5.023435129756891, 1.8821820339811675, 0.21063440135789158]]\n",
      "2021-01-14 19:48:25,852 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:48:25,854 : INFO : built Dictionary(376 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 2345 corpus positions)\n",
      "2021-01-14 19:48:26,312 : INFO : token count processed\n",
      "2021-01-14 19:48:26,373 : INFO : frequencies processed\n",
      "2021-01-14 19:48:35,820 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:48:35,821 : INFO : entropies processed\n",
      "2021-01-14 19:48:35,822 : INFO : extropies processed\n",
      "2021-01-14 19:48:35,830 : INFO : token count processed\n",
      "2021-01-14 19:48:35,834 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:48:35,839 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:48:35,840 : INFO : vocab #32006\n",
      "2021-01-14 19:48:35,848 : INFO : diff #set()\n",
      "2021-01-14 19:48:54,995 : INFO : alphabet #32006\n",
      "2021-01-14 19:49:04,414 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.0947908506594066, 0.4773746265338213], [0.7403530478477478, 0.25964695], [3.986187965046303, 1.388501016626658], [5.234069531114783, 7.1219284286457345, 7.383418847988016, 4.972579111772503, 2.1493493168732325, 0.2614904193422811]]\n",
      "2021-01-14 19:49:04,420 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:49:04,422 : INFO : built Dictionary(294 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 2328 corpus positions)\n",
      "2021-01-14 19:49:04,705 : INFO : token count processed\n",
      "2021-01-14 19:49:04,738 : INFO : frequencies processed\n",
      "2021-01-14 19:49:14,149 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:49:14,151 : INFO : entropies processed\n",
      "2021-01-14 19:49:14,151 : INFO : extropies processed\n",
      "2021-01-14 19:49:14,159 : INFO : token count processed\n",
      "2021-01-14 19:49:14,163 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:49:14,168 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:49:14,169 : INFO : vocab #32006\n",
      "2021-01-14 19:49:14,175 : INFO : diff #set()\n",
      "2021-01-14 19:49:33,302 : INFO : alphabet #32006\n",
      "2021-01-14 19:49:42,487 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.1166003120546824, 0.4724557557252051], [0.7365453541278839, 0.26345465], [4.103465189601647, 1.3939977576138491], [5.234069531114783, 6.41099024988467, 6.54913416529598, 5.095925615703473, 1.315064634181197, 0.13814391541130977]]\n",
      "2021-01-14 19:49:42,491 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:49:42,492 : INFO : built Dictionary(177 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 727 corpus positions)\n",
      "2021-01-14 19:49:42,646 : INFO : token count processed\n",
      "2021-01-14 19:49:42,680 : INFO : frequencies processed\n",
      "2021-01-14 19:49:52,155 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:49:52,156 : INFO : entropies processed\n",
      "2021-01-14 19:49:52,157 : INFO : extropies processed\n",
      "2021-01-14 19:49:52,163 : INFO : token count processed\n",
      "2021-01-14 19:49:52,170 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:49:52,175 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:49:52,176 : INFO : vocab #32006\n",
      "2021-01-14 19:49:52,182 : INFO : diff #set()\n",
      "2021-01-14 19:50:11,079 : INFO : alphabet #32006\n",
      "2021-01-14 19:50:20,520 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.0952600528369674, 0.47726772562002845], [0.7256374061107635, 0.2743626], [3.690116517593665, 1.3778211807876928], [5.234069531114783, 6.077866832717642, 6.38392204981708, 4.9280143140153445, 1.1498525187022972, 0.3060552170994386]]\n",
      "2021-01-14 19:50:20,524 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:50:20,526 : INFO : built Dictionary(153 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 492 corpus positions)\n",
      "2021-01-14 19:50:20,648 : INFO : token count processed\n",
      "2021-01-14 19:50:20,680 : INFO : frequencies processed\n",
      "2021-01-14 19:50:30,203 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:50:30,204 : INFO : entropies processed\n",
      "2021-01-14 19:50:30,205 : INFO : extropies processed\n",
      "2021-01-14 19:50:30,212 : INFO : token count processed\n",
      "2021-01-14 19:50:30,216 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:50:30,220 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:50:30,221 : INFO : vocab #32006\n",
      "2021-01-14 19:50:30,228 : INFO : diff #set()\n",
      "2021-01-14 19:50:49,090 : INFO : alphabet #32006\n",
      "2021-01-14 19:50:58,495 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.1022343920261728, 0.47568434984843977], [0.73421710729599, 0.2657829], [3.690116517593665, 1.3778211807876928], [5.234069531114783, 5.977547459003844, 6.320500822237277, 4.891116167881351, 1.0864312911224943, 0.3429533632334332]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 19:50:58,501 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:50:58,503 : INFO : built Dictionary(253 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 2203 corpus positions)\n",
      "2021-01-14 19:50:58,740 : INFO : token count processed\n",
      "2021-01-14 19:50:58,772 : INFO : frequencies processed\n",
      "2021-01-14 19:51:08,284 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:51:08,285 : INFO : entropies processed\n",
      "2021-01-14 19:51:08,286 : INFO : extropies processed\n",
      "2021-01-14 19:51:08,293 : INFO : token count processed\n",
      "2021-01-14 19:51:08,298 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:51:08,302 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:51:08,303 : INFO : vocab #32006\n",
      "2021-01-14 19:51:08,309 : INFO : diff #set()\n",
      "2021-01-14 19:51:27,191 : INFO : alphabet #32006\n",
      "2021-01-14 19:51:36,727 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.1009869952929725, 0.4759667728740772], [0.7168556451797485, 0.28314435], [4.106377316818026, 1.3949215669506878], [5.234069531114783, 6.4614394051846435, 6.625934394157575, 5.069574542141851, 1.3918648630427919, 0.16449498897293147]]\n",
      "2021-01-14 19:51:36,732 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 19:51:36,733 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:51:36,734 : INFO : built Dictionary(223 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1427 corpus positions)\n",
      "2021-01-14 19:51:36,932 : INFO : token count processed\n",
      "2021-01-14 19:51:36,965 : INFO : frequencies processed\n",
      "2021-01-14 19:51:46,370 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:51:46,371 : INFO : entropies processed\n",
      "2021-01-14 19:51:46,372 : INFO : extropies processed\n",
      "2021-01-14 19:51:46,386 : INFO : token count processed\n",
      "2021-01-14 19:51:46,390 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:51:46,396 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:51:46,397 : INFO : vocab #32006\n",
      "2021-01-14 19:51:46,405 : INFO : diff #set()\n",
      "2021-01-14 19:52:05,261 : INFO : alphabet #32006\n",
      "2021-01-14 19:52:14,777 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.1202719633210658, 0.4716376093723658], [0.7431735694408417, 0.25682643], [3.3248629576173565, 1.3574960179923317], [5.234069531114783, 6.327195724598159, 6.550131987755658, 5.011133267957285, 1.316062456640875, 0.22293626315749915]]\n",
      "2021-01-14 19:52:14,789 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 19:52:14,790 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:52:14,793 : INFO : built Dictionary(428 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 6319 corpus positions)\n",
      "2021-01-14 19:52:15,303 : INFO : token count processed\n",
      "2021-01-14 19:52:15,358 : INFO : frequencies processed\n",
      "2021-01-14 19:52:24,766 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:52:24,767 : INFO : entropies processed\n",
      "2021-01-14 19:52:24,768 : INFO : extropies processed\n",
      "2021-01-14 19:52:24,777 : INFO : token count processed\n",
      "2021-01-14 19:52:24,782 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:52:24,786 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:52:24,787 : INFO : vocab #32006\n",
      "2021-01-14 19:52:24,795 : INFO : diff #set()\n",
      "2021-01-14 19:52:43,774 : INFO : alphabet #32006\n",
      "2021-01-14 19:52:53,183 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.1069123220777775, 0.47462819858295213], [0.7002415359020233, 0.29975846], [4.240156860408859, 1.3959702130568121], [5.234069531114783, 6.9079058562486315, 7.038103303304762, 5.1038720840586524, 1.804033772189979, 0.13019744705613068]]\n",
      "2021-01-14 19:52:53,190 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:52:53,192 : INFO : built Dictionary(333 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 2713 corpus positions)\n",
      "2021-01-14 19:52:53,549 : INFO : token count processed\n",
      "2021-01-14 19:52:53,603 : INFO : frequencies processed\n",
      "2021-01-14 19:53:03,013 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:53:03,014 : INFO : entropies processed\n",
      "2021-01-14 19:53:03,015 : INFO : extropies processed\n",
      "2021-01-14 19:53:03,022 : INFO : token count processed\n",
      "2021-01-14 19:53:03,027 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:53:03,031 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:53:03,032 : INFO : vocab #32006\n",
      "2021-01-14 19:53:03,038 : INFO : diff #set()\n",
      "2021-01-14 19:53:22,017 : INFO : alphabet #32006\n",
      "2021-01-14 19:53:31,424 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.1277808870050843, 0.46997320358842504], [0.7288559079170227, 0.2711441], [4.000645725252347, 1.3838678096427857], [5.234069531114783, 6.61034830706307, 6.818649889116784, 5.02576794906107, 1.5845803580020013, 0.20830158205371418]]\n",
      "2021-01-14 19:53:31,428 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:53:31,430 : INFO : built Dictionary(224 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 760 corpus positions)\n",
      "2021-01-14 19:53:31,630 : INFO : token count processed\n",
      "2021-01-14 19:53:31,662 : INFO : frequencies processed\n",
      "2021-01-14 19:53:41,062 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:53:41,063 : INFO : entropies processed\n",
      "2021-01-14 19:53:41,064 : INFO : extropies processed\n",
      "2021-01-14 19:53:41,071 : INFO : token count processed\n",
      "2021-01-14 19:53:41,075 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:53:41,079 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:53:41,080 : INFO : vocab #32006\n",
      "2021-01-14 19:53:41,088 : INFO : diff #set()\n",
      "2021-01-14 19:54:00,036 : INFO : alphabet #32006\n",
      "2021-01-14 19:54:09,390 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.1425561380771039, 0.4667322280280962], [0.8094967603683472, 0.19050324], [3.6416041678685938, 1.372961825232017], [5.234069531114783, 6.616715366949855, 6.886447619569939, 4.9643372784947, 1.6523780884551558, 0.26973225262008427]]\n",
      "2021-01-14 19:54:09,397 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:54:09,400 : INFO : built Dictionary(436 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 2794 corpus positions)\n",
      "2021-01-14 19:54:09,921 : INFO : token count processed\n",
      "2021-01-14 19:54:09,953 : INFO : frequencies processed\n",
      "2021-01-14 19:54:19,374 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:54:19,375 : INFO : entropies processed\n",
      "2021-01-14 19:54:19,376 : INFO : extropies processed\n",
      "2021-01-14 19:54:19,391 : INFO : token count processed\n",
      "2021-01-14 19:54:19,396 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:54:19,401 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:54:19,402 : INFO : vocab #32006\n",
      "2021-01-14 19:54:19,409 : INFO : diff #set()\n",
      "2021-01-14 19:54:38,377 : INFO : alphabet #32006\n",
      "2021-01-14 19:54:47,787 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.0485494953825707, 0.4881502752332806], [0.6602384448051453, 0.33976156], [4.197159723424148, 1.3919225008023173], [5.234069531114783, 7.32185870753746, 7.5418683861933395, 5.014059852458903, 2.3077988550785564, 0.22000967865587917]]\n",
      "2021-01-14 19:54:47,790 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:54:47,791 : INFO : built Dictionary(86 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 159 corpus positions)\n",
      "2021-01-14 19:54:47,838 : INFO : token count processed\n",
      "2021-01-14 19:54:47,871 : INFO : frequencies processed\n",
      "2021-01-14 19:54:57,270 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:54:57,271 : INFO : entropies processed\n",
      "2021-01-14 19:54:57,272 : INFO : extropies processed\n",
      "2021-01-14 19:54:57,279 : INFO : token count processed\n",
      "2021-01-14 19:54:57,283 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:54:57,288 : INFO : alphabet_target #32008\n",
      "2021-01-14 19:54:57,289 : INFO : vocab #32006\n",
      "2021-01-14 19:54:57,295 : INFO : diff #set()\n",
      "2021-01-14 19:55:16,277 : INFO : alphabet #32006\n",
      "2021-01-14 19:55:25,695 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/fireException.py')[[1.1483696999536133, 0.465469234658072], [0.8211785107851028, 0.17882149], [2.128085278891394, 1.2238339714721664], [5.234069531114783, 5.176618657501385, 6.100306824886672, 4.310381363729497, 0.8662372937718885, 0.9236881673852864]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 19:55:25,698 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:55:25,700 : INFO : built Dictionary(175 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 552 corpus positions)\n",
      "2021-01-14 19:55:25,843 : INFO : token count processed\n",
      "2021-01-14 19:55:25,874 : INFO : frequencies processed\n",
      "2021-01-14 19:55:35,404 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:55:35,405 : INFO : entropies processed\n",
      "2021-01-14 19:55:35,406 : INFO : extropies processed\n",
      "2021-01-14 19:55:35,420 : INFO : token count processed\n",
      "2021-01-14 19:55:35,425 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:55:35,429 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:55:35,430 : INFO : vocab #32006\n",
      "2021-01-14 19:55:35,437 : INFO : diff #set()\n",
      "2021-01-14 19:55:54,493 : INFO : alphabet #32006\n",
      "2021-01-14 19:56:04,039 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.1290169248591067, 0.46970035246017483], [0.7995065599679947, 0.20049344], [3.4677201004744997, 1.365430749834613], [5.234069531114783, 6.468846789852156, 6.821533766063383, 4.881382554903555, 1.5874642349486, 0.3526869762112268]]\n",
      "2021-01-14 19:56:04,045 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:56:04,047 : INFO : built Dictionary(382 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 2601 corpus positions)\n",
      "2021-01-14 19:56:04,554 : INFO : token count processed\n",
      "2021-01-14 19:56:04,586 : INFO : frequencies processed\n",
      "2021-01-14 19:56:13,984 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:56:13,985 : INFO : entropies processed\n",
      "2021-01-14 19:56:13,986 : INFO : extropies processed\n",
      "2021-01-14 19:56:13,994 : INFO : token count processed\n",
      "2021-01-14 19:56:13,998 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:56:14,002 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:56:14,003 : INFO : vocab #32006\n",
      "2021-01-14 19:56:14,010 : INFO : diff #set()\n",
      "2021-01-14 19:56:32,873 : INFO : alphabet #32006\n",
      "2021-01-14 19:56:42,404 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.1078252461058573, 0.4744226315001537], [0.7256849706172943, 0.27431503], [4.185867302411998, 1.3907284515103375], [5.234069531114783, 6.957796704012729, 7.139408296676406, 5.0524579384511075, 1.9053387655616225, 0.18161159266367655]]\n",
      "2021-01-14 19:56:42,411 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:56:42,414 : INFO : built Dictionary(298 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 3098 corpus positions)\n",
      "2021-01-14 19:56:42,709 : INFO : token count processed\n",
      "2021-01-14 19:56:42,741 : INFO : frequencies processed\n",
      "2021-01-14 19:56:52,135 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:56:52,136 : INFO : entropies processed\n",
      "2021-01-14 19:56:52,137 : INFO : extropies processed\n",
      "2021-01-14 19:56:52,145 : INFO : token count processed\n",
      "2021-01-14 19:56:52,149 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:56:52,153 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:56:52,154 : INFO : vocab #32006\n",
      "2021-01-14 19:56:52,161 : INFO : diff #set()\n",
      "2021-01-14 19:57:11,015 : INFO : alphabet #32006\n",
      "2021-01-14 19:57:20,548 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.0729362032074006, 0.4824075137733259], [0.723150759935379, 0.27684924], [3.7544418457133446, 1.3809703887568134], [5.234069531114783, 6.441859572014148, 6.638607760733294, 5.037321342395638, 1.4045382296185105, 0.19674818871914557]]\n",
      "2021-01-14 19:57:20,553 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:57:20,557 : INFO : built Dictionary(321 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1574 corpus positions)\n",
      "2021-01-14 19:57:20,908 : INFO : token count processed\n",
      "2021-01-14 19:57:20,969 : INFO : frequencies processed\n",
      "2021-01-14 19:57:30,417 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:57:30,418 : INFO : entropies processed\n",
      "2021-01-14 19:57:30,419 : INFO : extropies processed\n",
      "2021-01-14 19:57:30,426 : INFO : token count processed\n",
      "2021-01-14 19:57:30,431 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:57:30,435 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:57:30,436 : INFO : vocab #32006\n",
      "2021-01-14 19:57:30,444 : INFO : diff #set()\n",
      "2021-01-14 19:57:49,816 : INFO : alphabet #32006\n",
      "2021-01-14 19:57:59,271 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.1094650574935379, 0.47405383485621605], [0.7473744750022888, 0.25262552], [4.0220552088742005, 1.3910594777968566], [5.234069531114783, 6.998955278238291, 7.226487027379101, 5.0065377819739725, 1.9924174962643182, 0.22753174914081065]]\n",
      "2021-01-14 19:57:59,281 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:57:59,284 : INFO : built Dictionary(237 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1393 corpus positions)\n",
      "2021-01-14 19:57:59,512 : INFO : token count processed\n",
      "2021-01-14 19:57:59,544 : INFO : frequencies processed\n",
      "2021-01-14 19:58:08,943 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:58:08,944 : INFO : entropies processed\n",
      "2021-01-14 19:58:08,945 : INFO : extropies processed\n",
      "2021-01-14 19:58:08,952 : INFO : token count processed\n",
      "2021-01-14 19:58:08,955 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:58:08,959 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:58:08,960 : INFO : vocab #32006\n",
      "2021-01-14 19:58:08,965 : INFO : diff #set()\n",
      "2021-01-14 19:58:27,901 : INFO : alphabet #32006\n",
      "2021-01-14 19:58:37,337 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.0891605750932192, 0.4786611483683487], [0.7111638784408569, 0.28883612], [3.936180434129755, 1.3877783194135342], [5.234069531114783, 6.492983191376071, 6.694601299489366, 5.032451423001487, 1.460531768374583, 0.20161810811329506]]\n",
      "2021-01-14 19:58:37,344 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:58:37,347 : INFO : built Dictionary(454 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 3332 corpus positions)\n",
      "2021-01-14 19:58:37,941 : INFO : token count processed\n",
      "2021-01-14 19:58:37,977 : INFO : frequencies processed\n",
      "2021-01-14 19:58:47,389 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:58:47,390 : INFO : entropies processed\n",
      "2021-01-14 19:58:47,392 : INFO : extropies processed\n",
      "2021-01-14 19:58:47,406 : INFO : token count processed\n",
      "2021-01-14 19:58:47,411 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:58:47,417 : INFO : alphabet_target #32008\n",
      "2021-01-14 19:58:47,418 : INFO : vocab #32006\n",
      "2021-01-14 19:58:47,424 : INFO : diff #set()\n",
      "2021-01-14 19:59:06,394 : INFO : alphabet #32006\n",
      "2021-01-14 19:59:15,811 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.1437963552401795, 0.46646221669127025], [0.7535140812397003, 0.24648592], [3.0269868333592873, 1.3385887750658594], [5.234069531114783, 6.560342487747443, 6.8091529897969405, 4.9852590290652845, 1.5750834586821574, 0.24881050204949773]]\n",
      "2021-01-14 19:59:15,819 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 19:59:15,820 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:59:15,822 : INFO : built Dictionary(458 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 3537 corpus positions)\n",
      "2021-01-14 19:59:16,402 : INFO : token count processed\n",
      "2021-01-14 19:59:16,434 : INFO : frequencies processed\n",
      "2021-01-14 19:59:25,832 : INFO : scalar_distribution processed\n",
      "2021-01-14 19:59:25,833 : INFO : entropies processed\n",
      "2021-01-14 19:59:25,834 : INFO : extropies processed\n",
      "2021-01-14 19:59:25,842 : INFO : token count processed\n",
      "2021-01-14 19:59:25,846 : INFO : alphabet_source #32006\n",
      "2021-01-14 19:59:25,850 : INFO : alphabet_target #32009\n",
      "2021-01-14 19:59:25,851 : INFO : vocab #32006\n",
      "2021-01-14 19:59:25,858 : INFO : diff #set()\n",
      "2021-01-14 19:59:45,118 : INFO : alphabet #32006\n",
      "2021-01-14 19:59:54,527 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.1121973238473142, 0.473440614998283], [0.720183402299881, 0.2798166], [4.300497519854927, 1.4003920045428815], [5.234069531114783, 7.046173750105238, 7.266324272290593, 5.013919008929427, 2.0322547411758096, 0.2201505221853548]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 19:59:54,538 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 19:59:54,539 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 19:59:54,542 : INFO : built Dictionary(505 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 5646 corpus positions)\n",
      "2021-01-14 19:59:55,263 : INFO : token count processed\n",
      "2021-01-14 19:59:55,296 : INFO : frequencies processed\n",
      "2021-01-14 20:00:04,797 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:00:04,798 : INFO : entropies processed\n",
      "2021-01-14 20:00:04,798 : INFO : extropies processed\n",
      "2021-01-14 20:00:04,807 : INFO : token count processed\n",
      "2021-01-14 20:00:04,811 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:00:04,815 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:00:04,816 : INFO : vocab #32006\n",
      "2021-01-14 20:00:04,823 : INFO : diff #set()\n",
      "2021-01-14 20:00:23,676 : INFO : alphabet #32006\n",
      "2021-01-14 20:00:33,095 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.0805475482534408, 0.4806427042917], [0.6738207042217255, 0.3261793], [4.66126308502903, 1.408413099051593], [5.234069531114783, 7.009229588004272, 7.134371246208306, 5.108927872910749, 1.9003017150935229, 0.12514165820403367]]\n",
      "2021-01-14 20:00:33,108 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:00:33,111 : INFO : built Dictionary(587 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 6587 corpus positions)\n",
      "2021-01-14 20:00:33,934 : INFO : token count processed\n",
      "2021-01-14 20:00:33,966 : INFO : frequencies processed\n",
      "2021-01-14 20:00:43,473 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:00:43,474 : INFO : entropies processed\n",
      "2021-01-14 20:00:43,475 : INFO : extropies processed\n",
      "2021-01-14 20:00:43,484 : INFO : token count processed\n",
      "2021-01-14 20:00:43,491 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:00:43,495 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:00:43,496 : INFO : vocab #32006\n",
      "2021-01-14 20:00:43,504 : INFO : diff #set()\n",
      "2021-01-14 20:01:02,431 : INFO : alphabet #32006\n",
      "2021-01-14 20:01:11,987 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.0908275973192885, 0.4782795106024665], [0.7076362371444702, 0.29236376], [4.472321731245031, 1.4021757051196455], [5.234069531114783, 7.376088004590871, 7.554219104816831, 5.055938430888823, 2.3201495737020474, 0.17813110022595957]]\n",
      "2021-01-14 20:01:11,991 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:01:11,992 : INFO : built Dictionary(153 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 373 corpus positions)\n",
      "2021-01-14 20:01:12,120 : INFO : token count processed\n",
      "2021-01-14 20:01:12,155 : INFO : frequencies processed\n",
      "2021-01-14 20:01:21,555 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:01:21,556 : INFO : entropies processed\n",
      "2021-01-14 20:01:21,556 : INFO : extropies processed\n",
      "2021-01-14 20:01:21,564 : INFO : token count processed\n",
      "2021-01-14 20:01:21,568 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:01:21,572 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:01:21,573 : INFO : vocab #32006\n",
      "2021-01-14 20:01:21,580 : INFO : diff #set()\n",
      "2021-01-14 20:01:40,451 : INFO : alphabet #32006\n",
      "2021-01-14 20:01:49,965 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.1148215986817125, 0.47285312417054776], [0.7921105474233627, 0.20788945], [3.188721875540867, 1.3469079016509695], [5.234069531114783, 6.2993628166120885, 6.70215086194896, 4.831281485777913, 1.4680813308341767, 0.4027880453368713]]\n",
      "2021-01-14 20:01:49,968 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:01:49,969 : INFO : built Dictionary(59 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 89 corpus positions)\n",
      "2021-01-14 20:01:49,991 : INFO : token count processed\n",
      "2021-01-14 20:01:50,022 : INFO : frequencies processed\n",
      "2021-01-14 20:01:59,415 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:01:59,416 : INFO : entropies processed\n",
      "2021-01-14 20:01:59,417 : INFO : extropies processed\n",
      "2021-01-14 20:01:59,424 : INFO : token count processed\n",
      "2021-01-14 20:01:59,430 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:01:59,435 : INFO : alphabet_target #32008\n",
      "2021-01-14 20:01:59,436 : INFO : vocab #32006\n",
      "2021-01-14 20:01:59,443 : INFO : diff #set()\n",
      "2021-01-14 20:02:18,320 : INFO : alphabet #32006\n",
      "2021-01-14 20:02:27,790 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.260491934447191, 0.44238158285866774], [0.9026680961251259, 0.097331904], [0.0, 0.0], [5.234069531114783, 3.8936606896881862, 5.62243685234006, 3.50529336846291, 0.3883673212252772, 1.728776162651874]]\n",
      "2021-01-14 20:02:27,813 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:02:27,818 : INFO : built Dictionary(738 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 12531 corpus positions)\n",
      "2021-01-14 20:02:29,041 : INFO : token count processed\n",
      "2021-01-14 20:02:29,112 : INFO : frequencies processed\n",
      "2021-01-14 20:02:38,541 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:02:38,543 : INFO : entropies processed\n",
      "2021-01-14 20:02:38,543 : INFO : extropies processed\n",
      "2021-01-14 20:02:38,554 : INFO : token count processed\n",
      "2021-01-14 20:02:38,559 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:02:38,563 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:02:38,564 : INFO : vocab #32006\n",
      "2021-01-14 20:02:38,570 : INFO : diff #set()\n",
      "2021-01-14 20:02:57,548 : INFO : alphabet #32006\n",
      "2021-01-14 20:03:06,955 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.0300814968661403, 0.4925910617596935], [0.6682805716991425, 0.33171943], [4.878494982701806, 1.4125374228146892], [5.234069531114783, 7.434393313070278, 7.641872573513986, 5.0265902706710754, 2.4078030423992027, 0.20747926044370768]]\n",
      "2021-01-14 20:03:06,964 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:03:06,967 : INFO : built Dictionary(495 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 4158 corpus positions)\n",
      "2021-01-14 20:03:07,617 : INFO : token count processed\n",
      "2021-01-14 20:03:07,650 : INFO : frequencies processed\n",
      "2021-01-14 20:03:17,055 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:03:17,056 : INFO : entropies processed\n",
      "2021-01-14 20:03:17,057 : INFO : extropies processed\n",
      "2021-01-14 20:03:17,065 : INFO : token count processed\n",
      "2021-01-14 20:03:17,069 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:03:17,074 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:03:17,075 : INFO : vocab #32006\n",
      "2021-01-14 20:03:17,082 : INFO : diff #set()\n",
      "2021-01-14 20:03:36,063 : INFO : alphabet #32006\n",
      "2021-01-14 20:03:45,501 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.0880224216659482, 0.47892206023445894], [0.6991201639175415, 0.30087984], [4.558518613048906, 1.4031195605627262], [5.234069531114783, 7.2991514951718255, 7.50773607672985, 5.025484949556757, 2.2736665456150673, 0.20858458155802495]]\n",
      "2021-01-14 20:03:45,509 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:03:45,512 : INFO : built Dictionary(465 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 3575 corpus positions)\n",
      "2021-01-14 20:03:46,080 : INFO : token count processed\n",
      "2021-01-14 20:03:46,117 : INFO : frequencies processed\n",
      "2021-01-14 20:03:55,530 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:03:55,531 : INFO : entropies processed\n",
      "2021-01-14 20:03:55,532 : INFO : extropies processed\n",
      "2021-01-14 20:03:55,540 : INFO : token count processed\n",
      "2021-01-14 20:03:55,547 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:03:55,551 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:03:55,552 : INFO : vocab #32006\n",
      "2021-01-14 20:03:55,559 : INFO : diff #set()\n",
      "2021-01-14 20:04:14,546 : INFO : alphabet #32006\n",
      "2021-01-14 20:04:23,973 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.0850406029655761, 0.47960696716298423], [0.715752124786377, 0.28424788], [4.317033095124792, 1.3952305964386365], [5.234069531114783, 7.170319527000998, 7.379589043470524, 5.0248000146452565, 2.145519512355741, 0.20926951646952663]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 20:04:23,977 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:04:23,978 : INFO : built Dictionary(188 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 611 corpus positions)\n",
      "2021-01-14 20:04:24,139 : INFO : token count processed\n",
      "2021-01-14 20:04:24,171 : INFO : frequencies processed\n",
      "2021-01-14 20:04:33,575 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:04:33,576 : INFO : entropies processed\n",
      "2021-01-14 20:04:33,577 : INFO : extropies processed\n",
      "2021-01-14 20:04:33,584 : INFO : token count processed\n",
      "2021-01-14 20:04:33,588 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:04:33,592 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:04:33,594 : INFO : vocab #32006\n",
      "2021-01-14 20:04:33,600 : INFO : diff #set()\n",
      "2021-01-14 20:04:52,735 : INFO : alphabet #32006\n",
      "2021-01-14 20:05:02,156 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.1217013559298026, 0.47131986658026404], [0.7581189721822739, 0.24188103], [3.0271691184406184, 1.334157713553925], [5.234069531114783, 6.353654804387375, 6.719236142037453, 4.868488193464706, 1.4851666109226702, 0.3655813376500783]]\n",
      "2021-01-14 20:05:02,160 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:05:02,163 : INFO : built Dictionary(182 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 739 corpus positions)\n",
      "2021-01-14 20:05:02,320 : INFO : token count processed\n",
      "2021-01-14 20:05:02,352 : INFO : frequencies processed\n",
      "2021-01-14 20:05:11,881 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:05:11,882 : INFO : entropies processed\n",
      "2021-01-14 20:05:11,883 : INFO : extropies processed\n",
      "2021-01-14 20:05:11,890 : INFO : token count processed\n",
      "2021-01-14 20:05:11,894 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:05:11,899 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:05:11,900 : INFO : vocab #32006\n",
      "2021-01-14 20:05:11,907 : INFO : diff #set()\n",
      "2021-01-14 20:05:30,817 : INFO : alphabet #32006\n",
      "2021-01-14 20:05:40,255 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.1087619887598177, 0.4742118860877748], [0.7517528831958771, 0.24824712], [3.188721875540867, 1.3469079016509693], [5.234069531114783, 6.245180322479091, 6.5628460008696194, 4.916403852724255, 1.3287764697548363, 0.3176656783905285]]\n",
      "2021-01-14 20:05:40,260 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:05:40,262 : INFO : built Dictionary(401 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1811 corpus positions)\n",
      "2021-01-14 20:05:40,753 : INFO : token count processed\n",
      "2021-01-14 20:05:40,784 : INFO : frequencies processed\n",
      "2021-01-14 20:05:50,335 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:05:50,336 : INFO : entropies processed\n",
      "2021-01-14 20:05:50,337 : INFO : extropies processed\n",
      "2021-01-14 20:05:50,344 : INFO : token count processed\n",
      "2021-01-14 20:05:50,348 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:05:50,352 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:05:50,353 : INFO : vocab #32006\n",
      "2021-01-14 20:05:50,360 : INFO : diff #set()\n",
      "2021-01-14 20:06:09,255 : INFO : alphabet #32006\n",
      "2021-01-14 20:06:18,689 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.1064353306187273, 0.47473567570017355], [0.693248450756073, 0.30675155], [4.043801694853791, 1.3854355168137273], [5.234069531114783, 7.2691387000368, 7.5125948706791945, 4.990613360472388, 2.2785253395644114, 0.24345617064239455]]\n",
      "2021-01-14 20:06:18,694 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:06:18,697 : INFO : built Dictionary(328 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1648 corpus positions)\n",
      "2021-01-14 20:06:19,060 : INFO : token count processed\n",
      "2021-01-14 20:06:19,092 : INFO : frequencies processed\n",
      "2021-01-14 20:06:28,601 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:06:28,602 : INFO : entropies processed\n",
      "2021-01-14 20:06:28,603 : INFO : extropies processed\n",
      "2021-01-14 20:06:28,617 : INFO : token count processed\n",
      "2021-01-14 20:06:28,621 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:06:28,624 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:06:28,625 : INFO : vocab #32006\n",
      "2021-01-14 20:06:28,630 : INFO : diff #set()\n",
      "2021-01-14 20:06:47,397 : INFO : alphabet #32006\n",
      "2021-01-14 20:06:56,902 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.0603650050395859, 0.4853508953772912], [0.6715110838413239, 0.32848892], [3.9262723741395074, 1.376905493149557], [5.234069531114783, 7.08857858466988, 7.301900266914193, 5.02074784887047, 2.0678307357994097, 0.21332168224431314]]\n",
      "2021-01-14 20:06:56,905 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:06:56,907 : INFO : built Dictionary(164 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 571 corpus positions)\n",
      "2021-01-14 20:06:57,043 : INFO : token count processed\n",
      "2021-01-14 20:06:57,076 : INFO : frequencies processed\n",
      "2021-01-14 20:07:06,821 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:07:06,822 : INFO : entropies processed\n",
      "2021-01-14 20:07:06,825 : INFO : extropies processed\n",
      "2021-01-14 20:07:06,837 : INFO : token count processed\n",
      "2021-01-14 20:07:06,841 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:07:06,847 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:07:06,848 : INFO : vocab #32006\n",
      "2021-01-14 20:07:06,856 : INFO : diff #set()\n",
      "2021-01-14 20:07:25,870 : INFO : alphabet #32006\n",
      "2021-01-14 20:07:35,301 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.1430019613407636, 0.4666351305503952], [0.7668334692716599, 0.23316653], [2.6416041678685938, 1.2962416748397703], [5.234069531114783, 6.0479231618016716, 6.451245169600638, 4.830747523315816, 1.2171756384858545, 0.4033220077989661]]\n",
      "2021-01-14 20:07:35,305 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:07:35,306 : INFO : built Dictionary(162 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 690 corpus positions)\n",
      "2021-01-14 20:07:35,441 : INFO : token count processed\n",
      "2021-01-14 20:07:35,478 : INFO : frequencies processed\n",
      "2021-01-14 20:07:44,885 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:07:44,886 : INFO : entropies processed\n",
      "2021-01-14 20:07:44,887 : INFO : extropies processed\n",
      "2021-01-14 20:07:44,894 : INFO : token count processed\n",
      "2021-01-14 20:07:44,900 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:07:44,904 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:07:44,905 : INFO : vocab #32006\n",
      "2021-01-14 20:07:44,912 : INFO : diff #set()\n",
      "2021-01-14 20:08:03,918 : INFO : alphabet #32006\n",
      "2021-01-14 20:08:13,343 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.119940411839627, 0.47171137189286705], [0.7575951367616653, 0.24240486], [3.334679141051595, 1.3571063325330468], [5.234069531114783, 6.036583168403119, 6.373819348707622, 4.896833350810281, 1.1397498175928389, 0.33723618030450275]]\n",
      "2021-01-14 20:08:13,356 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 20:08:13,357 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:08:13,361 : INFO : built Dictionary(563 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 7048 corpus positions)\n",
      "2021-01-14 20:08:14,202 : INFO : token count processed\n",
      "2021-01-14 20:08:14,235 : INFO : frequencies processed\n",
      "2021-01-14 20:08:23,647 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:08:23,648 : INFO : entropies processed\n",
      "2021-01-14 20:08:23,649 : INFO : extropies processed\n",
      "2021-01-14 20:08:23,661 : INFO : token count processed\n",
      "2021-01-14 20:08:23,666 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:08:23,671 : INFO : alphabet_target #32010\n",
      "2021-01-14 20:08:23,672 : INFO : vocab #32006\n",
      "2021-01-14 20:08:23,680 : INFO : diff #set()\n",
      "2021-01-14 20:08:42,680 : INFO : alphabet #32006\n",
      "2021-01-14 20:08:52,091 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.0829396047040352, 0.4800907322236499], [0.7363923192024231, 0.26360768], [4.514718016821338, 1.4072621816600537], [5.234069531114783, 7.29352035514053, 7.467333220246465, 5.060256666008849, 2.2332636891316815, 0.1738128651059343]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 20:08:52,098 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:08:52,101 : INFO : built Dictionary(374 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 3291 corpus positions)\n",
      "2021-01-14 20:08:52,517 : INFO : token count processed\n",
      "2021-01-14 20:08:52,549 : INFO : frequencies processed\n",
      "2021-01-14 20:09:01,967 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:09:01,968 : INFO : entropies processed\n",
      "2021-01-14 20:09:01,969 : INFO : extropies processed\n",
      "2021-01-14 20:09:01,976 : INFO : token count processed\n",
      "2021-01-14 20:09:01,983 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:09:01,987 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:09:01,988 : INFO : vocab #32006\n",
      "2021-01-14 20:09:01,996 : INFO : diff #set()\n",
      "2021-01-14 20:09:21,357 : INFO : alphabet #32006\n",
      "2021-01-14 20:09:30,775 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.1038381190091417, 0.4753217421837458], [0.7257002890110016, 0.2742997], [4.233946664525225, 1.3907643910215695], [5.234069531114783, 6.8153433747477745, 6.991408619303433, 5.0580042865591235, 1.7573390881886501, 0.17606524455565875]]\n",
      "2021-01-14 20:09:30,779 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:09:30,780 : INFO : built Dictionary(140 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 312 corpus positions)\n",
      "2021-01-14 20:09:30,881 : INFO : token count processed\n",
      "2021-01-14 20:09:30,913 : INFO : frequencies processed\n",
      "2021-01-14 20:09:40,442 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:09:40,443 : INFO : entropies processed\n",
      "2021-01-14 20:09:40,443 : INFO : extropies processed\n",
      "2021-01-14 20:09:40,450 : INFO : token count processed\n",
      "2021-01-14 20:09:40,458 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:09:40,462 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:09:40,463 : INFO : vocab #32006\n",
      "2021-01-14 20:09:40,470 : INFO : diff #set()\n",
      "2021-01-14 20:09:59,313 : INFO : alphabet #32006\n",
      "2021-01-14 20:10:08,723 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.114061409886071, 0.4730231559611558], [0.7867683619260788, 0.21323164], [2.8464393446710154, 1.3178207096846455], [5.234069531114783, 6.150121915859574, 6.589537936916679, 4.794653510057678, 1.3554684058018962, 0.439416021057105]]\n",
      "2021-01-14 20:10:08,727 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:10:08,729 : INFO : built Dictionary(289 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1023 corpus positions)\n",
      "2021-01-14 20:10:09,017 : INFO : token count processed\n",
      "2021-01-14 20:10:09,066 : INFO : frequencies processed\n",
      "2021-01-14 20:10:18,585 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:10:18,586 : INFO : entropies processed\n",
      "2021-01-14 20:10:18,587 : INFO : extropies processed\n",
      "2021-01-14 20:10:18,594 : INFO : token count processed\n",
      "2021-01-14 20:10:18,602 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:10:18,606 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:10:18,607 : INFO : vocab #32006\n",
      "2021-01-14 20:10:18,614 : INFO : diff #set()\n",
      "2021-01-14 20:10:37,463 : INFO : alphabet #32006\n",
      "2021-01-14 20:10:46,866 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.047482440318743, 0.4884046770356304], [0.6727506518363953, 0.32724935], [4.370859077232684, 1.3989500140632753], [5.234069531114783, 7.0391145208191315, 7.318463581160585, 4.954720470773329, 2.084394050045802, 0.27934906034145346]]\n",
      "2021-01-14 20:10:46,875 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 20:10:46,876 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:10:46,879 : INFO : built Dictionary(585 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 4398 corpus positions)\n",
      "2021-01-14 20:10:47,810 : INFO : token count processed\n",
      "2021-01-14 20:10:47,842 : INFO : frequencies processed\n",
      "2021-01-14 20:10:57,360 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:10:57,361 : INFO : entropies processed\n",
      "2021-01-14 20:10:57,362 : INFO : extropies processed\n",
      "2021-01-14 20:10:57,371 : INFO : token count processed\n",
      "2021-01-14 20:10:57,376 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:10:57,380 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:10:57,381 : INFO : vocab #32006\n",
      "2021-01-14 20:10:57,389 : INFO : diff #set()\n",
      "2021-01-14 20:11:16,211 : INFO : alphabet #32006\n",
      "2021-01-14 20:11:25,756 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.0281824253662188, 0.493052295243824], [0.6212832033634186, 0.3787168], [4.828904753566516, 1.4112221210813087], [5.234069531114783, 7.482466367279176, 7.752446583431515, 4.964089314962444, 2.5183770523167315, 0.269980216152339]]\n",
      "2021-01-14 20:11:25,760 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:11:25,761 : INFO : built Dictionary(188 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 620 corpus positions)\n",
      "2021-01-14 20:11:25,921 : INFO : token count processed\n",
      "2021-01-14 20:11:25,956 : INFO : frequencies processed\n",
      "2021-01-14 20:11:35,364 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:11:35,365 : INFO : entropies processed\n",
      "2021-01-14 20:11:35,366 : INFO : extropies processed\n",
      "2021-01-14 20:11:35,373 : INFO : token count processed\n",
      "2021-01-14 20:11:35,381 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:11:35,385 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:11:35,386 : INFO : vocab #32006\n",
      "2021-01-14 20:11:35,393 : INFO : diff #set()\n",
      "2021-01-14 20:11:54,271 : INFO : alphabet #32006\n",
      "2021-01-14 20:12:03,793 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.1223756787553356, 0.47117011847141443], [0.7598782777786255, 0.24012172], [3.0271691184406184, 1.334157713553925], [5.234069531114783, 6.372162341197667, 6.732129520918662, 4.874102351393788, 1.4980599898038784, 0.3599671797209947]]\n",
      "2021-01-14 20:12:03,798 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:12:03,800 : INFO : built Dictionary(329 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 2041 corpus positions)\n",
      "2021-01-14 20:12:04,142 : INFO : token count processed\n",
      "2021-01-14 20:12:04,174 : INFO : frequencies processed\n",
      "2021-01-14 20:12:13,576 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:12:13,577 : INFO : entropies processed\n",
      "2021-01-14 20:12:13,578 : INFO : extropies processed\n",
      "2021-01-14 20:12:13,585 : INFO : token count processed\n",
      "2021-01-14 20:12:13,589 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:12:13,594 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:12:13,595 : INFO : vocab #32006\n",
      "2021-01-14 20:12:13,602 : INFO : diff #set()\n",
      "2021-01-14 20:12:32,554 : INFO : alphabet #32006\n",
      "2021-01-14 20:12:41,970 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.0969518164350087, 0.47688267902124837], [0.7388002872467041, 0.2611997], [3.7821222241453056, 1.3706381016974656], [5.234069531114783, 6.798155919669889, 7.038937626413933, 4.993287824370739, 1.8048680952991498, 0.24078170674404387]]\n",
      "2021-01-14 20:12:41,974 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:12:41,975 : INFO : built Dictionary(190 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 757 corpus positions)\n",
      "2021-01-14 20:12:42,133 : INFO : token count processed\n",
      "2021-01-14 20:12:42,165 : INFO : frequencies processed\n",
      "2021-01-14 20:12:51,746 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:12:51,747 : INFO : entropies processed\n",
      "2021-01-14 20:12:51,748 : INFO : extropies processed\n",
      "2021-01-14 20:12:51,755 : INFO : token count processed\n",
      "2021-01-14 20:12:51,759 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:12:51,763 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:12:51,764 : INFO : vocab #32006\n",
      "2021-01-14 20:12:51,772 : INFO : diff #set()\n",
      "2021-01-14 20:13:10,742 : INFO : alphabet #32006\n",
      "2021-01-14 20:13:20,142 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.1119039875197783, 0.47350637429990405], [0.7414061725139618, 0.25859383], [3.454822399946606, 1.3672062119190402], [5.234069531114783, 6.271631856729336, 6.598476290557853, 4.9072250972862665, 1.36440675944307, 0.32684443382851747]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 20:13:20,149 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:13:20,151 : INFO : built Dictionary(354 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 3272 corpus positions)\n",
      "2021-01-14 20:13:20,585 : INFO : token count processed\n",
      "2021-01-14 20:13:20,653 : INFO : frequencies processed\n",
      "2021-01-14 20:13:30,059 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:13:30,060 : INFO : entropies processed\n",
      "2021-01-14 20:13:30,061 : INFO : extropies processed\n",
      "2021-01-14 20:13:30,069 : INFO : token count processed\n",
      "2021-01-14 20:13:30,075 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:13:30,080 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:13:30,081 : INFO : vocab #32006\n",
      "2021-01-14 20:13:30,088 : INFO : diff #set()\n",
      "2021-01-14 20:13:49,098 : INFO : alphabet #32006\n",
      "2021-01-14 20:13:58,538 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.070756928812026, 0.4829152017246615], [0.6878425180912018, 0.31215748], [4.431623565847431, 1.3982614701069989], [5.234069531114783, 6.873598627629562, 7.051730661759789, 5.055937496984557, 1.8176611306450061, 0.17813203413022727]]\n",
      "2021-01-14 20:13:58,541 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:13:58,543 : INFO : built Dictionary(121 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 220 corpus positions)\n",
      "2021-01-14 20:13:58,624 : INFO : token count processed\n",
      "2021-01-14 20:13:58,656 : INFO : frequencies processed\n",
      "2021-01-14 20:14:08,083 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:14:08,084 : INFO : entropies processed\n",
      "2021-01-14 20:14:08,085 : INFO : extropies processed\n",
      "2021-01-14 20:14:08,092 : INFO : token count processed\n",
      "2021-01-14 20:14:08,099 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:14:08,103 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:14:08,104 : INFO : vocab #32006\n",
      "2021-01-14 20:14:08,111 : INFO : diff #set()\n",
      "2021-01-14 20:14:27,088 : INFO : alphabet #32006\n",
      "2021-01-14 20:14:36,492 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.1041760561858038, 0.4752454040431765], [0.7558091282844543, 0.24419087], [2.8464393446710154, 1.3178207096846455], [5.234069531114783, 6.049830202851529, 6.580047054225751, 4.70385267974056, 1.3459775231109683, 0.530216851374222]]\n",
      "2021-01-14 20:14:36,497 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:14:36,500 : INFO : built Dictionary(259 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1300 corpus positions)\n",
      "2021-01-14 20:14:36,763 : INFO : token count processed\n",
      "2021-01-14 20:14:36,795 : INFO : frequencies processed\n",
      "2021-01-14 20:14:46,183 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:14:46,184 : INFO : entropies processed\n",
      "2021-01-14 20:14:46,185 : INFO : extropies processed\n",
      "2021-01-14 20:14:46,192 : INFO : token count processed\n",
      "2021-01-14 20:14:46,199 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:14:46,203 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:14:46,204 : INFO : vocab #32006\n",
      "2021-01-14 20:14:46,211 : INFO : diff #set()\n",
      "2021-01-14 20:15:05,234 : INFO : alphabet #32006\n",
      "2021-01-14 20:15:14,667 : INFO : Computed distances or similarities ('275', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.1045175265119707, 0.475168292685783], [0.7493913471698761, 0.25060865], [3.872905595320056, 1.3771992956325185], [5.234069531114783, 6.778844940588858, 7.034221627841264, 4.978692843862378, 1.8001520967264808, 0.25537668725240614]]\n",
      "2021-01-14 20:15:14,670 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:15:14,671 : INFO : built Dictionary(168 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 333 corpus positions)\n",
      "2021-01-14 20:15:14,812 : INFO : token count processed\n",
      "2021-01-14 20:15:14,842 : INFO : frequencies processed\n",
      "2021-01-14 20:15:24,266 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:15:24,267 : INFO : entropies processed\n",
      "2021-01-14 20:15:24,268 : INFO : extropies processed\n",
      "2021-01-14 20:15:24,275 : INFO : token count processed\n",
      "2021-01-14 20:15:24,282 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:15:24,286 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:15:24,287 : INFO : vocab #32006\n",
      "2021-01-14 20:15:24,294 : INFO : diff #set()\n",
      "2021-01-14 20:15:43,161 : INFO : alphabet #32006\n",
      "2021-01-14 20:15:52,586 : INFO : Computed distances or similarities ('275', 'sacp-python-common/setup.py')[[1.120565498672367, 0.471572323809887], [0.7539503127336502, 0.24604969], [2.8464393446710154, 1.3178207096846455], [5.234069531114783, 6.469677430851302, 6.928536972926825, 4.775209989039261, 1.6944674418120416, 0.4588595420755226]]\n",
      "2021-01-14 20:15:52,590 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:15:52,592 : INFO : built Dictionary(230 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1235 corpus positions)\n",
      "2021-01-14 20:15:52,798 : INFO : token count processed\n",
      "2021-01-14 20:15:52,831 : INFO : frequencies processed\n",
      "2021-01-14 20:16:02,351 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:16:02,352 : INFO : entropies processed\n",
      "2021-01-14 20:16:02,353 : INFO : extropies processed\n",
      "2021-01-14 20:16:02,360 : INFO : token count processed\n",
      "2021-01-14 20:16:02,367 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:16:02,371 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:16:02,372 : INFO : vocab #32006\n",
      "2021-01-14 20:16:02,379 : INFO : diff #set()\n",
      "2021-01-14 20:16:21,237 : INFO : alphabet #32006\n",
      "2021-01-14 20:16:30,785 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.1105759447573402, 0.4738043198511737], [0.7522770017385483, 0.247723], [3.795088586397732, 1.378800483023463], [5.234069531114783, 6.459180448028249, 6.722092740786751, 4.971157238356282, 1.4880232096719679, 0.2629122927585019]]\n",
      "2021-01-14 20:16:30,789 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:16:30,790 : INFO : built Dictionary(145 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 385 corpus positions)\n",
      "2021-01-14 20:16:30,905 : INFO : token count processed\n",
      "2021-01-14 20:16:30,975 : INFO : frequencies processed\n",
      "2021-01-14 20:16:40,394 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:16:40,395 : INFO : entropies processed\n",
      "2021-01-14 20:16:40,396 : INFO : extropies processed\n",
      "2021-01-14 20:16:40,403 : INFO : token count processed\n",
      "2021-01-14 20:16:40,409 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:16:40,413 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:16:40,414 : INFO : vocab #32006\n",
      "2021-01-14 20:16:40,421 : INFO : diff #set()\n",
      "2021-01-14 20:16:59,296 : INFO : alphabet #32006\n",
      "2021-01-14 20:17:08,834 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.1142309314410914, 0.47298522840094154], [0.7362820506095886, 0.26371795], [3.334679141051595, 1.3571063325330468], [5.234069531114783, 6.097125733496388, 6.518030793221934, 4.813164471389237, 1.2839612621071508, 0.4209050597255457]]\n",
      "2021-01-14 20:17:08,838 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:17:08,839 : INFO : built Dictionary(137 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 381 corpus positions)\n",
      "2021-01-14 20:17:08,951 : INFO : token count processed\n",
      "2021-01-14 20:17:09,017 : INFO : frequencies processed\n",
      "2021-01-14 20:17:18,441 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:17:18,442 : INFO : entropies processed\n",
      "2021-01-14 20:17:18,444 : INFO : extropies processed\n",
      "2021-01-14 20:17:18,458 : INFO : token count processed\n",
      "2021-01-14 20:17:18,462 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:17:18,467 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:17:18,468 : INFO : vocab #32006\n",
      "2021-01-14 20:17:18,477 : INFO : diff #set()\n",
      "2021-01-14 20:17:37,362 : INFO : alphabet #32006\n",
      "2021-01-14 20:17:46,917 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.1130628678306973, 0.4732466862316384], [0.7429031431674957, 0.25709686], [3.334679141051595, 1.3571063325330468], [5.234069531114783, 6.0695858597523715, 6.491904790494493, 4.811750600372662, 1.2578352593797097, 0.42231893074212135]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 20:17:46,920 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 20:17:46,921 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:17:46,922 : INFO : built Dictionary(139 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 477 corpus positions)\n",
      "2021-01-14 20:17:47,026 : INFO : token count processed\n",
      "2021-01-14 20:17:47,058 : INFO : frequencies processed\n",
      "2021-01-14 20:17:56,495 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:17:56,496 : INFO : entropies processed\n",
      "2021-01-14 20:17:56,497 : INFO : extropies processed\n",
      "2021-01-14 20:17:56,511 : INFO : token count processed\n",
      "2021-01-14 20:17:56,515 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:17:56,520 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:17:56,521 : INFO : vocab #32006\n",
      "2021-01-14 20:17:56,527 : INFO : diff #set()\n",
      "2021-01-14 20:18:15,500 : INFO : alphabet #32006\n",
      "2021-01-14 20:18:24,919 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.0977928643386785, 0.47669148703832886], [0.7337937653064728, 0.26620623], [3.3248629576173565, 1.3574960179923317], [5.234069531114783, 6.104787343210121, 6.462259892679037, 4.876596981645867, 1.2281903615642538, 0.35747254946891616]]\n",
      "2021-01-14 20:18:24,936 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 20:18:24,937 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:18:24,940 : INFO : built Dictionary(418 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 9162 corpus positions)\n",
      "2021-01-14 20:18:25,447 : INFO : token count processed\n",
      "2021-01-14 20:18:25,479 : INFO : frequencies processed\n",
      "2021-01-14 20:18:34,900 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:18:34,901 : INFO : entropies processed\n",
      "2021-01-14 20:18:34,902 : INFO : extropies processed\n",
      "2021-01-14 20:18:34,912 : INFO : token count processed\n",
      "2021-01-14 20:18:34,919 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:18:34,924 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:18:34,925 : INFO : vocab #32006\n",
      "2021-01-14 20:18:34,933 : INFO : diff #set()\n",
      "2021-01-14 20:18:54,166 : INFO : alphabet #32006\n",
      "2021-01-14 20:19:03,587 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.1167088927187627, 0.47243152019622814], [0.7215225398540497, 0.27847746], [4.042433003725894, 1.3838528601778162], [5.234069531114783, 6.89087415148015, 7.06571408615192, 5.059229596443013, 1.831644555037137, 0.17483993467176973]]\n",
      "2021-01-14 20:19:03,593 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:19:03,595 : INFO : built Dictionary(276 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 2322 corpus positions)\n",
      "2021-01-14 20:19:03,861 : INFO : token count processed\n",
      "2021-01-14 20:19:03,921 : INFO : frequencies processed\n",
      "2021-01-14 20:19:13,347 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:19:13,348 : INFO : entropies processed\n",
      "2021-01-14 20:19:13,349 : INFO : extropies processed\n",
      "2021-01-14 20:19:13,357 : INFO : token count processed\n",
      "2021-01-14 20:19:13,361 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:19:13,365 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:19:13,366 : INFO : vocab #32006\n",
      "2021-01-14 20:19:13,373 : INFO : diff #set()\n",
      "2021-01-14 20:19:32,223 : INFO : alphabet #32006\n",
      "2021-01-14 20:19:41,646 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.0941180783672917, 0.47752799153506376], [0.6951156258583069, 0.30488437], [4.093149802473811, 1.3895526821130288], [5.234069531114783, 6.655493573668506, 6.839828816502722, 5.0497342882805665, 1.605759285387939, 0.18433524283421576]]\n",
      "2021-01-14 20:19:41,651 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:19:41,652 : INFO : built Dictionary(255 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1329 corpus positions)\n",
      "2021-01-14 20:19:41,898 : INFO : token count processed\n",
      "2021-01-14 20:19:41,964 : INFO : frequencies processed\n",
      "2021-01-14 20:19:51,381 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:19:51,383 : INFO : entropies processed\n",
      "2021-01-14 20:19:51,383 : INFO : extropies processed\n",
      "2021-01-14 20:19:51,391 : INFO : token count processed\n",
      "2021-01-14 20:19:51,396 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:19:51,400 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:19:51,402 : INFO : vocab #32006\n",
      "2021-01-14 20:19:51,408 : INFO : diff #set()\n",
      "2021-01-14 20:20:10,557 : INFO : alphabet #32006\n",
      "2021-01-14 20:20:19,962 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.0815366840420502, 0.4804143052901384], [0.7033252120018005, 0.2966748], [4.053508854797679, 1.3937927964478642], [5.234069531114783, 6.6236746347295465, 6.824739356594764, 5.033004809249567, 1.5906698254799805, 0.20106472186521707]]\n",
      "2021-01-14 20:20:19,967 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:20:19,969 : INFO : built Dictionary(256 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1268 corpus positions)\n",
      "2021-01-14 20:20:20,224 : INFO : token count processed\n",
      "2021-01-14 20:20:20,289 : INFO : frequencies processed\n",
      "2021-01-14 20:20:29,832 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:20:29,833 : INFO : entropies processed\n",
      "2021-01-14 20:20:29,834 : INFO : extropies processed\n",
      "2021-01-14 20:20:29,841 : INFO : token count processed\n",
      "2021-01-14 20:20:29,848 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:20:29,854 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:20:29,855 : INFO : vocab #32006\n",
      "2021-01-14 20:20:29,863 : INFO : diff #set()\n",
      "2021-01-14 20:20:48,796 : INFO : alphabet #32006\n",
      "2021-01-14 20:20:58,339 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.088333691687587, 0.47885067601045017], [0.6721376180648804, 0.32786238], [4.026986833359287, 1.3922018630035278], [5.234069531114783, 6.75472436518627, 6.9513067825625106, 5.0374871137385435, 1.7172372514477274, 0.1965824173762405]]\n",
      "2021-01-14 20:20:58,344 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:20:58,345 : INFO : built Dictionary(214 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1063 corpus positions)\n",
      "2021-01-14 20:20:58,539 : INFO : token count processed\n",
      "2021-01-14 20:20:58,571 : INFO : frequencies processed\n",
      "2021-01-14 20:21:08,001 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:21:08,002 : INFO : entropies processed\n",
      "2021-01-14 20:21:08,003 : INFO : extropies processed\n",
      "2021-01-14 20:21:08,010 : INFO : token count processed\n",
      "2021-01-14 20:21:08,014 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:21:08,018 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:21:08,019 : INFO : vocab #32006\n",
      "2021-01-14 20:21:08,027 : INFO : diff #set()\n",
      "2021-01-14 20:21:27,089 : INFO : alphabet #32006\n",
      "2021-01-14 20:21:36,647 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.0855648629457733, 0.47948640570571455], [0.6610729396343231, 0.33892706], [3.456564762130954, 1.3654661895235272], [5.234069531114783, 6.597313085495733, 6.820473537009047, 5.010909079601468, 1.5864040058942637, 0.22316045151331387]]\n",
      "2021-01-14 20:21:36,651 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:21:36,653 : INFO : built Dictionary(234 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1029 corpus positions)\n",
      "2021-01-14 20:21:36,855 : INFO : token count processed\n",
      "2021-01-14 20:21:36,900 : INFO : frequencies processed\n",
      "2021-01-14 20:21:46,504 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:21:46,505 : INFO : entropies processed\n",
      "2021-01-14 20:21:46,506 : INFO : extropies processed\n",
      "2021-01-14 20:21:46,513 : INFO : token count processed\n",
      "2021-01-14 20:21:46,517 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:21:46,521 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:21:46,522 : INFO : vocab #32006\n",
      "2021-01-14 20:21:46,529 : INFO : diff #set()\n",
      "2021-01-14 20:22:05,430 : INFO : alphabet #32006\n",
      "2021-01-14 20:22:15,012 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.0857912247019559, 0.479434369153074], [0.707693874835968, 0.29230613], [3.8868421881310113, 1.3828481841556175], [5.234069531114783, 6.659481538516613, 6.880244890814325, 5.013306178817071, 1.6461753596995417, 0.2207633522977117]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 20:22:15,016 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:22:15,019 : INFO : built Dictionary(257 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1191 corpus positions)\n",
      "2021-01-14 20:22:15,278 : INFO : token count processed\n",
      "2021-01-14 20:22:15,313 : INFO : frequencies processed\n",
      "2021-01-14 20:22:24,729 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:22:24,730 : INFO : entropies processed\n",
      "2021-01-14 20:22:24,731 : INFO : extropies processed\n",
      "2021-01-14 20:22:24,738 : INFO : token count processed\n",
      "2021-01-14 20:22:24,744 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:22:24,748 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:22:24,749 : INFO : vocab #32006\n",
      "2021-01-14 20:22:24,756 : INFO : diff #set()\n",
      "2021-01-14 20:22:43,773 : INFO : alphabet #32006\n",
      "2021-01-14 20:22:53,210 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.1023624826678355, 0.4756553678274498], [0.6932785511016846, 0.30672145], [4.1336606896881865, 1.396507583562077], [5.234069531114783, 6.774682571479102, 6.958606239987313, 5.050145862606572, 1.72453670887253, 0.1839236685082115]]\n",
      "2021-01-14 20:22:53,225 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:22:53,228 : INFO : built Dictionary(434 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 7928 corpus positions)\n",
      "2021-01-14 20:22:53,773 : INFO : token count processed\n",
      "2021-01-14 20:22:53,816 : INFO : frequencies processed\n",
      "2021-01-14 20:23:03,239 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:23:03,240 : INFO : entropies processed\n",
      "2021-01-14 20:23:03,241 : INFO : extropies processed\n",
      "2021-01-14 20:23:03,250 : INFO : token count processed\n",
      "2021-01-14 20:23:03,254 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:23:03,258 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:23:03,259 : INFO : vocab #32006\n",
      "2021-01-14 20:23:03,267 : INFO : diff #set()\n",
      "2021-01-14 20:23:22,232 : INFO : alphabet #32006\n",
      "2021-01-14 20:23:31,649 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.1153093044609481, 0.47274410313948567], [0.7282832562923431, 0.27171674], [4.054229296672175, 1.38538800132636], [5.234069531114783, 6.839453716525233, 7.007613682105228, 5.0659095655347866, 1.7735441509904453, 0.16815996557999568]]\n",
      "2021-01-14 20:23:31,656 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:23:31,658 : INFO : built Dictionary(334 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 2370 corpus positions)\n",
      "2021-01-14 20:23:32,018 : INFO : token count processed\n",
      "2021-01-14 20:23:32,081 : INFO : frequencies processed\n",
      "2021-01-14 20:23:41,308 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:23:41,310 : INFO : entropies processed\n",
      "2021-01-14 20:23:41,311 : INFO : extropies processed\n",
      "2021-01-14 20:23:41,318 : INFO : token count processed\n",
      "2021-01-14 20:23:41,325 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:23:41,330 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:23:41,331 : INFO : vocab #32006\n",
      "2021-01-14 20:23:41,338 : INFO : diff #set()\n",
      "2021-01-14 20:24:00,557 : INFO : alphabet #32006\n",
      "2021-01-14 20:24:10,141 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.0851114711510947, 0.4795906664155206], [0.6678848564624786, 0.33211514], [4.043801694853791, 1.3854355168137273], [5.234069531114783, 6.86432793886027, 7.021263412580729, 5.077134057394324, 1.7871938814659458, 0.15693547372045913]]\n",
      "2021-01-14 20:24:10,145 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:24:10,146 : INFO : built Dictionary(174 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 572 corpus positions)\n",
      "2021-01-14 20:24:10,301 : INFO : token count processed\n",
      "2021-01-14 20:24:10,372 : INFO : frequencies processed\n",
      "2021-01-14 20:24:19,785 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:24:19,786 : INFO : entropies processed\n",
      "2021-01-14 20:24:19,788 : INFO : extropies processed\n",
      "2021-01-14 20:24:19,799 : INFO : token count processed\n",
      "2021-01-14 20:24:19,804 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:24:19,808 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:24:19,809 : INFO : vocab #32006\n",
      "2021-01-14 20:24:19,815 : INFO : diff #set()\n",
      "2021-01-14 20:24:38,811 : INFO : alphabet #32006\n",
      "2021-01-14 20:24:48,250 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.0702619709996954, 0.48303065699319037], [0.6568885743618011, 0.34311143], [3.5724312513221195, 1.3732570370060984], [5.234069531114783, 6.431978396403875, 6.716486138516584, 4.949561789002074, 1.4824166074018006, 0.2845077421127087]]\n",
      "2021-01-14 20:24:48,254 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:24:48,256 : INFO : built Dictionary(231 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 726 corpus positions)\n",
      "2021-01-14 20:24:48,466 : INFO : token count processed\n",
      "2021-01-14 20:24:48,497 : INFO : frequencies processed\n",
      "2021-01-14 20:24:57,915 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:24:57,916 : INFO : entropies processed\n",
      "2021-01-14 20:24:57,917 : INFO : extropies processed\n",
      "2021-01-14 20:24:57,928 : INFO : token count processed\n",
      "2021-01-14 20:24:57,933 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:24:57,938 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:24:57,939 : INFO : vocab #32006\n",
      "2021-01-14 20:24:57,945 : INFO : diff #set()\n",
      "2021-01-14 20:25:16,925 : INFO : alphabet #32006\n",
      "2021-01-14 20:25:26,341 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/test_auth_utility.py')[[1.0830442981151482, 0.4800666029545576], [0.6714036464691162, 0.32859635], [3.7841837197791888, 1.3830098939805733], [5.234069531114783, 6.911818353685893, 7.133325122990129, 5.012562761810546, 1.8992555918753462, 0.22150676930423607]]\n",
      "2021-01-14 20:25:26,355 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:25:26,361 : INFO : built Dictionary(320 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 7256 corpus positions)\n",
      "2021-01-14 20:25:26,718 : INFO : token count processed\n",
      "2021-01-14 20:25:26,788 : INFO : frequencies processed\n",
      "2021-01-14 20:25:36,316 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:25:36,317 : INFO : entropies processed\n",
      "2021-01-14 20:25:36,318 : INFO : extropies processed\n",
      "2021-01-14 20:25:36,327 : INFO : token count processed\n",
      "2021-01-14 20:25:36,333 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:25:36,338 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:25:36,339 : INFO : vocab #32006\n",
      "2021-01-14 20:25:36,346 : INFO : diff #set()\n",
      "2021-01-14 20:25:55,214 : INFO : alphabet #32006\n",
      "2021-01-14 20:26:04,632 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.098123914122597, 0.476616272885], [0.6938193142414093, 0.3061807], [3.9690016298759923, 1.3907663955465712], [5.234069531114783, 6.363791471162389, 6.424891801222732, 5.17296920105444, 1.1908222701079492, 0.06110033006034321]]\n",
      "2021-01-14 20:26:04,636 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:26:04,638 : INFO : built Dictionary(221 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1326 corpus positions)\n",
      "2021-01-14 20:26:04,841 : INFO : token count processed\n",
      "2021-01-14 20:26:04,873 : INFO : frequencies processed\n",
      "2021-01-14 20:26:14,558 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:26:14,559 : INFO : entropies processed\n",
      "2021-01-14 20:26:14,560 : INFO : extropies processed\n",
      "2021-01-14 20:26:14,567 : INFO : token count processed\n",
      "2021-01-14 20:26:14,575 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:26:14,579 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:26:14,580 : INFO : vocab #32006\n",
      "2021-01-14 20:26:14,587 : INFO : diff #set()\n",
      "2021-01-14 20:26:33,473 : INFO : alphabet #32006\n",
      "2021-01-14 20:26:42,983 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.1141422054289674, 0.4730050785761104], [0.7135534882545471, 0.2864465], [3.8820451081368623, 1.3816526416158188], [5.234069531114783, 6.29000629755059, 6.597116171722574, 4.926959656942801, 1.3630466406077906, 0.3071098741719833]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 20:26:42,988 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:26:42,989 : INFO : built Dictionary(229 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1279 corpus positions)\n",
      "2021-01-14 20:26:43,199 : INFO : token count processed\n",
      "2021-01-14 20:26:43,231 : INFO : frequencies processed\n",
      "2021-01-14 20:26:52,626 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:26:52,627 : INFO : entropies processed\n",
      "2021-01-14 20:26:52,628 : INFO : extropies processed\n",
      "2021-01-14 20:26:52,635 : INFO : token count processed\n",
      "2021-01-14 20:26:52,639 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:26:52,644 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:26:52,645 : INFO : vocab #32006\n",
      "2021-01-14 20:26:52,652 : INFO : diff #set()\n",
      "2021-01-14 20:27:11,555 : INFO : alphabet #32006\n",
      "2021-01-14 20:27:21,118 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.0684342825753534, 0.48345746752704477], [0.6907816231250763, 0.30921838], [4.060262039120377, 1.3905566447542497], [5.234069531114783, 6.361621244785958, 6.638331756320789, 4.957359019579953, 1.404262225206006, 0.27671051153483095]]\n",
      "2021-01-14 20:27:21,123 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:27:21,126 : INFO : built Dictionary(239 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1302 corpus positions)\n",
      "2021-01-14 20:27:21,354 : INFO : token count processed\n",
      "2021-01-14 20:27:21,398 : INFO : frequencies processed\n",
      "2021-01-14 20:27:30,821 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:27:30,822 : INFO : entropies processed\n",
      "2021-01-14 20:27:30,823 : INFO : extropies processed\n",
      "2021-01-14 20:27:30,830 : INFO : token count processed\n",
      "2021-01-14 20:27:30,838 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:27:30,842 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:27:30,843 : INFO : vocab #32006\n",
      "2021-01-14 20:27:30,850 : INFO : diff #set()\n",
      "2021-01-14 20:27:49,834 : INFO : alphabet #32006\n",
      "2021-01-14 20:27:59,133 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.070780635184367, 0.482909673293795], [0.660010814666748, 0.3399892], [3.7541634277688805, 1.3685002053922837], [5.234069531114783, 6.620594433343389, 6.898319807080515, 4.956344157377657, 1.664250275965732, 0.2777253737371259]]\n",
      "2021-01-14 20:27:59,137 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:27:59,139 : INFO : built Dictionary(213 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1395 corpus positions)\n",
      "2021-01-14 20:27:59,334 : INFO : token count processed\n",
      "2021-01-14 20:27:59,368 : INFO : frequencies processed\n",
      "2021-01-14 20:28:08,787 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:28:08,788 : INFO : entropies processed\n",
      "2021-01-14 20:28:08,789 : INFO : extropies processed\n",
      "2021-01-14 20:28:08,796 : INFO : token count processed\n",
      "2021-01-14 20:28:08,800 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:28:08,805 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:28:08,806 : INFO : vocab #32006\n",
      "2021-01-14 20:28:08,812 : INFO : diff #set()\n",
      "2021-01-14 20:28:27,798 : INFO : alphabet #32006\n",
      "2021-01-14 20:28:37,500 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.0825775162670364, 0.4801742034517269], [0.6650943458080292, 0.33490565], [3.5739348962840563, 1.357246593146981], [5.234069531114783, 6.207411496248084, 6.423260413272425, 5.018220614090442, 1.189190882157642, 0.21584891702434117]]\n",
      "2021-01-14 20:28:37,504 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:28:37,505 : INFO : built Dictionary(154 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 299 corpus positions)\n",
      "2021-01-14 20:28:37,624 : INFO : token count processed\n",
      "2021-01-14 20:28:37,657 : INFO : frequencies processed\n",
      "2021-01-14 20:28:47,074 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:28:47,075 : INFO : entropies processed\n",
      "2021-01-14 20:28:47,076 : INFO : extropies processed\n",
      "2021-01-14 20:28:47,083 : INFO : token count processed\n",
      "2021-01-14 20:28:47,087 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:28:47,091 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:28:47,092 : INFO : vocab #32006\n",
      "2021-01-14 20:28:47,099 : INFO : diff #set()\n",
      "2021-01-14 20:29:06,068 : INFO : alphabet #32006\n",
      "2021-01-14 20:29:15,501 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.0681136871188202, 0.48353241227910637], [0.6424226462841034, 0.35757735], [3.6835423624332306, 1.377974449419992], [5.234069531114783, 6.5805228788529595, 6.874354132528044, 4.940238277439699, 1.6402846014132608, 0.29383125367508445]]\n",
      "2021-01-14 20:29:15,506 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 20:29:15,507 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:29:15,509 : INFO : built Dictionary(240 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1260 corpus positions)\n",
      "2021-01-14 20:29:15,751 : INFO : token count processed\n",
      "2021-01-14 20:29:15,787 : INFO : frequencies processed\n",
      "2021-01-14 20:29:25,189 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:29:25,190 : INFO : entropies processed\n",
      "2021-01-14 20:29:25,191 : INFO : extropies processed\n",
      "2021-01-14 20:29:25,198 : INFO : token count processed\n",
      "2021-01-14 20:29:25,202 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:29:25,206 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:29:25,207 : INFO : vocab #32006\n",
      "2021-01-14 20:29:25,215 : INFO : diff #set()\n",
      "2021-01-14 20:29:44,206 : INFO : alphabet #32006\n",
      "2021-01-14 20:29:53,634 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.1230822494175512, 0.4710133110831392], [0.7081479132175446, 0.2918521], [3.334679141051595, 1.3571063325330468], [5.234069531114783, 6.422089779976135, 6.630529653510326, 5.025629657580591, 1.3964601223955428, 0.20843987353419102]]\n",
      "2021-01-14 20:29:53,639 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:29:53,641 : INFO : built Dictionary(247 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1542 corpus positions)\n",
      "2021-01-14 20:29:53,863 : INFO : token count processed\n",
      "2021-01-14 20:29:53,895 : INFO : frequencies processed\n",
      "2021-01-14 20:30:03,425 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:30:03,426 : INFO : entropies processed\n",
      "2021-01-14 20:30:03,426 : INFO : extropies processed\n",
      "2021-01-14 20:30:03,434 : INFO : token count processed\n",
      "2021-01-14 20:30:03,438 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:30:03,444 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:30:03,445 : INFO : vocab #32006\n",
      "2021-01-14 20:30:03,453 : INFO : diff #set()\n",
      "2021-01-14 20:30:22,314 : INFO : alphabet #32006\n",
      "2021-01-14 20:30:31,758 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.0798174452467164, 0.48081143000576], [0.6904830634593964, 0.30951694], [3.6732696895151085, 1.3629961045503014], [5.234069531114783, 6.485445644653597, 6.793830555093397, 4.925684620674982, 1.5597610239786137, 0.3083849104398002]]\n",
      "2021-01-14 20:30:31,763 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:30:31,764 : INFO : built Dictionary(224 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1428 corpus positions)\n",
      "2021-01-14 20:30:31,970 : INFO : token count processed\n",
      "2021-01-14 20:30:32,017 : INFO : frequencies processed\n",
      "2021-01-14 20:30:41,539 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:30:41,540 : INFO : entropies processed\n",
      "2021-01-14 20:30:41,541 : INFO : extropies processed\n",
      "2021-01-14 20:30:41,548 : INFO : token count processed\n",
      "2021-01-14 20:30:41,555 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:30:41,559 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:30:41,560 : INFO : vocab #32006\n",
      "2021-01-14 20:30:41,567 : INFO : diff #set()\n",
      "2021-01-14 20:31:00,826 : INFO : alphabet #32006\n",
      "2021-01-14 20:31:10,255 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.0882618798680872, 0.47886714288112603], [0.6708848476409912, 0.32911515], [3.447401504705915, 1.3464969825090003], [5.234069531114783, 6.2276600107346916, 6.4418200136523165, 5.019909528197159, 1.2077504825375334, 0.21416000291762494]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 20:31:10,260 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:31:10,262 : INFO : built Dictionary(210 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1626 corpus positions)\n",
      "2021-01-14 20:31:10,460 : INFO : token count processed\n",
      "2021-01-14 20:31:10,501 : INFO : frequencies processed\n",
      "2021-01-14 20:31:20,014 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:31:20,015 : INFO : entropies processed\n",
      "2021-01-14 20:31:20,016 : INFO : extropies processed\n",
      "2021-01-14 20:31:20,024 : INFO : token count processed\n",
      "2021-01-14 20:31:20,029 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:31:20,034 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:31:20,035 : INFO : vocab #32006\n",
      "2021-01-14 20:31:20,042 : INFO : diff #set()\n",
      "2021-01-14 20:31:38,948 : INFO : alphabet #32006\n",
      "2021-01-14 20:31:48,498 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.1126588012136645, 0.47333719927966006], [0.7069990932941437, 0.2930009], [3.9171886421180044, 1.380226849804935], [5.234069531114783, 6.253918170574241, 6.507906971754336, 4.980080729934688, 1.2738374406395527, 0.25398880118009526]]\n",
      "2021-01-14 20:31:48,502 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:31:48,503 : INFO : built Dictionary(187 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 723 corpus positions)\n",
      "2021-01-14 20:31:48,668 : INFO : token count processed\n",
      "2021-01-14 20:31:48,703 : INFO : frequencies processed\n",
      "2021-01-14 20:31:58,112 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:31:58,113 : INFO : entropies processed\n",
      "2021-01-14 20:31:58,114 : INFO : extropies processed\n",
      "2021-01-14 20:31:58,127 : INFO : token count processed\n",
      "2021-01-14 20:31:58,132 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:31:58,136 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:31:58,137 : INFO : vocab #32006\n",
      "2021-01-14 20:31:58,144 : INFO : diff #set()\n",
      "2021-01-14 20:32:17,074 : INFO : alphabet #32006\n",
      "2021-01-14 20:32:26,520 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.0914711287094907, 0.47813234726172593], [0.6720218360424042, 0.32797816], [3.3320953986601696, 1.3383685641551244], [5.234069531114783, 6.374522245625576, 6.698541974878157, 4.9100498018622005, 1.4644724437633743, 0.3240197292525817]]\n",
      "2021-01-14 20:32:26,525 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:32:26,527 : INFO : built Dictionary(303 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 2005 corpus positions)\n",
      "2021-01-14 20:32:26,859 : INFO : token count processed\n",
      "2021-01-14 20:32:26,921 : INFO : frequencies processed\n",
      "2021-01-14 20:32:36,356 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:32:36,357 : INFO : entropies processed\n",
      "2021-01-14 20:32:36,358 : INFO : extropies processed\n",
      "2021-01-14 20:32:36,366 : INFO : token count processed\n",
      "2021-01-14 20:32:36,372 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:32:36,376 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:32:36,377 : INFO : vocab #32006\n",
      "2021-01-14 20:32:36,384 : INFO : diff #set()\n",
      "2021-01-14 20:32:55,377 : INFO : alphabet #32006\n",
      "2021-01-14 20:33:04,793 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.090865632819745, 0.4782708100909374], [0.7202486097812653, 0.2797514], [3.964735178725505, 1.380575753977742], [5.234069531114783, 6.731238669067808, 7.039940077734394, 4.925368122448197, 1.8058705466196106, 0.308701408666586]]\n",
      "2021-01-14 20:33:04,798 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:33:04,799 : INFO : built Dictionary(227 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1397 corpus positions)\n",
      "2021-01-14 20:33:05,006 : INFO : token count processed\n",
      "2021-01-14 20:33:05,050 : INFO : frequencies processed\n",
      "2021-01-14 20:33:14,471 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:33:14,472 : INFO : entropies processed\n",
      "2021-01-14 20:33:14,474 : INFO : extropies processed\n",
      "2021-01-14 20:33:14,488 : INFO : token count processed\n",
      "2021-01-14 20:33:14,492 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:33:14,498 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:33:14,499 : INFO : vocab #32006\n",
      "2021-01-14 20:33:14,507 : INFO : diff #set()\n",
      "2021-01-14 20:33:33,484 : INFO : alphabet #32006\n",
      "2021-01-14 20:33:42,899 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.0771737067845875, 0.4814233863704999], [0.6778301000595093, 0.3221699], [4.159199529386523, 1.3960073509815607], [5.234069531114783, 6.503741451859337, 6.752026609771913, 4.9857843732022085, 1.5179570786571297, 0.24828515791257555]]\n",
      "2021-01-14 20:33:42,904 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:33:42,905 : INFO : built Dictionary(246 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 1719 corpus positions)\n",
      "2021-01-14 20:33:43,138 : INFO : token count processed\n",
      "2021-01-14 20:33:43,170 : INFO : frequencies processed\n",
      "2021-01-14 20:33:52,572 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:33:52,573 : INFO : entropies processed\n",
      "2021-01-14 20:33:52,574 : INFO : extropies processed\n",
      "2021-01-14 20:33:52,581 : INFO : token count processed\n",
      "2021-01-14 20:33:52,586 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:33:52,590 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:33:52,591 : INFO : vocab #32006\n",
      "2021-01-14 20:33:52,602 : INFO : diff #set()\n",
      "2021-01-14 20:34:11,587 : INFO : alphabet #32006\n",
      "2021-01-14 20:34:20,994 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.0653570231627203, 0.48417779046679355], [0.6893655359745026, 0.31063446], [4.252715278979704, 1.3930676526991819], [5.234069531114783, 6.334729224484471, 6.517444800865823, 5.051353954733431, 1.2833752697510397, 0.1827155763813515]]\n",
      "2021-01-14 20:34:21,000 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:34:21,003 : INFO : built Dictionary(250 unique tokens: ['\"', '.', '1', ':', '_']...) from 2 documents (total 2068 corpus positions)\n",
      "2021-01-14 20:34:21,252 : INFO : token count processed\n",
      "2021-01-14 20:34:21,313 : INFO : frequencies processed\n",
      "2021-01-14 20:34:30,750 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:34:30,751 : INFO : entropies processed\n",
      "2021-01-14 20:34:30,752 : INFO : extropies processed\n",
      "2021-01-14 20:34:30,760 : INFO : token count processed\n",
      "2021-01-14 20:34:30,767 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:34:30,771 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:34:30,772 : INFO : vocab #32006\n",
      "2021-01-14 20:34:30,779 : INFO : diff #set()\n",
      "2021-01-14 20:34:49,757 : INFO : alphabet #32006\n",
      "2021-01-14 20:34:59,202 : INFO : Computed distances or similarities ('275', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.096467857787418, 0.4769927648952298], [0.7540451884269714, 0.24595481], [3.6800893536948065, 1.366444577264692], [5.234069531114783, 6.21319712067992, 6.469967469560586, 4.977299182234116, 1.2358979384458033, 0.2567703488806661]]\n",
      "2021-01-14 20:34:59,207 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 20:34:59,208 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:34:59,209 : INFO : built Dictionary(274 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1411 corpus positions)\n",
      "2021-01-14 20:34:59,338 : INFO : token count processed\n",
      "2021-01-14 20:34:59,370 : INFO : frequencies processed\n",
      "2021-01-14 20:35:08,903 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:35:08,904 : INFO : entropies processed\n",
      "2021-01-14 20:35:08,905 : INFO : extropies processed\n",
      "2021-01-14 20:35:08,913 : INFO : token count processed\n",
      "2021-01-14 20:35:08,919 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:35:08,924 : INFO : alphabet_target #32010\n",
      "2021-01-14 20:35:08,925 : INFO : vocab #32006\n",
      "2021-01-14 20:35:08,932 : INFO : diff #set()\n",
      "2021-01-14 20:35:27,839 : INFO : alphabet #32006\n",
      "2021-01-14 20:35:37,298 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.1845618655196626, 0.45775769310251163], [0.8225109428167343, 0.17748906], [2.0, 1.2451124978365313], [4.1219280948873624, 6.905617163738059, 7.041440511624259, 3.9861047470011624, 2.9195124167368967, 0.1358233478862001]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 20:35:37,304 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:35:37,306 : INFO : built Dictionary(359 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 2301 corpus positions)\n",
      "2021-01-14 20:35:37,489 : INFO : token count processed\n",
      "2021-01-14 20:35:37,521 : INFO : frequencies processed\n",
      "2021-01-14 20:35:47,053 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:35:47,054 : INFO : entropies processed\n",
      "2021-01-14 20:35:47,054 : INFO : extropies processed\n",
      "2021-01-14 20:35:47,062 : INFO : token count processed\n",
      "2021-01-14 20:35:47,070 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:35:47,074 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:35:47,075 : INFO : vocab #32006\n",
      "2021-01-14 20:35:47,082 : INFO : diff #set()\n",
      "2021-01-14 20:36:05,971 : INFO : alphabet #32006\n",
      "2021-01-14 20:36:15,356 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.1710522749643177, 0.4606061362646994], [0.8101181387901306, 0.18988186], [3.251629167387823, 1.3589504783379556], [4.1219280948873624, 7.1219284286457345, 7.327947188855914, 3.915909334677183, 3.2060190939685516, 0.2060187602101795]]\n",
      "2021-01-14 20:36:15,362 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:36:15,366 : INFO : built Dictionary(282 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 2284 corpus positions)\n",
      "2021-01-14 20:36:15,495 : INFO : token count processed\n",
      "2021-01-14 20:36:15,521 : INFO : frequencies processed\n",
      "2021-01-14 20:36:24,985 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:36:24,986 : INFO : entropies processed\n",
      "2021-01-14 20:36:24,987 : INFO : extropies processed\n",
      "2021-01-14 20:36:25,001 : INFO : token count processed\n",
      "2021-01-14 20:36:25,005 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:36:25,010 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:36:25,011 : INFO : vocab #32006\n",
      "2021-01-14 20:36:25,018 : INFO : diff #set()\n",
      "2021-01-14 20:36:43,937 : INFO : alphabet #32006\n",
      "2021-01-14 20:36:53,506 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2339197982557766, 0.4476436444946639], [0.8887080177664757, 0.11129198], [2.584962500721156, 1.315172029168969], [4.1219280948873624, 6.41099024988467, 6.482069666022798, 4.050848678749234, 2.3601415711354354, 0.07107941613812763]]\n",
      "2021-01-14 20:36:53,510 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:36:53,511 : INFO : built Dictionary(163 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 683 corpus positions)\n",
      "2021-01-14 20:36:53,587 : INFO : token count processed\n",
      "2021-01-14 20:36:53,653 : INFO : frequencies processed\n",
      "2021-01-14 20:37:03,074 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:37:03,076 : INFO : entropies processed\n",
      "2021-01-14 20:37:03,076 : INFO : extropies processed\n",
      "2021-01-14 20:37:03,083 : INFO : token count processed\n",
      "2021-01-14 20:37:03,089 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:37:03,095 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:37:03,096 : INFO : vocab #32006\n",
      "2021-01-14 20:37:03,103 : INFO : diff #set()\n",
      "2021-01-14 20:37:21,987 : INFO : alphabet #32006\n",
      "2021-01-14 20:37:31,537 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.1927183486717525, 0.4560549240652607], [0.852224200963974, 0.1477758], [1.584962500721156, 1.1699250014423124], [4.1219280948873624, 6.077866832717642, 6.230036047057928, 3.9697588805470767, 2.108107952170566, 0.15216921434028663]]\n",
      "2021-01-14 20:37:31,540 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:37:31,542 : INFO : built Dictionary(139 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 448 corpus positions)\n",
      "2021-01-14 20:37:31,610 : INFO : token count processed\n",
      "2021-01-14 20:37:31,642 : INFO : frequencies processed\n",
      "2021-01-14 20:37:41,059 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:37:41,061 : INFO : entropies processed\n",
      "2021-01-14 20:37:41,062 : INFO : extropies processed\n",
      "2021-01-14 20:37:41,068 : INFO : token count processed\n",
      "2021-01-14 20:37:41,076 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:37:41,081 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:37:41,082 : INFO : vocab #32006\n",
      "2021-01-14 20:37:41,089 : INFO : diff #set()\n",
      "2021-01-14 20:38:00,068 : INFO : alphabet #32006\n",
      "2021-01-14 20:38:09,467 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.1728903875515748, 0.4602164958384329], [0.8344714939594269, 0.1655285], [1.584962500721156, 1.1699250014423124], [4.1219280948873624, 5.977547459003844, 6.122517846131769, 3.9769577077594374, 2.000589751244407, 0.1449703871279251]]\n",
      "2021-01-14 20:38:09,472 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:38:09,474 : INFO : built Dictionary(243 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 2159 corpus positions)\n",
      "2021-01-14 20:38:09,588 : INFO : token count processed\n",
      "2021-01-14 20:38:09,619 : INFO : frequencies processed\n",
      "2021-01-14 20:38:19,012 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:38:19,013 : INFO : entropies processed\n",
      "2021-01-14 20:38:19,014 : INFO : extropies processed\n",
      "2021-01-14 20:38:19,022 : INFO : token count processed\n",
      "2021-01-14 20:38:19,029 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:38:19,034 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:38:19,035 : INFO : vocab #32006\n",
      "2021-01-14 20:38:19,042 : INFO : diff #set()\n",
      "2021-01-14 20:38:38,043 : INFO : alphabet #32006\n",
      "2021-01-14 20:38:47,493 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.230927426888317, 0.4482440746155483], [0.8789827451109886, 0.121017255], [2.0, 1.2451124978365313], [4.1219280948873624, 6.4614394051846435, 6.56678019132607, 4.016587308745936, 2.4448520964387077, 0.10534078614142661]]\n",
      "2021-01-14 20:38:47,498 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 20:38:47,499 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:38:47,500 : INFO : built Dictionary(206 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1383 corpus positions)\n",
      "2021-01-14 20:38:47,594 : INFO : token count processed\n",
      "2021-01-14 20:38:47,627 : INFO : frequencies processed\n",
      "2021-01-14 20:38:57,026 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:38:57,027 : INFO : entropies processed\n",
      "2021-01-14 20:38:57,028 : INFO : extropies processed\n",
      "2021-01-14 20:38:57,036 : INFO : token count processed\n",
      "2021-01-14 20:38:57,041 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:38:57,046 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:38:57,047 : INFO : vocab #32006\n",
      "2021-01-14 20:38:57,054 : INFO : diff #set()\n",
      "2021-01-14 20:39:16,068 : INFO : alphabet #32006\n",
      "2021-01-14 20:39:25,653 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.1926042681807345, 0.45607865245547846], [0.8435489237308502, 0.15645108], [1.584962500721156, 1.1699250014423124], [4.1219280948873624, 6.327195724598159, 6.4446185158926275, 4.004505303592894, 2.322690421005265, 0.11742279129446853]]\n",
      "2021-01-14 20:39:25,665 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 20:39:25,666 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:39:25,669 : INFO : built Dictionary(417 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 6275 corpus positions)\n",
      "2021-01-14 20:39:25,899 : INFO : token count processed\n",
      "2021-01-14 20:39:25,932 : INFO : frequencies processed\n",
      "2021-01-14 20:39:35,339 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:39:35,340 : INFO : entropies processed\n",
      "2021-01-14 20:39:35,341 : INFO : extropies processed\n",
      "2021-01-14 20:39:35,357 : INFO : token count processed\n",
      "2021-01-14 20:39:35,361 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:39:35,365 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:39:35,366 : INFO : vocab #32006\n",
      "2021-01-14 20:39:35,373 : INFO : diff #set()\n",
      "2021-01-14 20:39:54,383 : INFO : alphabet #32006\n",
      "2021-01-14 20:40:03,820 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.197211590930279, 0.4551223032537386], [0.8461754024028778, 0.1538246], [2.94770277922009, 1.3393100707180505], [4.1219280948873624, 6.9079058562486315, 7.012992699143836, 4.016841251992159, 2.8910646042564734, 0.10508684289520431]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 20:40:03,827 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:40:03,829 : INFO : built Dictionary(316 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 2669 corpus positions)\n",
      "2021-01-14 20:40:03,986 : INFO : token count processed\n",
      "2021-01-14 20:40:04,019 : INFO : frequencies processed\n",
      "2021-01-14 20:40:13,445 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:40:13,446 : INFO : entropies processed\n",
      "2021-01-14 20:40:13,447 : INFO : extropies processed\n",
      "2021-01-14 20:40:13,454 : INFO : token count processed\n",
      "2021-01-14 20:40:13,462 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:40:13,466 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:40:13,467 : INFO : vocab #32006\n",
      "2021-01-14 20:40:13,474 : INFO : diff #set()\n",
      "2021-01-14 20:40:32,281 : INFO : alphabet #32006\n",
      "2021-01-14 20:40:41,744 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.152197756333819, 0.46464131702444444], [0.8011154532432556, 0.19888455], [3.4182958340544896, 1.369895090630202], [4.1219280948873624, 6.61034830706307, 6.755254239301154, 3.9770221626492788, 2.6333261444137914, 0.14490593223808368]]\n",
      "2021-01-14 20:40:41,748 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:40:41,749 : INFO : built Dictionary(207 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 716 corpus positions)\n",
      "2021-01-14 20:40:41,838 : INFO : token count processed\n",
      "2021-01-14 20:40:41,870 : INFO : frequencies processed\n",
      "2021-01-14 20:40:51,422 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:40:51,423 : INFO : entropies processed\n",
      "2021-01-14 20:40:51,425 : INFO : extropies processed\n",
      "2021-01-14 20:40:51,438 : INFO : token count processed\n",
      "2021-01-14 20:40:51,442 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:40:51,447 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:40:51,448 : INFO : vocab #32006\n",
      "2021-01-14 20:40:51,455 : INFO : diff #set()\n",
      "2021-01-14 20:41:10,325 : INFO : alphabet #32006\n",
      "2021-01-14 20:41:19,871 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.194370623455507, 0.45571153264223235], [0.840717151761055, 0.15928285], [2.521640636343318, 1.2998438251349493], [4.1219280948873624, 6.616715366949855, 6.750435153332136, 3.9882083085050812, 2.6285070584447734, 0.1337197863822812]]\n",
      "2021-01-14 20:41:19,877 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:41:19,879 : INFO : built Dictionary(423 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 2750 corpus positions)\n",
      "2021-01-14 20:41:20,122 : INFO : token count processed\n",
      "2021-01-14 20:41:20,159 : INFO : frequencies processed\n",
      "2021-01-14 20:41:29,740 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:41:29,741 : INFO : entropies processed\n",
      "2021-01-14 20:41:29,742 : INFO : extropies processed\n",
      "2021-01-14 20:41:29,757 : INFO : token count processed\n",
      "2021-01-14 20:41:29,761 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:41:29,766 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:41:29,767 : INFO : vocab #32006\n",
      "2021-01-14 20:41:29,776 : INFO : diff #set()\n",
      "2021-01-14 20:41:48,669 : INFO : alphabet #32006\n",
      "2021-01-14 20:41:58,226 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.1796893470779812, 0.4587809732338999], [0.8078912198543549, 0.19210878], [3.251629167387823, 1.3589504783379556], [4.1219280948873624, 7.32185870753746, 7.5122574771778625, 3.9315293252469603, 3.3903293822905, 0.19039876964040214]]\n",
      "2021-01-14 20:41:58,229 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:41:58,230 : INFO : built Dictionary(65 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 115 corpus positions)\n",
      "2021-01-14 20:41:58,253 : INFO : token count processed\n",
      "2021-01-14 20:41:58,285 : INFO : frequencies processed\n",
      "2021-01-14 20:42:07,713 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:42:07,714 : INFO : entropies processed\n",
      "2021-01-14 20:42:07,715 : INFO : extropies processed\n",
      "2021-01-14 20:42:07,722 : INFO : token count processed\n",
      "2021-01-14 20:42:07,726 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:42:07,730 : INFO : alphabet_target #32008\n",
      "2021-01-14 20:42:07,731 : INFO : vocab #32006\n",
      "2021-01-14 20:42:07,738 : INFO : diff #set()\n",
      "2021-01-14 20:42:26,653 : INFO : alphabet #32006\n",
      "2021-01-14 20:42:36,199 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/fireException.py')[[1.1968840484273682, 0.45519015931489265], [0.8659995049238205, 0.1340005], [0.0, 0.0], [4.1219280948873624, 5.176618657501385, 5.691884294427158, 3.6066624579615905, 1.5699561995397957, 0.5152656369257729]]\n",
      "2021-01-14 20:42:36,203 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:42:36,204 : INFO : built Dictionary(159 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 508 corpus positions)\n",
      "2021-01-14 20:42:36,267 : INFO : token count processed\n",
      "2021-01-14 20:42:36,299 : INFO : frequencies processed\n",
      "2021-01-14 20:42:45,708 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:42:45,709 : INFO : entropies processed\n",
      "2021-01-14 20:42:45,710 : INFO : extropies processed\n",
      "2021-01-14 20:42:45,717 : INFO : token count processed\n",
      "2021-01-14 20:42:45,723 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:42:45,728 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:42:45,729 : INFO : vocab #32006\n",
      "2021-01-14 20:42:45,736 : INFO : diff #set()\n",
      "2021-01-14 20:43:04,727 : INFO : alphabet #32006\n",
      "2021-01-14 20:43:14,137 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.1936726918539844, 0.4558565203065227], [0.8499423712491989, 0.15005763], [1.584962500721156, 1.1699250014423124], [4.1219280948873624, 6.468846789852156, 6.658441008867934, 3.932333875871585, 2.5365129139805713, 0.18959421901577755]]\n",
      "2021-01-14 20:43:14,143 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:43:14,145 : INFO : built Dictionary(369 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 2557 corpus positions)\n",
      "2021-01-14 20:43:14,345 : INFO : token count processed\n",
      "2021-01-14 20:43:14,377 : INFO : frequencies processed\n",
      "2021-01-14 20:43:23,816 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:43:23,817 : INFO : entropies processed\n",
      "2021-01-14 20:43:23,818 : INFO : extropies processed\n",
      "2021-01-14 20:43:23,831 : INFO : token count processed\n",
      "2021-01-14 20:43:23,836 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:43:23,842 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:43:23,843 : INFO : vocab #32006\n",
      "2021-01-14 20:43:23,852 : INFO : diff #set()\n",
      "2021-01-14 20:43:42,846 : INFO : alphabet #32006\n",
      "2021-01-14 20:43:52,279 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.2169681616422376, 0.4510664687485821], [0.8663887977600098, 0.1336112], [3.2776134368191157, 1.3618978811135465], [4.1219280948873624, 6.957796704012729, 7.091332991422222, 3.9883918074778695, 2.9694048965348596, 0.13353628740949297]]\n",
      "2021-01-14 20:43:52,286 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:43:52,288 : INFO : built Dictionary(283 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 3054 corpus positions)\n",
      "2021-01-14 20:43:52,422 : INFO : token count processed\n",
      "2021-01-14 20:43:52,455 : INFO : frequencies processed\n",
      "2021-01-14 20:44:02,054 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:44:02,055 : INFO : entropies processed\n",
      "2021-01-14 20:44:02,056 : INFO : extropies processed\n",
      "2021-01-14 20:44:02,064 : INFO : token count processed\n",
      "2021-01-14 20:44:02,071 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:44:02,075 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:44:02,076 : INFO : vocab #32006\n",
      "2021-01-14 20:44:02,083 : INFO : diff #set()\n",
      "2021-01-14 20:44:21,061 : INFO : alphabet #32006\n",
      "2021-01-14 20:44:30,482 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.222247832459297, 0.4499948139867591], [0.8813758045434952, 0.118624195], [2.321928094887362, 1.2877123795494492], [4.1219280948873624, 6.441859572014148, 6.588245675121481, 3.9755419917800303, 2.4663175802341186, 0.14638610310733302]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 20:44:30,487 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:44:30,490 : INFO : built Dictionary(305 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1530 corpus positions)\n",
      "2021-01-14 20:44:30,655 : INFO : token count processed\n",
      "2021-01-14 20:44:30,725 : INFO : frequencies processed\n",
      "2021-01-14 20:44:40,134 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:44:40,135 : INFO : entropies processed\n",
      "2021-01-14 20:44:40,136 : INFO : extropies processed\n",
      "2021-01-14 20:44:40,143 : INFO : token count processed\n",
      "2021-01-14 20:44:40,150 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:44:40,155 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:44:40,156 : INFO : vocab #32006\n",
      "2021-01-14 20:44:40,163 : INFO : diff #set()\n",
      "2021-01-14 20:44:59,029 : INFO : alphabet #32006\n",
      "2021-01-14 20:45:08,456 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.1723576626333503, 0.46032935423156407], [0.8063687682151794, 0.19363123], [3.095795255000934, 1.3487605247277434], [4.1219280948873624, 6.998955278238291, 7.149383506895684, 3.97149986622997, 3.0274554120083215, 0.15042822865739325]]\n",
      "2021-01-14 20:45:08,460 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:45:08,462 : INFO : built Dictionary(223 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1349 corpus positions)\n",
      "2021-01-14 20:45:08,569 : INFO : token count processed\n",
      "2021-01-14 20:45:08,604 : INFO : frequencies processed\n",
      "2021-01-14 20:45:18,124 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:45:18,126 : INFO : entropies processed\n",
      "2021-01-14 20:45:18,128 : INFO : extropies processed\n",
      "2021-01-14 20:45:18,136 : INFO : token count processed\n",
      "2021-01-14 20:45:18,140 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:45:18,144 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:45:18,145 : INFO : vocab #32006\n",
      "2021-01-14 20:45:18,152 : INFO : diff #set()\n",
      "2021-01-14 20:45:36,996 : INFO : alphabet #32006\n",
      "2021-01-14 20:45:46,415 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.211007429490745, 0.45228251459576835], [0.858785942196846, 0.14121406], [2.584962500721156, 1.315172029168969], [4.1219280948873624, 6.492983191376071, 6.604410610097682, 4.010500676165751, 2.48248251521032, 0.1114274187216111]]\n",
      "2021-01-14 20:45:46,422 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:45:46,425 : INFO : built Dictionary(436 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 3288 corpus positions)\n",
      "2021-01-14 20:45:46,669 : INFO : token count processed\n",
      "2021-01-14 20:45:46,729 : INFO : frequencies processed\n",
      "2021-01-14 20:45:56,252 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:45:56,253 : INFO : entropies processed\n",
      "2021-01-14 20:45:56,254 : INFO : extropies processed\n",
      "2021-01-14 20:45:56,262 : INFO : token count processed\n",
      "2021-01-14 20:45:56,270 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:45:56,275 : INFO : alphabet_target #32008\n",
      "2021-01-14 20:45:56,276 : INFO : vocab #32006\n",
      "2021-01-14 20:45:56,284 : INFO : diff #set()\n",
      "2021-01-14 20:46:15,160 : INFO : alphabet #32006\n",
      "2021-01-14 20:46:24,676 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.1948460540380892, 0.4556128199334048], [0.8486798852682114, 0.15132011], [1.0, 1.0], [4.1219280948873624, 6.560342487747443, 6.751181674450915, 3.93108890818389, 2.629253579563553, 0.19083918670347266]]\n",
      "2021-01-14 20:46:24,683 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 20:46:24,684 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:46:24,686 : INFO : built Dictionary(445 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 3493 corpus positions)\n",
      "2021-01-14 20:46:24,947 : INFO : token count processed\n",
      "2021-01-14 20:46:24,981 : INFO : frequencies processed\n",
      "2021-01-14 20:46:34,406 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:46:34,407 : INFO : entropies processed\n",
      "2021-01-14 20:46:34,408 : INFO : extropies processed\n",
      "2021-01-14 20:46:34,423 : INFO : token count processed\n",
      "2021-01-14 20:46:34,428 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:46:34,434 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:46:34,435 : INFO : vocab #32006\n",
      "2021-01-14 20:46:34,442 : INFO : diff #set()\n",
      "2021-01-14 20:46:53,322 : INFO : alphabet #32006\n",
      "2021-01-14 20:47:02,885 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.1581021868610077, 0.4633700878893576], [0.8036374151706696, 0.19636258], [3.251629167387823, 1.3589504783379556], [4.1219280948873624, 7.046173750105238, 7.225118777446566, 3.9429830675460344, 3.1031906825592035, 0.178945027341328]]\n",
      "2021-01-14 20:47:02,896 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 20:47:02,897 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:47:02,904 : INFO : built Dictionary(500 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 5602 corpus positions)\n",
      "2021-01-14 20:47:03,208 : INFO : token count processed\n",
      "2021-01-14 20:47:03,269 : INFO : frequencies processed\n",
      "2021-01-14 20:47:12,687 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:47:12,688 : INFO : entropies processed\n",
      "2021-01-14 20:47:12,689 : INFO : extropies processed\n",
      "2021-01-14 20:47:12,698 : INFO : token count processed\n",
      "2021-01-14 20:47:12,706 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:47:12,711 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:47:12,712 : INFO : vocab #32006\n",
      "2021-01-14 20:47:12,719 : INFO : diff #set()\n",
      "2021-01-14 20:47:31,600 : INFO : alphabet #32006\n",
      "2021-01-14 20:47:41,130 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.1720044815087312, 0.4604042065812746], [0.8212723433971405, 0.17872766], [3.121928094887362, 1.3519647487142497], [4.1219280948873624, 7.009229588004272, 7.112845298755901, 4.018312384135734, 2.9909172038685385, 0.10361571075162868]]\n",
      "2021-01-14 20:47:41,143 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:47:41,146 : INFO : built Dictionary(576 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 6543 corpus positions)\n",
      "2021-01-14 20:47:41,524 : INFO : token count processed\n",
      "2021-01-14 20:47:41,571 : INFO : frequencies processed\n",
      "2021-01-14 20:47:50,983 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:47:50,985 : INFO : entropies processed\n",
      "2021-01-14 20:47:50,985 : INFO : extropies processed\n",
      "2021-01-14 20:47:50,994 : INFO : token count processed\n",
      "2021-01-14 20:47:51,000 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:47:51,005 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:47:51,006 : INFO : vocab #32006\n",
      "2021-01-14 20:47:51,013 : INFO : diff #set()\n",
      "2021-01-14 20:48:09,984 : INFO : alphabet #32006\n",
      "2021-01-14 20:48:19,389 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.1833766239946826, 0.45800618592792786], [0.8190177828073502, 0.18098222], [3.521640636343319, 1.3740281872300928], [4.1219280948873624, 7.376088004590871, 7.534469444850127, 3.9635466546281064, 3.4125413499627646, 0.15838144025925605]]\n",
      "2021-01-14 20:48:19,393 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:48:19,394 : INFO : built Dictionary(134 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 329 corpus positions)\n",
      "2021-01-14 20:48:19,456 : INFO : token count processed\n",
      "2021-01-14 20:48:19,489 : INFO : frequencies processed\n",
      "2021-01-14 20:48:28,877 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:48:28,878 : INFO : entropies processed\n",
      "2021-01-14 20:48:28,879 : INFO : extropies processed\n",
      "2021-01-14 20:48:28,886 : INFO : token count processed\n",
      "2021-01-14 20:48:28,890 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:48:28,895 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:48:28,896 : INFO : vocab #32006\n",
      "2021-01-14 20:48:28,903 : INFO : diff #set()\n",
      "2021-01-14 20:48:47,865 : INFO : alphabet #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 20:48:57,166 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.160512927835127, 0.4628530508271562], [0.8188300281763077, 0.18116997], [2.0, 1.2451124978365313], [4.1219280948873624, 6.2993628166120885, 6.481367015208275, 3.939923896291176, 2.3594389203209127, 0.18200419859618666]]\n",
      "2021-01-14 20:48:57,169 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:48:57,170 : INFO : built Dictionary(34 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 45 corpus positions)\n",
      "2021-01-14 20:48:57,189 : INFO : token count processed\n",
      "2021-01-14 20:48:57,221 : INFO : frequencies processed\n",
      "2021-01-14 20:49:06,680 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:49:06,681 : INFO : entropies processed\n",
      "2021-01-14 20:49:06,682 : INFO : extropies processed\n",
      "2021-01-14 20:49:06,689 : INFO : token count processed\n",
      "2021-01-14 20:49:06,695 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:49:06,700 : INFO : alphabet_target #32008\n",
      "2021-01-14 20:49:06,701 : INFO : vocab #32006\n",
      "2021-01-14 20:49:06,708 : INFO : diff #set()\n",
      "2021-01-14 20:49:25,692 : INFO : alphabet #32006\n",
      "2021-01-14 20:49:35,155 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.220842013836942, 0.4502796658967663], [0.8499092161655426, 0.15009078], [0.0, 0.0], [4.1219280948873624, 3.8936606896881862, 4.813136197106743, 3.202452587468806, 0.6912081022193801, 0.9194755074185563]]\n",
      "2021-01-14 20:49:35,178 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:49:35,183 : INFO : built Dictionary(738 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 12487 corpus positions)\n",
      "2021-01-14 20:49:35,716 : INFO : token count processed\n",
      "2021-01-14 20:49:35,752 : INFO : frequencies processed\n",
      "2021-01-14 20:49:45,167 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:49:45,168 : INFO : entropies processed\n",
      "2021-01-14 20:49:45,169 : INFO : extropies processed\n",
      "2021-01-14 20:49:45,180 : INFO : token count processed\n",
      "2021-01-14 20:49:45,184 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:49:45,188 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:49:45,189 : INFO : vocab #32006\n",
      "2021-01-14 20:49:45,198 : INFO : diff #set()\n",
      "2021-01-14 20:50:05,305 : INFO : alphabet #32006\n",
      "2021-01-14 20:50:14,736 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.1771320180982288, 0.4593198720551275], [0.8147179782390594, 0.18528202], [3.121928094887362, 1.3519647487142497], [4.1219280948873624, 7.434393313070278, 7.637724529042856, 3.9185968789147836, 3.5157964341554937, 0.20333121597257797]]\n",
      "2021-01-14 20:50:14,745 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:50:14,748 : INFO : built Dictionary(488 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 4114 corpus positions)\n",
      "2021-01-14 20:50:15,042 : INFO : token count processed\n",
      "2021-01-14 20:50:15,074 : INFO : frequencies processed\n",
      "2021-01-14 20:50:24,607 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:50:24,608 : INFO : entropies processed\n",
      "2021-01-14 20:50:24,609 : INFO : extropies processed\n",
      "2021-01-14 20:50:24,617 : INFO : token count processed\n",
      "2021-01-14 20:50:24,621 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:50:24,629 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:50:24,630 : INFO : vocab #32006\n",
      "2021-01-14 20:50:24,640 : INFO : diff #set()\n",
      "2021-01-14 20:50:43,490 : INFO : alphabet #32006\n",
      "2021-01-14 20:50:52,903 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.1633859728546065, 0.4622383673314158], [0.7934914082288742, 0.20650859], [3.251629167387823, 1.3589504783379556], [4.1219280948873624, 7.2991514951718255, 7.480823372903905, 3.940256217155283, 3.3588952780165426, 0.1816718777320796]]\n",
      "2021-01-14 20:50:52,911 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:50:52,913 : INFO : built Dictionary(455 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 3531 corpus positions)\n",
      "2021-01-14 20:50:53,185 : INFO : token count processed\n",
      "2021-01-14 20:50:53,252 : INFO : frequencies processed\n",
      "2021-01-14 20:51:02,758 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:51:02,759 : INFO : entropies processed\n",
      "2021-01-14 20:51:02,760 : INFO : extropies processed\n",
      "2021-01-14 20:51:02,768 : INFO : token count processed\n",
      "2021-01-14 20:51:02,772 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:51:02,776 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:51:02,776 : INFO : vocab #32006\n",
      "2021-01-14 20:51:02,782 : INFO : diff #set()\n",
      "2021-01-14 20:51:21,655 : INFO : alphabet #32006\n",
      "2021-01-14 20:51:31,055 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.176587558134054, 0.45943476809050604], [0.8137192577123642, 0.18628074], [3.095795255000934, 1.3487605247277434], [4.1219280948873624, 7.170319527000998, 7.347159207369826, 3.945088414518535, 3.2252311124824633, 0.17683968036882813]]\n",
      "2021-01-14 20:51:31,059 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:51:31,061 : INFO : built Dictionary(168 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 567 corpus positions)\n",
      "2021-01-14 20:51:31,134 : INFO : token count processed\n",
      "2021-01-14 20:51:31,165 : INFO : frequencies processed\n",
      "2021-01-14 20:51:40,679 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:51:40,680 : INFO : entropies processed\n",
      "2021-01-14 20:51:40,681 : INFO : extropies processed\n",
      "2021-01-14 20:51:40,689 : INFO : token count processed\n",
      "2021-01-14 20:51:40,695 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:51:40,700 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:51:40,701 : INFO : vocab #32006\n",
      "2021-01-14 20:51:40,708 : INFO : diff #set()\n",
      "2021-01-14 20:51:59,631 : INFO : alphabet #32006\n",
      "2021-01-14 20:52:09,200 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.1700702111496437, 0.46081458326190633], [0.8306958079338074, 0.16930419], [2.0, 1.2451124978365313], [4.1219280948873624, 6.353654804387375, 6.537998407820528, 3.937584491454209, 2.416070312933166, 0.18434360343315337]]\n",
      "2021-01-14 20:52:09,203 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:52:09,205 : INFO : built Dictionary(164 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 695 corpus positions)\n",
      "2021-01-14 20:52:09,276 : INFO : token count processed\n",
      "2021-01-14 20:52:09,337 : INFO : frequencies processed\n",
      "2021-01-14 20:52:18,765 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:52:18,766 : INFO : entropies processed\n",
      "2021-01-14 20:52:18,767 : INFO : extropies processed\n",
      "2021-01-14 20:52:18,774 : INFO : token count processed\n",
      "2021-01-14 20:52:18,780 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:52:18,784 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:52:18,785 : INFO : vocab #32006\n",
      "2021-01-14 20:52:18,792 : INFO : diff #set()\n",
      "2021-01-14 20:52:37,782 : INFO : alphabet #32006\n",
      "2021-01-14 20:52:47,212 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.182737602932239, 0.4581402724068268], [0.8353806883096695, 0.16461931], [1.584962500721156, 1.1699250014423124], [4.1219280948873624, 6.245180322479091, 6.402697310818528, 3.9644111065479244, 2.2807692159311657, 0.15751698833943717]]\n",
      "2021-01-14 20:52:47,217 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:52:47,219 : INFO : built Dictionary(387 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1767 corpus positions)\n",
      "2021-01-14 20:52:47,428 : INFO : token count processed\n",
      "2021-01-14 20:52:47,460 : INFO : frequencies processed\n",
      "2021-01-14 20:52:57,242 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:52:57,243 : INFO : entropies processed\n",
      "2021-01-14 20:52:57,244 : INFO : extropies processed\n",
      "2021-01-14 20:52:57,252 : INFO : token count processed\n",
      "2021-01-14 20:52:57,259 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:52:57,263 : INFO : alphabet_target #32009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 20:52:57,264 : INFO : vocab #32006\n",
      "2021-01-14 20:52:57,271 : INFO : diff #set()\n",
      "2021-01-14 20:53:16,119 : INFO : alphabet #32006\n",
      "2021-01-14 20:53:25,519 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.1732455113364622, 0.46014129318736685], [0.8108231872320175, 0.18917681], [3.095795255000934, 1.3487605247277434], [4.1219280948873624, 7.2691387000368, 7.46093538193273, 3.9301314129914324, 3.3390072870453675, 0.19179668189593002]]\n",
      "2021-01-14 20:53:25,524 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:53:25,526 : INFO : built Dictionary(315 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1604 corpus positions)\n",
      "2021-01-14 20:53:25,683 : INFO : token count processed\n",
      "2021-01-14 20:53:25,715 : INFO : frequencies processed\n",
      "2021-01-14 20:53:35,111 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:53:35,112 : INFO : entropies processed\n",
      "2021-01-14 20:53:35,113 : INFO : extropies processed\n",
      "2021-01-14 20:53:35,120 : INFO : token count processed\n",
      "2021-01-14 20:53:35,126 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:53:35,132 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:53:35,133 : INFO : vocab #32006\n",
      "2021-01-14 20:53:35,140 : INFO : diff #set()\n",
      "2021-01-14 20:53:54,104 : INFO : alphabet #32006\n",
      "2021-01-14 20:54:03,508 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.1744388836391546, 0.4598887591296168], [0.8132499754428864, 0.18675002], [2.75, 1.3226647836567116], [4.1219280948873624, 7.08857858466988, 7.246669699407458, 3.9638369801497833, 3.1247416045200955, 0.1580911147375783]]\n",
      "2021-01-14 20:54:03,511 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:54:03,513 : INFO : built Dictionary(143 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 527 corpus positions)\n",
      "2021-01-14 20:54:03,568 : INFO : token count processed\n",
      "2021-01-14 20:54:03,600 : INFO : frequencies processed\n",
      "2021-01-14 20:54:13,007 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:54:13,008 : INFO : entropies processed\n",
      "2021-01-14 20:54:13,009 : INFO : extropies processed\n",
      "2021-01-14 20:54:13,016 : INFO : token count processed\n",
      "2021-01-14 20:54:13,021 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:54:13,026 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:54:13,027 : INFO : vocab #32006\n",
      "2021-01-14 20:54:13,034 : INFO : diff #set()\n",
      "2021-01-14 20:54:32,040 : INFO : alphabet #32006\n",
      "2021-01-14 20:54:41,644 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.1879905130702189, 0.45704037290215943], [0.8485182523727417, 0.15148175], [1.584962500721156, 1.1699250014423124], [4.1219280948873624, 6.0479231618016716, 6.234877157229581, 3.9349740994594526, 2.112949062342219, 0.1869539954279098]]\n",
      "2021-01-14 20:54:41,648 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:54:41,649 : INFO : built Dictionary(146 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 646 corpus positions)\n",
      "2021-01-14 20:54:41,713 : INFO : token count processed\n",
      "2021-01-14 20:54:41,749 : INFO : frequencies processed\n",
      "2021-01-14 20:54:51,258 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:54:51,259 : INFO : entropies processed\n",
      "2021-01-14 20:54:51,260 : INFO : extropies processed\n",
      "2021-01-14 20:54:51,267 : INFO : token count processed\n",
      "2021-01-14 20:54:51,274 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:54:51,279 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:54:51,280 : INFO : vocab #32006\n",
      "2021-01-14 20:54:51,287 : INFO : diff #set()\n",
      "2021-01-14 20:55:10,122 : INFO : alphabet #32006\n",
      "2021-01-14 20:55:19,534 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.2009314093211916, 0.4543530960414703], [0.8507259041070938, 0.1492741], [1.0, 1.0], [4.1219280948873624, 6.036583168403119, 6.204373698381965, 3.9541375649085166, 2.0824456034946026, 0.1677905299788458]]\n",
      "2021-01-14 20:55:19,547 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 20:55:19,548 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:55:19,552 : INFO : built Dictionary(556 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 7004 corpus positions)\n",
      "2021-01-14 20:55:19,892 : INFO : token count processed\n",
      "2021-01-14 20:55:19,924 : INFO : frequencies processed\n",
      "2021-01-14 20:55:29,439 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:55:29,440 : INFO : entropies processed\n",
      "2021-01-14 20:55:29,441 : INFO : extropies processed\n",
      "2021-01-14 20:55:29,450 : INFO : token count processed\n",
      "2021-01-14 20:55:29,456 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:55:29,462 : INFO : alphabet_target #32010\n",
      "2021-01-14 20:55:29,463 : INFO : vocab #32006\n",
      "2021-01-14 20:55:29,470 : INFO : diff #set()\n",
      "2021-01-14 20:55:48,322 : INFO : alphabet #32006\n",
      "2021-01-14 20:55:57,732 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.2035661263398316, 0.4538098439827719], [0.8469952046871185, 0.1530048], [2.75, 1.3226647836567116], [4.1219280948873624, 7.29352035514053, 7.448763607859744, 3.9666848421681475, 3.326835512972382, 0.15524325271921402]]\n",
      "2021-01-14 20:55:57,739 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:55:57,741 : INFO : built Dictionary(363 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 3247 corpus positions)\n",
      "2021-01-14 20:55:57,949 : INFO : token count processed\n",
      "2021-01-14 20:55:57,983 : INFO : frequencies processed\n",
      "2021-01-14 20:56:07,499 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:56:07,500 : INFO : entropies processed\n",
      "2021-01-14 20:56:07,501 : INFO : extropies processed\n",
      "2021-01-14 20:56:07,512 : INFO : token count processed\n",
      "2021-01-14 20:56:07,516 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:56:07,521 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:56:07,522 : INFO : vocab #32006\n",
      "2021-01-14 20:56:07,528 : INFO : diff #set()\n",
      "2021-01-14 20:56:26,393 : INFO : alphabet #32006\n",
      "2021-01-14 20:56:35,922 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.1675296545944711, 0.4613547029819496], [0.7990549802780151, 0.20094502], [3.121928094887362, 1.3519647487142497], [4.1219280948873624, 6.8153433747477745, 6.948425212178915, 3.9888462574562222, 2.8264971172915523, 0.13308183743114022]]\n",
      "2021-01-14 20:56:35,925 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:56:35,927 : INFO : built Dictionary(121 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 268 corpus positions)\n",
      "2021-01-14 20:56:35,977 : INFO : token count processed\n",
      "2021-01-14 20:56:36,011 : INFO : frequencies processed\n",
      "2021-01-14 20:56:45,441 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:56:45,443 : INFO : entropies processed\n",
      "2021-01-14 20:56:45,444 : INFO : extropies processed\n",
      "2021-01-14 20:56:45,455 : INFO : token count processed\n",
      "2021-01-14 20:56:45,459 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:56:45,464 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:56:45,465 : INFO : vocab #32006\n",
      "2021-01-14 20:56:45,471 : INFO : diff #set()\n",
      "2021-01-14 20:57:04,310 : INFO : alphabet #32006\n",
      "2021-01-14 20:57:13,796 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.1840697060960061, 0.4578608444633784], [0.8397666662931442, 0.16023333], [1.0, 1.0], [4.1219280948873624, 6.150121915859574, 6.351634930623446, 3.920415080123492, 2.229706835736083, 0.2015130147638713]]\n",
      "2021-01-14 20:57:13,800 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:57:13,802 : INFO : built Dictionary(284 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 979 corpus positions)\n",
      "2021-01-14 20:57:13,945 : INFO : token count processed\n",
      "2021-01-14 20:57:13,973 : INFO : frequencies processed\n",
      "2021-01-14 20:57:23,216 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:57:23,217 : INFO : entropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 20:57:23,218 : INFO : extropies processed\n",
      "2021-01-14 20:57:23,224 : INFO : token count processed\n",
      "2021-01-14 20:57:23,231 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:57:23,235 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:57:23,236 : INFO : vocab #32006\n",
      "2021-01-14 20:57:23,243 : INFO : diff #set()\n",
      "2021-01-14 20:57:42,320 : INFO : alphabet #32006\n",
      "2021-01-14 20:57:51,902 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.1756812498200802, 0.4596261516169686], [0.8139386624097824, 0.18606134], [1.9219280948873623, 1.2148067842293933], [4.1219280948873624, 7.0391145208191315, 7.272855114530577, 3.888187501175917, 3.1509270196432144, 0.2337405937114454]]\n",
      "2021-01-14 20:57:51,911 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 20:57:51,912 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:57:51,915 : INFO : built Dictionary(586 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 4354 corpus positions)\n",
      "2021-01-14 20:57:52,292 : INFO : token count processed\n",
      "2021-01-14 20:57:52,360 : INFO : frequencies processed\n",
      "2021-01-14 20:58:01,790 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:58:01,792 : INFO : entropies processed\n",
      "2021-01-14 20:58:01,792 : INFO : extropies processed\n",
      "2021-01-14 20:58:01,801 : INFO : token count processed\n",
      "2021-01-14 20:58:01,805 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:58:01,809 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:58:01,810 : INFO : vocab #32006\n",
      "2021-01-14 20:58:01,817 : INFO : diff #set()\n",
      "2021-01-14 20:58:20,863 : INFO : alphabet #32006\n",
      "2021-01-14 20:58:30,308 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.1886009979866519, 0.4569128867801508], [0.8142518848180771, 0.18574812], [2.725480556997868, 1.3192201298976014], [4.1219280948873624, 7.482466367279176, 7.7461254143790725, 3.8582690477874664, 3.62419731949171, 0.26365904709989696]]\n",
      "2021-01-14 20:58:30,312 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:58:30,313 : INFO : built Dictionary(169 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 576 corpus positions)\n",
      "2021-01-14 20:58:30,381 : INFO : token count processed\n",
      "2021-01-14 20:58:30,413 : INFO : frequencies processed\n",
      "2021-01-14 20:58:39,834 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:58:39,835 : INFO : entropies processed\n",
      "2021-01-14 20:58:39,836 : INFO : extropies processed\n",
      "2021-01-14 20:58:39,843 : INFO : token count processed\n",
      "2021-01-14 20:58:39,847 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:58:39,851 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:58:39,852 : INFO : vocab #32006\n",
      "2021-01-14 20:58:39,860 : INFO : diff #set()\n",
      "2021-01-14 20:58:58,811 : INFO : alphabet #32006\n",
      "2021-01-14 20:59:08,248 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.1889135096084011, 0.4568476532354634], [0.8435746282339096, 0.15642537], [1.584962500721156, 1.1699250014423124], [4.1219280948873624, 6.372162341197667, 6.561540865619937, 3.932549570465092, 2.439612770732575, 0.18937852442227054]]\n",
      "2021-01-14 20:59:08,254 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:59:08,257 : INFO : built Dictionary(312 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1997 corpus positions)\n",
      "2021-01-14 20:59:08,412 : INFO : token count processed\n",
      "2021-01-14 20:59:08,443 : INFO : frequencies processed\n",
      "2021-01-14 20:59:17,891 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:59:17,892 : INFO : entropies processed\n",
      "2021-01-14 20:59:17,893 : INFO : extropies processed\n",
      "2021-01-14 20:59:17,908 : INFO : token count processed\n",
      "2021-01-14 20:59:17,913 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:59:17,917 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:59:17,918 : INFO : vocab #32006\n",
      "2021-01-14 20:59:17,928 : INFO : diff #set()\n",
      "2021-01-14 20:59:36,959 : INFO : alphabet #32006\n",
      "2021-01-14 20:59:46,391 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.1692536552525683, 0.4609880442421424], [0.8155437409877777, 0.18445626], [3.121928094887362, 1.3519647487142497], [4.1219280948873624, 6.798155919669889, 6.97179570874694, 3.94828830581031, 2.849867613859578, 0.17363978907705135]]\n",
      "2021-01-14 20:59:46,395 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 20:59:46,397 : INFO : built Dictionary(174 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 713 corpus positions)\n",
      "2021-01-14 20:59:46,465 : INFO : token count processed\n",
      "2021-01-14 20:59:46,496 : INFO : frequencies processed\n",
      "2021-01-14 20:59:55,923 : INFO : scalar_distribution processed\n",
      "2021-01-14 20:59:55,924 : INFO : entropies processed\n",
      "2021-01-14 20:59:55,925 : INFO : extropies processed\n",
      "2021-01-14 20:59:55,932 : INFO : token count processed\n",
      "2021-01-14 20:59:55,939 : INFO : alphabet_source #32006\n",
      "2021-01-14 20:59:55,943 : INFO : alphabet_target #32009\n",
      "2021-01-14 20:59:55,944 : INFO : vocab #32006\n",
      "2021-01-14 20:59:55,951 : INFO : diff #set()\n",
      "2021-01-14 21:00:14,971 : INFO : alphabet #32006\n",
      "2021-01-14 21:00:24,456 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.186168550628836, 0.4574212723499096], [0.8426304310560226, 0.15736957], [1.584962500721156, 1.1699250014423124], [4.1219280948873624, 6.271631856729336, 6.4549824084941045, 3.9385775431225936, 2.333054313606742, 0.1833505517647689]]\n",
      "2021-01-14 21:00:24,463 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:00:24,465 : INFO : built Dictionary(346 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 3228 corpus positions)\n",
      "2021-01-14 21:00:24,656 : INFO : token count processed\n",
      "2021-01-14 21:00:24,689 : INFO : frequencies processed\n",
      "2021-01-14 21:00:34,230 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:00:34,231 : INFO : entropies processed\n",
      "2021-01-14 21:00:34,232 : INFO : extropies processed\n",
      "2021-01-14 21:00:34,240 : INFO : token count processed\n",
      "2021-01-14 21:00:34,247 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:00:34,251 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:00:34,252 : INFO : vocab #32006\n",
      "2021-01-14 21:00:34,260 : INFO : diff #set()\n",
      "2021-01-14 21:00:53,174 : INFO : alphabet #32006\n",
      "2021-01-14 21:01:02,610 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.155263514860207, 0.46398038713371026], [0.7896243333816528, 0.21037567], [3.095795255000934, 1.3487605247277434], [4.1219280948873624, 6.873598627629562, 7.015707878420742, 3.979818844096182, 2.89377978353338, 0.1421092507911803]]\n",
      "2021-01-14 21:01:02,613 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:01:02,614 : INFO : built Dictionary(100 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 176 corpus positions)\n",
      "2021-01-14 21:01:02,661 : INFO : token count processed\n",
      "2021-01-14 21:01:02,694 : INFO : frequencies processed\n",
      "2021-01-14 21:01:12,235 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:01:12,236 : INFO : entropies processed\n",
      "2021-01-14 21:01:12,237 : INFO : extropies processed\n",
      "2021-01-14 21:01:12,243 : INFO : token count processed\n",
      "2021-01-14 21:01:12,250 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:01:12,255 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:01:12,256 : INFO : vocab #32006\n",
      "2021-01-14 21:01:12,263 : INFO : diff #set()\n",
      "2021-01-14 21:01:31,168 : INFO : alphabet #32006\n",
      "2021-01-14 21:01:40,636 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.160773713184065, 0.46279718875625514], [0.8152991533279419, 0.18470085], [1.9219280948873623, 1.2148067842293933], [4.1219280948873624, 6.049830202851529, 6.306990048980601, 3.864768248758291, 2.1850619540932383, 0.2571598461290714]]\n",
      "2021-01-14 21:01:40,641 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:01:40,642 : INFO : built Dictionary(247 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1256 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 21:01:40,756 : INFO : token count processed\n",
      "2021-01-14 21:01:40,782 : INFO : frequencies processed\n",
      "2021-01-14 21:01:50,190 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:01:50,191 : INFO : entropies processed\n",
      "2021-01-14 21:01:50,192 : INFO : extropies processed\n",
      "2021-01-14 21:01:50,199 : INFO : token count processed\n",
      "2021-01-14 21:01:50,203 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:01:50,208 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:01:50,209 : INFO : vocab #32006\n",
      "2021-01-14 21:01:50,217 : INFO : diff #set()\n",
      "2021-01-14 21:02:09,097 : INFO : alphabet #32006\n",
      "2021-01-14 21:02:18,626 : INFO : Computed distances or similarities ('216', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.1812302665824135, 0.45845686964852916], [0.8199358731508255, 0.18006413], [2.235926350629032, 1.2653331222512112], [4.1219280948873624, 6.778844940588858, 6.962585604994169, 3.938187430482051, 2.840657510106807, 0.18374066440531145]]\n",
      "2021-01-14 21:02:18,630 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:02:18,631 : INFO : built Dictionary(147 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 289 corpus positions)\n",
      "2021-01-14 21:02:18,694 : INFO : token count processed\n",
      "2021-01-14 21:02:18,726 : INFO : frequencies processed\n",
      "2021-01-14 21:02:28,140 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:02:28,141 : INFO : entropies processed\n",
      "2021-01-14 21:02:28,142 : INFO : extropies processed\n",
      "2021-01-14 21:02:28,149 : INFO : token count processed\n",
      "2021-01-14 21:02:28,153 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:02:28,157 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:02:28,158 : INFO : vocab #32006\n",
      "2021-01-14 21:02:28,165 : INFO : diff #set()\n",
      "2021-01-14 21:02:47,068 : INFO : alphabet #32006\n",
      "2021-01-14 21:02:56,619 : INFO : Computed distances or similarities ('216', 'sacp-python-common/setup.py')[[1.1706989907072445, 0.4606811005491765], [0.8090151399374008, 0.19098486], [1.9219280948873623, 1.2148067842293933], [4.1219280948873624, 6.469677430851302, 6.727838848785083, 3.8637666769535812, 2.605910753897721, 0.2581614179337812]]\n",
      "2021-01-14 21:02:56,623 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:02:56,625 : INFO : built Dictionary(217 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1191 corpus positions)\n",
      "2021-01-14 21:02:56,714 : INFO : token count processed\n",
      "2021-01-14 21:02:56,746 : INFO : frequencies processed\n",
      "2021-01-14 21:03:06,167 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:03:06,169 : INFO : entropies processed\n",
      "2021-01-14 21:03:06,169 : INFO : extropies processed\n",
      "2021-01-14 21:03:06,177 : INFO : token count processed\n",
      "2021-01-14 21:03:06,183 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:03:06,188 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:03:06,189 : INFO : vocab #32006\n",
      "2021-01-14 21:03:06,196 : INFO : diff #set()\n",
      "2021-01-14 21:03:25,210 : INFO : alphabet #32006\n",
      "2021-01-14 21:03:35,023 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.1561141140917743, 0.4637973442427154], [0.8027228266000748, 0.19727717], [2.0, 1.2451124978365313], [4.1219280948873624, 6.459180448028249, 6.623102332677344, 3.958006210238268, 2.5011742377899813, 0.16392188464909463]]\n",
      "2021-01-14 21:03:35,026 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:03:35,028 : INFO : built Dictionary(128 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 341 corpus positions)\n",
      "2021-01-14 21:03:35,085 : INFO : token count processed\n",
      "2021-01-14 21:03:35,119 : INFO : frequencies processed\n",
      "2021-01-14 21:03:44,546 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:03:44,547 : INFO : entropies processed\n",
      "2021-01-14 21:03:44,548 : INFO : extropies processed\n",
      "2021-01-14 21:03:44,562 : INFO : token count processed\n",
      "2021-01-14 21:03:44,567 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:03:44,572 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:03:44,573 : INFO : vocab #32006\n",
      "2021-01-14 21:03:44,581 : INFO : diff #set()\n",
      "2021-01-14 21:04:03,575 : INFO : alphabet #32006\n",
      "2021-01-14 21:04:13,018 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.1743791354501807, 0.45990139607964975], [0.83011394739151, 0.16988605], [1.584962500721156, 1.1699250014423124], [4.1219280948873624, 6.097125733496388, 6.304088335591723, 3.9149654927920263, 2.182160240704361, 0.20696260209533524]]\n",
      "2021-01-14 21:04:13,021 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:04:13,022 : INFO : built Dictionary(119 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 337 corpus positions)\n",
      "2021-01-14 21:04:13,067 : INFO : token count processed\n",
      "2021-01-14 21:04:13,099 : INFO : frequencies processed\n",
      "2021-01-14 21:04:22,519 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:04:22,520 : INFO : entropies processed\n",
      "2021-01-14 21:04:22,521 : INFO : extropies processed\n",
      "2021-01-14 21:04:22,528 : INFO : token count processed\n",
      "2021-01-14 21:04:22,533 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:04:22,539 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:04:22,540 : INFO : vocab #32006\n",
      "2021-01-14 21:04:22,547 : INFO : diff #set()\n",
      "2021-01-14 21:04:41,549 : INFO : alphabet #32006\n",
      "2021-01-14 21:04:50,991 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.1603340973622078, 0.46289136537770303], [0.8172132819890976, 0.18278672], [2.0, 1.2451124978365313], [4.1219280948873624, 6.0695858597523715, 6.260653844549853, 3.930860110089882, 2.1387257496624903, 0.19106798479748122]]\n",
      "2021-01-14 21:04:50,995 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 21:04:50,996 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:04:50,997 : INFO : built Dictionary(121 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 433 corpus positions)\n",
      "2021-01-14 21:04:51,042 : INFO : token count processed\n",
      "2021-01-14 21:04:51,074 : INFO : frequencies processed\n",
      "2021-01-14 21:05:00,499 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:05:00,500 : INFO : entropies processed\n",
      "2021-01-14 21:05:00,501 : INFO : extropies processed\n",
      "2021-01-14 21:05:00,508 : INFO : token count processed\n",
      "2021-01-14 21:05:00,512 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:05:00,516 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:05:00,517 : INFO : vocab #32006\n",
      "2021-01-14 21:05:00,526 : INFO : diff #set()\n",
      "2021-01-14 21:05:19,686 : INFO : alphabet #32006\n",
      "2021-01-14 21:05:29,118 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.133199523906618, 0.4687794033296323], [0.7814069986343384, 0.218593], [2.0, 1.2451124978365313], [4.1219280948873624, 6.104787343210121, 6.253940907328208, 3.972774530769274, 2.1320128124408457, 0.14915356411808744]]\n",
      "2021-01-14 21:05:29,134 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 21:05:29,135 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:05:29,139 : INFO : built Dictionary(407 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 9118 corpus positions)\n",
      "2021-01-14 21:05:29,371 : INFO : token count processed\n",
      "2021-01-14 21:05:29,412 : INFO : frequencies processed\n",
      "2021-01-14 21:05:39,235 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:05:39,236 : INFO : entropies processed\n",
      "2021-01-14 21:05:39,237 : INFO : extropies processed\n",
      "2021-01-14 21:05:39,246 : INFO : token count processed\n",
      "2021-01-14 21:05:39,252 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:05:39,257 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:05:39,259 : INFO : vocab #32006\n",
      "2021-01-14 21:05:39,267 : INFO : diff #set()\n",
      "2021-01-14 21:05:58,079 : INFO : alphabet #32006\n",
      "2021-01-14 21:06:07,653 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.1732044120048184, 0.46014999531382456], [0.8185143917798996, 0.18148561], [2.521640636343318, 1.2998438251349493], [4.1219280948873624, 6.89087415148015, 7.045985711743184, 3.9668165346243294, 2.924057616855822, 0.15511156026303397]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 21:06:07,659 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:06:07,661 : INFO : built Dictionary(265 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 2278 corpus positions)\n",
      "2021-01-14 21:06:07,792 : INFO : token count processed\n",
      "2021-01-14 21:06:07,827 : INFO : frequencies processed\n",
      "2021-01-14 21:06:17,260 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:06:17,261 : INFO : entropies processed\n",
      "2021-01-14 21:06:17,262 : INFO : extropies processed\n",
      "2021-01-14 21:06:17,269 : INFO : token count processed\n",
      "2021-01-14 21:06:17,274 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:06:17,279 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:06:17,280 : INFO : vocab #32006\n",
      "2021-01-14 21:06:17,287 : INFO : diff #set()\n",
      "2021-01-14 21:06:36,205 : INFO : alphabet #32006\n",
      "2021-01-14 21:06:45,758 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.1250554848312813, 0.47057594831666], [0.7679791301488876, 0.23202087], [2.584962500721156, 1.315172029168969], [4.1219280948873624, 6.655493573668506, 6.788798618817248, 3.988623049738621, 2.6668705239298856, 0.13330504514874164]]\n",
      "2021-01-14 21:06:45,763 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:06:45,765 : INFO : built Dictionary(244 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1285 corpus positions)\n",
      "2021-01-14 21:06:45,887 : INFO : token count processed\n",
      "2021-01-14 21:06:45,921 : INFO : frequencies processed\n",
      "2021-01-14 21:06:55,340 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:06:55,342 : INFO : entropies processed\n",
      "2021-01-14 21:06:55,342 : INFO : extropies processed\n",
      "2021-01-14 21:06:55,349 : INFO : token count processed\n",
      "2021-01-14 21:06:55,356 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:06:55,361 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:06:55,362 : INFO : vocab #32006\n",
      "2021-01-14 21:06:55,369 : INFO : diff #set()\n",
      "2021-01-14 21:07:14,280 : INFO : alphabet #32006\n",
      "2021-01-14 21:07:23,833 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.2042289880912809, 0.45367337304911093], [0.8495972454547882, 0.15040275], [2.0, 1.2451124978365313], [4.1219280948873624, 6.6236746347295465, 6.748495725532557, 3.997107004084352, 2.6265676306451944, 0.12482109080301029]]\n",
      "2021-01-14 21:07:23,838 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:07:23,841 : INFO : built Dictionary(244 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1224 corpus positions)\n",
      "2021-01-14 21:07:23,959 : INFO : token count processed\n",
      "2021-01-14 21:07:24,028 : INFO : frequencies processed\n",
      "2021-01-14 21:07:33,464 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:07:33,465 : INFO : entropies processed\n",
      "2021-01-14 21:07:33,466 : INFO : extropies processed\n",
      "2021-01-14 21:07:33,473 : INFO : token count processed\n",
      "2021-01-14 21:07:33,479 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:07:33,484 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:07:33,484 : INFO : vocab #32006\n",
      "2021-01-14 21:07:33,492 : INFO : diff #set()\n",
      "2021-01-14 21:07:52,508 : INFO : alphabet #32006\n",
      "2021-01-14 21:08:01,924 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.1435116185447067, 0.4665241799244034], [0.7690268009901047, 0.2309732], [2.321928094887362, 1.2877123795494492], [4.1219280948873624, 6.75472436518627, 6.87006784219877, 4.0065846178748625, 2.7481397473114075, 0.11534347701249992]]\n",
      "2021-01-14 21:08:01,929 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:08:01,930 : INFO : built Dictionary(198 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1019 corpus positions)\n",
      "2021-01-14 21:08:02,027 : INFO : token count processed\n",
      "2021-01-14 21:08:02,060 : INFO : frequencies processed\n",
      "2021-01-14 21:08:11,480 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:08:11,481 : INFO : entropies processed\n",
      "2021-01-14 21:08:11,482 : INFO : extropies processed\n",
      "2021-01-14 21:08:11,496 : INFO : token count processed\n",
      "2021-01-14 21:08:11,501 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:08:11,505 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:08:11,506 : INFO : vocab #32006\n",
      "2021-01-14 21:08:11,513 : INFO : diff #set()\n",
      "2021-01-14 21:08:30,662 : INFO : alphabet #32006\n",
      "2021-01-14 21:08:40,089 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.1765766529930586, 0.45943706996253864], [0.810537800192833, 0.1894622], [1.584962500721156, 1.1699250014423124], [4.1219280948873624, 6.597313085495733, 6.715311570670447, 4.003929609712649, 2.5933834757830843, 0.11799848517471379]]\n",
      "2021-01-14 21:08:40,094 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:08:40,095 : INFO : built Dictionary(221 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 985 corpus positions)\n",
      "2021-01-14 21:08:40,194 : INFO : token count processed\n",
      "2021-01-14 21:08:40,227 : INFO : frequencies processed\n",
      "2021-01-14 21:08:49,620 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:08:49,622 : INFO : entropies processed\n",
      "2021-01-14 21:08:49,622 : INFO : extropies processed\n",
      "2021-01-14 21:08:49,630 : INFO : token count processed\n",
      "2021-01-14 21:08:49,635 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:08:49,640 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:08:49,641 : INFO : vocab #32006\n",
      "2021-01-14 21:08:49,648 : INFO : diff #set()\n",
      "2021-01-14 21:09:08,632 : INFO : alphabet #32006\n",
      "2021-01-14 21:09:18,052 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.1443142922932976, 0.46634954754254876], [0.7827708870172501, 0.21722911], [2.321928094887362, 1.2877123795494492], [4.1219280948873624, 6.659481538516613, 6.7849956080276925, 3.996414025376282, 2.66306751314033, 0.12551406951107946]]\n",
      "2021-01-14 21:09:18,057 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:09:18,059 : INFO : built Dictionary(246 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1147 corpus positions)\n",
      "2021-01-14 21:09:18,173 : INFO : token count processed\n",
      "2021-01-14 21:09:18,233 : INFO : frequencies processed\n",
      "2021-01-14 21:09:27,642 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:09:27,643 : INFO : entropies processed\n",
      "2021-01-14 21:09:27,644 : INFO : extropies processed\n",
      "2021-01-14 21:09:27,651 : INFO : token count processed\n",
      "2021-01-14 21:09:27,655 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:09:27,660 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:09:27,661 : INFO : vocab #32006\n",
      "2021-01-14 21:09:27,668 : INFO : diff #set()\n",
      "2021-01-14 21:09:46,653 : INFO : alphabet #32006\n",
      "2021-01-14 21:09:56,040 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.1582258125928386, 0.4633435455016753], [0.8055999577045441, 0.19440004], [2.321928094887362, 1.2877123795494492], [4.1219280948873624, 6.774682571479102, 6.87498031374467, 4.021630352621793, 2.7530522188573077, 0.10029774226556842]]\n",
      "2021-01-14 21:09:56,053 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:09:56,057 : INFO : built Dictionary(424 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 7884 corpus positions)\n",
      "2021-01-14 21:09:56,297 : INFO : token count processed\n",
      "2021-01-14 21:09:56,350 : INFO : frequencies processed\n",
      "2021-01-14 21:10:05,883 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:10:05,884 : INFO : entropies processed\n",
      "2021-01-14 21:10:05,885 : INFO : extropies processed\n",
      "2021-01-14 21:10:05,894 : INFO : token count processed\n",
      "2021-01-14 21:10:05,900 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:10:05,906 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:10:05,907 : INFO : vocab #32006\n",
      "2021-01-14 21:10:05,914 : INFO : diff #set()\n",
      "2021-01-14 21:10:24,890 : INFO : alphabet #32006\n",
      "2021-01-14 21:10:34,298 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.1829298128708776, 0.458099932532806], [0.8320074826478958, 0.16799252], [2.2516291673878226, 1.2667563532600834], [4.1219280948873624, 6.839453716525233, 6.9859052786692, 3.9754765327433947, 2.863977183781838, 0.1464515621439677]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 21:10:34,304 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:10:34,306 : INFO : built Dictionary(323 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 2326 corpus positions)\n",
      "2021-01-14 21:10:34,471 : INFO : token count processed\n",
      "2021-01-14 21:10:34,532 : INFO : frequencies processed\n",
      "2021-01-14 21:10:44,216 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:10:44,217 : INFO : entropies processed\n",
      "2021-01-14 21:10:44,218 : INFO : extropies processed\n",
      "2021-01-14 21:10:44,225 : INFO : token count processed\n",
      "2021-01-14 21:10:44,231 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:10:44,236 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:10:44,237 : INFO : vocab #32006\n",
      "2021-01-14 21:10:44,244 : INFO : diff #set()\n",
      "2021-01-14 21:11:03,138 : INFO : alphabet #32006\n",
      "2021-01-14 21:11:12,709 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.1347174024128603, 0.46844608043655095], [0.7580366432666779, 0.24196336], [2.521640636343318, 1.2998438251349491], [4.1219280948873624, 6.86432793886027, 6.972638994133748, 4.0136170396138855, 2.850710899246385, 0.10831105527347784]]\n",
      "2021-01-14 21:11:12,713 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:11:12,714 : INFO : built Dictionary(159 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 528 corpus positions)\n",
      "2021-01-14 21:11:12,788 : INFO : token count processed\n",
      "2021-01-14 21:11:12,823 : INFO : frequencies processed\n",
      "2021-01-14 21:11:22,340 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:11:22,341 : INFO : entropies processed\n",
      "2021-01-14 21:11:22,342 : INFO : extropies processed\n",
      "2021-01-14 21:11:22,356 : INFO : token count processed\n",
      "2021-01-14 21:11:22,360 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:11:22,364 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:11:22,365 : INFO : vocab #32006\n",
      "2021-01-14 21:11:22,372 : INFO : diff #set()\n",
      "2021-01-14 21:11:41,256 : INFO : alphabet #32006\n",
      "2021-01-14 21:11:50,779 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.157205415587212, 0.46356271534196497], [0.7874330133199692, 0.21256699], [1.584962500721156, 1.1699250014423124], [4.1219280948873624, 6.431978396403875, 6.5782352262612775, 3.975671265029959, 2.456307131373915, 0.14625682985740251]]\n",
      "2021-01-14 21:11:50,783 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:11:50,785 : INFO : built Dictionary(218 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 682 corpus positions)\n",
      "2021-01-14 21:11:50,890 : INFO : token count processed\n",
      "2021-01-14 21:11:50,953 : INFO : frequencies processed\n",
      "2021-01-14 21:12:00,357 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:12:00,359 : INFO : entropies processed\n",
      "2021-01-14 21:12:00,359 : INFO : extropies processed\n",
      "2021-01-14 21:12:00,366 : INFO : token count processed\n",
      "2021-01-14 21:12:00,374 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:12:00,378 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:12:00,379 : INFO : vocab #32006\n",
      "2021-01-14 21:12:00,386 : INFO : diff #set()\n",
      "2021-01-14 21:12:19,231 : INFO : alphabet #32006\n",
      "2021-01-14 21:12:29,041 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/test_auth_utility.py')[[1.1987028324089173, 0.4548136224959476], [0.8306391835212708, 0.16936082], [1.5, 1.1225562489182657], [4.1219280948873624, 6.911818353685893, 7.042001625035464, 3.9917448235377915, 2.920073530148102, 0.13018327134957097]]\n",
      "2021-01-14 21:12:29,054 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:12:29,057 : INFO : built Dictionary(309 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 7212 corpus positions)\n",
      "2021-01-14 21:12:29,216 : INFO : token count processed\n",
      "2021-01-14 21:12:29,284 : INFO : frequencies processed\n",
      "2021-01-14 21:12:38,678 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:12:38,680 : INFO : entropies processed\n",
      "2021-01-14 21:12:38,680 : INFO : extropies processed\n",
      "2021-01-14 21:12:38,689 : INFO : token count processed\n",
      "2021-01-14 21:12:38,696 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:12:38,702 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:12:38,703 : INFO : vocab #32006\n",
      "2021-01-14 21:12:38,711 : INFO : diff #set()\n",
      "2021-01-14 21:12:57,685 : INFO : alphabet #32006\n",
      "2021-01-14 21:13:07,083 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.1842803607611063, 0.4578166878044688], [0.821738600730896, 0.1782614], [1.584962500721156, 1.1699250014423124], [4.1219280948873624, 6.363791471162389, 6.397756011371552, 4.0879635546782005, 2.2758279164841895, 0.03396454020916284]]\n",
      "2021-01-14 21:13:07,088 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:13:07,090 : INFO : built Dictionary(210 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1282 corpus positions)\n",
      "2021-01-14 21:13:07,199 : INFO : token count processed\n",
      "2021-01-14 21:13:07,235 : INFO : frequencies processed\n",
      "2021-01-14 21:13:16,656 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:13:16,657 : INFO : entropies processed\n",
      "2021-01-14 21:13:16,658 : INFO : extropies processed\n",
      "2021-01-14 21:13:16,665 : INFO : token count processed\n",
      "2021-01-14 21:13:16,670 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:13:16,676 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:13:16,677 : INFO : vocab #32006\n",
      "2021-01-14 21:13:16,684 : INFO : diff #set()\n",
      "2021-01-14 21:13:35,799 : INFO : alphabet #32006\n",
      "2021-01-14 21:13:45,203 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.1811418023265565, 0.45847546405893047], [0.84234619140625, 0.15765381], [1.584962500721156, 1.1699250014423124], [4.1219280948873624, 6.29000629755059, 6.504686862156181, 3.907247530281772, 2.382758767268818, 0.21468056460559026]]\n",
      "2021-01-14 21:13:45,214 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:13:45,215 : INFO : built Dictionary(219 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1235 corpus positions)\n",
      "2021-01-14 21:13:45,310 : INFO : token count processed\n",
      "2021-01-14 21:13:45,341 : INFO : frequencies processed\n",
      "2021-01-14 21:13:54,754 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:13:54,755 : INFO : entropies processed\n",
      "2021-01-14 21:13:54,756 : INFO : extropies processed\n",
      "2021-01-14 21:13:54,764 : INFO : token count processed\n",
      "2021-01-14 21:13:54,769 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:13:54,774 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:13:54,775 : INFO : vocab #32006\n",
      "2021-01-14 21:13:54,782 : INFO : diff #set()\n",
      "2021-01-14 21:14:13,966 : INFO : alphabet #32006\n",
      "2021-01-14 21:14:23,399 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.1625755786245415, 0.4624115845403322], [0.8210173398256302, 0.17898266], [2.0, 1.2451124978365313], [4.1219280948873624, 6.361621244785958, 6.558261459131893, 3.9252878805414273, 2.436333364244531, 0.19664021434593515]]\n",
      "2021-01-14 21:14:23,404 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:14:23,405 : INFO : built Dictionary(226 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1258 corpus positions)\n",
      "2021-01-14 21:14:23,509 : INFO : token count processed\n",
      "2021-01-14 21:14:23,576 : INFO : frequencies processed\n",
      "2021-01-14 21:14:33,284 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:14:33,286 : INFO : entropies processed\n",
      "2021-01-14 21:14:33,287 : INFO : extropies processed\n",
      "2021-01-14 21:14:33,298 : INFO : token count processed\n",
      "2021-01-14 21:14:33,304 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:14:33,308 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:14:33,309 : INFO : vocab #32006\n",
      "2021-01-14 21:14:33,316 : INFO : diff #set()\n",
      "2021-01-14 21:14:52,294 : INFO : alphabet #32006\n",
      "2021-01-14 21:15:01,712 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.1326127315762862, 0.4689083888479209], [0.7770148813724518, 0.22298512], [2.321928094887362, 1.2877123795494492], [4.1219280948873624, 6.620594433343389, 6.818176894854117, 3.9243456333766336, 2.696248799966755, 0.197582461510728]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 21:15:01,717 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:15:01,719 : INFO : built Dictionary(199 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1351 corpus positions)\n",
      "2021-01-14 21:15:01,814 : INFO : token count processed\n",
      "2021-01-14 21:15:01,865 : INFO : frequencies processed\n",
      "2021-01-14 21:15:11,405 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:15:11,406 : INFO : entropies processed\n",
      "2021-01-14 21:15:11,407 : INFO : extropies processed\n",
      "2021-01-14 21:15:11,414 : INFO : token count processed\n",
      "2021-01-14 21:15:11,420 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:15:11,424 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:15:11,425 : INFO : vocab #32006\n",
      "2021-01-14 21:15:11,432 : INFO : diff #set()\n",
      "2021-01-14 21:15:30,313 : INFO : alphabet #32006\n",
      "2021-01-14 21:15:39,735 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.0969833996556886, 0.47687549656530115], [0.7492466568946838, 0.25075334], [2.0, 1.2451124978365313], [4.1219280948873624, 6.207411496248084, 6.326618271304623, 4.002721319830824, 2.2046901764172606, 0.1192067750565391]]\n",
      "2021-01-14 21:15:39,738 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:15:39,740 : INFO : built Dictionary(142 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 255 corpus positions)\n",
      "2021-01-14 21:15:39,805 : INFO : token count processed\n",
      "2021-01-14 21:15:39,865 : INFO : frequencies processed\n",
      "2021-01-14 21:15:49,387 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:15:49,389 : INFO : entropies processed\n",
      "2021-01-14 21:15:49,390 : INFO : extropies processed\n",
      "2021-01-14 21:15:49,403 : INFO : token count processed\n",
      "2021-01-14 21:15:49,407 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:15:49,412 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:15:49,413 : INFO : vocab #32006\n",
      "2021-01-14 21:15:49,420 : INFO : diff #set()\n",
      "2021-01-14 21:16:08,595 : INFO : alphabet #32006\n",
      "2021-01-14 21:16:18,166 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.1986782131751934, 0.45481871517517913], [0.818752333521843, 0.18124767], [0.0, 0.0], [4.1219280948873624, 6.5805228788529595, 6.790120789996098, 3.912330183744225, 2.6681926951087354, 0.2095979111431383]]\n",
      "2021-01-14 21:16:18,171 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 21:16:18,172 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:16:18,176 : INFO : built Dictionary(222 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1216 corpus positions)\n",
      "2021-01-14 21:16:18,278 : INFO : token count processed\n",
      "2021-01-14 21:16:18,337 : INFO : frequencies processed\n",
      "2021-01-14 21:16:28,271 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:16:28,273 : INFO : entropies processed\n",
      "2021-01-14 21:16:28,273 : INFO : extropies processed\n",
      "2021-01-14 21:16:28,281 : INFO : token count processed\n",
      "2021-01-14 21:16:28,285 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:16:28,291 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:16:28,292 : INFO : vocab #32006\n",
      "2021-01-14 21:16:28,300 : INFO : diff #set()\n",
      "2021-01-14 21:16:47,186 : INFO : alphabet #32006\n",
      "2021-01-14 21:16:56,751 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.1917230163932293, 0.45626203335019605], [0.8377752006053925, 0.1622248], [1.9219280948873623, 1.2148067842293933], [4.1219280948873624, 6.422089779976135, 6.516693705106244, 4.027324169757254, 2.3947656102188812, 0.09460392513010873]]\n",
      "2021-01-14 21:16:56,756 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:16:56,758 : INFO : built Dictionary(233 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1498 corpus positions)\n",
      "2021-01-14 21:16:56,862 : INFO : token count processed\n",
      "2021-01-14 21:16:56,894 : INFO : frequencies processed\n",
      "2021-01-14 21:17:06,314 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:17:06,315 : INFO : entropies processed\n",
      "2021-01-14 21:17:06,316 : INFO : extropies processed\n",
      "2021-01-14 21:17:06,323 : INFO : token count processed\n",
      "2021-01-14 21:17:06,327 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:17:06,333 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:17:06,334 : INFO : vocab #32006\n",
      "2021-01-14 21:17:06,341 : INFO : diff #set()\n",
      "2021-01-14 21:17:25,334 : INFO : alphabet #32006\n",
      "2021-01-14 21:17:34,753 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.170725997507995, 0.4606753690461188], [0.8194454163312912, 0.18055458], [2.321928094887362, 1.2877123795494492], [4.1219280948873624, 6.485445644653597, 6.714466042381213, 3.892907697159746, 2.5925379474938506, 0.2290203977276164]]\n",
      "2021-01-14 21:17:34,758 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:17:34,759 : INFO : built Dictionary(209 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1384 corpus positions)\n",
      "2021-01-14 21:17:34,854 : INFO : token count processed\n",
      "2021-01-14 21:17:34,889 : INFO : frequencies processed\n",
      "2021-01-14 21:17:44,295 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:17:44,296 : INFO : entropies processed\n",
      "2021-01-14 21:17:44,297 : INFO : extropies processed\n",
      "2021-01-14 21:17:44,305 : INFO : token count processed\n",
      "2021-01-14 21:17:44,313 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:17:44,317 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:17:44,318 : INFO : vocab #32006\n",
      "2021-01-14 21:17:44,325 : INFO : diff #set()\n",
      "2021-01-14 21:18:03,350 : INFO : alphabet #32006\n",
      "2021-01-14 21:18:12,793 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.121545243546085, 0.47135454831429224], [0.7679494470357895, 0.23205055], [2.0, 1.2451124978365313], [4.1219280948873624, 6.2276600107346916, 6.347761637019152, 4.001826468602902, 2.22583354213179, 0.12010162628446075]]\n",
      "2021-01-14 21:18:12,798 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:18:12,800 : INFO : built Dictionary(200 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1582 corpus positions)\n",
      "2021-01-14 21:18:12,887 : INFO : token count processed\n",
      "2021-01-14 21:18:12,920 : INFO : frequencies processed\n",
      "2021-01-14 21:18:22,343 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:18:22,344 : INFO : entropies processed\n",
      "2021-01-14 21:18:22,345 : INFO : extropies processed\n",
      "2021-01-14 21:18:22,352 : INFO : token count processed\n",
      "2021-01-14 21:18:22,358 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:18:22,363 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:18:22,364 : INFO : vocab #32006\n",
      "2021-01-14 21:18:22,372 : INFO : diff #set()\n",
      "2021-01-14 21:18:41,284 : INFO : alphabet #32006\n",
      "2021-01-14 21:18:50,709 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.1770762146448737, 0.4593316454762337], [0.831116259098053, 0.16888374], [1.584962500721156, 1.1699250014423124], [4.1219280948873624, 6.253918170574241, 6.42770414711835, 3.948142118343254, 2.3057760522309874, 0.1737859765441092]]\n",
      "2021-01-14 21:18:50,713 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:18:50,715 : INFO : built Dictionary(172 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 679 corpus positions)\n",
      "2021-01-14 21:18:50,792 : INFO : token count processed\n",
      "2021-01-14 21:18:50,826 : INFO : frequencies processed\n",
      "2021-01-14 21:19:00,248 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:19:00,249 : INFO : entropies processed\n",
      "2021-01-14 21:19:00,250 : INFO : extropies processed\n",
      "2021-01-14 21:19:00,257 : INFO : token count processed\n",
      "2021-01-14 21:19:00,265 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:19:00,269 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:19:00,270 : INFO : vocab #32006\n",
      "2021-01-14 21:19:00,277 : INFO : diff #set()\n",
      "2021-01-14 21:19:19,260 : INFO : alphabet #32006\n",
      "2021-01-14 21:19:28,689 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.137209201366547, 0.4678999132890654], [0.7736854553222656, 0.22631454], [1.584962500721156, 1.1699250014423124], [4.1219280948873624, 6.374522245625576, 6.568857644339917, 3.927592696173021, 2.4469295494525545, 0.19433539871434125]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 21:19:28,694 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:19:28,696 : INFO : built Dictionary(292 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1961 corpus positions)\n",
      "2021-01-14 21:19:28,845 : INFO : token count processed\n",
      "2021-01-14 21:19:28,879 : INFO : frequencies processed\n",
      "2021-01-14 21:19:38,299 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:19:38,300 : INFO : entropies processed\n",
      "2021-01-14 21:19:38,301 : INFO : extropies processed\n",
      "2021-01-14 21:19:38,312 : INFO : token count processed\n",
      "2021-01-14 21:19:38,317 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:19:38,322 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:19:38,323 : INFO : vocab #32006\n",
      "2021-01-14 21:19:38,331 : INFO : diff #set()\n",
      "2021-01-14 21:19:57,364 : INFO : alphabet #32006\n",
      "2021-01-14 21:20:07,060 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.186115315673645, 0.45743241119549677], [0.8389413505792618, 0.16105865], [2.321928094887362, 1.2877123795494492], [4.1219280948873624, 6.731238669067808, 6.987625218296355, 3.8655415456588145, 2.8656971234089923, 0.2563865492285471]]\n",
      "2021-01-14 21:20:07,064 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:20:07,066 : INFO : built Dictionary(219 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1353 corpus positions)\n",
      "2021-01-14 21:20:07,171 : INFO : token count processed\n",
      "2021-01-14 21:20:07,206 : INFO : frequencies processed\n",
      "2021-01-14 21:20:16,747 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:20:16,748 : INFO : entropies processed\n",
      "2021-01-14 21:20:16,749 : INFO : extropies processed\n",
      "2021-01-14 21:20:16,757 : INFO : token count processed\n",
      "2021-01-14 21:20:16,763 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:20:16,768 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:20:16,769 : INFO : vocab #32006\n",
      "2021-01-14 21:20:16,776 : INFO : diff #set()\n",
      "2021-01-14 21:20:35,657 : INFO : alphabet #32006\n",
      "2021-01-14 21:20:45,080 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.1656973197964524, 0.4617450420513921], [0.8156350702047348, 0.18436493], [1.584962500721156, 1.1699250014423124], [4.1219280948873624, 6.503741451859337, 6.683596785724532, 3.9420727610221675, 2.5616686908371697, 0.17985533386519492]]\n",
      "2021-01-14 21:20:45,085 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:20:45,087 : INFO : built Dictionary(238 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 1675 corpus positions)\n",
      "2021-01-14 21:20:45,213 : INFO : token count processed\n",
      "2021-01-14 21:20:45,245 : INFO : frequencies processed\n",
      "2021-01-14 21:20:54,773 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:20:54,775 : INFO : entropies processed\n",
      "2021-01-14 21:20:54,775 : INFO : extropies processed\n",
      "2021-01-14 21:20:54,783 : INFO : token count processed\n",
      "2021-01-14 21:20:54,788 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:20:54,793 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:20:54,794 : INFO : vocab #32006\n",
      "2021-01-14 21:20:54,801 : INFO : diff #set()\n",
      "2021-01-14 21:21:13,659 : INFO : alphabet #32006\n",
      "2021-01-14 21:21:23,059 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.1711206568199695, 0.46059162896303313], [0.8197339475154877, 0.18026605], [2.584962500721156, 1.315172029168969], [4.1219280948873624, 6.334729224484471, 6.454725509142396, 4.001931810229438, 2.332797414255033, 0.11999628465792433]]\n",
      "2021-01-14 21:21:23,064 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:21:23,066 : INFO : built Dictionary(236 unique tokens: ['-', '.', '/', '4:', '74']...) from 2 documents (total 2024 corpus positions)\n",
      "2021-01-14 21:21:23,176 : INFO : token count processed\n",
      "2021-01-14 21:21:23,207 : INFO : frequencies processed\n",
      "2021-01-14 21:21:32,737 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:21:32,738 : INFO : entropies processed\n",
      "2021-01-14 21:21:32,739 : INFO : extropies processed\n",
      "2021-01-14 21:21:32,746 : INFO : token count processed\n",
      "2021-01-14 21:21:32,752 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:21:32,756 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:21:32,758 : INFO : vocab #32006\n",
      "2021-01-14 21:21:32,764 : INFO : diff #set()\n",
      "2021-01-14 21:21:51,611 : INFO : alphabet #32006\n",
      "2021-01-14 21:22:01,139 : INFO : Computed distances or similarities ('216', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.18458769596068, 0.4577522806015103], [0.8361243158578873, 0.16387568], [2.321928094887362, 1.2877123795494492], [4.1219280948873624, 6.21319712067992, 6.397369362218086, 3.9377558533491968, 2.2754412673307236, 0.1841722415381657]]\n",
      "2021-01-14 21:22:01,143 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 21:22:01,144 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:22:01,146 : INFO : built Dictionary(271 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1409 corpus positions)\n",
      "2021-01-14 21:22:01,272 : INFO : token count processed\n",
      "2021-01-14 21:22:01,333 : INFO : frequencies processed\n",
      "2021-01-14 21:22:10,741 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:22:10,742 : INFO : entropies processed\n",
      "2021-01-14 21:22:10,743 : INFO : extropies processed\n",
      "2021-01-14 21:22:10,750 : INFO : token count processed\n",
      "2021-01-14 21:22:10,758 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:22:10,762 : INFO : alphabet_target #32010\n",
      "2021-01-14 21:22:10,763 : INFO : vocab #32006\n",
      "2021-01-14 21:22:10,770 : INFO : diff #set()\n",
      "2021-01-14 21:22:29,808 : INFO : alphabet #32006\n",
      "2021-01-14 21:22:39,313 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.1359454759977876, 0.4681767447892644], [0.6492801606655121, 0.35071984], [2.2359263506290326, 1.2653331222512112], [3.94770277922009, 6.905617163738059, 7.0215085364537035, 3.8318114065044444, 3.0738057572336137, 0.11589137271564454]]\n",
      "2021-01-14 21:22:39,319 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:22:39,321 : INFO : built Dictionary(361 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 2299 corpus positions)\n",
      "2021-01-14 21:22:39,499 : INFO : token count processed\n",
      "2021-01-14 21:22:39,526 : INFO : frequencies processed\n",
      "2021-01-14 21:22:48,888 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:22:48,889 : INFO : entropies processed\n",
      "2021-01-14 21:22:48,890 : INFO : extropies processed\n",
      "2021-01-14 21:22:48,898 : INFO : token count processed\n",
      "2021-01-14 21:22:48,902 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:22:48,907 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:22:48,908 : INFO : vocab #32006\n",
      "2021-01-14 21:22:48,915 : INFO : diff #set()\n",
      "2021-01-14 21:23:08,291 : INFO : alphabet #32006\n",
      "2021-01-14 21:23:17,720 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.198335791135631, 0.4548895596534019], [0.7481960356235504, 0.25180396], [2.5, 1.2968140217166513], [3.94770277922009, 7.1219284286457345, 7.328793685696888, 3.7408375221689374, 3.381090906476798, 0.20686525705115333]]\n",
      "2021-01-14 21:23:17,726 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:23:17,728 : INFO : built Dictionary(282 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 2282 corpus positions)\n",
      "2021-01-14 21:23:17,850 : INFO : token count processed\n",
      "2021-01-14 21:23:17,882 : INFO : frequencies processed\n",
      "2021-01-14 21:23:27,285 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:23:27,287 : INFO : entropies processed\n",
      "2021-01-14 21:23:27,287 : INFO : extropies processed\n",
      "2021-01-14 21:23:27,295 : INFO : token count processed\n",
      "2021-01-14 21:23:27,300 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:23:27,305 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:23:27,306 : INFO : vocab #32006\n",
      "2021-01-14 21:23:27,313 : INFO : diff #set()\n",
      "2021-01-14 21:23:46,315 : INFO : alphabet #32006\n",
      "2021-01-14 21:23:55,755 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2456509482779414, 0.445305180115744], [0.8482479304075241, 0.15175207], [1.9182958340544896, 1.2183406773511978], [3.94770277922009, 6.41099024988467, 6.477341165276699, 3.881351863828061, 2.529638386056609, 0.06635091539202875]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 21:23:55,759 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:23:55,761 : INFO : built Dictionary(161 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 681 corpus positions)\n",
      "2021-01-14 21:23:55,820 : INFO : token count processed\n",
      "2021-01-14 21:23:55,852 : INFO : frequencies processed\n",
      "2021-01-14 21:24:05,269 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:24:05,270 : INFO : entropies processed\n",
      "2021-01-14 21:24:05,271 : INFO : extropies processed\n",
      "2021-01-14 21:24:05,277 : INFO : token count processed\n",
      "2021-01-14 21:24:05,282 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:24:05,288 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:24:05,289 : INFO : vocab #32006\n",
      "2021-01-14 21:24:05,296 : INFO : diff #set()\n",
      "2021-01-14 21:24:24,304 : INFO : alphabet #32006\n",
      "2021-01-14 21:24:33,739 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.218273824125291, 0.45080097376811434], [0.8007653653621674, 0.19923463], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 6.077866832717642, 6.210142369882693, 3.815427242055039, 2.262439590662603, 0.13227553716505103]]\n",
      "2021-01-14 21:24:33,742 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:24:33,744 : INFO : built Dictionary(137 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 446 corpus positions)\n",
      "2021-01-14 21:24:33,799 : INFO : token count processed\n",
      "2021-01-14 21:24:33,833 : INFO : frequencies processed\n",
      "2021-01-14 21:24:43,256 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:24:43,257 : INFO : entropies processed\n",
      "2021-01-14 21:24:43,258 : INFO : extropies processed\n",
      "2021-01-14 21:24:43,265 : INFO : token count processed\n",
      "2021-01-14 21:24:43,272 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:24:43,276 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:24:43,277 : INFO : vocab #32006\n",
      "2021-01-14 21:24:43,284 : INFO : diff #set()\n",
      "2021-01-14 21:25:02,293 : INFO : alphabet #32006\n",
      "2021-01-14 21:25:11,731 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.2183183962681141, 0.4507919159315921], [0.8028639703989029, 0.19713603], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 5.977547459003844, 6.0989001148028334, 3.8263501234211015, 2.1511973355827436, 0.12135265579898924]]\n",
      "2021-01-14 21:25:11,736 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:25:11,740 : INFO : built Dictionary(241 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 2157 corpus positions)\n",
      "2021-01-14 21:25:11,843 : INFO : token count processed\n",
      "2021-01-14 21:25:11,875 : INFO : frequencies processed\n",
      "2021-01-14 21:25:21,287 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:25:21,289 : INFO : entropies processed\n",
      "2021-01-14 21:25:21,290 : INFO : extropies processed\n",
      "2021-01-14 21:25:21,297 : INFO : token count processed\n",
      "2021-01-14 21:25:21,304 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:25:21,308 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:25:21,309 : INFO : vocab #32006\n",
      "2021-01-14 21:25:21,317 : INFO : diff #set()\n",
      "2021-01-14 21:25:40,331 : INFO : alphabet #32006\n",
      "2021-01-14 21:25:49,763 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.2312683957992343, 0.4481755766731966], [0.8170160055160522, 0.182984], [1.9182958340544896, 1.2183406773511978], [3.94770277922009, 6.4614394051846435, 6.557266933470348, 3.851875250934384, 2.6095641542502586, 0.09582752828570484]]\n",
      "2021-01-14 21:25:49,767 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 21:25:49,768 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:25:49,770 : INFO : built Dictionary(204 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1381 corpus positions)\n",
      "2021-01-14 21:25:49,860 : INFO : token count processed\n",
      "2021-01-14 21:25:49,892 : INFO : frequencies processed\n",
      "2021-01-14 21:25:59,442 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:25:59,443 : INFO : entropies processed\n",
      "2021-01-14 21:25:59,444 : INFO : extropies processed\n",
      "2021-01-14 21:25:59,451 : INFO : token count processed\n",
      "2021-01-14 21:25:59,456 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:25:59,461 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:25:59,462 : INFO : vocab #32006\n",
      "2021-01-14 21:25:59,469 : INFO : diff #set()\n",
      "2021-01-14 21:26:18,362 : INFO : alphabet #32006\n",
      "2021-01-14 21:26:27,922 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.1817095353586817, 0.45835615777129385], [0.7277335822582245, 0.27226642], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 6.327195724598159, 6.428733269350916, 3.8461652344673336, 2.481030490130826, 0.10153754475275711]]\n",
      "2021-01-14 21:26:27,934 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 21:26:27,935 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:26:27,938 : INFO : built Dictionary(416 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 6273 corpus positions)\n",
      "2021-01-14 21:26:28,137 : INFO : token count processed\n",
      "2021-01-14 21:26:28,169 : INFO : frequencies processed\n",
      "2021-01-14 21:26:37,584 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:26:37,585 : INFO : entropies processed\n",
      "2021-01-14 21:26:37,586 : INFO : extropies processed\n",
      "2021-01-14 21:26:37,594 : INFO : token count processed\n",
      "2021-01-14 21:26:37,599 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:26:37,603 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:26:37,604 : INFO : vocab #32006\n",
      "2021-01-14 21:26:37,610 : INFO : diff #set()\n",
      "2021-01-14 21:26:56,406 : INFO : alphabet #32006\n",
      "2021-01-14 21:27:05,963 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.2190292033232848, 0.45064751671693637], [0.7886801660060883, 0.21131983], [2.725480556997868, 1.3192201298976014], [3.94770277922009, 6.9079058562486315, 7.01121740094832, 3.844391234520401, 3.0635146217282303, 0.10331154469968862]]\n",
      "2021-01-14 21:27:05,970 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:27:05,972 : INFO : built Dictionary(320 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 2667 corpus positions)\n",
      "2021-01-14 21:27:06,116 : INFO : token count processed\n",
      "2021-01-14 21:27:06,148 : INFO : frequencies processed\n",
      "2021-01-14 21:27:15,571 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:27:15,572 : INFO : entropies processed\n",
      "2021-01-14 21:27:15,573 : INFO : extropies processed\n",
      "2021-01-14 21:27:15,580 : INFO : token count processed\n",
      "2021-01-14 21:27:15,585 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:27:15,589 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:27:15,590 : INFO : vocab #32006\n",
      "2021-01-14 21:27:15,599 : INFO : diff #set()\n",
      "2021-01-14 21:27:34,499 : INFO : alphabet #32006\n",
      "2021-01-14 21:27:44,063 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.1777667303045003, 0.4591860028370337], [0.7310774624347687, 0.26892254], [2.2359263506290326, 1.2653331222512112], [3.94770277922009, 6.61034830706307, 6.757943309097719, 3.800107777185442, 2.810240529877629, 0.14759500203464881]]\n",
      "2021-01-14 21:27:44,067 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:27:44,068 : INFO : built Dictionary(205 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 714 corpus positions)\n",
      "2021-01-14 21:27:44,148 : INFO : token count processed\n",
      "2021-01-14 21:27:44,180 : INFO : frequencies processed\n",
      "2021-01-14 21:27:53,597 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:27:53,599 : INFO : entropies processed\n",
      "2021-01-14 21:27:53,600 : INFO : extropies processed\n",
      "2021-01-14 21:27:53,607 : INFO : token count processed\n",
      "2021-01-14 21:27:53,611 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:27:53,616 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:27:53,618 : INFO : vocab #32006\n",
      "2021-01-14 21:27:53,625 : INFO : diff #set()\n",
      "2021-01-14 21:28:12,641 : INFO : alphabet #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 21:28:22,084 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.1914770921450195, 0.456313234386219], [0.7887708246707916, 0.21122918], [2.5, 1.2968140217166513], [3.94770277922009, 6.616715366949855, 6.730601297365444, 3.833816848804501, 2.7828985181453545, 0.1138859304155897]]\n",
      "2021-01-14 21:28:22,093 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:28:22,095 : INFO : built Dictionary(424 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 2748 corpus positions)\n",
      "2021-01-14 21:28:22,314 : INFO : token count processed\n",
      "2021-01-14 21:28:22,357 : INFO : frequencies processed\n",
      "2021-01-14 21:28:31,768 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:28:31,769 : INFO : entropies processed\n",
      "2021-01-14 21:28:31,770 : INFO : extropies processed\n",
      "2021-01-14 21:28:31,778 : INFO : token count processed\n",
      "2021-01-14 21:28:31,782 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:28:31,788 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:28:31,789 : INFO : vocab #32006\n",
      "2021-01-14 21:28:31,797 : INFO : diff #set()\n",
      "2021-01-14 21:28:50,981 : INFO : alphabet #32006\n",
      "2021-01-14 21:29:00,413 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.1955509543250016, 0.45546654156675637], [0.7347922027111053, 0.2652078], [2.725480556997868, 1.3192201298976014], [3.94770277922009, 7.32185870753746, 7.512199755550065, 3.7573617312074843, 3.564496976329975, 0.19034104801260465]]\n",
      "2021-01-14 21:29:00,416 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:29:00,417 : INFO : built Dictionary(61 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 113 corpus positions)\n",
      "2021-01-14 21:29:00,438 : INFO : token count processed\n",
      "2021-01-14 21:29:00,470 : INFO : frequencies processed\n",
      "2021-01-14 21:29:09,899 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:29:09,900 : INFO : entropies processed\n",
      "2021-01-14 21:29:09,901 : INFO : extropies processed\n",
      "2021-01-14 21:29:09,912 : INFO : token count processed\n",
      "2021-01-14 21:29:09,917 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:29:09,922 : INFO : alphabet_target #32008\n",
      "2021-01-14 21:29:09,924 : INFO : vocab #32006\n",
      "2021-01-14 21:29:09,932 : INFO : diff #set()\n",
      "2021-01-14 21:29:28,940 : INFO : alphabet #32006\n",
      "2021-01-14 21:29:38,385 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/fireException.py')[[1.1369619431693572, 0.4679540518708941], [0.7144605219364166, 0.28553948], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 5.176618657501385, 5.552778408576105, 3.5715430281453697, 1.6050756293560156, 0.3761597510747201]]\n",
      "2021-01-14 21:29:38,389 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:29:38,391 : INFO : built Dictionary(157 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 506 corpus positions)\n",
      "2021-01-14 21:29:38,459 : INFO : token count processed\n",
      "2021-01-14 21:29:38,529 : INFO : frequencies processed\n",
      "2021-01-14 21:29:47,936 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:29:47,937 : INFO : entropies processed\n",
      "2021-01-14 21:29:47,938 : INFO : extropies processed\n",
      "2021-01-14 21:29:47,945 : INFO : token count processed\n",
      "2021-01-14 21:29:47,949 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:29:47,955 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:29:47,956 : INFO : vocab #32006\n",
      "2021-01-14 21:29:47,964 : INFO : diff #set()\n",
      "2021-01-14 21:30:06,979 : INFO : alphabet #32006\n",
      "2021-01-14 21:30:16,419 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.2100861512202878, 0.4524710493515626], [0.7763113081455231, 0.22368869], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 6.468846789852156, 6.6377298150075035, 3.7788197540647417, 2.6900270357874136, 0.16888302515534726]]\n",
      "2021-01-14 21:30:16,425 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:30:16,430 : INFO : built Dictionary(373 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 2555 corpus positions)\n",
      "2021-01-14 21:30:16,612 : INFO : token count processed\n",
      "2021-01-14 21:30:16,645 : INFO : frequencies processed\n",
      "2021-01-14 21:30:26,194 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:30:26,195 : INFO : entropies processed\n",
      "2021-01-14 21:30:26,196 : INFO : extropies processed\n",
      "2021-01-14 21:30:26,204 : INFO : token count processed\n",
      "2021-01-14 21:30:26,209 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:30:26,213 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:30:26,214 : INFO : vocab #32006\n",
      "2021-01-14 21:30:26,222 : INFO : diff #set()\n",
      "2021-01-14 21:30:45,073 : INFO : alphabet #32006\n",
      "2021-01-14 21:30:54,484 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.2307682762012537, 0.4482760538906744], [0.8110213875770569, 0.18897861], [1.9182958340544896, 1.2183406773511978], [3.94770277922009, 6.957796704012729, 7.091449613415368, 3.8140498698174516, 3.1437468341952783, 0.13365290940263908]]\n",
      "2021-01-14 21:30:54,491 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:30:54,493 : INFO : built Dictionary(283 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 3052 corpus positions)\n",
      "2021-01-14 21:30:54,619 : INFO : token count processed\n",
      "2021-01-14 21:30:54,651 : INFO : frequencies processed\n",
      "2021-01-14 21:31:03,994 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:31:03,995 : INFO : entropies processed\n",
      "2021-01-14 21:31:03,996 : INFO : extropies processed\n",
      "2021-01-14 21:31:04,003 : INFO : token count processed\n",
      "2021-01-14 21:31:04,010 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:31:04,015 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:31:04,015 : INFO : vocab #32006\n",
      "2021-01-14 21:31:04,022 : INFO : diff #set()\n",
      "2021-01-14 21:31:22,989 : INFO : alphabet #32006\n",
      "2021-01-14 21:31:32,551 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.23031699295529, 0.44836675824943895], [0.8320820778608322, 0.16791792], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 6.441859572014148, 6.583554602874242, 3.8060077483599954, 2.6358518236541526, 0.14169503086009438]]\n",
      "2021-01-14 21:31:32,556 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:31:32,558 : INFO : built Dictionary(307 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1528 corpus positions)\n",
      "2021-01-14 21:31:32,687 : INFO : token count processed\n",
      "2021-01-14 21:31:32,719 : INFO : frequencies processed\n",
      "2021-01-14 21:31:42,141 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:31:42,142 : INFO : entropies processed\n",
      "2021-01-14 21:31:42,143 : INFO : extropies processed\n",
      "2021-01-14 21:31:42,150 : INFO : token count processed\n",
      "2021-01-14 21:31:42,154 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:31:42,159 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:31:42,160 : INFO : vocab #32006\n",
      "2021-01-14 21:31:42,167 : INFO : diff #set()\n",
      "2021-01-14 21:32:01,066 : INFO : alphabet #32006\n",
      "2021-01-14 21:32:10,620 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.190632912135592, 0.4564890787772953], [0.7434216439723969, 0.25657836], [2.2359263506290326, 1.2653331222512112], [3.94770277922009, 6.998955278238291, 7.1484131416445, 3.7982449158138802, 3.2007103624244104, 0.1494578634062096]]\n",
      "2021-01-14 21:32:10,625 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:32:10,627 : INFO : built Dictionary(222 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1347 corpus positions)\n",
      "2021-01-14 21:32:10,722 : INFO : token count processed\n",
      "2021-01-14 21:32:10,754 : INFO : frequencies processed\n",
      "2021-01-14 21:32:20,186 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:32:20,190 : INFO : entropies processed\n",
      "2021-01-14 21:32:20,191 : INFO : extropies processed\n",
      "2021-01-14 21:32:20,200 : INFO : token count processed\n",
      "2021-01-14 21:32:20,204 : INFO : alphabet_source #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 21:32:20,208 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:32:20,209 : INFO : vocab #32006\n",
      "2021-01-14 21:32:20,215 : INFO : diff #set()\n",
      "2021-01-14 21:32:39,109 : INFO : alphabet #32006\n",
      "2021-01-14 21:32:48,660 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.2093034341785778, 0.45263135182325076], [0.7904177010059357, 0.2095823], [2.2359263506290326, 1.2653331222512112], [3.94770277922009, 6.492983191376071, 6.59474650523282, 3.8459394653633403, 2.64704372601273, 0.10176331385674864]]\n",
      "2021-01-14 21:32:48,667 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:32:48,670 : INFO : built Dictionary(432 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 3286 corpus positions)\n",
      "2021-01-14 21:32:48,896 : INFO : token count processed\n",
      "2021-01-14 21:32:48,929 : INFO : frequencies processed\n",
      "2021-01-14 21:32:58,349 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:32:58,350 : INFO : entropies processed\n",
      "2021-01-14 21:32:58,351 : INFO : extropies processed\n",
      "2021-01-14 21:32:58,359 : INFO : token count processed\n",
      "2021-01-14 21:32:58,363 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:32:58,368 : INFO : alphabet_target #32008\n",
      "2021-01-14 21:32:58,369 : INFO : vocab #32006\n",
      "2021-01-14 21:32:58,377 : INFO : diff #set()\n",
      "2021-01-14 21:33:17,703 : INFO : alphabet #32006\n",
      "2021-01-14 21:33:27,141 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.2566481841267223, 0.44313509169661736], [0.8744122684001923, 0.12558773], [1.9219280948873623, 1.2148067842293933], [3.94770277922009, 6.560342487747443, 6.746132908093196, 3.7619123588743353, 2.7984301288731066, 0.18579042034575366]]\n",
      "2021-01-14 21:33:27,149 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 21:33:27,150 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:33:27,152 : INFO : built Dictionary(446 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 3491 corpus positions)\n",
      "2021-01-14 21:33:27,387 : INFO : token count processed\n",
      "2021-01-14 21:33:27,420 : INFO : frequencies processed\n",
      "2021-01-14 21:33:36,834 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:33:36,835 : INFO : entropies processed\n",
      "2021-01-14 21:33:36,836 : INFO : extropies processed\n",
      "2021-01-14 21:33:36,844 : INFO : token count processed\n",
      "2021-01-14 21:33:36,848 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:33:36,852 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:33:36,852 : INFO : vocab #32006\n",
      "2021-01-14 21:33:36,859 : INFO : diff #set()\n",
      "2021-01-14 21:33:55,891 : INFO : alphabet #32006\n",
      "2021-01-14 21:34:05,301 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.1739031565604414, 0.46000209208132536], [0.713381439447403, 0.28661856], [2.725480556997868, 1.3192201298976014], [3.94770277922009, 7.046173750105238, 7.224364677811342, 3.769511851513985, 3.276661898591252, 0.17819092770610379]]\n",
      "2021-01-14 21:34:05,312 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 21:34:05,313 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:34:05,320 : INFO : built Dictionary(502 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 5600 corpus positions)\n",
      "2021-01-14 21:34:05,591 : INFO : token count processed\n",
      "2021-01-14 21:34:05,624 : INFO : frequencies processed\n",
      "2021-01-14 21:34:15,045 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:34:15,046 : INFO : entropies processed\n",
      "2021-01-14 21:34:15,047 : INFO : extropies processed\n",
      "2021-01-14 21:34:15,055 : INFO : token count processed\n",
      "2021-01-14 21:34:15,060 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:34:15,064 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:34:15,065 : INFO : vocab #32006\n",
      "2021-01-14 21:34:15,074 : INFO : diff #set()\n",
      "2021-01-14 21:34:34,060 : INFO : alphabet #32006\n",
      "2021-01-14 21:34:43,479 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.2085700268085526, 0.452781658657674], [0.7732341289520264, 0.22676587], [2.2359263506290326, 1.2653331222512112], [3.94770277922009, 7.009229588004272, 7.112172551443706, 3.8447598157806553, 3.164469772223616, 0.10294296343943365]]\n",
      "2021-01-14 21:34:43,492 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:34:43,495 : INFO : built Dictionary(580 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 6541 corpus positions)\n",
      "2021-01-14 21:34:43,860 : INFO : token count processed\n",
      "2021-01-14 21:34:43,931 : INFO : frequencies processed\n",
      "2021-01-14 21:34:53,487 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:34:53,488 : INFO : entropies processed\n",
      "2021-01-14 21:34:53,489 : INFO : extropies processed\n",
      "2021-01-14 21:34:53,498 : INFO : token count processed\n",
      "2021-01-14 21:34:53,504 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:34:53,510 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:34:53,511 : INFO : vocab #32006\n",
      "2021-01-14 21:34:53,519 : INFO : diff #set()\n",
      "2021-01-14 21:35:12,525 : INFO : alphabet #32006\n",
      "2021-01-14 21:35:21,853 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.1740454618317557, 0.4599719819830464], [0.6934414505958557, 0.30655855], [2.5, 1.2968140217166513], [3.94770277922009, 7.376088004590871, 7.535362270568297, 3.788428513242663, 3.587659491348207, 0.15927426597742578]]\n",
      "2021-01-14 21:35:21,856 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:35:21,858 : INFO : built Dictionary(133 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 327 corpus positions)\n",
      "2021-01-14 21:35:21,912 : INFO : token count processed\n",
      "2021-01-14 21:35:21,941 : INFO : frequencies processed\n",
      "2021-01-14 21:35:31,476 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:35:31,477 : INFO : entropies processed\n",
      "2021-01-14 21:35:31,478 : INFO : extropies processed\n",
      "2021-01-14 21:35:31,485 : INFO : token count processed\n",
      "2021-01-14 21:35:31,490 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:35:31,494 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:35:31,495 : INFO : vocab #32006\n",
      "2021-01-14 21:35:31,501 : INFO : diff #set()\n",
      "2021-01-14 21:35:50,526 : INFO : alphabet #32006\n",
      "2021-01-14 21:35:59,955 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.1788358243681412, 0.4589606930526757], [0.7247341871261597, 0.2752658], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 6.2993628166120885, 6.4533668963749236, 3.7936986994572557, 2.5056641171548337, 0.15400407976283503]]\n",
      "2021-01-14 21:35:59,958 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:35:59,959 : INFO : built Dictionary(32 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 43 corpus positions)\n",
      "2021-01-14 21:35:59,971 : INFO : token count processed\n",
      "2021-01-14 21:36:00,003 : INFO : frequencies processed\n",
      "2021-01-14 21:36:09,519 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:36:09,520 : INFO : entropies processed\n",
      "2021-01-14 21:36:09,521 : INFO : extropies processed\n",
      "2021-01-14 21:36:09,528 : INFO : token count processed\n",
      "2021-01-14 21:36:09,532 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:36:09,537 : INFO : alphabet_target #32008\n",
      "2021-01-14 21:36:09,537 : INFO : vocab #32006\n",
      "2021-01-14 21:36:09,543 : INFO : diff #set()\n",
      "2021-01-14 21:36:28,377 : INFO : alphabet #32006\n",
      "2021-01-14 21:36:37,806 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.1533773774352893, 0.4643867863007913], [0.7364198565483093, 0.26358014], [0.0, 0.0], [3.94770277922009, 3.8936606896881862, 4.668169992264658, 3.1731934766436183, 0.720467213044568, 0.7745093025764715]]\n",
      "2021-01-14 21:36:37,829 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:36:37,834 : INFO : built Dictionary(738 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 12485 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 21:36:38,333 : INFO : token count processed\n",
      "2021-01-14 21:36:38,366 : INFO : frequencies processed\n",
      "2021-01-14 21:36:47,887 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:36:47,888 : INFO : entropies processed\n",
      "2021-01-14 21:36:47,889 : INFO : extropies processed\n",
      "2021-01-14 21:36:47,900 : INFO : token count processed\n",
      "2021-01-14 21:36:47,904 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:36:47,909 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:36:47,910 : INFO : vocab #32006\n",
      "2021-01-14 21:36:47,917 : INFO : diff #set()\n",
      "2021-01-14 21:37:06,765 : INFO : alphabet #32006\n",
      "2021-01-14 21:37:16,313 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.189726489582963, 0.456678039361186], [0.7290163934230804, 0.2709836], [2.725480556997868, 1.3192201298976014], [3.94770277922009, 7.434393313070278, 7.636745669278902, 3.745350423011466, 3.689042890058812, 0.2023523562086238]]\n",
      "2021-01-14 21:37:16,322 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:37:16,325 : INFO : built Dictionary(490 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 4112 corpus positions)\n",
      "2021-01-14 21:37:16,582 : INFO : token count processed\n",
      "2021-01-14 21:37:16,626 : INFO : frequencies processed\n",
      "2021-01-14 21:37:26,023 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:37:26,024 : INFO : entropies processed\n",
      "2021-01-14 21:37:26,025 : INFO : extropies processed\n",
      "2021-01-14 21:37:26,033 : INFO : token count processed\n",
      "2021-01-14 21:37:26,038 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:37:26,042 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:37:26,043 : INFO : vocab #32006\n",
      "2021-01-14 21:37:26,051 : INFO : diff #set()\n",
      "2021-01-14 21:37:45,021 : INFO : alphabet #32006\n",
      "2021-01-14 21:37:55,066 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.182532278293268, 0.4581833725648247], [0.7172350287437439, 0.28276497], [2.5, 1.2968140217166513], [3.94770277922009, 7.2991514951718255, 7.48243957637187, 3.764414698020045, 3.53473679715178, 0.18328808120004414]]\n",
      "2021-01-14 21:37:55,074 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:37:55,077 : INFO : built Dictionary(456 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 3529 corpus positions)\n",
      "2021-01-14 21:37:55,326 : INFO : token count processed\n",
      "2021-01-14 21:37:55,359 : INFO : frequencies processed\n",
      "2021-01-14 21:38:04,971 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:38:04,972 : INFO : entropies processed\n",
      "2021-01-14 21:38:04,973 : INFO : extropies processed\n",
      "2021-01-14 21:38:04,981 : INFO : token count processed\n",
      "2021-01-14 21:38:04,987 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:38:04,992 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:38:04,993 : INFO : vocab #32006\n",
      "2021-01-14 21:38:05,000 : INFO : diff #set()\n",
      "2021-01-14 21:38:23,999 : INFO : alphabet #32006\n",
      "2021-01-14 21:38:33,447 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.1842547080425616, 0.45782206457788], [0.7263176441192627, 0.27368236], [2.5, 1.2968140217166513], [3.94770277922009, 7.170319527000998, 7.346704902659565, 3.7713174035615227, 3.399002123439475, 0.17638537565856716]]\n",
      "2021-01-14 21:38:33,451 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:38:33,453 : INFO : built Dictionary(167 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 565 corpus positions)\n",
      "2021-01-14 21:38:33,523 : INFO : token count processed\n",
      "2021-01-14 21:38:33,555 : INFO : frequencies processed\n",
      "2021-01-14 21:38:42,979 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:38:42,980 : INFO : entropies processed\n",
      "2021-01-14 21:38:42,981 : INFO : extropies processed\n",
      "2021-01-14 21:38:42,988 : INFO : token count processed\n",
      "2021-01-14 21:38:42,993 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:38:42,997 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:38:42,998 : INFO : vocab #32006\n",
      "2021-01-14 21:38:43,005 : INFO : diff #set()\n",
      "2021-01-14 21:39:01,997 : INFO : alphabet #32006\n",
      "2021-01-14 21:39:11,416 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.1846587379022666, 0.4577373951595804], [0.7363186478614807, 0.26368135], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 6.353654804387375, 6.521083897513064, 3.780273686094402, 2.573381118292974, 0.16742909312568877]]\n",
      "2021-01-14 21:39:11,420 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:39:11,422 : INFO : built Dictionary(162 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 693 corpus positions)\n",
      "2021-01-14 21:39:11,480 : INFO : token count processed\n",
      "2021-01-14 21:39:11,512 : INFO : frequencies processed\n",
      "2021-01-14 21:39:20,912 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:39:20,913 : INFO : entropies processed\n",
      "2021-01-14 21:39:20,914 : INFO : extropies processed\n",
      "2021-01-14 21:39:20,921 : INFO : token count processed\n",
      "2021-01-14 21:39:20,926 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:39:20,930 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:39:20,931 : INFO : vocab #32006\n",
      "2021-01-14 21:39:20,938 : INFO : diff #set()\n",
      "2021-01-14 21:39:39,818 : INFO : alphabet #32006\n",
      "2021-01-14 21:39:49,242 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.1693844737903725, 0.4609602456741055], [0.7158467173576355, 0.28415328], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 6.245180322479091, 6.377779860329845, 3.8151032413693358, 2.430077081109755, 0.13259953785075407]]\n",
      "2021-01-14 21:39:49,248 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:39:49,250 : INFO : built Dictionary(385 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1765 corpus positions)\n",
      "2021-01-14 21:39:49,440 : INFO : token count processed\n",
      "2021-01-14 21:39:49,471 : INFO : frequencies processed\n",
      "2021-01-14 21:39:59,163 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:39:59,165 : INFO : entropies processed\n",
      "2021-01-14 21:39:59,165 : INFO : extropies processed\n",
      "2021-01-14 21:39:59,173 : INFO : token count processed\n",
      "2021-01-14 21:39:59,177 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:39:59,182 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:39:59,182 : INFO : vocab #32006\n",
      "2021-01-14 21:39:59,189 : INFO : diff #set()\n",
      "2021-01-14 21:40:18,338 : INFO : alphabet #32006\n",
      "2021-01-14 21:40:27,764 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.1740246765551887, 0.4599763796538555], [0.7040949761867523, 0.29590502], [3.095795255000934, 1.3487605247277434], [3.94770277922009, 7.2691387000368, 7.454566076916334, 3.7622754023405545, 3.5068632976962446, 0.18542737687953448]]\n",
      "2021-01-14 21:40:27,769 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:40:27,772 : INFO : built Dictionary(316 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1602 corpus positions)\n",
      "2021-01-14 21:40:27,929 : INFO : token count processed\n",
      "2021-01-14 21:40:27,993 : INFO : frequencies processed\n",
      "2021-01-14 21:40:37,525 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:40:37,526 : INFO : entropies processed\n",
      "2021-01-14 21:40:37,527 : INFO : extropies processed\n",
      "2021-01-14 21:40:37,534 : INFO : token count processed\n",
      "2021-01-14 21:40:37,542 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:40:37,546 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:40:37,547 : INFO : vocab #32006\n",
      "2021-01-14 21:40:37,554 : INFO : diff #set()\n",
      "2021-01-14 21:40:56,463 : INFO : alphabet #32006\n",
      "2021-01-14 21:41:05,896 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.1780163354504836, 0.45913337917787833], [0.7178739011287689, 0.2821261], [1.9182958340544896, 1.2183406773511978], [3.94770277922009, 7.08857858466988, 7.242781701230385, 3.7934996626595847, 3.295078922010295, 0.15420311656050512]]\n",
      "2021-01-14 21:41:05,899 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 21:41:05,901 : INFO : built Dictionary(142 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 525 corpus positions)\n",
      "2021-01-14 21:41:05,959 : INFO : token count processed\n",
      "2021-01-14 21:41:06,003 : INFO : frequencies processed\n",
      "2021-01-14 21:41:15,525 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:41:15,526 : INFO : entropies processed\n",
      "2021-01-14 21:41:15,527 : INFO : extropies processed\n",
      "2021-01-14 21:41:15,541 : INFO : token count processed\n",
      "2021-01-14 21:41:15,546 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:41:15,551 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:41:15,552 : INFO : vocab #32006\n",
      "2021-01-14 21:41:15,561 : INFO : diff #set()\n",
      "2021-01-14 21:41:34,424 : INFO : alphabet #32006\n",
      "2021-01-14 21:41:43,955 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.197033581330573, 0.4551591784930194], [0.7629205882549286, 0.23707941], [0.9182958340544896, 0.9182958340544896], [3.94770277922009, 6.0479231618016716, 6.2184455926853985, 3.777180348336362, 2.2707428134653087, 0.17052243088372698]]\n",
      "2021-01-14 21:41:43,959 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:41:43,961 : INFO : built Dictionary(143 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 644 corpus positions)\n",
      "2021-01-14 21:41:44,012 : INFO : token count processed\n",
      "2021-01-14 21:41:44,051 : INFO : frequencies processed\n",
      "2021-01-14 21:41:53,455 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:41:53,456 : INFO : entropies processed\n",
      "2021-01-14 21:41:53,456 : INFO : extropies processed\n",
      "2021-01-14 21:41:53,463 : INFO : token count processed\n",
      "2021-01-14 21:41:53,468 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:41:53,474 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:41:53,475 : INFO : vocab #32006\n",
      "2021-01-14 21:41:53,483 : INFO : diff #set()\n",
      "2021-01-14 21:42:12,731 : INFO : alphabet #32006\n",
      "2021-01-14 21:42:22,257 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.177237267513354, 0.4592976681600305], [0.7338165938854218, 0.2661834], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 6.036583168403119, 6.172807332915459, 3.8114786147077506, 2.2251045536953695, 0.1362241645123401]]\n",
      "2021-01-14 21:42:22,270 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 21:42:22,271 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:42:22,274 : INFO : built Dictionary(554 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 7002 corpus positions)\n",
      "2021-01-14 21:42:22,614 : INFO : token count processed\n",
      "2021-01-14 21:42:22,645 : INFO : frequencies processed\n",
      "2021-01-14 21:42:32,066 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:42:32,067 : INFO : entropies processed\n",
      "2021-01-14 21:42:32,068 : INFO : extropies processed\n",
      "2021-01-14 21:42:32,077 : INFO : token count processed\n",
      "2021-01-14 21:42:32,083 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:42:32,088 : INFO : alphabet_target #32010\n",
      "2021-01-14 21:42:32,090 : INFO : vocab #32006\n",
      "2021-01-14 21:42:32,097 : INFO : diff #set()\n",
      "2021-01-14 21:42:51,101 : INFO : alphabet #32006\n",
      "2021-01-14 21:43:00,528 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.2031444342096582, 0.4538967053055391], [0.761548712849617, 0.23845129], [2.725480556997868, 1.3192201298976014], [3.94770277922009, 7.29352035514053, 7.445764211198288, 3.7954589231623324, 3.498061431978198, 0.15224385605775748]]\n",
      "2021-01-14 21:43:00,535 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:43:00,538 : INFO : built Dictionary(364 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 3245 corpus positions)\n",
      "2021-01-14 21:43:00,715 : INFO : token count processed\n",
      "2021-01-14 21:43:00,748 : INFO : frequencies processed\n",
      "2021-01-14 21:43:10,172 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:43:10,173 : INFO : entropies processed\n",
      "2021-01-14 21:43:10,174 : INFO : extropies processed\n",
      "2021-01-14 21:43:10,181 : INFO : token count processed\n",
      "2021-01-14 21:43:10,186 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:43:10,190 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:43:10,191 : INFO : vocab #32006\n",
      "2021-01-14 21:43:10,198 : INFO : diff #set()\n",
      "2021-01-14 21:43:29,237 : INFO : alphabet #32006\n",
      "2021-01-14 21:43:38,558 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.1716235408603182, 0.46048496950987944], [0.7114275693893433, 0.28857243], [2.5, 1.2968140217166513], [3.94770277922009, 6.8153433747477745, 6.9482902358128005, 3.814755918155063, 3.0005874565927106, 0.13294686106502596]]\n",
      "2021-01-14 21:43:38,561 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:43:38,562 : INFO : built Dictionary(118 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 266 corpus positions)\n",
      "2021-01-14 21:43:38,615 : INFO : token count processed\n",
      "2021-01-14 21:43:38,677 : INFO : frequencies processed\n",
      "2021-01-14 21:43:48,105 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:43:48,106 : INFO : entropies processed\n",
      "2021-01-14 21:43:48,107 : INFO : extropies processed\n",
      "2021-01-14 21:43:48,121 : INFO : token count processed\n",
      "2021-01-14 21:43:48,125 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:43:48,129 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:43:48,130 : INFO : vocab #32006\n",
      "2021-01-14 21:43:48,137 : INFO : diff #set()\n",
      "2021-01-14 21:44:07,321 : INFO : alphabet #32006\n",
      "2021-01-14 21:44:16,741 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.1927858993094762, 0.45604087490479894], [0.7556691765785217, 0.24433082], [1.5, 1.1225562489182657], [3.94770277922009, 6.150121915859574, 6.309271733867577, 3.788552961212087, 2.3615689546474874, 0.15914981800800287]]\n",
      "2021-01-14 21:44:16,745 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:44:16,747 : INFO : built Dictionary(280 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 977 corpus positions)\n",
      "2021-01-14 21:44:16,867 : INFO : token count processed\n",
      "2021-01-14 21:44:16,899 : INFO : frequencies processed\n",
      "2021-01-14 21:44:26,319 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:44:26,320 : INFO : entropies processed\n",
      "2021-01-14 21:44:26,321 : INFO : extropies processed\n",
      "2021-01-14 21:44:26,334 : INFO : token count processed\n",
      "2021-01-14 21:44:26,339 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:44:26,343 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:44:26,344 : INFO : vocab #32006\n",
      "2021-01-14 21:44:26,351 : INFO : diff #set()\n",
      "2021-01-14 21:44:45,309 : INFO : alphabet #32006\n",
      "2021-01-14 21:44:54,725 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.1669679256025882, 0.4614742969589276], [0.7115732431411743, 0.28842676], [2.5, 1.2968140217166513], [3.94770277922009, 7.0391145208191315, 7.2529461380714135, 3.733871161967807, 3.3052433588513237, 0.21383161725228206]]\n",
      "2021-01-14 21:44:54,734 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 21:44:54,735 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:44:54,738 : INFO : built Dictionary(585 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 4352 corpus positions)\n",
      "2021-01-14 21:44:55,088 : INFO : token count processed\n",
      "2021-01-14 21:44:55,122 : INFO : frequencies processed\n",
      "2021-01-14 21:45:04,551 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:45:04,552 : INFO : entropies processed\n",
      "2021-01-14 21:45:04,553 : INFO : extropies processed\n",
      "2021-01-14 21:45:04,561 : INFO : token count processed\n",
      "2021-01-14 21:45:04,567 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:45:04,572 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:45:04,573 : INFO : vocab #32006\n",
      "2021-01-14 21:45:04,580 : INFO : diff #set()\n",
      "2021-01-14 21:45:23,574 : INFO : alphabet #32006\n",
      "2021-01-14 21:45:33,005 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.2014950911560744, 0.45423676119798595], [0.7577269375324249, 0.24227306], [2.5, 1.2968140217166513], [3.94770277922009, 7.482466367279176, 7.744983340206397, 3.6851858062928686, 3.797280560986307, 0.2625169729272212]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 21:45:33,009 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:45:33,010 : INFO : built Dictionary(167 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 574 corpus positions)\n",
      "2021-01-14 21:45:33,073 : INFO : token count processed\n",
      "2021-01-14 21:45:33,106 : INFO : frequencies processed\n",
      "2021-01-14 21:45:42,679 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:45:42,681 : INFO : entropies processed\n",
      "2021-01-14 21:45:42,682 : INFO : extropies processed\n",
      "2021-01-14 21:45:42,696 : INFO : token count processed\n",
      "2021-01-14 21:45:42,701 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:45:42,705 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:45:42,706 : INFO : vocab #32006\n",
      "2021-01-14 21:45:42,713 : INFO : diff #set()\n",
      "2021-01-14 21:46:01,565 : INFO : alphabet #32006\n",
      "2021-01-14 21:46:10,981 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.1840026200885498, 0.4578749085747229], [0.7321501672267914, 0.26784983], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 6.372162341197667, 6.53613252980416, 3.783732590613596, 2.58842975058407, 0.16397018860649304]]\n",
      "2021-01-14 21:46:10,987 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:46:10,989 : INFO : built Dictionary(315 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1995 corpus positions)\n",
      "2021-01-14 21:46:11,130 : INFO : token count processed\n",
      "2021-01-14 21:46:11,165 : INFO : frequencies processed\n",
      "2021-01-14 21:46:20,712 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:46:20,713 : INFO : entropies processed\n",
      "2021-01-14 21:46:20,714 : INFO : extropies processed\n",
      "2021-01-14 21:46:20,722 : INFO : token count processed\n",
      "2021-01-14 21:46:20,728 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:46:20,732 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:46:20,733 : INFO : vocab #32006\n",
      "2021-01-14 21:46:20,739 : INFO : diff #set()\n",
      "2021-01-14 21:46:39,558 : INFO : alphabet #32006\n",
      "2021-01-14 21:46:49,094 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.1854192428189658, 0.45757810694029716], [0.7400273978710175, 0.2599726], [1.9182958340544896, 1.2183406773511978], [3.94770277922009, 6.798155919669889, 6.972349026992919, 3.7735096718970595, 3.0246462477728295, 0.17419310732303028]]\n",
      "2021-01-14 21:46:49,097 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:46:49,099 : INFO : built Dictionary(172 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 711 corpus positions)\n",
      "2021-01-14 21:46:49,162 : INFO : token count processed\n",
      "2021-01-14 21:46:49,194 : INFO : frequencies processed\n",
      "2021-01-14 21:46:58,579 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:46:58,580 : INFO : entropies processed\n",
      "2021-01-14 21:46:58,581 : INFO : extropies processed\n",
      "2021-01-14 21:46:58,594 : INFO : token count processed\n",
      "2021-01-14 21:46:58,599 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:46:58,604 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:46:58,605 : INFO : vocab #32006\n",
      "2021-01-14 21:46:58,613 : INFO : diff #set()\n",
      "2021-01-14 21:47:17,450 : INFO : alphabet #32006\n",
      "2021-01-14 21:47:26,987 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.1948086737895305, 0.45562057957125346], [0.7519771903753281, 0.24802281], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 6.271631856729336, 6.435263283936659, 3.7840713520127673, 2.487560504716569, 0.16363142720732338]]\n",
      "2021-01-14 21:47:26,994 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:47:26,996 : INFO : built Dictionary(348 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 3226 corpus positions)\n",
      "2021-01-14 21:47:27,178 : INFO : token count processed\n",
      "2021-01-14 21:47:27,210 : INFO : frequencies processed\n",
      "2021-01-14 21:47:36,602 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:47:36,604 : INFO : entropies processed\n",
      "2021-01-14 21:47:36,604 : INFO : extropies processed\n",
      "2021-01-14 21:47:36,613 : INFO : token count processed\n",
      "2021-01-14 21:47:36,617 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:47:36,621 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:47:36,622 : INFO : vocab #32006\n",
      "2021-01-14 21:47:36,628 : INFO : diff #set()\n",
      "2021-01-14 21:47:55,399 : INFO : alphabet #32006\n",
      "2021-01-14 21:48:04,888 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.1677413920228703, 0.46130963946157355], [0.7065629959106445, 0.293437], [2.2359263506290326, 1.2653331222512112], [3.94770277922009, 6.873598627629562, 7.017524974610135, 3.803776432239518, 3.069822195390045, 0.14392634698057272]]\n",
      "2021-01-14 21:48:04,891 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:48:04,892 : INFO : built Dictionary(99 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 174 corpus positions)\n",
      "2021-01-14 21:48:04,934 : INFO : token count processed\n",
      "2021-01-14 21:48:04,968 : INFO : frequencies processed\n",
      "2021-01-14 21:48:14,388 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:48:14,389 : INFO : entropies processed\n",
      "2021-01-14 21:48:14,390 : INFO : extropies processed\n",
      "2021-01-14 21:48:14,396 : INFO : token count processed\n",
      "2021-01-14 21:48:14,404 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:48:14,409 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:48:14,410 : INFO : vocab #32006\n",
      "2021-01-14 21:48:14,417 : INFO : diff #set()\n",
      "2021-01-14 21:48:33,401 : INFO : alphabet #32006\n",
      "2021-01-14 21:48:42,827 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.169520198262816, 0.4609314081522369], [0.7461417317390442, 0.25385827], [1.5, 1.1225562489182657], [3.94770277922009, 6.049830202851529, 6.283694969562843, 3.7138380125087753, 2.335992190342753, 0.23386476671131362]]\n",
      "2021-01-14 21:48:42,831 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:48:42,833 : INFO : built Dictionary(246 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1254 corpus positions)\n",
      "2021-01-14 21:48:42,946 : INFO : token count processed\n",
      "2021-01-14 21:48:42,980 : INFO : frequencies processed\n",
      "2021-01-14 21:48:52,388 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:48:52,389 : INFO : entropies processed\n",
      "2021-01-14 21:48:52,390 : INFO : extropies processed\n",
      "2021-01-14 21:48:52,397 : INFO : token count processed\n",
      "2021-01-14 21:48:52,405 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:48:52,410 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:48:52,411 : INFO : vocab #32006\n",
      "2021-01-14 21:48:52,418 : INFO : diff #set()\n",
      "2021-01-14 21:49:11,409 : INFO : alphabet #32006\n",
      "2021-01-14 21:49:20,833 : INFO : Computed distances or similarities ('272', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.1636760377903705, 0.4621763991162182], [0.7104584276676178, 0.28954157], [1.9182958340544893, 1.2183406773511978], [3.94770277922009, 6.778844940588858, 6.950784665382248, 3.775763054426701, 3.0030818861621578, 0.17193972479338981]]\n",
      "2021-01-14 21:49:20,836 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:49:20,838 : INFO : built Dictionary(143 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 287 corpus positions)\n",
      "2021-01-14 21:49:20,890 : INFO : token count processed\n",
      "2021-01-14 21:49:20,922 : INFO : frequencies processed\n",
      "2021-01-14 21:49:30,313 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:49:30,314 : INFO : entropies processed\n",
      "2021-01-14 21:49:30,315 : INFO : extropies processed\n",
      "2021-01-14 21:49:30,322 : INFO : token count processed\n",
      "2021-01-14 21:49:30,327 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:49:30,332 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:49:30,333 : INFO : vocab #32006\n",
      "2021-01-14 21:49:30,340 : INFO : diff #set()\n",
      "2021-01-14 21:49:49,353 : INFO : alphabet #32006\n",
      "2021-01-14 21:49:58,797 : INFO : Computed distances or similarities ('272', 'sacp-python-common/setup.py')[[1.1750580032732514, 0.4597578540411782], [0.7334468364715576, 0.26655316], [2.5, 1.2968140217166513], [3.94770277922009, 6.469677430851302, 6.682951923899079, 3.7344282861723137, 2.7352491446789893, 0.21327449304777701]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 21:49:58,801 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:49:58,803 : INFO : built Dictionary(216 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1189 corpus positions)\n",
      "2021-01-14 21:49:58,894 : INFO : token count processed\n",
      "2021-01-14 21:49:58,928 : INFO : frequencies processed\n",
      "2021-01-14 21:50:08,332 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:50:08,333 : INFO : entropies processed\n",
      "2021-01-14 21:50:08,334 : INFO : extropies processed\n",
      "2021-01-14 21:50:08,341 : INFO : token count processed\n",
      "2021-01-14 21:50:08,346 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:50:08,351 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:50:08,352 : INFO : vocab #32006\n",
      "2021-01-14 21:50:08,359 : INFO : diff #set()\n",
      "2021-01-14 21:50:27,393 : INFO : alphabet #32006\n",
      "2021-01-14 21:50:36,828 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.1856198978106067, 0.4575360981119025], [0.7440442442893982, 0.25595576], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 6.459180448028249, 6.616468862325625, 3.7904143649227144, 2.6687660831055355, 0.15728841429737628]]\n",
      "2021-01-14 21:50:36,831 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:50:36,833 : INFO : built Dictionary(126 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 339 corpus positions)\n",
      "2021-01-14 21:50:36,885 : INFO : token count processed\n",
      "2021-01-14 21:50:36,921 : INFO : frequencies processed\n",
      "2021-01-14 21:50:46,430 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:50:46,431 : INFO : entropies processed\n",
      "2021-01-14 21:50:46,432 : INFO : extropies processed\n",
      "2021-01-14 21:50:46,439 : INFO : token count processed\n",
      "2021-01-14 21:50:46,444 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:50:46,449 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:50:46,450 : INFO : vocab #32006\n",
      "2021-01-14 21:50:46,458 : INFO : diff #set()\n",
      "2021-01-14 21:51:05,359 : INFO : alphabet #32006\n",
      "2021-01-14 21:51:14,920 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.177385884187469, 0.45926631896631775], [0.7395032644271851, 0.26049674], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 6.097125733496388, 6.266957483315942, 3.7778710294005364, 2.3192547040958518, 0.1698317498195534]]\n",
      "2021-01-14 21:51:14,923 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:51:14,924 : INFO : built Dictionary(118 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 335 corpus positions)\n",
      "2021-01-14 21:51:14,971 : INFO : token count processed\n",
      "2021-01-14 21:51:15,033 : INFO : frequencies processed\n",
      "2021-01-14 21:51:24,466 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:51:24,467 : INFO : entropies processed\n",
      "2021-01-14 21:51:24,468 : INFO : extropies processed\n",
      "2021-01-14 21:51:24,475 : INFO : token count processed\n",
      "2021-01-14 21:51:24,480 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:51:24,485 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:51:24,487 : INFO : vocab #32006\n",
      "2021-01-14 21:51:24,493 : INFO : diff #set()\n",
      "2021-01-14 21:51:43,541 : INFO : alphabet #32006\n",
      "2021-01-14 21:51:53,095 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.1870603212366828, 0.45723475950336184], [0.7464726567268372, 0.25352734], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 6.0695858597523715, 6.238377357031767, 3.7789112819406947, 2.290674577811677, 0.16879149727939513]]\n",
      "2021-01-14 21:51:53,099 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 21:51:53,100 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:51:53,101 : INFO : built Dictionary(120 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 431 corpus positions)\n",
      "2021-01-14 21:51:53,161 : INFO : token count processed\n",
      "2021-01-14 21:51:53,217 : INFO : frequencies processed\n",
      "2021-01-14 21:52:02,646 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:52:02,647 : INFO : entropies processed\n",
      "2021-01-14 21:52:02,648 : INFO : extropies processed\n",
      "2021-01-14 21:52:02,660 : INFO : token count processed\n",
      "2021-01-14 21:52:02,664 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:52:02,668 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:52:02,669 : INFO : vocab #32006\n",
      "2021-01-14 21:52:02,676 : INFO : diff #set()\n",
      "2021-01-14 21:52:21,432 : INFO : alphabet #32006\n",
      "2021-01-14 21:52:30,973 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.1984492516236602, 0.45486608310901516], [0.7734638154506683, 0.22653618], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 6.104787343210121, 6.241560485257218, 3.8109296371729924, 2.2938577060371284, 0.13677314204709745]]\n",
      "2021-01-14 21:52:30,990 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 21:52:30,991 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:52:30,994 : INFO : built Dictionary(407 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 9116 corpus positions)\n",
      "2021-01-14 21:52:31,206 : INFO : token count processed\n",
      "2021-01-14 21:52:31,270 : INFO : frequencies processed\n",
      "2021-01-14 21:52:40,671 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:52:40,672 : INFO : entropies processed\n",
      "2021-01-14 21:52:40,673 : INFO : extropies processed\n",
      "2021-01-14 21:52:40,689 : INFO : token count processed\n",
      "2021-01-14 21:52:40,693 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:52:40,699 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:52:40,700 : INFO : vocab #32006\n",
      "2021-01-14 21:52:40,708 : INFO : diff #set()\n",
      "2021-01-14 21:52:59,682 : INFO : alphabet #32006\n",
      "2021-01-14 21:53:09,098 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.2202721610234308, 0.4503952342216693], [0.7964684218168259, 0.20353158], [1.9182958340544896, 1.2183406773511978], [3.94770277922009, 6.89087415148015, 7.0446458393253355, 3.7939310913749047, 3.0969430601052457, 0.15377168784518513]]\n",
      "2021-01-14 21:53:09,104 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:53:09,106 : INFO : built Dictionary(266 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 2276 corpus positions)\n",
      "2021-01-14 21:53:09,225 : INFO : token count processed\n",
      "2021-01-14 21:53:09,260 : INFO : frequencies processed\n",
      "2021-01-14 21:53:18,660 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:53:18,661 : INFO : entropies processed\n",
      "2021-01-14 21:53:18,662 : INFO : extropies processed\n",
      "2021-01-14 21:53:18,670 : INFO : token count processed\n",
      "2021-01-14 21:53:18,676 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:53:18,682 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:53:18,683 : INFO : vocab #32006\n",
      "2021-01-14 21:53:18,690 : INFO : diff #set()\n",
      "2021-01-14 21:53:37,695 : INFO : alphabet #32006\n",
      "2021-01-14 21:53:47,129 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.1929643984452794, 0.4560037548758012], [0.7418085038661957, 0.2581915], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 6.655493573668506, 6.785501772166895, 3.8176945807217004, 2.837798992946805, 0.13000819849838852]]\n",
      "2021-01-14 21:53:47,134 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:53:47,136 : INFO : built Dictionary(243 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1283 corpus positions)\n",
      "2021-01-14 21:53:47,242 : INFO : token count processed\n",
      "2021-01-14 21:53:47,311 : INFO : frequencies processed\n",
      "2021-01-14 21:53:56,710 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:53:56,712 : INFO : entropies processed\n",
      "2021-01-14 21:53:56,712 : INFO : extropies processed\n",
      "2021-01-14 21:53:56,720 : INFO : token count processed\n",
      "2021-01-14 21:53:56,724 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:53:56,728 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:53:56,729 : INFO : vocab #32006\n",
      "2021-01-14 21:53:56,736 : INFO : diff #set()\n",
      "2021-01-14 21:54:15,703 : INFO : alphabet #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 21:54:25,130 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.1841082654350046, 0.4578527611591781], [0.7314770817756653, 0.26852292], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 6.6236746347295465, 6.732400399077935, 3.838977014871702, 2.7846976198578455, 0.10872576434838876]]\n",
      "2021-01-14 21:54:25,135 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:54:25,138 : INFO : built Dictionary(243 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1222 corpus positions)\n",
      "2021-01-14 21:54:25,240 : INFO : token count processed\n",
      "2021-01-14 21:54:25,272 : INFO : frequencies processed\n",
      "2021-01-14 21:54:34,670 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:54:34,671 : INFO : entropies processed\n",
      "2021-01-14 21:54:34,672 : INFO : extropies processed\n",
      "2021-01-14 21:54:34,680 : INFO : token count processed\n",
      "2021-01-14 21:54:34,684 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:54:34,688 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:54:34,689 : INFO : vocab #32006\n",
      "2021-01-14 21:54:34,696 : INFO : diff #set()\n",
      "2021-01-14 21:54:53,857 : INFO : alphabet #32006\n",
      "2021-01-14 21:55:03,302 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.1660121148481446, 0.46167793483006825], [0.7002817392349243, 0.29971826], [1.9182958340544896, 1.2183406773511978], [3.94770277922009, 6.75472436518627, 6.860174896077838, 3.8422522483285224, 2.9124721168577485, 0.1054505308915683]]\n",
      "2021-01-14 21:55:03,307 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:55:03,309 : INFO : built Dictionary(196 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1017 corpus positions)\n",
      "2021-01-14 21:55:03,399 : INFO : token count processed\n",
      "2021-01-14 21:55:03,432 : INFO : frequencies processed\n",
      "2021-01-14 21:55:12,836 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:55:12,837 : INFO : entropies processed\n",
      "2021-01-14 21:55:12,838 : INFO : extropies processed\n",
      "2021-01-14 21:55:12,845 : INFO : token count processed\n",
      "2021-01-14 21:55:12,852 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:55:12,857 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:55:12,858 : INFO : vocab #32006\n",
      "2021-01-14 21:55:12,865 : INFO : diff #set()\n",
      "2021-01-14 21:55:31,866 : INFO : alphabet #32006\n",
      "2021-01-14 21:55:41,300 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.178015770355018, 0.4591334983019885], [0.7201902568340302, 0.27980974], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 6.597313085495733, 6.69792144598765, 3.8470944187281715, 2.7502186667675605, 0.10060836049191746]]\n",
      "2021-01-14 21:55:41,304 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:55:41,306 : INFO : built Dictionary(220 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 983 corpus positions)\n",
      "2021-01-14 21:55:41,397 : INFO : token count processed\n",
      "2021-01-14 21:55:41,433 : INFO : frequencies processed\n",
      "2021-01-14 21:55:50,943 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:55:50,944 : INFO : entropies processed\n",
      "2021-01-14 21:55:50,945 : INFO : extropies processed\n",
      "2021-01-14 21:55:50,956 : INFO : token count processed\n",
      "2021-01-14 21:55:50,960 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:55:50,965 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:55:50,966 : INFO : vocab #32006\n",
      "2021-01-14 21:55:50,972 : INFO : diff #set()\n",
      "2021-01-14 21:56:11,317 : INFO : alphabet #32006\n",
      "2021-01-14 21:56:22,956 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.172687973338307, 0.46025937100554426], [0.7084135711193085, 0.29158643], [1.9182958340544896, 1.2183406773511978], [3.94770277922009, 6.659481538516613, 6.775343743005328, 3.831840574731375, 2.827640963785238, 0.11586220448871476]]\n",
      "2021-01-14 21:56:22,960 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:56:22,962 : INFO : built Dictionary(246 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1145 corpus positions)\n",
      "2021-01-14 21:56:23,080 : INFO : token count processed\n",
      "2021-01-14 21:56:23,150 : INFO : frequencies processed\n",
      "2021-01-14 21:56:32,582 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:56:32,583 : INFO : entropies processed\n",
      "2021-01-14 21:56:32,585 : INFO : extropies processed\n",
      "2021-01-14 21:56:32,598 : INFO : token count processed\n",
      "2021-01-14 21:56:32,603 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:56:32,608 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:56:32,609 : INFO : vocab #32006\n",
      "2021-01-14 21:56:32,617 : INFO : diff #set()\n",
      "2021-01-14 21:56:51,536 : INFO : alphabet #32006\n",
      "2021-01-14 21:57:01,100 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.1703065651412858, 0.460764398938682], [0.7053624391555786, 0.29463756], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 6.774682571479102, 6.8651243697185755, 3.857260980980616, 2.9174215904984857, 0.09044179823947385]]\n",
      "2021-01-14 21:57:01,114 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:57:01,118 : INFO : built Dictionary(423 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 7882 corpus positions)\n",
      "2021-01-14 21:57:01,357 : INFO : token count processed\n",
      "2021-01-14 21:57:01,390 : INFO : frequencies processed\n",
      "2021-01-14 21:57:10,815 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:57:10,817 : INFO : entropies processed\n",
      "2021-01-14 21:57:10,817 : INFO : extropies processed\n",
      "2021-01-14 21:57:10,827 : INFO : token count processed\n",
      "2021-01-14 21:57:10,832 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:57:10,837 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:57:10,838 : INFO : vocab #32006\n",
      "2021-01-14 21:57:10,845 : INFO : diff #set()\n",
      "2021-01-14 21:57:29,748 : INFO : alphabet #32006\n",
      "2021-01-14 21:57:39,321 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.2279109965602235, 0.4488509646677749], [0.8129855245351791, 0.18701448], [1.9182958340544896, 1.2183406773511978], [3.94770277922009, 6.839453716525233, 6.984105276057058, 3.803051219688264, 3.036402496836968, 0.14465155953182496]]\n",
      "2021-01-14 21:57:39,327 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:57:39,329 : INFO : built Dictionary(323 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 2324 corpus positions)\n",
      "2021-01-14 21:57:39,472 : INFO : token count processed\n",
      "2021-01-14 21:57:39,504 : INFO : frequencies processed\n",
      "2021-01-14 21:57:49,212 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:57:49,213 : INFO : entropies processed\n",
      "2021-01-14 21:57:49,214 : INFO : extropies processed\n",
      "2021-01-14 21:57:49,221 : INFO : token count processed\n",
      "2021-01-14 21:57:49,226 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:57:49,230 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:57:49,231 : INFO : vocab #32006\n",
      "2021-01-14 21:57:49,238 : INFO : diff #set()\n",
      "2021-01-14 21:58:08,428 : INFO : alphabet #32006\n",
      "2021-01-14 21:58:17,848 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.1446183238140244, 0.46628343556329577], [0.6568678021430969, 0.3431322], [1.9182958340544896, 1.2183406773511978], [3.94770277922009, 6.86432793886027, 6.966658003041707, 3.8453727150386525, 3.0189552238216173, 0.10233006418143731]]\n",
      "2021-01-14 21:58:17,852 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:58:17,853 : INFO : built Dictionary(157 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 526 corpus positions)\n",
      "2021-01-14 21:58:17,914 : INFO : token count processed\n",
      "2021-01-14 21:58:17,947 : INFO : frequencies processed\n",
      "2021-01-14 21:58:27,363 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:58:27,364 : INFO : entropies processed\n",
      "2021-01-14 21:58:27,365 : INFO : extropies processed\n",
      "2021-01-14 21:58:27,372 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 21:58:27,379 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:58:27,383 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:58:27,384 : INFO : vocab #32006\n",
      "2021-01-14 21:58:27,391 : INFO : diff #set()\n",
      "2021-01-14 21:58:46,403 : INFO : alphabet #32006\n",
      "2021-01-14 21:58:55,839 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.1438376042065923, 0.46645324162512186], [0.6518071889877319, 0.3481928], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 6.431978396403875, 6.5468024879465325, 3.8328786876774323, 2.5990997087264427, 0.11482409154265749]]\n",
      "2021-01-14 21:58:55,843 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:58:55,845 : INFO : built Dictionary(213 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 680 corpus positions)\n",
      "2021-01-14 21:58:55,940 : INFO : token count processed\n",
      "2021-01-14 21:58:55,975 : INFO : frequencies processed\n",
      "2021-01-14 21:59:05,391 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:59:05,392 : INFO : entropies processed\n",
      "2021-01-14 21:59:05,393 : INFO : extropies processed\n",
      "2021-01-14 21:59:05,400 : INFO : token count processed\n",
      "2021-01-14 21:59:05,407 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:59:05,411 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:59:05,412 : INFO : vocab #32006\n",
      "2021-01-14 21:59:05,419 : INFO : diff #set()\n",
      "2021-01-14 21:59:24,449 : INFO : alphabet #32006\n",
      "2021-01-14 21:59:33,895 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/test_auth_utility.py')[[1.1417325220070016, 0.466911712702064], [0.658631831407547, 0.34136817], [2.5, 1.2968140217166513], [3.94770277922009, 6.911818353685893, 7.002490763363786, 3.857030369542196, 3.0547879841436965, 0.09067240967789303]]\n",
      "2021-01-14 21:59:33,908 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 21:59:33,911 : INFO : built Dictionary(306 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 7210 corpus positions)\n",
      "2021-01-14 21:59:34,049 : INFO : token count processed\n",
      "2021-01-14 21:59:34,081 : INFO : frequencies processed\n",
      "2021-01-14 21:59:43,508 : INFO : scalar_distribution processed\n",
      "2021-01-14 21:59:43,509 : INFO : entropies processed\n",
      "2021-01-14 21:59:43,510 : INFO : extropies processed\n",
      "2021-01-14 21:59:43,519 : INFO : token count processed\n",
      "2021-01-14 21:59:43,524 : INFO : alphabet_source #32006\n",
      "2021-01-14 21:59:43,530 : INFO : alphabet_target #32009\n",
      "2021-01-14 21:59:43,531 : INFO : vocab #32006\n",
      "2021-01-14 21:59:43,540 : INFO : diff #set()\n",
      "2021-01-14 22:00:02,551 : INFO : alphabet #32006\n",
      "2021-01-14 22:00:11,971 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.1581448644102146, 0.4633609246955178], [0.7069623172283173, 0.29303768], [1.9182958340544893, 1.2183406773511978], [3.94770277922009, 6.363791471162389, 6.3935862889288595, 3.9179079614536194, 2.4458835097087697, 0.029794817766470416]]\n",
      "2021-01-14 22:00:11,975 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:00:11,977 : INFO : built Dictionary(208 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1280 corpus positions)\n",
      "2021-01-14 22:00:12,062 : INFO : token count processed\n",
      "2021-01-14 22:00:12,098 : INFO : frequencies processed\n",
      "2021-01-14 22:00:21,805 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:00:21,806 : INFO : entropies processed\n",
      "2021-01-14 22:00:21,807 : INFO : extropies processed\n",
      "2021-01-14 22:00:21,814 : INFO : token count processed\n",
      "2021-01-14 22:00:21,822 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:00:21,826 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:00:21,827 : INFO : vocab #32006\n",
      "2021-01-14 22:00:21,834 : INFO : diff #set()\n",
      "2021-01-14 22:00:40,631 : INFO : alphabet #32006\n",
      "2021-01-14 22:00:50,049 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.1960740724178025, 0.4553580466887595], [0.7643316835165024, 0.23566832], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 6.29000629755059, 6.493581176173404, 3.7441279005972774, 2.545878396953314, 0.20357487862281332]]\n",
      "2021-01-14 22:00:50,053 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:00:50,055 : INFO : built Dictionary(217 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1233 corpus positions)\n",
      "2021-01-14 22:00:50,150 : INFO : token count processed\n",
      "2021-01-14 22:00:50,184 : INFO : frequencies processed\n",
      "2021-01-14 22:00:59,730 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:00:59,731 : INFO : entropies processed\n",
      "2021-01-14 22:00:59,732 : INFO : extropies processed\n",
      "2021-01-14 22:00:59,739 : INFO : token count processed\n",
      "2021-01-14 22:00:59,744 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:00:59,748 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:00:59,749 : INFO : vocab #32006\n",
      "2021-01-14 22:00:59,755 : INFO : diff #set()\n",
      "2021-01-14 22:01:18,666 : INFO : alphabet #32006\n",
      "2021-01-14 22:01:28,090 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.1905024515190312, 0.4565162660770078], [0.7570587694644928, 0.24294123], [1.9182958340544896, 1.2183406773511978], [3.94770277922009, 6.361621244785958, 6.544560275876197, 3.7647637481298517, 2.5968574966561073, 0.182939031090239]]\n",
      "2021-01-14 22:01:28,094 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:01:28,096 : INFO : built Dictionary(227 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1256 corpus positions)\n",
      "2021-01-14 22:01:28,196 : INFO : token count processed\n",
      "2021-01-14 22:01:28,230 : INFO : frequencies processed\n",
      "2021-01-14 22:01:37,762 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:01:37,763 : INFO : entropies processed\n",
      "2021-01-14 22:01:37,764 : INFO : extropies processed\n",
      "2021-01-14 22:01:37,771 : INFO : token count processed\n",
      "2021-01-14 22:01:37,776 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:01:37,781 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:01:37,782 : INFO : vocab #32006\n",
      "2021-01-14 22:01:37,790 : INFO : diff #set()\n",
      "2021-01-14 22:01:56,679 : INFO : alphabet #32006\n",
      "2021-01-14 22:02:06,229 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.1939941619059236, 0.45578972695683917], [0.756862610578537, 0.24313739], [0.9182958340544896, 0.9182958340544896], [3.94770277922009, 6.620594433343389, 6.820106315661082, 3.748190896902397, 2.8724035364409923, 0.1995118823176929]]\n",
      "2021-01-14 22:02:06,234 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:02:06,235 : INFO : built Dictionary(199 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1349 corpus positions)\n",
      "2021-01-14 22:02:06,324 : INFO : token count processed\n",
      "2021-01-14 22:02:06,358 : INFO : frequencies processed\n",
      "2021-01-14 22:02:15,783 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:02:15,785 : INFO : entropies processed\n",
      "2021-01-14 22:02:15,786 : INFO : extropies processed\n",
      "2021-01-14 22:02:15,793 : INFO : token count processed\n",
      "2021-01-14 22:02:15,799 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:02:15,804 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:02:15,805 : INFO : vocab #32006\n",
      "2021-01-14 22:02:15,812 : INFO : diff #set()\n",
      "2021-01-14 22:02:34,814 : INFO : alphabet #32006\n",
      "2021-01-14 22:02:44,406 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.1963920023491175, 0.45529213315768097], [0.7679421752691269, 0.23205782], [0.9182958340544896, 0.9182958340544896], [3.94770277922009, 6.207411496248084, 6.3267793746475265, 3.8283349008206473, 2.3790765954274367, 0.11936787839944252]]\n",
      "2021-01-14 22:02:44,409 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:02:44,410 : INFO : built Dictionary(139 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 253 corpus positions)\n",
      "2021-01-14 22:02:44,459 : INFO : token count processed\n",
      "2021-01-14 22:02:44,495 : INFO : frequencies processed\n",
      "2021-01-14 22:02:53,900 : INFO : scalar_distribution processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 22:02:53,901 : INFO : entropies processed\n",
      "2021-01-14 22:02:53,902 : INFO : extropies processed\n",
      "2021-01-14 22:02:53,909 : INFO : token count processed\n",
      "2021-01-14 22:02:53,915 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:02:53,919 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:02:53,921 : INFO : vocab #32006\n",
      "2021-01-14 22:02:53,927 : INFO : diff #set()\n",
      "2021-01-14 22:03:12,934 : INFO : alphabet #32006\n",
      "2021-01-14 22:03:22,378 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.1803046332559273, 0.4586515043572898], [0.7105052769184113, 0.28949472], [1.0, 1.0], [3.94770277922009, 6.5805228788529595, 6.7389027977843385, 3.789322860288711, 2.7912000185642487, 0.15837991893137904]]\n",
      "2021-01-14 22:03:22,383 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 22:03:22,384 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:03:22,387 : INFO : built Dictionary(220 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1214 corpus positions)\n",
      "2021-01-14 22:03:22,494 : INFO : token count processed\n",
      "2021-01-14 22:03:22,527 : INFO : frequencies processed\n",
      "2021-01-14 22:03:31,939 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:03:31,940 : INFO : entropies processed\n",
      "2021-01-14 22:03:31,941 : INFO : extropies processed\n",
      "2021-01-14 22:03:31,948 : INFO : token count processed\n",
      "2021-01-14 22:03:31,952 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:03:31,957 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:03:31,958 : INFO : vocab #32006\n",
      "2021-01-14 22:03:31,965 : INFO : diff #set()\n",
      "2021-01-14 22:03:50,976 : INFO : alphabet #32006\n",
      "2021-01-14 22:04:00,407 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.1780667017341395, 0.45912276203654234], [0.7427670061588287, 0.257233], [1.9182958340544896, 1.2183406773511978], [3.94770277922009, 6.422089779976135, 6.501228332428765, 3.868564226767459, 2.553525553208675, 0.0791385524526298]]\n",
      "2021-01-14 22:04:00,412 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:04:00,416 : INFO : built Dictionary(234 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1496 corpus positions)\n",
      "2021-01-14 22:04:00,513 : INFO : token count processed\n",
      "2021-01-14 22:04:00,546 : INFO : frequencies processed\n",
      "2021-01-14 22:04:09,962 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:04:09,963 : INFO : entropies processed\n",
      "2021-01-14 22:04:09,964 : INFO : extropies processed\n",
      "2021-01-14 22:04:09,971 : INFO : token count processed\n",
      "2021-01-14 22:04:09,978 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:04:09,983 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:04:09,984 : INFO : vocab #32006\n",
      "2021-01-14 22:04:09,990 : INFO : diff #set()\n",
      "2021-01-14 22:04:29,031 : INFO : alphabet #32006\n",
      "2021-01-14 22:04:38,845 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.2068452326986618, 0.45313553718361144], [0.7860300689935684, 0.21396993], [0.9182958340544896, 0.9182958340544896], [3.94770277922009, 6.485445644653597, 6.714126141156713, 3.7190222827169723, 2.7664233619366234, 0.22868049650311661]]\n",
      "2021-01-14 22:04:38,849 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:04:38,851 : INFO : built Dictionary(209 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1382 corpus positions)\n",
      "2021-01-14 22:04:38,943 : INFO : token count processed\n",
      "2021-01-14 22:04:38,978 : INFO : frequencies processed\n",
      "2021-01-14 22:04:48,323 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:04:48,324 : INFO : entropies processed\n",
      "2021-01-14 22:04:48,325 : INFO : extropies processed\n",
      "2021-01-14 22:04:48,332 : INFO : token count processed\n",
      "2021-01-14 22:04:48,336 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:04:48,341 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:04:48,342 : INFO : vocab #32006\n",
      "2021-01-14 22:04:48,348 : INFO : diff #set()\n",
      "2021-01-14 22:05:07,295 : INFO : alphabet #32006\n",
      "2021-01-14 22:05:16,740 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.198169960734907, 0.4549238766167441], [0.7714392691850662, 0.22856073], [0.9182958340544896, 0.9182958340544896], [3.94770277922009, 6.2276600107346916, 6.344454717211003, 3.830908072743779, 2.3967519379909135, 0.11679470647631174]]\n",
      "2021-01-14 22:05:16,745 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:05:16,747 : INFO : built Dictionary(197 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1580 corpus positions)\n",
      "2021-01-14 22:05:16,823 : INFO : token count processed\n",
      "2021-01-14 22:05:16,855 : INFO : frequencies processed\n",
      "2021-01-14 22:05:26,396 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:05:26,397 : INFO : entropies processed\n",
      "2021-01-14 22:05:26,398 : INFO : extropies processed\n",
      "2021-01-14 22:05:26,405 : INFO : token count processed\n",
      "2021-01-14 22:05:26,409 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:05:26,414 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:05:26,415 : INFO : vocab #32006\n",
      "2021-01-14 22:05:26,421 : INFO : diff #set()\n",
      "2021-01-14 22:05:45,618 : INFO : alphabet #32006\n",
      "2021-01-14 22:05:55,058 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.200146990008153, 0.4545150867380431], [0.7713419795036316, 0.22865802], [1.9182958340544896, 1.2183406773511978], [3.94770277922009, 6.253918170574241, 6.415704234611363, 3.7859167151829674, 2.4680014553912732, 0.16178606403712248]]\n",
      "2021-01-14 22:05:55,062 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:05:55,064 : INFO : built Dictionary(171 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 677 corpus positions)\n",
      "2021-01-14 22:05:55,125 : INFO : token count processed\n",
      "2021-01-14 22:05:55,158 : INFO : frequencies processed\n",
      "2021-01-14 22:06:04,868 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:06:04,869 : INFO : entropies processed\n",
      "2021-01-14 22:06:04,869 : INFO : extropies processed\n",
      "2021-01-14 22:06:04,877 : INFO : token count processed\n",
      "2021-01-14 22:06:04,884 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:06:04,888 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:06:04,889 : INFO : vocab #32006\n",
      "2021-01-14 22:06:04,896 : INFO : diff #set()\n",
      "2021-01-14 22:06:23,755 : INFO : alphabet #32006\n",
      "2021-01-14 22:06:33,178 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.179754550244554, 0.45876724968313826], [0.7385807931423187, 0.2614192], [0.9182958340544896, 0.9182958340544896], [3.94770277922009, 6.374522245625576, 6.56006751330237, 3.762157511543295, 2.6123647340822798, 0.1855452676767939]]\n",
      "2021-01-14 22:06:33,184 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:06:33,185 : INFO : built Dictionary(293 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1959 corpus positions)\n",
      "2021-01-14 22:06:33,310 : INFO : token count processed\n",
      "2021-01-14 22:06:33,342 : INFO : frequencies processed\n",
      "2021-01-14 22:06:42,854 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:06:42,856 : INFO : entropies processed\n",
      "2021-01-14 22:06:42,856 : INFO : extropies processed\n",
      "2021-01-14 22:06:42,864 : INFO : token count processed\n",
      "2021-01-14 22:06:42,869 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:06:42,875 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:06:42,876 : INFO : vocab #32006\n",
      "2021-01-14 22:06:42,884 : INFO : diff #set()\n",
      "2021-01-14 22:07:01,766 : INFO : alphabet #32006\n",
      "2021-01-14 22:07:11,310 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.2302468333481744, 0.4483808630718881], [0.8200259357690811, 0.17997406], [0.9182958340544896, 0.9182958340544896], [3.94770277922009, 6.731238669067808, 6.987025422966266, 3.691916025321632, 3.039322643746176, 0.25578675389845795]]\n",
      "2021-01-14 22:07:11,315 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:07:11,316 : INFO : built Dictionary(217 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1351 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 22:07:11,407 : INFO : token count processed\n",
      "2021-01-14 22:07:11,469 : INFO : frequencies processed\n",
      "2021-01-14 22:07:20,877 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:07:20,878 : INFO : entropies processed\n",
      "2021-01-14 22:07:20,879 : INFO : extropies processed\n",
      "2021-01-14 22:07:20,886 : INFO : token count processed\n",
      "2021-01-14 22:07:20,890 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:07:20,894 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:07:20,895 : INFO : vocab #32006\n",
      "2021-01-14 22:07:20,903 : INFO : diff #set()\n",
      "2021-01-14 22:07:39,768 : INFO : alphabet #32006\n",
      "2021-01-14 22:07:49,303 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.1989171432180192, 0.45476929546173966], [0.7596077173948288, 0.24039228], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 6.503741451859337, 6.675597275359843, 3.775846955719585, 2.7278944961397533, 0.17185582350050588]]\n",
      "2021-01-14 22:07:49,308 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:07:49,310 : INFO : built Dictionary(237 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 1673 corpus positions)\n",
      "2021-01-14 22:07:49,409 : INFO : token count processed\n",
      "2021-01-14 22:07:49,441 : INFO : frequencies processed\n",
      "2021-01-14 22:07:58,860 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:07:58,862 : INFO : entropies processed\n",
      "2021-01-14 22:07:58,862 : INFO : extropies processed\n",
      "2021-01-14 22:07:58,870 : INFO : token count processed\n",
      "2021-01-14 22:07:58,876 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:07:58,880 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:07:58,881 : INFO : vocab #32006\n",
      "2021-01-14 22:07:58,888 : INFO : diff #set()\n",
      "2021-01-14 22:08:18,049 : INFO : alphabet #32006\n",
      "2021-01-14 22:08:27,480 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.1915836401541509, 0.45629104984998997], [0.7723140567541122, 0.22768594], [2.2359263506290326, 1.2653331222512112], [3.94770277922009, 6.334729224484471, 6.446480793086417, 3.835951210618143, 2.4987780138663274, 0.1117515686019459]]\n",
      "2021-01-14 22:08:27,486 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:08:27,488 : INFO : built Dictionary(236 unique tokens: ['.', 'ating', 'aut', 'ens', 'h']...) from 2 documents (total 2022 corpus positions)\n",
      "2021-01-14 22:08:27,582 : INFO : token count processed\n",
      "2021-01-14 22:08:27,614 : INFO : frequencies processed\n",
      "2021-01-14 22:08:37,015 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:08:37,016 : INFO : entropies processed\n",
      "2021-01-14 22:08:37,017 : INFO : extropies processed\n",
      "2021-01-14 22:08:37,024 : INFO : token count processed\n",
      "2021-01-14 22:08:37,028 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:08:37,033 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:08:37,034 : INFO : vocab #32006\n",
      "2021-01-14 22:08:37,041 : INFO : diff #set()\n",
      "2021-01-14 22:08:56,015 : INFO : alphabet #32006\n",
      "2021-01-14 22:09:05,313 : INFO : Computed distances or similarities ('272', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.216751615396879, 0.4511105317593121], [0.8276094943284988, 0.1723905], [1.5219280948873621, 1.1419011889093373], [3.94770277922009, 6.21319712067992, 6.393783979564223, 3.7671159203357867, 2.446081200344133, 0.18058685888430226]]\n",
      "2021-01-14 22:09:05,317 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 22:09:05,318 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:09:05,320 : INFO : built Dictionary(279 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 1417 corpus positions)\n",
      "2021-01-14 22:09:05,471 : INFO : token count processed\n",
      "2021-01-14 22:09:05,501 : INFO : frequencies processed\n",
      "2021-01-14 22:09:14,904 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:09:14,905 : INFO : entropies processed\n",
      "2021-01-14 22:09:14,906 : INFO : extropies processed\n",
      "2021-01-14 22:09:14,913 : INFO : token count processed\n",
      "2021-01-14 22:09:14,919 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:09:14,924 : INFO : alphabet_target #32010\n",
      "2021-01-14 22:09:14,925 : INFO : vocab #32006\n",
      "2021-01-14 22:09:14,932 : INFO : diff #set()\n",
      "2021-01-14 22:09:34,284 : INFO : alphabet #32006\n",
      "2021-01-14 22:09:43,727 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.2677559644301049, 0.4409645551307385], [0.9415600299835205, 0.05843997], [1.5, 1.1225562489182657], [4.3637132757501895, 6.905617163738059, 7.064581128652401, 4.204749310835847, 2.7008678529022117, 0.15896396491434217]]\n",
      "2021-01-14 22:09:43,733 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:09:43,735 : INFO : built Dictionary(370 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 2307 corpus positions)\n",
      "2021-01-14 22:09:43,960 : INFO : token count processed\n",
      "2021-01-14 22:09:43,992 : INFO : frequencies processed\n",
      "2021-01-14 22:09:53,413 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:09:53,414 : INFO : entropies processed\n",
      "2021-01-14 22:09:53,415 : INFO : extropies processed\n",
      "2021-01-14 22:09:53,431 : INFO : token count processed\n",
      "2021-01-14 22:09:53,436 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:09:53,440 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:09:53,441 : INFO : vocab #32006\n",
      "2021-01-14 22:09:53,447 : INFO : diff #set()\n",
      "2021-01-14 22:10:12,414 : INFO : alphabet #32006\n",
      "2021-01-14 22:10:21,838 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.2605182411593259, 0.4423764346564802], [0.9307096302509308, 0.06929037], [1.5, 1.1225562489182657], [4.3637132757501895, 7.1219284286457345, 7.354798753731058, 4.130842950664866, 2.991085477980868, 0.23287032508532324]]\n",
      "2021-01-14 22:10:21,844 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:10:21,846 : INFO : built Dictionary(289 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 2290 corpus positions)\n",
      "2021-01-14 22:10:22,007 : INFO : token count processed\n",
      "2021-01-14 22:10:22,039 : INFO : frequencies processed\n",
      "2021-01-14 22:10:31,452 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:10:31,453 : INFO : entropies processed\n",
      "2021-01-14 22:10:31,454 : INFO : extropies processed\n",
      "2021-01-14 22:10:31,467 : INFO : token count processed\n",
      "2021-01-14 22:10:31,472 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:10:31,477 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:10:31,478 : INFO : vocab #32006\n",
      "2021-01-14 22:10:31,486 : INFO : diff #set()\n",
      "2021-01-14 22:10:50,451 : INFO : alphabet #32006\n",
      "2021-01-14 22:11:00,044 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2707655432153722, 0.4403801189373404], [0.9442997425794601, 0.055700257], [1.5219280948873621, 1.1419011889093373], [4.3637132757501895, 6.41099024988467, 6.5027315744932705, 4.271971951141588, 2.139018298743081, 0.0917413246086003]]\n",
      "2021-01-14 22:11:00,048 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:11:00,049 : INFO : built Dictionary(169 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 689 corpus positions)\n",
      "2021-01-14 22:11:00,135 : INFO : token count processed\n",
      "2021-01-14 22:11:00,171 : INFO : frequencies processed\n",
      "2021-01-14 22:11:09,700 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:11:09,701 : INFO : entropies processed\n",
      "2021-01-14 22:11:09,702 : INFO : extropies processed\n",
      "2021-01-14 22:11:09,709 : INFO : token count processed\n",
      "2021-01-14 22:11:09,714 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:11:09,719 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:11:09,721 : INFO : vocab #32006\n",
      "2021-01-14 22:11:09,727 : INFO : diff #set()\n",
      "2021-01-14 22:11:28,596 : INFO : alphabet #32006\n",
      "2021-01-14 22:11:38,135 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.2738829766765172, 0.43977636943374687], [0.9504231996834278, 0.0495768], [0.0, 0.0], [4.3637132757501895, 6.077866832717642, 6.28427773315772, 4.157302375310112, 1.9205644574075302, 0.20641090044007804]]\n",
      "2021-01-14 22:11:38,139 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 22:11:38,140 : INFO : built Dictionary(146 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 454 corpus positions)\n",
      "2021-01-14 22:11:38,212 : INFO : token count processed\n",
      "2021-01-14 22:11:38,240 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 22:11:38,240 : INFO : frequencies processed\n",
      "2021-01-14 22:11:38,241 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 22:11:38,246 : INFO : token count processed\n",
      "2021-01-14 22:11:38,251 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:11:38,255 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:11:38,256 : INFO : vocab #32006\n",
      "2021-01-14 22:11:38,262 : INFO : diff #set()\n",
      "2021-01-14 22:11:57,100 : INFO : alphabet #32006\n",
      "2021-01-14 22:12:06,520 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.27299893024992, 0.43994741338925675], [0.9435373395681381, 0.05646266], [nan, nan], [4.3637132757501895, 5.977547459003844, 6.199411078638098, 4.1418496561159355, 1.8356978028879087, 0.22186361963425405]]\n",
      "2021-01-14 22:12:06,525 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:12:06,529 : INFO : built Dictionary(250 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 2165 corpus positions)\n",
      "2021-01-14 22:12:06,661 : INFO : token count processed\n",
      "2021-01-14 22:12:06,693 : INFO : frequencies processed\n",
      "2021-01-14 22:12:16,216 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:12:16,218 : INFO : entropies processed\n",
      "2021-01-14 22:12:16,219 : INFO : extropies processed\n",
      "2021-01-14 22:12:16,226 : INFO : token count processed\n",
      "2021-01-14 22:12:16,231 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:12:16,236 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:12:16,237 : INFO : vocab #32006\n",
      "2021-01-14 22:12:16,244 : INFO : diff #set()\n",
      "2021-01-14 22:12:35,140 : INFO : alphabet #32006\n",
      "2021-01-14 22:12:44,579 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.2712674482703412, 0.44028280366609357], [0.9434174299240112, 0.05658257], [0.0, 0.0], [4.3637132757501895, 6.4614394051846435, 6.58795457111909, 4.237198109815743, 2.2242412953689, 0.12651516593444612]]\n",
      "2021-01-14 22:12:44,584 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 22:12:44,585 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:12:44,587 : INFO : built Dictionary(212 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 1389 corpus positions)\n",
      "2021-01-14 22:12:44,711 : INFO : token count processed\n",
      "2021-01-14 22:12:44,769 : INFO : frequencies processed\n",
      "2021-01-14 22:12:54,288 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:12:54,289 : INFO : entropies processed\n",
      "2021-01-14 22:12:54,290 : INFO : extropies processed\n",
      "2021-01-14 22:12:54,303 : INFO : token count processed\n",
      "2021-01-14 22:12:54,308 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:12:54,314 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:12:54,315 : INFO : vocab #32006\n",
      "2021-01-14 22:12:54,322 : INFO : diff #set()\n",
      "2021-01-14 22:13:13,159 : INFO : alphabet #32006\n",
      "2021-01-14 22:13:22,680 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.272609394409298, 0.4400228224260783], [0.9469965547323227, 0.053003445], [0.0, 0.0], [4.3637132757501895, 6.327195724598159, 6.471333216867913, 4.219575783480435, 2.107619941117724, 0.14413749226975447]]\n",
      "2021-01-14 22:13:22,692 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 22:13:22,693 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:13:22,696 : INFO : built Dictionary(426 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 6281 corpus positions)\n",
      "2021-01-14 22:13:22,972 : INFO : token count processed\n",
      "2021-01-14 22:13:23,019 : INFO : frequencies processed\n",
      "2021-01-14 22:13:32,436 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:13:32,437 : INFO : entropies processed\n",
      "2021-01-14 22:13:32,438 : INFO : extropies processed\n",
      "2021-01-14 22:13:32,454 : INFO : token count processed\n",
      "2021-01-14 22:13:32,458 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:13:32,463 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:13:32,464 : INFO : vocab #32006\n",
      "2021-01-14 22:13:32,470 : INFO : diff #set()\n",
      "2021-01-14 22:13:51,454 : INFO : alphabet #32006\n",
      "2021-01-14 22:14:00,900 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.269919947101268, 0.4405441704131546], [0.9417845457792282, 0.058215454], [1.4591479170272446, 1.1091703386755989], [4.3637132757501895, 6.9079058562486315, 7.02376522613045, 4.2478539058683715, 2.660051950380261, 0.11585936988181889]]\n",
      "2021-01-14 22:14:00,907 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:14:00,909 : INFO : built Dictionary(329 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 2675 corpus positions)\n",
      "2021-01-14 22:14:01,109 : INFO : token count processed\n",
      "2021-01-14 22:14:01,141 : INFO : frequencies processed\n",
      "2021-01-14 22:14:10,568 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:14:10,569 : INFO : entropies processed\n",
      "2021-01-14 22:14:10,570 : INFO : extropies processed\n",
      "2021-01-14 22:14:10,578 : INFO : token count processed\n",
      "2021-01-14 22:14:10,585 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:14:10,590 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:14:10,591 : INFO : vocab #32006\n",
      "2021-01-14 22:14:10,598 : INFO : diff #set()\n",
      "2021-01-14 22:14:29,627 : INFO : alphabet #32006\n",
      "2021-01-14 22:14:39,072 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.2659234854251493, 0.44132116835903334], [0.9421208165585995, 0.057879183], [0.9182958340544896, 0.9182958340544896], [4.3637132757501895, 6.61034830706307, 6.783295504137689, 4.190766078675571, 2.4195822283874993, 0.17294719707461859]]\n",
      "2021-01-14 22:14:39,076 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:14:39,078 : INFO : built Dictionary(217 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 722 corpus positions)\n",
      "2021-01-14 22:14:39,186 : INFO : token count processed\n",
      "2021-01-14 22:14:39,214 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 22:14:39,214 : INFO : frequencies processed\n",
      "2021-01-14 22:14:39,218 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 22:14:39,223 : INFO : token count processed\n",
      "2021-01-14 22:14:39,230 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:14:39,233 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:14:39,234 : INFO : vocab #32006\n",
      "2021-01-14 22:14:39,242 : INFO : diff #set()\n",
      "2021-01-14 22:14:58,147 : INFO : alphabet #32006\n",
      "2021-01-14 22:15:07,691 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.2780637528911492, 0.4389692776292474], [0.9458793848752975, 0.054120615], [nan, nan], [4.3637132757501895, 6.616715366949855, 6.811227586733541, 4.169201055966504, 2.447514310983351, 0.19451221978368594]]\n",
      "2021-01-14 22:15:07,698 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:15:07,700 : INFO : built Dictionary(434 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 2756 corpus positions)\n",
      "2021-01-14 22:15:07,982 : INFO : token count processed\n",
      "2021-01-14 22:15:08,014 : INFO : frequencies processed\n",
      "2021-01-14 22:15:17,427 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:15:17,429 : INFO : entropies processed\n",
      "2021-01-14 22:15:17,429 : INFO : extropies processed\n",
      "2021-01-14 22:15:17,437 : INFO : token count processed\n",
      "2021-01-14 22:15:17,441 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:15:17,448 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:15:17,449 : INFO : vocab #32006\n",
      "2021-01-14 22:15:17,457 : INFO : diff #set()\n",
      "2021-01-14 22:15:36,582 : INFO : alphabet #32006\n",
      "2021-01-14 22:15:46,001 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.2582495413402293, 0.44282085823275247], [0.9275755062699318, 0.072424494], [1.4591479170272446, 1.1091703386755989], [4.3637132757501895, 7.32185870753746, 7.534297245192242, 4.151274738095408, 3.1705839694420526, 0.2124385376547817]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 22:15:46,004 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:15:46,005 : INFO : built Dictionary(70 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 121 corpus positions)\n",
      "2021-01-14 22:15:46,032 : INFO : token count processed\n",
      "2021-01-14 22:15:46,059 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 22:15:46,060 : INFO : frequencies processed\n",
      "2021-01-14 22:15:46,061 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 22:15:46,067 : INFO : token count processed\n",
      "2021-01-14 22:15:46,071 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:15:46,077 : INFO : alphabet_target #32008\n",
      "2021-01-14 22:15:46,078 : INFO : vocab #32006\n",
      "2021-01-14 22:15:46,085 : INFO : diff #set()\n",
      "2021-01-14 22:16:04,976 : INFO : alphabet #32006\n",
      "2021-01-14 22:16:14,804 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2793368844248727, 0.4387240898145349], [0.9658111818134785, 0.03418882], [nan, nan], [4.3637132757501895, 5.176618657501385, 5.8226921227038835, 3.717639810547692, 1.458978846953694, 0.6460734652024982]]\n",
      "2021-01-14 22:16:14,808 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:16:14,809 : INFO : built Dictionary(166 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 514 corpus positions)\n",
      "2021-01-14 22:16:14,886 : INFO : token count processed\n",
      "2021-01-14 22:16:14,914 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 22:16:14,917 : INFO : frequencies processed\n",
      "2021-01-14 22:16:14,917 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 22:16:14,923 : INFO : token count processed\n",
      "2021-01-14 22:16:14,926 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:16:14,930 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:16:14,931 : INFO : vocab #32006\n",
      "2021-01-14 22:16:14,938 : INFO : diff #set()\n",
      "2021-01-14 22:16:33,828 : INFO : alphabet #32006\n",
      "2021-01-14 22:16:43,373 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.2796080013404487, 0.4386719117549959], [0.9620320945978165, 0.037967905], [nan, nan], [4.3637132757501895, 6.468846789852156, 6.723104761501191, 4.109455304101155, 2.3593914857510017, 0.25425797164903496]]\n",
      "2021-01-14 22:16:43,379 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:16:43,381 : INFO : built Dictionary(380 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 2563 corpus positions)\n",
      "2021-01-14 22:16:43,613 : INFO : token count processed\n",
      "2021-01-14 22:16:43,655 : INFO : frequencies processed\n",
      "2021-01-14 22:16:53,084 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:16:53,085 : INFO : entropies processed\n",
      "2021-01-14 22:16:53,086 : INFO : extropies processed\n",
      "2021-01-14 22:16:53,094 : INFO : token count processed\n",
      "2021-01-14 22:16:53,100 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:16:53,104 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:16:53,105 : INFO : vocab #32006\n",
      "2021-01-14 22:16:53,113 : INFO : diff #set()\n",
      "2021-01-14 22:17:11,968 : INFO : alphabet #32006\n",
      "2021-01-14 22:17:21,483 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.2674062787251892, 0.44103256191132767], [0.9404605962336063, 0.059539404], [1.5219280948873621, 1.1419011889093373], [4.3637132757501895, 6.957796704012729, 7.113596529268253, 4.207913450494666, 2.749883253518063, 0.1557998252555235]]\n",
      "2021-01-14 22:17:21,490 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:17:21,494 : INFO : built Dictionary(290 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 3060 corpus positions)\n",
      "2021-01-14 22:17:21,662 : INFO : token count processed\n",
      "2021-01-14 22:17:21,713 : INFO : frequencies processed\n",
      "2021-01-14 22:17:31,050 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:17:31,051 : INFO : entropies processed\n",
      "2021-01-14 22:17:31,052 : INFO : extropies processed\n",
      "2021-01-14 22:17:31,060 : INFO : token count processed\n",
      "2021-01-14 22:17:31,065 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:17:31,070 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:17:31,071 : INFO : vocab #32006\n",
      "2021-01-14 22:17:31,078 : INFO : diff #set()\n",
      "2021-01-14 22:17:49,978 : INFO : alphabet #32006\n",
      "2021-01-14 22:17:59,532 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.266428637936308, 0.44122280457528457], [0.9427282735705376, 0.057271726], [0.9709505944546686, 0.9709505944546686], [4.3637132757501895, 6.441859572014148, 6.604282208030444, 4.2012906397338945, 2.2405689322802544, 0.16242263601629592]]\n",
      "2021-01-14 22:17:59,537 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:17:59,539 : INFO : built Dictionary(317 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 1536 corpus positions)\n",
      "2021-01-14 22:17:59,727 : INFO : token count processed\n",
      "2021-01-14 22:17:59,760 : INFO : frequencies processed\n",
      "2021-01-14 22:18:09,184 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:18:09,185 : INFO : entropies processed\n",
      "2021-01-14 22:18:09,186 : INFO : extropies processed\n",
      "2021-01-14 22:18:09,193 : INFO : token count processed\n",
      "2021-01-14 22:18:09,200 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:18:09,204 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:18:09,205 : INFO : vocab #32006\n",
      "2021-01-14 22:18:09,212 : INFO : diff #set()\n",
      "2021-01-14 22:18:28,347 : INFO : alphabet #32006\n",
      "2021-01-14 22:18:37,757 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.2671098081449361, 0.44109023586213075], [0.9300089478492737, 0.06999105], [0.0, 0.0], [4.3637132757501895, 6.998955278238291, 7.18695194648887, 4.175716607499611, 2.8232386707386805, 0.18799666825057937]]\n",
      "2021-01-14 22:18:37,762 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:18:37,763 : INFO : built Dictionary(232 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 1355 corpus positions)\n",
      "2021-01-14 22:18:37,881 : INFO : token count processed\n",
      "2021-01-14 22:18:37,914 : INFO : frequencies processed\n",
      "2021-01-14 22:18:47,317 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:18:47,318 : INFO : entropies processed\n",
      "2021-01-14 22:18:47,319 : INFO : extropies processed\n",
      "2021-01-14 22:18:47,326 : INFO : token count processed\n",
      "2021-01-14 22:18:47,331 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:18:47,335 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:18:47,336 : INFO : vocab #32006\n",
      "2021-01-14 22:18:47,345 : INFO : diff #set()\n",
      "2021-01-14 22:19:06,525 : INFO : alphabet #32006\n",
      "2021-01-14 22:19:15,957 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.263855052607933, 0.44172439346238723], [0.9310531839728355, 0.068946816], [0.0, 0.0], [4.3637132757501895, 6.492983191376071, 6.638241900929445, 4.218454566196816, 2.274528625179255, 0.1452587095533735]]\n",
      "2021-01-14 22:19:15,964 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:19:15,966 : INFO : built Dictionary(440 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 3294 corpus positions)\n",
      "2021-01-14 22:19:16,252 : INFO : token count processed\n",
      "2021-01-14 22:19:16,298 : INFO : frequencies processed\n",
      "2021-01-14 22:19:25,883 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:19:25,885 : INFO : entropies processed\n",
      "2021-01-14 22:19:25,885 : INFO : extropies processed\n",
      "2021-01-14 22:19:25,893 : INFO : token count processed\n",
      "2021-01-14 22:19:25,899 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:19:25,904 : INFO : alphabet_target #32008\n",
      "2021-01-14 22:19:25,905 : INFO : vocab #32006\n",
      "2021-01-14 22:19:25,912 : INFO : diff #set()\n",
      "2021-01-14 22:19:44,904 : INFO : alphabet #32006\n",
      "2021-01-14 22:19:54,330 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.217580012523075, 0.4509420153287906], [0.8661600798368454, 0.13383992], [0.9182958340544896, 0.9182958340544896], [4.3637132757501895, 6.560342487747443, 6.76110517892908, 4.162950584568552, 2.3973919031788906, 0.20076269118163736]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 22:19:54,337 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 22:19:54,338 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:19:54,341 : INFO : built Dictionary(457 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 3499 corpus positions)\n",
      "2021-01-14 22:19:54,630 : INFO : token count processed\n",
      "2021-01-14 22:19:54,662 : INFO : frequencies processed\n",
      "2021-01-14 22:20:04,067 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:20:04,068 : INFO : entropies processed\n",
      "2021-01-14 22:20:04,069 : INFO : extropies processed\n",
      "2021-01-14 22:20:04,077 : INFO : token count processed\n",
      "2021-01-14 22:20:04,081 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:20:04,085 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:20:04,086 : INFO : vocab #32006\n",
      "2021-01-14 22:20:04,095 : INFO : diff #set()\n",
      "2021-01-14 22:20:23,094 : INFO : alphabet #32006\n",
      "2021-01-14 22:20:32,524 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.265579036078876, 0.44138826501976214], [0.9414542503654957, 0.05854575], [0.9182958340544896, 0.9182958340544896], [4.3637132757501895, 7.046173750105238, 7.246209826185289, 4.163677199670138, 2.8824965504351, 0.20003607608005147]]\n",
      "2021-01-14 22:20:32,535 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 22:20:32,536 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:20:32,538 : INFO : built Dictionary(509 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 5608 corpus positions)\n",
      "2021-01-14 22:20:32,901 : INFO : token count processed\n",
      "2021-01-14 22:20:32,948 : INFO : frequencies processed\n",
      "2021-01-14 22:20:42,448 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:20:42,449 : INFO : entropies processed\n",
      "2021-01-14 22:20:42,450 : INFO : extropies processed\n",
      "2021-01-14 22:20:42,458 : INFO : token count processed\n",
      "2021-01-14 22:20:42,463 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:20:42,467 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:20:42,468 : INFO : vocab #32006\n",
      "2021-01-14 22:20:42,477 : INFO : diff #set()\n",
      "2021-01-14 22:21:01,338 : INFO : alphabet #32006\n",
      "2021-01-14 22:21:10,753 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.2689091680011741, 0.44074042897052745], [0.9417725503444672, 0.05822745], [1.9219280948873623, 1.2148067842293933], [4.3637132757501895, 7.009229588004272, 7.126302108930997, 4.246640754823465, 2.762588833180807, 0.11707252092672427]]\n",
      "2021-01-14 22:21:10,765 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:21:10,769 : INFO : built Dictionary(586 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 6549 corpus positions)\n",
      "2021-01-14 22:21:11,213 : INFO : token count processed\n",
      "2021-01-14 22:21:11,245 : INFO : frequencies processed\n",
      "2021-01-14 22:21:20,751 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:21:20,752 : INFO : entropies processed\n",
      "2021-01-14 22:21:20,752 : INFO : extropies processed\n",
      "2021-01-14 22:21:20,761 : INFO : token count processed\n",
      "2021-01-14 22:21:20,766 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:21:20,770 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:21:20,771 : INFO : vocab #32006\n",
      "2021-01-14 22:21:20,778 : INFO : diff #set()\n",
      "2021-01-14 22:21:39,582 : INFO : alphabet #32006\n",
      "2021-01-14 22:21:48,980 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.2640802256610302, 0.44168046196686156], [0.9329769983887672, 0.067023], [2.4193819456463714, 1.2761517340193214], [4.3637132757501895, 7.376088004590871, 7.5462570324881195, 4.193544247852941, 3.18254375673793, 0.17016902789724853]]\n",
      "2021-01-14 22:21:48,983 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:21:48,984 : INFO : built Dictionary(142 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 335 corpus positions)\n",
      "2021-01-14 22:21:49,046 : INFO : token count processed\n",
      "2021-01-14 22:21:49,073 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 22:21:49,074 : INFO : frequencies processed\n",
      "2021-01-14 22:21:49,074 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 22:21:49,079 : INFO : token count processed\n",
      "2021-01-14 22:21:49,083 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:21:49,087 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:21:49,087 : INFO : vocab #32006\n",
      "2021-01-14 22:21:49,095 : INFO : diff #set()\n",
      "2021-01-14 22:22:08,103 : INFO : alphabet #32006\n",
      "2021-01-14 22:22:17,521 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.2824836480106163, 0.43811923948352804], [0.9565736353397369, 0.043426365], [nan, nan], [4.3637132757501895, 6.2993628166120885, 6.577535136929892, 4.0855409554323865, 2.213821861179702, 0.27817232031780303]]\n",
      "2021-01-14 22:22:17,524 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:22:17,525 : INFO : built Dictionary(39 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 51 corpus positions)\n",
      "2021-01-14 22:22:17,539 : INFO : token count processed\n",
      "2021-01-14 22:22:17,566 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 22:22:17,567 : INFO : frequencies processed\n",
      "2021-01-14 22:22:17,568 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 22:22:17,574 : INFO : token count processed\n",
      "2021-01-14 22:22:17,580 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:22:17,584 : INFO : alphabet_target #32008\n",
      "2021-01-14 22:22:17,585 : INFO : vocab #32006\n",
      "2021-01-14 22:22:17,592 : INFO : diff #set()\n",
      "2021-01-14 22:22:36,635 : INFO : alphabet #32006\n",
      "2021-01-14 22:22:46,067 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.3059188913765702, 0.43366659761524723], [0.9858596045523882, 0.014140395], [nan, nan], [4.3637132757501895, 3.8936606896881862, 5.053285864435066, 3.2040881010033093, 0.6895725886848769, 1.1596251747468802]]\n",
      "2021-01-14 22:22:46,089 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:22:46,094 : INFO : built Dictionary(745 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 12493 corpus positions)\n",
      "2021-01-14 22:22:46,765 : INFO : token count processed\n",
      "2021-01-14 22:22:46,797 : INFO : frequencies processed\n",
      "2021-01-14 22:22:56,319 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:22:56,320 : INFO : entropies processed\n",
      "2021-01-14 22:22:56,321 : INFO : extropies processed\n",
      "2021-01-14 22:22:56,332 : INFO : token count processed\n",
      "2021-01-14 22:22:56,339 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:22:56,344 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:22:56,345 : INFO : vocab #32006\n",
      "2021-01-14 22:22:56,352 : INFO : diff #set()\n",
      "2021-01-14 22:23:15,387 : INFO : alphabet #32006\n",
      "2021-01-14 22:23:24,808 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.256104384685187, 0.4432419026301118], [0.9256488308310509, 0.07435117], [2.521640636343318, 1.2998438251349493], [4.3637132757501895, 7.434393313070278, 7.643453372592473, 4.154653216227993, 3.279740096842284, 0.2090600595221952]]\n",
      "2021-01-14 22:23:24,816 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:23:24,819 : INFO : built Dictionary(495 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 4120 corpus positions)\n",
      "2021-01-14 22:23:25,172 : INFO : token count processed\n",
      "2021-01-14 22:23:25,217 : INFO : frequencies processed\n",
      "2021-01-14 22:23:34,721 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:23:34,722 : INFO : entropies processed\n",
      "2021-01-14 22:23:34,723 : INFO : extropies processed\n",
      "2021-01-14 22:23:34,731 : INFO : token count processed\n",
      "2021-01-14 22:23:34,735 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:23:34,740 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:23:34,741 : INFO : vocab #32006\n",
      "2021-01-14 22:23:34,749 : INFO : diff #set()\n",
      "2021-01-14 22:23:53,643 : INFO : alphabet #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 22:24:03,088 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.2557373983428863, 0.4433140137387542], [0.9276094138622284, 0.072390586], [2.725480556997868, 1.3192201298976014], [4.3637132757501895, 7.2991514951718255, 7.49491845096004, 4.167946319961975, 3.1312051752098506, 0.19576695578821468]]\n",
      "2021-01-14 22:24:03,096 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:24:03,098 : INFO : built Dictionary(463 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 3537 corpus positions)\n",
      "2021-01-14 22:24:03,402 : INFO : token count processed\n",
      "2021-01-14 22:24:03,444 : INFO : frequencies processed\n",
      "2021-01-14 22:24:12,979 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:24:12,980 : INFO : entropies processed\n",
      "2021-01-14 22:24:12,981 : INFO : extropies processed\n",
      "2021-01-14 22:24:12,989 : INFO : token count processed\n",
      "2021-01-14 22:24:12,993 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:24:12,997 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:24:12,998 : INFO : vocab #32006\n",
      "2021-01-14 22:24:13,005 : INFO : diff #set()\n",
      "2021-01-14 22:24:31,899 : INFO : alphabet #32006\n",
      "2021-01-14 22:24:41,427 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.2682056843742837, 0.44087712454343136], [0.9412723146378994, 0.058727685], [2.2359263506290326, 1.2653331222512112], [4.3637132757501895, 7.170319527000998, 7.3643860690700045, 4.1696467336811835, 3.000672793319815, 0.19406654206900686]]\n",
      "2021-01-14 22:24:41,431 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:24:41,432 : INFO : built Dictionary(176 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 573 corpus positions)\n",
      "2021-01-14 22:24:41,512 : INFO : token count processed\n",
      "2021-01-14 22:24:41,539 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 22:24:41,542 : INFO : frequencies processed\n",
      "2021-01-14 22:24:41,543 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 22:24:41,550 : INFO : token count processed\n",
      "2021-01-14 22:24:41,554 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:24:41,560 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:24:41,561 : INFO : vocab #32006\n",
      "2021-01-14 22:24:41,568 : INFO : diff #set()\n",
      "2021-01-14 22:25:00,491 : INFO : alphabet #32006\n",
      "2021-01-14 22:25:10,045 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.281902707445126, 0.438230778524131], [0.9598209485411644, 0.04017905], [nan, nan], [4.3637132757501895, 6.353654804387375, 6.607835102702889, 4.109532977434675, 2.2441218269527, 0.25418029831551436]]\n",
      "2021-01-14 22:25:10,049 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:25:10,051 : INFO : built Dictionary(171 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 701 corpus positions)\n",
      "2021-01-14 22:25:10,128 : INFO : token count processed\n",
      "2021-01-14 22:25:10,156 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 22:25:10,158 : INFO : frequencies processed\n",
      "2021-01-14 22:25:10,159 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 22:25:10,164 : INFO : token count processed\n",
      "2021-01-14 22:25:10,170 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:25:10,174 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:25:10,177 : INFO : vocab #32006\n",
      "2021-01-14 22:25:10,183 : INFO : diff #set()\n",
      "2021-01-14 22:25:29,086 : INFO : alphabet #32006\n",
      "2021-01-14 22:25:38,530 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.282684433462341, 0.43808070241369945], [0.9635938294231892, 0.03640617], [nan, nan], [4.3637132757501895, 6.245180322479091, 6.458633217793121, 4.150260380436158, 2.0949199420429316, 0.21345289531403022]]\n",
      "2021-01-14 22:25:38,535 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:25:38,537 : INFO : built Dictionary(397 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 1773 corpus positions)\n",
      "2021-01-14 22:25:38,784 : INFO : token count processed\n",
      "2021-01-14 22:25:38,829 : INFO : frequencies processed\n",
      "2021-01-14 22:25:48,374 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:25:48,375 : INFO : entropies processed\n",
      "2021-01-14 22:25:48,376 : INFO : extropies processed\n",
      "2021-01-14 22:25:48,384 : INFO : token count processed\n",
      "2021-01-14 22:25:48,391 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:25:48,395 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:25:48,396 : INFO : vocab #32006\n",
      "2021-01-14 22:25:48,403 : INFO : diff #set()\n",
      "2021-01-14 22:26:07,164 : INFO : alphabet #32006\n",
      "2021-01-14 22:26:16,588 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.2594263579005656, 0.4425902160976776], [0.9341055080294609, 0.06589449], [1.5, 1.1225562489182657], [4.3637132757501895, 7.2691387000368, 7.4906896364138085, 4.142162339373181, 3.126976360663619, 0.2215509363770085]]\n",
      "2021-01-14 22:26:16,593 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:26:16,595 : INFO : built Dictionary(321 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 1610 corpus positions)\n",
      "2021-01-14 22:26:16,781 : INFO : token count processed\n",
      "2021-01-14 22:26:16,813 : INFO : frequencies processed\n",
      "2021-01-14 22:26:26,320 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:26:26,321 : INFO : entropies processed\n",
      "2021-01-14 22:26:26,322 : INFO : extropies processed\n",
      "2021-01-14 22:26:26,329 : INFO : token count processed\n",
      "2021-01-14 22:26:26,333 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:26:26,338 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:26:26,339 : INFO : vocab #32006\n",
      "2021-01-14 22:26:26,346 : INFO : diff #set()\n",
      "2021-01-14 22:26:45,197 : INFO : alphabet #32006\n",
      "2021-01-14 22:26:54,728 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.251490740638158, 0.44415017212842817], [0.9262082725763321, 0.07379173], [2.251629167387823, 1.2667563532600834], [4.3637132757501895, 7.08857858466988, 7.2725239705157545, 4.179767889904314, 2.908810694765565, 0.18394538584587483]]\n",
      "2021-01-14 22:26:54,732 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:26:54,733 : INFO : built Dictionary(150 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 533 corpus positions)\n",
      "2021-01-14 22:26:54,806 : INFO : token count processed\n",
      "2021-01-14 22:26:54,834 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 22:26:54,835 : INFO : frequencies processed\n",
      "2021-01-14 22:26:54,836 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 22:26:54,841 : INFO : token count processed\n",
      "2021-01-14 22:26:54,846 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:26:54,850 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:26:54,851 : INFO : vocab #32006\n",
      "2021-01-14 22:26:54,857 : INFO : diff #set()\n",
      "2021-01-14 22:27:13,711 : INFO : alphabet #32006\n",
      "2021-01-14 22:27:23,404 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.2838124536680369, 0.43786432567783645], [0.9617477655410767, 0.038252234], [nan, nan], [4.3637132757501895, 6.0479231618016716, 6.3026056051042385, 4.109030832447623, 1.938892329354049, 0.25468244330256695]]\n",
      "2021-01-14 22:27:23,408 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:27:23,409 : INFO : built Dictionary(152 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 652 corpus positions)\n",
      "2021-01-14 22:27:23,484 : INFO : token count processed\n",
      "2021-01-14 22:27:23,513 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 22:27:23,514 : INFO : frequencies processed\n",
      "2021-01-14 22:27:23,515 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 22:27:23,520 : INFO : token count processed\n",
      "2021-01-14 22:27:23,525 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:27:23,529 : INFO : alphabet_target #32009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 22:27:23,530 : INFO : vocab #32006\n",
      "2021-01-14 22:27:23,536 : INFO : diff #set()\n",
      "2021-01-14 22:27:42,658 : INFO : alphabet #32006\n",
      "2021-01-14 22:27:52,082 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.2838716680945899, 0.43785297307632415], [0.9663815014064312, 0.0336185], [nan, nan], [4.3637132757501895, 6.036583168403119, 6.257193640672153, 4.143102803481156, 1.8934803649219631, 0.2206104722690334]]\n",
      "2021-01-14 22:27:52,096 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 22:27:52,097 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:27:52,100 : INFO : built Dictionary(564 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 7010 corpus positions)\n",
      "2021-01-14 22:27:52,523 : INFO : token count processed\n",
      "2021-01-14 22:27:52,569 : INFO : frequencies processed\n",
      "2021-01-14 22:28:02,096 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:28:02,097 : INFO : entropies processed\n",
      "2021-01-14 22:28:02,098 : INFO : extropies processed\n",
      "2021-01-14 22:28:02,107 : INFO : token count processed\n",
      "2021-01-14 22:28:02,111 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:28:02,117 : INFO : alphabet_target #32010\n",
      "2021-01-14 22:28:02,118 : INFO : vocab #32006\n",
      "2021-01-14 22:28:02,127 : INFO : diff #set()\n",
      "2021-01-14 22:28:21,166 : INFO : alphabet #32006\n",
      "2021-01-14 22:28:30,579 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.2671906081978868, 0.44107451591591856], [0.9394089318811893, 0.060591068], [1.4591479170272448, 1.1091703386755989], [4.3637132757501895, 7.29352035514053, 7.457172913642385, 4.200060717248334, 3.0934596378921952, 0.16365255850185445]]\n",
      "2021-01-14 22:28:30,586 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:28:30,588 : INFO : built Dictionary(374 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 3253 corpus positions)\n",
      "2021-01-14 22:28:30,814 : INFO : token count processed\n",
      "2021-01-14 22:28:30,862 : INFO : frequencies processed\n",
      "2021-01-14 22:28:40,367 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:28:40,368 : INFO : entropies processed\n",
      "2021-01-14 22:28:40,369 : INFO : extropies processed\n",
      "2021-01-14 22:28:40,376 : INFO : token count processed\n",
      "2021-01-14 22:28:40,383 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:28:40,387 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:28:40,388 : INFO : vocab #32006\n",
      "2021-01-14 22:28:40,396 : INFO : diff #set()\n",
      "2021-01-14 22:28:59,237 : INFO : alphabet #32006\n",
      "2021-01-14 22:29:08,674 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.2705264582348066, 0.4404264906815655], [0.94332180544734, 0.056678195], [0.9182958340544896, 0.9182958340544896], [4.3637132757501895, 6.8153433747477745, 6.970004664733318, 4.2090519857646465, 2.606291388983128, 0.15466128998554307]]\n",
      "2021-01-14 22:29:08,678 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:29:08,679 : INFO : built Dictionary(127 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 274 corpus positions)\n",
      "2021-01-14 22:29:08,738 : INFO : token count processed\n",
      "2021-01-14 22:29:08,765 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 22:29:08,766 : INFO : frequencies processed\n",
      "2021-01-14 22:29:08,766 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 22:29:08,772 : INFO : token count processed\n",
      "2021-01-14 22:29:08,776 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:29:08,780 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:29:08,781 : INFO : vocab #32006\n",
      "2021-01-14 22:29:08,787 : INFO : diff #set()\n",
      "2021-01-14 22:29:27,737 : INFO : alphabet #32006\n",
      "2021-01-14 22:29:37,161 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.28325492589162, 0.4379712438853039], [0.9650842845439911, 0.034915715], [nan, nan], [4.3637132757501895, 6.150121915859574, 6.439494517945879, 4.074340673663886, 2.0757812421956894, 0.28937260208630455]]\n",
      "2021-01-14 22:29:37,166 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:29:37,167 : INFO : built Dictionary(290 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 985 corpus positions)\n",
      "2021-01-14 22:29:37,340 : INFO : token count processed\n",
      "2021-01-14 22:29:37,375 : INFO : frequencies processed\n",
      "2021-01-14 22:29:46,906 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:29:46,907 : INFO : entropies processed\n",
      "2021-01-14 22:29:46,908 : INFO : extropies processed\n",
      "2021-01-14 22:29:46,915 : INFO : token count processed\n",
      "2021-01-14 22:29:46,920 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:29:46,924 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:29:46,925 : INFO : vocab #32006\n",
      "2021-01-14 22:29:46,933 : INFO : diff #set()\n",
      "2021-01-14 22:30:05,792 : INFO : alphabet #32006\n",
      "2021-01-14 22:30:15,093 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.2559541041962665, 0.4432714292103349], [0.9276338443160057, 0.072366156], [0.9182958340544896, 0.9182958340544896], [4.3637132757501895, 7.0391145208191315, 7.310478786833679, 4.092349009735642, 2.9467655110834894, 0.27136426601454744]]\n",
      "2021-01-14 22:30:15,102 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 22:30:15,103 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:30:15,106 : INFO : built Dictionary(592 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 4360 corpus positions)\n",
      "2021-01-14 22:30:15,558 : INFO : token count processed\n",
      "2021-01-14 22:30:15,590 : INFO : frequencies processed\n",
      "2021-01-14 22:30:25,129 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:30:25,130 : INFO : entropies processed\n",
      "2021-01-14 22:30:25,131 : INFO : extropies processed\n",
      "2021-01-14 22:30:25,146 : INFO : token count processed\n",
      "2021-01-14 22:30:25,150 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:30:25,154 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:30:25,155 : INFO : vocab #32006\n",
      "2021-01-14 22:30:25,162 : INFO : diff #set()\n",
      "2021-01-14 22:30:44,050 : INFO : alphabet #32006\n",
      "2021-01-14 22:30:53,598 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.2448488666623372, 0.4454642870844172], [0.9081984609365463, 0.09180154], [2.2516291673878226, 1.2667563532600834], [4.3637132757501895, 7.482466367279176, 7.758240239902772, 4.087939403126594, 3.3945269641525826, 0.27577387262359654]]\n",
      "2021-01-14 22:30:53,602 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:30:53,603 : INFO : built Dictionary(176 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 582 corpus positions)\n",
      "2021-01-14 22:30:53,696 : INFO : token count processed\n",
      "2021-01-14 22:30:53,726 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 22:30:53,729 : INFO : frequencies processed\n",
      "2021-01-14 22:30:53,729 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 22:30:53,735 : INFO : token count processed\n",
      "2021-01-14 22:30:53,739 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:30:53,743 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:30:53,744 : INFO : vocab #32006\n",
      "2021-01-14 22:30:53,750 : INFO : diff #set()\n",
      "2021-01-14 22:31:12,604 : INFO : alphabet #32006\n",
      "2021-01-14 22:31:22,004 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.2822347518566528, 0.4381670199292496], [0.9605901800096035, 0.03940982], [nan, nan], [4.3637132757501895, 6.372162341197667, 6.621730735119646, 4.11414488182821, 2.258017459369457, 0.2495683939219795]]\n",
      "2021-01-14 22:31:22,010 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:31:22,012 : INFO : built Dictionary(324 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 2003 corpus positions)\n",
      "2021-01-14 22:31:22,205 : INFO : token count processed\n",
      "2021-01-14 22:31:22,238 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 22:31:31,761 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:31:31,762 : INFO : entropies processed\n",
      "2021-01-14 22:31:31,763 : INFO : extropies processed\n",
      "2021-01-14 22:31:31,777 : INFO : token count processed\n",
      "2021-01-14 22:31:31,782 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:31:31,787 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:31:31,788 : INFO : vocab #32006\n",
      "2021-01-14 22:31:31,796 : INFO : diff #set()\n",
      "2021-01-14 22:31:50,673 : INFO : alphabet #32006\n",
      "2021-01-14 22:32:00,201 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.2706566720152368, 0.44040123384769003], [0.9418618716299534, 0.05813813], [0.0, 0.0], [4.3637132757501895, 6.798155919669889, 7.003432730527932, 4.158436464892146, 2.639719454777742, 0.2052768108580425]]\n",
      "2021-01-14 22:32:00,205 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:32:00,206 : INFO : built Dictionary(180 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 719 corpus positions)\n",
      "2021-01-14 22:32:00,300 : INFO : token count processed\n",
      "2021-01-14 22:32:00,332 : INFO : frequencies processed\n",
      "2021-01-14 22:32:09,898 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:32:09,898 : INFO : entropies processed\n",
      "2021-01-14 22:32:09,901 : INFO : extropies processed\n",
      "2021-01-14 22:32:09,908 : INFO : token count processed\n",
      "2021-01-14 22:32:09,913 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:32:09,917 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:32:09,918 : INFO : vocab #32006\n",
      "2021-01-14 22:32:09,924 : INFO : diff #set()\n",
      "2021-01-14 22:32:28,795 : INFO : alphabet #32006\n",
      "2021-01-14 22:32:38,318 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.2751920779302766, 0.43952333066740096], [0.9502691552042961, 0.049730845], [0.0, 0.0], [4.3637132757501895, 6.271631856729336, 6.503273337720593, 4.132071794758932, 2.1395600619704034, 0.23164148099125725]]\n",
      "2021-01-14 22:32:38,325 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:32:38,329 : INFO : built Dictionary(356 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 3234 corpus positions)\n",
      "2021-01-14 22:32:38,559 : INFO : token count processed\n",
      "2021-01-14 22:32:38,591 : INFO : frequencies processed\n",
      "2021-01-14 22:32:48,025 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:32:48,026 : INFO : entropies processed\n",
      "2021-01-14 22:32:48,027 : INFO : extropies processed\n",
      "2021-01-14 22:32:48,034 : INFO : token count processed\n",
      "2021-01-14 22:32:48,039 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:32:48,043 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:32:48,044 : INFO : vocab #32006\n",
      "2021-01-14 22:32:48,051 : INFO : diff #set()\n",
      "2021-01-14 22:33:06,945 : INFO : alphabet #32006\n",
      "2021-01-14 22:33:16,519 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.2612239095288813, 0.4422383806335865], [0.9416366517543793, 0.05836335], [1.5, 1.1225562489182657], [4.3637132757501895, 6.873598627629562, 7.0381027715394175, 4.199209131840334, 2.674389495789228, 0.16450414390985557]]\n",
      "2021-01-14 22:33:16,522 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:33:16,524 : INFO : built Dictionary(107 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 182 corpus positions)\n",
      "2021-01-14 22:33:16,581 : INFO : token count processed\n",
      "2021-01-14 22:33:16,645 : INFO : frequencies processed\n",
      "2021-01-14 22:33:26,082 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:33:26,083 : INFO : entropies processed\n",
      "2021-01-14 22:33:26,084 : INFO : extropies processed\n",
      "2021-01-14 22:33:26,091 : INFO : token count processed\n",
      "2021-01-14 22:33:26,095 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:33:26,099 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:33:26,100 : INFO : vocab #32006\n",
      "2021-01-14 22:33:26,107 : INFO : diff #set()\n",
      "2021-01-14 22:33:45,081 : INFO : alphabet #32006\n",
      "2021-01-14 22:33:54,833 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.266457143921511, 0.441217255169344], [0.9400240927934647, 0.059975907], [0.0, 0.0], [4.3637132757501895, 6.049830202851529, 6.432558735318368, 3.9809847432833507, 2.0688454595681787, 0.3827285324668388]]\n",
      "2021-01-14 22:33:54,838 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:33:54,839 : INFO : built Dictionary(256 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 1262 corpus positions)\n",
      "2021-01-14 22:33:54,987 : INFO : token count processed\n",
      "2021-01-14 22:33:55,045 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 22:33:55,047 : INFO : frequencies processed\n",
      "2021-01-14 22:33:55,047 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 22:33:55,053 : INFO : token count processed\n",
      "2021-01-14 22:33:55,062 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:33:55,069 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:33:55,072 : INFO : vocab #32006\n",
      "2021-01-14 22:33:55,079 : INFO : diff #set()\n",
      "2021-01-14 22:34:16,361 : INFO : alphabet #32006\n",
      "2021-01-14 22:34:26,455 : INFO : Computed distances or similarities ('271', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.2723390552993616, 0.4400751717345537], [0.9487018883228302, 0.05129811], [nan, nan], [4.3637132757501895, 6.778844940588858, 7.005857544321268, 4.1367006720177795, 2.6421442685710783, 0.22701260373241006]]\n",
      "2021-01-14 22:34:26,458 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:34:26,459 : INFO : built Dictionary(152 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 295 corpus positions)\n",
      "2021-01-14 22:34:26,535 : INFO : token count processed\n",
      "2021-01-14 22:34:26,567 : INFO : frequencies processed\n",
      "2021-01-14 22:34:35,992 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:34:35,994 : INFO : entropies processed\n",
      "2021-01-14 22:34:35,995 : INFO : extropies processed\n",
      "2021-01-14 22:34:36,002 : INFO : token count processed\n",
      "2021-01-14 22:34:36,006 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:34:36,011 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:34:36,012 : INFO : vocab #32006\n",
      "2021-01-14 22:34:36,019 : INFO : diff #set()\n",
      "2021-01-14 22:34:55,047 : INFO : alphabet #32006\n",
      "2021-01-14 22:35:04,489 : INFO : Computed distances or similarities ('271', 'sacp-python-common/setup.py')[[1.2389193672186531, 0.446644043837216], [0.9065147340297699, 0.093485266], [1.5, 1.1225562489182657], [4.3637132757501895, 6.469677430851302, 6.794148647101852, 4.03924205949964, 2.4304353713516624, 0.32447121625054987]]\n",
      "2021-01-14 22:35:04,494 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:35:04,495 : INFO : built Dictionary(224 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 1197 corpus positions)\n",
      "2021-01-14 22:35:04,611 : INFO : token count processed\n",
      "2021-01-14 22:35:04,643 : INFO : frequencies processed\n",
      "2021-01-14 22:35:14,050 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:35:14,051 : INFO : entropies processed\n",
      "2021-01-14 22:35:14,052 : INFO : extropies processed\n",
      "2021-01-14 22:35:14,066 : INFO : token count processed\n",
      "2021-01-14 22:35:14,072 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:35:14,076 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:35:14,077 : INFO : vocab #32006\n",
      "2021-01-14 22:35:14,083 : INFO : diff #set()\n",
      "2021-01-14 22:35:33,093 : INFO : alphabet #32006\n",
      "2021-01-14 22:35:42,534 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.2704016631283908, 0.44045069920451785], [0.9330219477415085, 0.06697805], [0.0, 0.0], [4.3637132757501895, 6.459180448028249, 6.659991929869913, 4.162901793908525, 2.2962786541197238, 0.2008114818416642]]\n",
      "2021-01-14 22:35:42,537 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:35:42,538 : INFO : built Dictionary(134 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 347 corpus positions)\n",
      "2021-01-14 22:35:42,597 : INFO : token count processed\n",
      "2021-01-14 22:35:42,629 : INFO : frequencies processed\n",
      "2021-01-14 22:35:52,044 : INFO : scalar_distribution processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 22:35:52,045 : INFO : entropies processed\n",
      "2021-01-14 22:35:52,046 : INFO : extropies processed\n",
      "2021-01-14 22:35:52,053 : INFO : token count processed\n",
      "2021-01-14 22:35:52,061 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:35:52,066 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:35:52,067 : INFO : vocab #32006\n",
      "2021-01-14 22:35:52,073 : INFO : diff #set()\n",
      "2021-01-14 22:36:11,040 : INFO : alphabet #32006\n",
      "2021-01-14 22:36:20,630 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.2697535368597148, 0.4405764695419468], [0.9378220289945602, 0.06217797], [0.0, 0.0], [4.3637132757501895, 6.097125733496388, 6.379817819710697, 4.08102118953588, 2.016104543960507, 0.2826920862143085]]\n",
      "2021-01-14 22:36:20,633 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:36:20,634 : INFO : built Dictionary(126 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 343 corpus positions)\n",
      "2021-01-14 22:36:20,688 : INFO : token count processed\n",
      "2021-01-14 22:36:20,720 : INFO : frequencies processed\n",
      "2021-01-14 22:36:30,247 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:36:30,248 : INFO : entropies processed\n",
      "2021-01-14 22:36:30,249 : INFO : extropies processed\n",
      "2021-01-14 22:36:30,260 : INFO : token count processed\n",
      "2021-01-14 22:36:30,265 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:36:30,270 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:36:30,270 : INFO : vocab #32006\n",
      "2021-01-14 22:36:30,277 : INFO : diff #set()\n",
      "2021-01-14 22:36:49,165 : INFO : alphabet #32006\n",
      "2021-01-14 22:36:58,609 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.2710726956407008, 0.4403205594957348], [0.9393108449876308, 0.060689155], [0.0, 0.0], [4.3637132757501895, 6.0695858597523715, 6.34813055534324, 4.085168580159322, 1.9844172795930506, 0.2785446955908686]]\n",
      "2021-01-14 22:36:58,612 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 22:36:58,613 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:36:58,615 : INFO : built Dictionary(128 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 439 corpus positions)\n",
      "2021-01-14 22:36:58,677 : INFO : token count processed\n",
      "2021-01-14 22:36:58,712 : INFO : frequencies processed\n",
      "2021-01-14 22:37:08,238 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:37:08,239 : INFO : entropies processed\n",
      "2021-01-14 22:37:08,240 : INFO : extropies processed\n",
      "2021-01-14 22:37:08,247 : INFO : token count processed\n",
      "2021-01-14 22:37:08,251 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:37:08,256 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:37:08,257 : INFO : vocab #32006\n",
      "2021-01-14 22:37:08,263 : INFO : diff #set()\n",
      "2021-01-14 22:37:27,131 : INFO : alphabet #32006\n",
      "2021-01-14 22:37:36,549 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.2591065555121792, 0.4426528698082071], [0.9139029383659363, 0.08609706], [0.0, 0.0], [4.3637132757501895, 6.104787343210121, 6.32936819968881, 4.139132419271499, 1.9656549239386205, 0.2245808564786893]]\n",
      "2021-01-14 22:37:36,566 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 22:37:36,567 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:37:36,570 : INFO : built Dictionary(415 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 9124 corpus positions)\n",
      "2021-01-14 22:37:36,844 : INFO : token count processed\n",
      "2021-01-14 22:37:36,876 : INFO : frequencies processed\n",
      "2021-01-14 22:37:46,405 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:37:46,406 : INFO : entropies processed\n",
      "2021-01-14 22:37:46,407 : INFO : extropies processed\n",
      "2021-01-14 22:37:46,427 : INFO : token count processed\n",
      "2021-01-14 22:37:46,431 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:37:46,436 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:37:46,437 : INFO : vocab #32006\n",
      "2021-01-14 22:37:46,443 : INFO : diff #set()\n",
      "2021-01-14 22:38:05,314 : INFO : alphabet #32006\n",
      "2021-01-14 22:38:14,867 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.2597183259588547, 0.44253303100317826], [0.9170772582292557, 0.08292274], [0.9182958340544896, 0.9182958340544896], [4.3637132757501895, 6.89087415148015, 7.053697826351337, 4.2008896008790035, 2.6899845506011477, 0.16282367487118687]]\n",
      "2021-01-14 22:38:14,873 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:38:14,875 : INFO : built Dictionary(274 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 2284 corpus positions)\n",
      "2021-01-14 22:38:15,035 : INFO : token count processed\n",
      "2021-01-14 22:38:15,102 : INFO : frequencies processed\n",
      "2021-01-14 22:38:24,529 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:38:24,530 : INFO : entropies processed\n",
      "2021-01-14 22:38:24,531 : INFO : extropies processed\n",
      "2021-01-14 22:38:24,542 : INFO : token count processed\n",
      "2021-01-14 22:38:24,546 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:38:24,550 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:38:24,551 : INFO : vocab #32006\n",
      "2021-01-14 22:38:24,557 : INFO : diff #set()\n",
      "2021-01-14 22:38:43,185 : INFO : alphabet #32006\n",
      "2021-01-14 22:38:52,652 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.2639092665809801, 0.44171381546144217], [0.9187232404947281, 0.08127676], [0.0, 0.0], [4.3637132757501895, 6.655493573668506, 6.815141460392237, 4.204065389026459, 2.4514281846420474, 0.15964788672373054]]\n",
      "2021-01-14 22:38:52,657 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:38:52,658 : INFO : built Dictionary(251 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 1291 corpus positions)\n",
      "2021-01-14 22:38:52,796 : INFO : token count processed\n",
      "2021-01-14 22:38:52,864 : INFO : frequencies processed\n",
      "2021-01-14 22:39:02,258 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:39:02,260 : INFO : entropies processed\n",
      "2021-01-14 22:39:02,260 : INFO : extropies processed\n",
      "2021-01-14 22:39:02,267 : INFO : token count processed\n",
      "2021-01-14 22:39:02,272 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:39:02,276 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:39:02,277 : INFO : vocab #32006\n",
      "2021-01-14 22:39:02,283 : INFO : diff #set()\n",
      "2021-01-14 22:39:21,502 : INFO : alphabet #32006\n",
      "2021-01-14 22:39:31,100 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.2735977222648338, 0.43983154548723535], [0.9463722892105579, 0.05362771], [0.0, 0.0], [4.3637132757501895, 6.6236746347295465, 6.780021173969596, 4.20736673651014, 2.416307898219406, 0.15634653924004915]]\n",
      "2021-01-14 22:39:31,104 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:39:31,107 : INFO : built Dictionary(252 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 1230 corpus positions)\n",
      "2021-01-14 22:39:31,248 : INFO : token count processed\n",
      "2021-01-14 22:39:31,312 : INFO : frequencies processed\n",
      "2021-01-14 22:39:40,745 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:39:40,746 : INFO : entropies processed\n",
      "2021-01-14 22:39:40,747 : INFO : extropies processed\n",
      "2021-01-14 22:39:40,754 : INFO : token count processed\n",
      "2021-01-14 22:39:40,758 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:39:40,763 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:39:40,764 : INFO : vocab #32006\n",
      "2021-01-14 22:39:40,770 : INFO : diff #set()\n",
      "2021-01-14 22:39:59,758 : INFO : alphabet #32006\n",
      "2021-01-14 22:40:09,192 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.2629401146527959, 0.44190298873791917], [0.91893570125103, 0.0810643], [0.0, 0.0], [4.3637132757501895, 6.75472436518627, 6.906312629327697, 4.212125011608762, 2.5425993535775078, 0.15158826414142723]]\n",
      "2021-01-14 22:40:09,196 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:40:09,197 : INFO : built Dictionary(205 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 1025 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 22:40:09,311 : INFO : token count processed\n",
      "2021-01-14 22:40:09,373 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 22:40:09,375 : INFO : frequencies processed\n",
      "2021-01-14 22:40:09,376 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 22:40:09,382 : INFO : token count processed\n",
      "2021-01-14 22:40:09,391 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:40:09,395 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:40:09,396 : INFO : vocab #32006\n",
      "2021-01-14 22:40:09,402 : INFO : diff #set()\n",
      "2021-01-14 22:40:28,300 : INFO : alphabet #32006\n",
      "2021-01-14 22:40:37,856 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.2686069059435021, 0.44079915184076596], [0.9310912564396858, 0.06890874], [nan, nan], [4.3637132757501895, 6.597313085495733, 6.7538737687691945, 4.207152592476728, 2.390160493019005, 0.15656068327346162]]\n",
      "2021-01-14 22:40:37,860 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:40:37,862 : INFO : built Dictionary(229 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 991 corpus positions)\n",
      "2021-01-14 22:40:37,986 : INFO : token count processed\n",
      "2021-01-14 22:40:38,020 : INFO : frequencies processed\n",
      "2021-01-14 22:40:47,451 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:40:47,452 : INFO : entropies processed\n",
      "2021-01-14 22:40:47,453 : INFO : extropies processed\n",
      "2021-01-14 22:40:47,460 : INFO : token count processed\n",
      "2021-01-14 22:40:47,468 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:40:47,472 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:40:47,473 : INFO : vocab #32006\n",
      "2021-01-14 22:40:47,481 : INFO : diff #set()\n",
      "2021-01-14 22:41:06,499 : INFO : alphabet #32006\n",
      "2021-01-14 22:41:15,942 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.271148543754071, 0.4403058543881328], [0.9332190901041031, 0.06678091], [0.0, 0.0], [4.3637132757501895, 6.659481538516613, 6.831189607226538, 4.192005207040264, 2.467476331476348, 0.1717080687099246]]\n",
      "2021-01-14 22:41:15,947 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:41:15,948 : INFO : built Dictionary(253 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 1153 corpus positions)\n",
      "2021-01-14 22:41:16,082 : INFO : token count processed\n",
      "2021-01-14 22:41:16,115 : INFO : frequencies processed\n",
      "2021-01-14 22:41:25,530 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:41:25,533 : INFO : entropies processed\n",
      "2021-01-14 22:41:25,534 : INFO : extropies processed\n",
      "2021-01-14 22:41:25,545 : INFO : token count processed\n",
      "2021-01-14 22:41:25,552 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:41:25,561 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:41:25,562 : INFO : vocab #32006\n",
      "2021-01-14 22:41:25,570 : INFO : diff #set()\n",
      "2021-01-14 22:41:44,574 : INFO : alphabet #32006\n",
      "2021-01-14 22:41:54,006 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.2708271410837795, 0.4403681733003852], [0.9361939430236816, 0.06380606], [1.0, 1.0], [4.3637132757501895, 6.774682571479102, 6.912486542037419, 4.2259093051918715, 2.5487732662872293, 0.13780397055831717]]\n",
      "2021-01-14 22:41:54,021 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:41:54,024 : INFO : built Dictionary(431 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 7890 corpus positions)\n",
      "2021-01-14 22:41:54,316 : INFO : token count processed\n",
      "2021-01-14 22:41:54,348 : INFO : frequencies processed\n",
      "2021-01-14 22:42:03,775 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:42:03,776 : INFO : entropies processed\n",
      "2021-01-14 22:42:03,777 : INFO : extropies processed\n",
      "2021-01-14 22:42:03,792 : INFO : token count processed\n",
      "2021-01-14 22:42:03,798 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:42:03,803 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:42:03,804 : INFO : vocab #32006\n",
      "2021-01-14 22:42:03,812 : INFO : diff #set()\n",
      "2021-01-14 22:42:22,812 : INFO : alphabet #32006\n",
      "2021-01-14 22:42:32,248 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.2586436832866477, 0.44274358430226485], [0.9207434132695198, 0.07925659], [0.9182958340544896, 0.9182958340544896], [4.3637132757501895, 6.839453716525233, 6.994214224559587, 4.208952767715835, 2.6305009488093978, 0.1547605080343546]]\n",
      "2021-01-14 22:42:32,254 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:42:32,256 : INFO : built Dictionary(332 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 2332 corpus positions)\n",
      "2021-01-14 22:42:32,462 : INFO : token count processed\n",
      "2021-01-14 22:42:32,528 : INFO : frequencies processed\n",
      "2021-01-14 22:42:41,955 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:42:41,956 : INFO : entropies processed\n",
      "2021-01-14 22:42:41,957 : INFO : extropies processed\n",
      "2021-01-14 22:42:41,965 : INFO : token count processed\n",
      "2021-01-14 22:42:41,969 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:42:41,973 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:42:41,974 : INFO : vocab #32006\n",
      "2021-01-14 22:42:41,981 : INFO : diff #set()\n",
      "2021-01-14 22:43:00,984 : INFO : alphabet #32006\n",
      "2021-01-14 22:43:10,434 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.2626217158904616, 0.44196517384102224], [0.9155313819646835, 0.08446862], [0.0, 0.0], [4.3637132757501895, 6.86432793886027, 6.997442429049356, 4.230598785561104, 2.633729153299167, 0.1331144901890866]]\n",
      "2021-01-14 22:43:10,438 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:43:10,439 : INFO : built Dictionary(166 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 534 corpus positions)\n",
      "2021-01-14 22:43:10,521 : INFO : token count processed\n",
      "2021-01-14 22:43:10,574 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 22:43:10,581 : INFO : frequencies processed\n",
      "2021-01-14 22:43:10,581 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 22:43:10,587 : INFO : token count processed\n",
      "2021-01-14 22:43:10,595 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:43:10,602 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:43:10,604 : INFO : vocab #32006\n",
      "2021-01-14 22:43:10,610 : INFO : diff #set()\n",
      "2021-01-14 22:43:29,491 : INFO : alphabet #32006\n",
      "2021-01-14 22:43:38,913 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.2693783169575363, 0.44064931462844836], [0.9307370856404305, 0.069262914], [nan, nan], [4.3637132757501895, 6.431978396403875, 6.643092075227676, 4.152599596926388, 2.2793787994774863, 0.21111367882380083]]\n",
      "2021-01-14 22:43:38,917 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:43:38,919 : INFO : built Dictionary(225 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 688 corpus positions)\n",
      "2021-01-14 22:43:39,047 : INFO : token count processed\n",
      "2021-01-14 22:43:39,109 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 22:43:39,112 : INFO : frequencies processed\n",
      "2021-01-14 22:43:39,113 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 22:43:39,120 : INFO : token count processed\n",
      "2021-01-14 22:43:39,128 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:43:39,132 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:43:39,133 : INFO : vocab #32006\n",
      "2021-01-14 22:43:39,139 : INFO : diff #set()\n",
      "2021-01-14 22:43:58,142 : INFO : alphabet #32006\n",
      "2021-01-14 22:44:07,575 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/test_auth_utility.py')[[1.276244168730686, 0.4393201809090785], [0.9443301893770695, 0.05566981], [nan, nan], [4.3637132757501895, 6.911818353685893, 7.090795523703402, 4.18473610573268, 2.727082247953213, 0.17897717001750912]]\n",
      "2021-01-14 22:44:07,588 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:44:07,591 : INFO : built Dictionary(314 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 7218 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 22:44:07,779 : INFO : token count processed\n",
      "2021-01-14 22:44:07,812 : INFO : frequencies processed\n",
      "2021-01-14 22:44:17,234 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:44:17,235 : INFO : entropies processed\n",
      "2021-01-14 22:44:17,235 : INFO : extropies processed\n",
      "2021-01-14 22:44:17,245 : INFO : token count processed\n",
      "2021-01-14 22:44:17,249 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:44:17,255 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:44:17,256 : INFO : vocab #32006\n",
      "2021-01-14 22:44:17,264 : INFO : diff #set()\n",
      "2021-01-14 22:44:36,275 : INFO : alphabet #32006\n",
      "2021-01-14 22:44:45,712 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.2698249847835947, 0.4405626013916399], [0.9367193505167961, 0.06328065], [1.0, 1.0], [4.3637132757501895, 6.363791471162389, 6.406564652015337, 4.320940094897242, 2.0428513762651477, 0.04277318085294812]]\n",
      "2021-01-14 22:44:45,716 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:44:45,718 : INFO : built Dictionary(216 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 1288 corpus positions)\n",
      "2021-01-14 22:44:45,827 : INFO : token count processed\n",
      "2021-01-14 22:44:45,860 : INFO : frequencies processed\n",
      "2021-01-14 22:44:55,450 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:44:55,451 : INFO : entropies processed\n",
      "2021-01-14 22:44:55,452 : INFO : extropies processed\n",
      "2021-01-14 22:44:55,459 : INFO : token count processed\n",
      "2021-01-14 22:44:55,466 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:44:55,471 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:44:55,472 : INFO : vocab #32006\n",
      "2021-01-14 22:44:55,478 : INFO : diff #set()\n",
      "2021-01-14 22:45:14,657 : INFO : alphabet #32006\n",
      "2021-01-14 22:45:24,094 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.2726640480239728, 0.44001224064307964], [0.9475402645766735, 0.052459735], [0.0, 0.0], [4.3637132757501895, 6.29000629755059, 6.538975842955638, 4.1147437303451415, 2.175262567205449, 0.248969545405048]]\n",
      "2021-01-14 22:45:24,098 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:45:24,100 : INFO : built Dictionary(226 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 1241 corpus positions)\n",
      "2021-01-14 22:45:24,217 : INFO : token count processed\n",
      "2021-01-14 22:45:24,250 : INFO : frequencies processed\n",
      "2021-01-14 22:45:33,779 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:45:33,780 : INFO : entropies processed\n",
      "2021-01-14 22:45:33,781 : INFO : extropies processed\n",
      "2021-01-14 22:45:33,788 : INFO : token count processed\n",
      "2021-01-14 22:45:33,796 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:45:33,803 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:45:33,804 : INFO : vocab #32006\n",
      "2021-01-14 22:45:33,810 : INFO : diff #set()\n",
      "2021-01-14 22:45:52,708 : INFO : alphabet #32006\n",
      "2021-01-14 22:46:02,146 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.270589932355631, 0.4404141786018344], [0.9343586042523384, 0.065641396], [0.0, 0.0], [4.3637132757501895, 6.361621244785958, 6.595264333653679, 4.1300701868824685, 2.2315510579034896, 0.233643088867721]]\n",
      "2021-01-14 22:46:02,150 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:46:02,152 : INFO : built Dictionary(234 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 1264 corpus positions)\n",
      "2021-01-14 22:46:02,278 : INFO : token count processed\n",
      "2021-01-14 22:46:02,310 : INFO : frequencies processed\n",
      "2021-01-14 22:46:11,839 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:46:11,840 : INFO : entropies processed\n",
      "2021-01-14 22:46:11,841 : INFO : extropies processed\n",
      "2021-01-14 22:46:11,848 : INFO : token count processed\n",
      "2021-01-14 22:46:11,852 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:46:11,858 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:46:11,860 : INFO : vocab #32006\n",
      "2021-01-14 22:46:11,868 : INFO : diff #set()\n",
      "2021-01-14 22:46:30,751 : INFO : alphabet #32006\n",
      "2021-01-14 22:46:40,181 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.2601708628325372, 0.44244442596997274], [0.9189868271350861, 0.08101317], [0.0, 0.0], [4.3637132757501895, 6.620594433343389, 6.859540508253534, 4.124767200840044, 2.495827232503345, 0.23894607491014508]]\n",
      "2021-01-14 22:46:40,185 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:46:40,187 : INFO : built Dictionary(206 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 1357 corpus positions)\n",
      "2021-01-14 22:46:40,300 : INFO : token count processed\n",
      "2021-01-14 22:46:40,333 : INFO : frequencies processed\n",
      "2021-01-14 22:46:50,037 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:46:50,038 : INFO : entropies processed\n",
      "2021-01-14 22:46:50,039 : INFO : extropies processed\n",
      "2021-01-14 22:46:50,046 : INFO : token count processed\n",
      "2021-01-14 22:46:50,054 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:46:50,058 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:46:50,059 : INFO : vocab #32006\n",
      "2021-01-14 22:46:50,066 : INFO : diff #set()\n",
      "2021-01-14 22:47:08,965 : INFO : alphabet #32006\n",
      "2021-01-14 22:47:18,894 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.2582500040697047, 0.44282076749600363], [0.9059650078415871, 0.09403499], [0.0, 0.0], [4.3637132757501895, 6.207411496248084, 6.367838734114638, 4.203286037883636, 2.0041254583644488, 0.16042723786655433]]\n",
      "2021-01-14 22:47:18,897 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:47:18,898 : INFO : built Dictionary(147 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 261 corpus positions)\n",
      "2021-01-14 22:47:18,973 : INFO : token count processed\n",
      "2021-01-14 22:47:19,002 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 22:47:19,002 : INFO : frequencies processed\n",
      "2021-01-14 22:47:19,003 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 22:47:19,009 : INFO : token count processed\n",
      "2021-01-14 22:47:19,013 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:47:19,017 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:47:19,018 : INFO : vocab #32006\n",
      "2021-01-14 22:47:19,025 : INFO : diff #set()\n",
      "2021-01-14 22:47:37,801 : INFO : alphabet #32006\n",
      "2021-01-14 22:47:47,367 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.267597850122821, 0.44099530256030034], [0.9277740642428398, 0.072225936], [nan, nan], [4.3637132757501895, 6.5805228788529595, 6.853644012223729, 4.090592142379421, 2.4899307364735392, 0.27312113337076926]]\n",
      "2021-01-14 22:47:47,372 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 22:47:47,373 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:47:47,374 : INFO : built Dictionary(228 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 1222 corpus positions)\n",
      "2021-01-14 22:47:47,496 : INFO : token count processed\n",
      "2021-01-14 22:47:47,529 : INFO : frequencies processed\n",
      "2021-01-14 22:47:56,950 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:47:56,952 : INFO : entropies processed\n",
      "2021-01-14 22:47:56,952 : INFO : extropies processed\n",
      "2021-01-14 22:47:56,960 : INFO : token count processed\n",
      "2021-01-14 22:47:56,964 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:47:56,971 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:47:56,972 : INFO : vocab #32006\n",
      "2021-01-14 22:47:56,979 : INFO : diff #set()\n",
      "2021-01-14 22:48:15,876 : INFO : alphabet #32006\n",
      "2021-01-14 22:48:25,437 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.2740391949688774, 0.43974615838302905], [0.9410775676369667, 0.058922432], [0.9182958340544896, 0.9182958340544896], [4.3637132757501895, 6.422089779976135, 6.550416356291113, 4.235386699435211, 2.1867030805409238, 0.12832657631497835]]\n",
      "2021-01-14 22:48:25,442 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:48:25,444 : INFO : built Dictionary(241 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 1504 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 22:48:25,569 : INFO : token count processed\n",
      "2021-01-14 22:48:25,602 : INFO : frequencies processed\n",
      "2021-01-14 22:48:35,029 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:48:35,030 : INFO : entropies processed\n",
      "2021-01-14 22:48:35,031 : INFO : extropies processed\n",
      "2021-01-14 22:48:35,038 : INFO : token count processed\n",
      "2021-01-14 22:48:35,044 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:48:35,048 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:48:35,049 : INFO : vocab #32006\n",
      "2021-01-14 22:48:35,056 : INFO : diff #set()\n",
      "2021-01-14 22:48:53,975 : INFO : alphabet #32006\n",
      "2021-01-14 22:49:03,536 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.2653489335280612, 0.4414330989807371], [0.9316762164235115, 0.06832378], [0.0, 0.0], [4.3637132757501895, 6.485445644653597, 6.750267463875703, 4.098891456528083, 2.3865541881255137, 0.26482181922210657]]\n",
      "2021-01-14 22:49:03,540 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:49:03,542 : INFO : built Dictionary(217 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 1390 corpus positions)\n",
      "2021-01-14 22:49:03,658 : INFO : token count processed\n",
      "2021-01-14 22:49:03,687 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 22:49:03,688 : INFO : frequencies processed\n",
      "2021-01-14 22:49:03,688 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 22:49:03,694 : INFO : token count processed\n",
      "2021-01-14 22:49:03,698 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:49:03,702 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:49:03,703 : INFO : vocab #32006\n",
      "2021-01-14 22:49:03,710 : INFO : diff #set()\n",
      "2021-01-14 22:49:22,607 : INFO : alphabet #32006\n",
      "2021-01-14 22:49:32,562 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.2598278765387143, 0.44251157815242953], [0.9089315086603165, 0.09106849], [nan, nan], [4.3637132757501895, 6.2276600107346916, 6.386147909803901, 4.20522537668098, 2.0224346340537114, 0.15848789906920935]]\n",
      "2021-01-14 22:49:32,566 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:49:32,568 : INFO : built Dictionary(207 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 1588 corpus positions)\n",
      "2021-01-14 22:49:32,682 : INFO : token count processed\n",
      "2021-01-14 22:49:32,711 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 22:49:32,712 : INFO : frequencies processed\n",
      "2021-01-14 22:49:32,713 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 22:49:32,719 : INFO : token count processed\n",
      "2021-01-14 22:49:32,723 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:49:32,727 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:49:32,728 : INFO : vocab #32006\n",
      "2021-01-14 22:49:32,734 : INFO : diff #set()\n",
      "2021-01-14 22:49:51,645 : INFO : alphabet #32006\n",
      "2021-01-14 22:50:01,093 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.2733330740007198, 0.43988274812724754], [0.9366667345166206, 0.063333265], [nan, nan], [4.3637132757501895, 6.253918170574241, 6.459448687113914, 4.158182759210517, 2.0957354113637248, 0.2055305165396737]]\n",
      "2021-01-14 22:50:01,097 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:50:01,098 : INFO : built Dictionary(179 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 685 corpus positions)\n",
      "2021-01-14 22:50:01,183 : INFO : token count processed\n",
      "2021-01-14 22:50:01,209 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-14 22:50:01,210 : INFO : frequencies processed\n",
      "2021-01-14 22:50:01,212 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-14 22:50:01,218 : INFO : token count processed\n",
      "2021-01-14 22:50:01,223 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:50:01,227 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:50:01,227 : INFO : vocab #32006\n",
      "2021-01-14 22:50:01,235 : INFO : diff #set()\n",
      "2021-01-14 22:50:20,242 : INFO : alphabet #32006\n",
      "2021-01-14 22:50:29,679 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.2602171003195823, 0.442435374840145], [0.9144879132509232, 0.08551209], [nan, nan], [4.3637132757501895, 6.374522245625576, 6.626925749949926, 4.111309771425839, 2.2632124741997366, 0.25240350432435044]]\n",
      "2021-01-14 22:50:29,685 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:50:29,687 : INFO : built Dictionary(298 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 1967 corpus positions)\n",
      "2021-01-14 22:50:29,859 : INFO : token count processed\n",
      "2021-01-14 22:50:29,891 : INFO : frequencies processed\n",
      "2021-01-14 22:50:39,420 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:50:39,422 : INFO : entropies processed\n",
      "2021-01-14 22:50:39,422 : INFO : extropies processed\n",
      "2021-01-14 22:50:39,437 : INFO : token count processed\n",
      "2021-01-14 22:50:39,442 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:50:39,446 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:50:39,447 : INFO : vocab #32006\n",
      "2021-01-14 22:50:39,454 : INFO : diff #set()\n",
      "2021-01-14 22:50:58,369 : INFO : alphabet #32006\n",
      "2021-01-14 22:51:07,805 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.2471458657419001, 0.4450089401160649], [0.9084770604968071, 0.09152294], [1.584962500721156, 1.1699250014423124], [4.3637132757501895, 6.731238669067808, 7.010932765582611, 4.084019179235385, 2.6472194898324215, 0.27969409651480337]]\n",
      "2021-01-14 22:51:07,810 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:51:07,812 : INFO : built Dictionary(225 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 1359 corpus positions)\n",
      "2021-01-14 22:51:07,928 : INFO : token count processed\n",
      "2021-01-14 22:51:07,961 : INFO : frequencies processed\n",
      "2021-01-14 22:51:17,501 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:51:17,502 : INFO : entropies processed\n",
      "2021-01-14 22:51:17,503 : INFO : extropies processed\n",
      "2021-01-14 22:51:17,511 : INFO : token count processed\n",
      "2021-01-14 22:51:17,516 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:51:17,521 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:51:17,522 : INFO : vocab #32006\n",
      "2021-01-14 22:51:17,528 : INFO : diff #set()\n",
      "2021-01-14 22:51:36,323 : INFO : alphabet #32006\n",
      "2021-01-14 22:51:45,757 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.2525870167568471, 0.4439340156722318], [0.9199770465493202, 0.08002295], [0.0, 0.0], [4.3637132757501895, 6.503741451859337, 6.715184945205583, 4.152269782403944, 2.3514716694553934, 0.21144349334624568]]\n",
      "2021-01-14 22:51:45,762 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:51:45,763 : INFO : built Dictionary(247 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 1681 corpus positions)\n",
      "2021-01-14 22:51:45,908 : INFO : token count processed\n",
      "2021-01-14 22:51:45,941 : INFO : frequencies processed\n",
      "2021-01-14 22:51:55,473 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:51:55,474 : INFO : entropies processed\n",
      "2021-01-14 22:51:55,475 : INFO : extropies processed\n",
      "2021-01-14 22:51:55,482 : INFO : token count processed\n",
      "2021-01-14 22:51:55,488 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:51:55,492 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:51:55,493 : INFO : vocab #32006\n",
      "2021-01-14 22:51:55,500 : INFO : diff #set()\n",
      "2021-01-14 22:52:14,386 : INFO : alphabet #32006\n",
      "2021-01-14 22:52:23,932 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.2727522668836688, 0.4399951611845363], [0.928905762732029, 0.07109424], [0.0, 0.0], [4.3637132757501895, 6.334729224484471, 6.487205645315655, 4.211236854919006, 2.123492369565465, 0.1524764208311833]]\n",
      "2021-01-14 22:52:23,937 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:52:23,939 : INFO : built Dictionary(243 unique tokens: ['-', '2:', '4:', '76', '79']...) from 2 documents (total 2030 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 22:52:24,072 : INFO : token count processed\n",
      "2021-01-14 22:52:24,133 : INFO : frequencies processed\n",
      "2021-01-14 22:52:33,540 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:52:33,541 : INFO : entropies processed\n",
      "2021-01-14 22:52:33,541 : INFO : extropies processed\n",
      "2021-01-14 22:52:33,549 : INFO : token count processed\n",
      "2021-01-14 22:52:33,553 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:52:33,557 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:52:33,558 : INFO : vocab #32006\n",
      "2021-01-14 22:52:33,566 : INFO : diff #set()\n",
      "2021-01-14 22:52:52,440 : INFO : alphabet #32006\n",
      "2021-01-14 22:53:02,247 : INFO : Computed distances or similarities ('271', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.2657830276492028, 0.4413485262256205], [0.9262433722615242, 0.07375663], [0.9182958340544896, 0.9182958340544896], [4.3637132757501895, 6.21319712067992, 6.424331740695829, 4.152578655734281, 2.060618464945639, 0.21113462001590833]]\n",
      "2021-01-14 22:53:02,252 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 22:53:02,253 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:53:02,255 : INFO : built Dictionary(288 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1432 corpus positions)\n",
      "2021-01-14 22:53:02,514 : INFO : token count processed\n",
      "2021-01-14 22:53:02,583 : INFO : frequencies processed\n",
      "2021-01-14 22:53:12,002 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:53:12,002 : INFO : entropies processed\n",
      "2021-01-14 22:53:12,003 : INFO : extropies processed\n",
      "2021-01-14 22:53:12,010 : INFO : token count processed\n",
      "2021-01-14 22:53:12,014 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:53:12,018 : INFO : alphabet_target #32010\n",
      "2021-01-14 22:53:12,019 : INFO : vocab #32006\n",
      "2021-01-14 22:53:12,026 : INFO : diff #set()\n",
      "2021-01-14 22:53:31,181 : INFO : alphabet #32006\n",
      "2021-01-14 22:53:40,595 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.2226436738599142, 0.4499146722260559], [0.8792147040367126, 0.120785296], [2.807354922057604, 1.3343545280186873], [5.064869077788814, 6.905617163738059, 7.10740043274938, 4.863085808777495, 2.0425313549605653, 0.2017832690113206]]\n",
      "2021-01-14 22:53:40,601 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:53:40,605 : INFO : built Dictionary(377 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 2322 corpus positions)\n",
      "2021-01-14 22:53:40,999 : INFO : token count processed\n",
      "2021-01-14 22:53:41,069 : INFO : frequencies processed\n",
      "2021-01-14 22:53:50,494 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:53:50,496 : INFO : entropies processed\n",
      "2021-01-14 22:53:50,497 : INFO : extropies processed\n",
      "2021-01-14 22:53:50,504 : INFO : token count processed\n",
      "2021-01-14 22:53:50,510 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:53:50,515 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:53:50,516 : INFO : vocab #32006\n",
      "2021-01-14 22:53:50,524 : INFO : diff #set()\n",
      "2021-01-14 22:54:09,544 : INFO : alphabet #32006\n",
      "2021-01-14 22:54:18,984 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.2085934271761372, 0.4527768613703518], [0.8816141858696938, 0.118385814], [3.121928094887362, 1.3519647487142497], [5.064869077788814, 7.1219284286457345, 7.378966920112707, 4.807830586321841, 2.3140978423238927, 0.25703849146697255]]\n",
      "2021-01-14 22:54:18,990 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:54:18,992 : INFO : built Dictionary(296 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 2305 corpus positions)\n",
      "2021-01-14 22:54:19,246 : INFO : token count processed\n",
      "2021-01-14 22:54:19,292 : INFO : frequencies processed\n",
      "2021-01-14 22:54:28,712 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:54:28,713 : INFO : entropies processed\n",
      "2021-01-14 22:54:28,714 : INFO : extropies processed\n",
      "2021-01-14 22:54:28,722 : INFO : token count processed\n",
      "2021-01-14 22:54:28,728 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:54:28,732 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:54:28,733 : INFO : vocab #32006\n",
      "2021-01-14 22:54:28,740 : INFO : diff #set()\n",
      "2021-01-14 22:54:47,744 : INFO : alphabet #32006\n",
      "2021-01-14 22:54:57,173 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2495074559859523, 0.4445417583920401], [0.9292048737406731, 0.070795126], [3.121928094887362, 1.3519647487142497], [5.064869077788814, 6.41099024988467, 6.53589160884029, 4.939967718833195, 1.4710225310514753, 0.12490135895561938]]\n",
      "2021-01-14 22:54:57,177 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:54:57,178 : INFO : built Dictionary(180 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 704 corpus positions)\n",
      "2021-01-14 22:54:57,318 : INFO : token count processed\n",
      "2021-01-14 22:54:57,385 : INFO : frequencies processed\n",
      "2021-01-14 22:55:06,785 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:55:06,786 : INFO : entropies processed\n",
      "2021-01-14 22:55:06,787 : INFO : extropies processed\n",
      "2021-01-14 22:55:06,795 : INFO : token count processed\n",
      "2021-01-14 22:55:06,802 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:55:06,806 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:55:06,807 : INFO : vocab #32006\n",
      "2021-01-14 22:55:06,814 : INFO : diff #set()\n",
      "2021-01-14 22:55:25,825 : INFO : alphabet #32006\n",
      "2021-01-14 22:55:35,259 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.2417650254107717, 0.44607708152497566], [0.9199302867054939, 0.08006971], [1.584962500721156, 1.1699250014423124], [5.064869077788814, 6.077866832717642, 6.369661854261491, 4.773074056244965, 1.3047927764726763, 0.29179502154384895]]\n",
      "2021-01-14 22:55:35,263 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:55:35,264 : INFO : built Dictionary(154 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 469 corpus positions)\n",
      "2021-01-14 22:55:35,374 : INFO : token count processed\n",
      "2021-01-14 22:55:35,409 : INFO : frequencies processed\n",
      "2021-01-14 22:55:44,606 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:55:44,607 : INFO : entropies processed\n",
      "2021-01-14 22:55:44,608 : INFO : extropies processed\n",
      "2021-01-14 22:55:44,622 : INFO : token count processed\n",
      "2021-01-14 22:55:44,625 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:55:44,629 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:55:44,630 : INFO : vocab #32006\n",
      "2021-01-14 22:55:44,635 : INFO : diff #set()\n",
      "2021-01-14 22:56:03,828 : INFO : alphabet #32006\n",
      "2021-01-14 22:56:13,197 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.238928060711579, 0.4466423095712056], [0.9156260043382645, 0.084373996], [2.321928094887362, 1.2877123795494492], [5.064869077788814, 5.977547459003844, 6.29908110167344, 4.743335435119218, 1.2342120238846253, 0.3215336426695954]]\n",
      "2021-01-14 22:56:13,202 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:56:13,204 : INFO : built Dictionary(259 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 2180 corpus positions)\n",
      "2021-01-14 22:56:13,415 : INFO : token count processed\n",
      "2021-01-14 22:56:13,468 : INFO : frequencies processed\n",
      "2021-01-14 22:56:22,988 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:56:22,989 : INFO : entropies processed\n",
      "2021-01-14 22:56:22,990 : INFO : extropies processed\n",
      "2021-01-14 22:56:22,997 : INFO : token count processed\n",
      "2021-01-14 22:56:23,001 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:56:23,006 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:56:23,007 : INFO : vocab #32006\n",
      "2021-01-14 22:56:23,014 : INFO : diff #set()\n",
      "2021-01-14 22:56:41,848 : INFO : alphabet #32006\n",
      "2021-01-14 22:56:51,390 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.2431759571936873, 0.4457965041899987], [0.916472040116787, 0.08352796], [2.251629167387823, 1.2667563532600834], [5.064869077788814, 6.4614394051846435, 6.621116141383551, 4.905192341589908, 1.5562470635947365, 0.15967673619890732]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 22:56:51,395 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 22:56:51,396 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:56:51,398 : INFO : built Dictionary(222 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1404 corpus positions)\n",
      "2021-01-14 22:56:51,575 : INFO : token count processed\n",
      "2021-01-14 22:56:51,637 : INFO : frequencies processed\n",
      "2021-01-14 22:57:01,066 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:57:01,067 : INFO : entropies processed\n",
      "2021-01-14 22:57:01,068 : INFO : extropies processed\n",
      "2021-01-14 22:57:01,081 : INFO : token count processed\n",
      "2021-01-14 22:57:01,086 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:57:01,090 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:57:01,091 : INFO : vocab #32006\n",
      "2021-01-14 22:57:01,097 : INFO : diff #set()\n",
      "2021-01-14 22:57:19,964 : INFO : alphabet #32006\n",
      "2021-01-14 22:57:29,497 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.2240415319780413, 0.4496318911412637], [0.8882625475525856, 0.11173745], [2.0, 1.2451124978365313], [5.064869077788814, 6.327195724598159, 6.522603887951707, 4.869460914435265, 1.4577348101628926, 0.19540816335354805]]\n",
      "2021-01-14 22:57:29,509 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 22:57:29,510 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:57:29,513 : INFO : built Dictionary(430 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 6296 corpus positions)\n",
      "2021-01-14 22:57:29,962 : INFO : token count processed\n",
      "2021-01-14 22:57:30,010 : INFO : frequencies processed\n",
      "2021-01-14 22:57:39,401 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:57:39,402 : INFO : entropies processed\n",
      "2021-01-14 22:57:39,403 : INFO : extropies processed\n",
      "2021-01-14 22:57:39,412 : INFO : token count processed\n",
      "2021-01-14 22:57:39,416 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:57:39,421 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:57:39,422 : INFO : vocab #32006\n",
      "2021-01-14 22:57:39,429 : INFO : diff #set()\n",
      "2021-01-14 22:57:58,336 : INFO : alphabet #32006\n",
      "2021-01-14 22:58:07,894 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.2352976416321044, 0.44736771576864776], [0.9079018011689186, 0.0920982], [3.521640636343319, 1.3740281872300928], [5.064869077788814, 6.9079058562486315, 7.03493797630949, 4.9378369577279555, 1.970068898520676, 0.12703212006085884]]\n",
      "2021-01-14 22:58:07,901 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:58:07,903 : INFO : built Dictionary(333 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 2690 corpus positions)\n",
      "2021-01-14 22:58:08,210 : INFO : token count processed\n",
      "2021-01-14 22:58:08,240 : INFO : frequencies processed\n",
      "2021-01-14 22:58:17,657 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:58:17,658 : INFO : entropies processed\n",
      "2021-01-14 22:58:17,659 : INFO : extropies processed\n",
      "2021-01-14 22:58:17,667 : INFO : token count processed\n",
      "2021-01-14 22:58:17,671 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:58:17,675 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:58:17,677 : INFO : vocab #32006\n",
      "2021-01-14 22:58:17,684 : INFO : diff #set()\n",
      "2021-01-14 22:58:36,702 : INFO : alphabet #32006\n",
      "2021-01-14 22:58:46,137 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.2129175824048772, 0.4518921120023164], [0.8798148408532143, 0.12018516], [3.4182958340544896, 1.369895090630202], [5.064869077788814, 6.61034830706307, 6.805619571057553, 4.86959781379433, 1.740750493268739, 0.19527126399448314]]\n",
      "2021-01-14 22:58:46,141 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:58:46,142 : INFO : built Dictionary(224 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 737 corpus positions)\n",
      "2021-01-14 22:58:46,321 : INFO : token count processed\n",
      "2021-01-14 22:58:46,354 : INFO : frequencies processed\n",
      "2021-01-14 22:58:55,762 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:58:55,763 : INFO : entropies processed\n",
      "2021-01-14 22:58:55,764 : INFO : extropies processed\n",
      "2021-01-14 22:58:55,771 : INFO : token count processed\n",
      "2021-01-14 22:58:55,775 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:58:55,781 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:58:55,782 : INFO : vocab #32006\n",
      "2021-01-14 22:58:55,789 : INFO : diff #set()\n",
      "2021-01-14 22:59:15,141 : INFO : alphabet #32006\n",
      "2021-01-14 22:59:24,564 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.2292884110421445, 0.44857363230651764], [0.9057594686746597, 0.09424053], [2.584962500721156, 1.315172029168969], [5.064869077788814, 6.616715366949855, 6.8687346190976, 4.812849825641068, 1.8038655413087854, 0.2520192521477451]]\n",
      "2021-01-14 22:59:24,570 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 22:59:24,572 : INFO : built Dictionary(440 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 2771 corpus positions)\n",
      "2021-01-14 22:59:25,011 : INFO : token count processed\n",
      "2021-01-14 22:59:25,043 : INFO : frequencies processed\n",
      "2021-01-14 22:59:34,434 : INFO : scalar_distribution processed\n",
      "2021-01-14 22:59:34,435 : INFO : entropies processed\n",
      "2021-01-14 22:59:34,436 : INFO : extropies processed\n",
      "2021-01-14 22:59:34,444 : INFO : token count processed\n",
      "2021-01-14 22:59:34,449 : INFO : alphabet_source #32006\n",
      "2021-01-14 22:59:34,454 : INFO : alphabet_target #32009\n",
      "2021-01-14 22:59:34,455 : INFO : vocab #32006\n",
      "2021-01-14 22:59:34,462 : INFO : diff #set()\n",
      "2021-01-14 22:59:53,476 : INFO : alphabet #32006\n",
      "2021-01-14 23:00:02,921 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.1894077050468275, 0.4567445330967317], [0.837186336517334, 0.16281366], [3.2776134368191157, 1.3618978811135465], [5.064869077788814, 7.32185870753746, 7.55164524825549, 4.835082537070786, 2.4867761704666753, 0.2297865407180293]]\n",
      "2021-01-14 23:00:02,924 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:00:02,925 : INFO : built Dictionary(81 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 136 corpus positions)\n",
      "2021-01-14 23:00:02,964 : INFO : token count processed\n",
      "2021-01-14 23:00:02,996 : INFO : frequencies processed\n",
      "2021-01-14 23:00:12,435 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:00:12,436 : INFO : entropies processed\n",
      "2021-01-14 23:00:12,437 : INFO : extropies processed\n",
      "2021-01-14 23:00:12,446 : INFO : token count processed\n",
      "2021-01-14 23:00:12,451 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:00:12,455 : INFO : alphabet_target #32008\n",
      "2021-01-14 23:00:12,455 : INFO : vocab #32006\n",
      "2021-01-14 23:00:12,462 : INFO : diff #set()\n",
      "2021-01-14 23:00:31,460 : INFO : alphabet #32006\n",
      "2021-01-14 23:00:40,903 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2290780326192527, 0.4486159682911421], [0.9018843099474907, 0.09811569], [1.0, 1.0], [5.064869077788814, 5.176618657501385, 6.047377988016916, 4.194109747273283, 0.9825089102281019, 0.870759330515531]]\n",
      "2021-01-14 23:00:40,907 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:00:40,908 : INFO : built Dictionary(175 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 529 corpus positions)\n",
      "2021-01-14 23:00:41,030 : INFO : token count processed\n",
      "2021-01-14 23:00:41,056 : INFO : frequencies processed\n",
      "2021-01-14 23:00:50,495 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:00:50,496 : INFO : entropies processed\n",
      "2021-01-14 23:00:50,497 : INFO : extropies processed\n",
      "2021-01-14 23:00:50,504 : INFO : token count processed\n",
      "2021-01-14 23:00:50,512 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:00:50,517 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:00:50,518 : INFO : vocab #32006\n",
      "2021-01-14 23:00:50,525 : INFO : diff #set()\n",
      "2021-01-14 23:01:09,418 : INFO : alphabet #32006\n",
      "2021-01-14 23:01:18,838 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.230572889996195, 0.4483153204653652], [0.9075010120868683, 0.09249899], [2.0, 1.2451124978365313], [5.064869077788814, 6.468846789852156, 6.802600388368698, 4.731115479272273, 1.7377313105798837, 0.33375359851654185]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 23:01:18,844 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:01:18,846 : INFO : built Dictionary(387 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 2578 corpus positions)\n",
      "2021-01-14 23:01:19,236 : INFO : token count processed\n",
      "2021-01-14 23:01:19,266 : INFO : frequencies processed\n",
      "2021-01-14 23:01:28,816 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:01:28,817 : INFO : entropies processed\n",
      "2021-01-14 23:01:28,818 : INFO : extropies processed\n",
      "2021-01-14 23:01:28,826 : INFO : token count processed\n",
      "2021-01-14 23:01:28,833 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:01:28,837 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:01:28,838 : INFO : vocab #32006\n",
      "2021-01-14 23:01:28,846 : INFO : diff #set()\n",
      "2021-01-14 23:01:47,754 : INFO : alphabet #32006\n",
      "2021-01-14 23:01:57,316 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.2290477151161936, 0.44862206996222737], [0.9060476645827293, 0.093952335], [3.121928094887362, 1.3519647487142497], [5.064869077788814, 6.957796704012729, 7.138237367246616, 4.884428414554926, 2.073368289457802, 0.1804406632338873]]\n",
      "2021-01-14 23:01:57,323 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:01:57,325 : INFO : built Dictionary(300 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 3075 corpus positions)\n",
      "2021-01-14 23:01:57,579 : INFO : token count processed\n",
      "2021-01-14 23:01:57,609 : INFO : frequencies processed\n",
      "2021-01-14 23:02:07,069 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:02:07,070 : INFO : entropies processed\n",
      "2021-01-14 23:02:07,071 : INFO : extropies processed\n",
      "2021-01-14 23:02:07,078 : INFO : token count processed\n",
      "2021-01-14 23:02:07,084 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:02:07,089 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:02:07,090 : INFO : vocab #32006\n",
      "2021-01-14 23:02:07,097 : INFO : diff #set()\n",
      "2021-01-14 23:02:25,980 : INFO : alphabet #32006\n",
      "2021-01-14 23:02:35,520 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.2332908181347542, 0.4477697180679768], [0.9123194962739944, 0.087680504], [2.251629167387823, 1.2667563532600834], [5.064869077788814, 6.441859572014148, 6.63107091157552, 4.875657738227442, 1.5662018337867059, 0.18921133956137215]]\n",
      "2021-01-14 23:02:35,525 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:02:35,527 : INFO : built Dictionary(322 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1551 corpus positions)\n",
      "2021-01-14 23:02:35,831 : INFO : token count processed\n",
      "2021-01-14 23:02:35,879 : INFO : frequencies processed\n",
      "2021-01-14 23:02:45,300 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:02:45,301 : INFO : entropies processed\n",
      "2021-01-14 23:02:45,302 : INFO : extropies processed\n",
      "2021-01-14 23:02:45,310 : INFO : token count processed\n",
      "2021-01-14 23:02:45,317 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:02:45,321 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:02:45,322 : INFO : vocab #32006\n",
      "2021-01-14 23:02:45,329 : INFO : diff #set()\n",
      "2021-01-14 23:03:04,405 : INFO : alphabet #32006\n",
      "2021-01-14 23:03:13,983 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.2159510744314805, 0.45127350126922716], [0.8850871622562408, 0.11491284], [3.169925001442312, 1.3594000115384994], [5.064869077788814, 6.998955278238291, 7.218568621256964, 4.845255734770141, 2.15369954346815, 0.21961334301867375]]\n",
      "2021-01-14 23:03:13,988 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:03:13,990 : INFO : built Dictionary(239 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1370 corpus positions)\n",
      "2021-01-14 23:03:14,191 : INFO : token count processed\n",
      "2021-01-14 23:03:14,226 : INFO : frequencies processed\n",
      "2021-01-14 23:03:23,618 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:03:23,619 : INFO : entropies processed\n",
      "2021-01-14 23:03:23,620 : INFO : extropies processed\n",
      "2021-01-14 23:03:23,631 : INFO : token count processed\n",
      "2021-01-14 23:03:23,636 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:03:23,641 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:03:23,642 : INFO : vocab #32006\n",
      "2021-01-14 23:03:23,648 : INFO : diff #set()\n",
      "2021-01-14 23:03:42,671 : INFO : alphabet #32006\n",
      "2021-01-14 23:03:52,096 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.2245808651990078, 0.44952288120600253], [0.9018372148275375, 0.098162785], [2.75, 1.3226647836567116], [5.064869077788814, 6.492983191376071, 6.682356413526122, 4.875495855638764, 1.6174873357373079, 0.18937322215005103]]\n",
      "2021-01-14 23:03:52,103 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:03:52,106 : INFO : built Dictionary(450 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 3309 corpus positions)\n",
      "2021-01-14 23:03:52,581 : INFO : token count processed\n",
      "2021-01-14 23:03:52,645 : INFO : frequencies processed\n",
      "2021-01-14 23:04:02,070 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:04:02,072 : INFO : entropies processed\n",
      "2021-01-14 23:04:02,072 : INFO : extropies processed\n",
      "2021-01-14 23:04:02,080 : INFO : token count processed\n",
      "2021-01-14 23:04:02,087 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:04:02,091 : INFO : alphabet_target #32008\n",
      "2021-01-14 23:04:02,092 : INFO : vocab #32006\n",
      "2021-01-14 23:04:02,100 : INFO : diff #set()\n",
      "2021-01-14 23:04:21,132 : INFO : alphabet #32006\n",
      "2021-01-14 23:04:30,601 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.241802995744969, 0.44606952613500817], [0.9173948839306831, 0.082605116], [2.321928094887362, 1.2877123795494492], [5.064869077788814, 6.560342487747443, 6.790145445769598, 4.83506611976666, 1.7252763679807837, 0.22980295802215522]]\n",
      "2021-01-14 23:04:30,609 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 23:04:30,610 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:04:30,612 : INFO : built Dictionary(461 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 3514 corpus positions)\n",
      "2021-01-14 23:04:31,132 : INFO : token count processed\n",
      "2021-01-14 23:04:31,200 : INFO : frequencies processed\n",
      "2021-01-14 23:04:41,009 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:04:41,010 : INFO : entropies processed\n",
      "2021-01-14 23:04:41,010 : INFO : extropies processed\n",
      "2021-01-14 23:04:41,018 : INFO : token count processed\n",
      "2021-01-14 23:04:41,023 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:04:41,027 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:04:41,028 : INFO : vocab #32006\n",
      "2021-01-14 23:04:41,035 : INFO : diff #set()\n",
      "2021-01-14 23:04:59,951 : INFO : alphabet #32006\n",
      "2021-01-14 23:05:09,394 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.2155661776702888, 0.45135189825452177], [0.8780476301908493, 0.12195237], [3.378783493486176, 1.3660934553878117], [5.064869077788814, 7.046173750105238, 7.261562569061411, 4.849480258832642, 2.1966934912725966, 0.215388818956173]]\n",
      "2021-01-14 23:05:09,405 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 23:05:09,406 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:05:09,409 : INFO : built Dictionary(513 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 5623 corpus positions)\n",
      "2021-01-14 23:05:10,086 : INFO : token count processed\n",
      "2021-01-14 23:05:10,157 : INFO : frequencies processed\n",
      "2021-01-14 23:05:19,586 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:05:19,587 : INFO : entropies processed\n",
      "2021-01-14 23:05:19,588 : INFO : extropies processed\n",
      "2021-01-14 23:05:19,596 : INFO : token count processed\n",
      "2021-01-14 23:05:19,601 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:05:19,607 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:05:19,608 : INFO : vocab #32006\n",
      "2021-01-14 23:05:19,616 : INFO : diff #set()\n",
      "2021-01-14 23:05:38,637 : INFO : alphabet #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 23:05:48,074 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.214735456693679, 0.4515211949931366], [0.8873348012566566, 0.1126652], [3.625, 1.3785939957689286], [5.064869077788814, 7.009229588004272, 7.134998786166345, 4.939099879626743, 2.0701297083775305, 0.12576919816207255]]\n",
      "2021-01-14 23:05:48,087 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:05:48,090 : INFO : built Dictionary(593 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 6564 corpus positions)\n",
      "2021-01-14 23:05:48,813 : INFO : token count processed\n",
      "2021-01-14 23:05:48,845 : INFO : frequencies processed\n",
      "2021-01-14 23:05:58,375 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:05:58,376 : INFO : entropies processed\n",
      "2021-01-14 23:05:58,376 : INFO : extropies processed\n",
      "2021-01-14 23:05:58,395 : INFO : token count processed\n",
      "2021-01-14 23:05:58,399 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:05:58,404 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:05:58,405 : INFO : vocab #32006\n",
      "2021-01-14 23:05:58,411 : INFO : diff #set()\n",
      "2021-01-14 23:06:17,302 : INFO : alphabet #32006\n",
      "2021-01-14 23:06:26,745 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.2126905440382834, 0.4519384794653411], [0.8732354044914246, 0.1267646], [3.5465935642949384, 1.3764678303056375], [5.064869077788814, 7.376088004590871, 7.556320862090112, 4.884636220289575, 2.4914517843012973, 0.18023285749924067]]\n",
      "2021-01-14 23:06:26,748 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:06:26,749 : INFO : built Dictionary(151 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 350 corpus positions)\n",
      "2021-01-14 23:06:26,849 : INFO : token count processed\n",
      "2021-01-14 23:06:26,884 : INFO : frequencies processed\n",
      "2021-01-14 23:06:36,419 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:06:36,420 : INFO : entropies processed\n",
      "2021-01-14 23:06:36,421 : INFO : extropies processed\n",
      "2021-01-14 23:06:36,432 : INFO : token count processed\n",
      "2021-01-14 23:06:36,437 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:06:36,442 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:06:36,443 : INFO : vocab #32006\n",
      "2021-01-14 23:06:36,451 : INFO : diff #set()\n",
      "2021-01-14 23:06:55,468 : INFO : alphabet #32006\n",
      "2021-01-14 23:07:04,879 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.2312021819226626, 0.44818887687635917], [0.9033751264214516, 0.09662487], [2.0, 1.2451124978365313], [5.064869077788814, 6.2993628166120885, 6.679451173149214, 4.684780721251688, 1.6145820953604, 0.3800883565371258]]\n",
      "2021-01-14 23:07:04,882 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:07:04,883 : INFO : built Dictionary(51 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 66 corpus positions)\n",
      "2021-01-14 23:07:04,902 : INFO : token count processed\n",
      "2021-01-14 23:07:04,935 : INFO : frequencies processed\n",
      "2021-01-14 23:07:14,480 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:07:14,481 : INFO : entropies processed\n",
      "2021-01-14 23:07:14,482 : INFO : extropies processed\n",
      "2021-01-14 23:07:14,489 : INFO : token count processed\n",
      "2021-01-14 23:07:14,494 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:07:14,498 : INFO : alphabet_target #32008\n",
      "2021-01-14 23:07:14,499 : INFO : vocab #32006\n",
      "2021-01-14 23:07:14,506 : INFO : diff #set()\n",
      "2021-01-14 23:07:33,424 : INFO : alphabet #32006\n",
      "2021-01-14 23:07:42,982 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.263238836757266, 0.4418446625071107], [0.9001852720975876, 0.09981473], [0.0, 0.0], [5.064869077788814, 3.8936606896881862, 5.463054250365621, 3.4954755171113785, 0.39818517257680686, 1.569393560677435]]\n",
      "2021-01-14 23:07:43,005 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:07:43,010 : INFO : built Dictionary(746 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 12508 corpus positions)\n",
      "2021-01-14 23:07:44,053 : INFO : token count processed\n",
      "2021-01-14 23:07:44,085 : INFO : frequencies processed\n",
      "2021-01-14 23:07:53,508 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:07:53,509 : INFO : entropies processed\n",
      "2021-01-14 23:07:53,510 : INFO : extropies processed\n",
      "2021-01-14 23:07:53,521 : INFO : token count processed\n",
      "2021-01-14 23:07:53,525 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:07:53,530 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:07:53,531 : INFO : vocab #32006\n",
      "2021-01-14 23:07:53,537 : INFO : diff #set()\n",
      "2021-01-14 23:08:12,533 : INFO : alphabet #32006\n",
      "2021-01-14 23:08:21,954 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.1893459192918436, 0.45675742293088867], [0.8508971631526947, 0.14910284], [4.088779347361361, 1.396606630271964], [5.064869077788814, 7.434393313070278, 7.645434312517296, 4.853828078341796, 2.580565234728482, 0.21104099944701815]]\n",
      "2021-01-14 23:08:21,962 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:08:21,965 : INFO : built Dictionary(494 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 4135 corpus positions)\n",
      "2021-01-14 23:08:22,548 : INFO : token count processed\n",
      "2021-01-14 23:08:22,596 : INFO : frequencies processed\n",
      "2021-01-14 23:08:32,009 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:08:32,010 : INFO : entropies processed\n",
      "2021-01-14 23:08:32,011 : INFO : extropies processed\n",
      "2021-01-14 23:08:32,024 : INFO : token count processed\n",
      "2021-01-14 23:08:32,030 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:08:32,035 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:08:32,037 : INFO : vocab #32006\n",
      "2021-01-14 23:08:32,043 : INFO : diff #set()\n",
      "2021-01-14 23:08:51,052 : INFO : alphabet #32006\n",
      "2021-01-14 23:09:00,436 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.1670242019971564, 0.461462312732081], [0.8224643617868423, 0.17753564], [4.315824333525707, 1.4034875937284337], [5.064869077788814, 7.2991514951718255, 7.496181821507706, 4.867838751452934, 2.431312743718892, 0.1970303263358808]]\n",
      "2021-01-14 23:09:00,444 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:09:00,446 : INFO : built Dictionary(468 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 3552 corpus positions)\n",
      "2021-01-14 23:09:00,931 : INFO : token count processed\n",
      "2021-01-14 23:09:00,968 : INFO : frequencies processed\n",
      "2021-01-14 23:09:10,764 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:09:10,765 : INFO : entropies processed\n",
      "2021-01-14 23:09:10,766 : INFO : extropies processed\n",
      "2021-01-14 23:09:10,781 : INFO : token count processed\n",
      "2021-01-14 23:09:10,786 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:09:10,790 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:09:10,790 : INFO : vocab #32006\n",
      "2021-01-14 23:09:10,797 : INFO : diff #set()\n",
      "2021-01-14 23:09:29,977 : INFO : alphabet #32006\n",
      "2021-01-14 23:09:39,422 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.2151187336575413, 0.45144306930618944], [0.8761445060372353, 0.123855494], [3.640223928941852, 1.3797477693995936], [5.064869077788814, 7.170319527000998, 7.379717883524906, 4.855470721264906, 2.3148488057360916, 0.20939835652390837]]\n",
      "2021-01-14 23:09:39,425 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:09:39,427 : INFO : built Dictionary(185 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 588 corpus positions)\n",
      "2021-01-14 23:09:39,554 : INFO : token count processed\n",
      "2021-01-14 23:09:39,586 : INFO : frequencies processed\n",
      "2021-01-14 23:09:49,029 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:09:49,030 : INFO : entropies processed\n",
      "2021-01-14 23:09:49,031 : INFO : extropies processed\n",
      "2021-01-14 23:09:49,044 : INFO : token count processed\n",
      "2021-01-14 23:09:49,049 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:09:49,053 : INFO : alphabet_target #32009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 23:09:49,054 : INFO : vocab #32006\n",
      "2021-01-14 23:09:49,061 : INFO : diff #set()\n",
      "2021-01-14 23:10:08,044 : INFO : alphabet #32006\n",
      "2021-01-14 23:10:17,733 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.2351678346605035, 0.4473936965686018], [0.9001837596297264, 0.09981624], [2.0, 1.2451124978365313], [5.064869077788814, 6.353654804387375, 6.684544119372259, 4.73397976280393, 1.6196750415834442, 0.3308893149848835]]\n",
      "2021-01-14 23:10:17,737 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:10:17,738 : INFO : built Dictionary(180 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 716 corpus positions)\n",
      "2021-01-14 23:10:17,870 : INFO : token count processed\n",
      "2021-01-14 23:10:17,906 : INFO : frequencies processed\n",
      "2021-01-14 23:10:27,449 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:10:27,450 : INFO : entropies processed\n",
      "2021-01-14 23:10:27,451 : INFO : extropies processed\n",
      "2021-01-14 23:10:27,458 : INFO : token count processed\n",
      "2021-01-14 23:10:27,463 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:10:27,467 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:10:27,468 : INFO : vocab #32006\n",
      "2021-01-14 23:10:27,475 : INFO : diff #set()\n",
      "2021-01-14 23:10:46,379 : INFO : alphabet #32006\n",
      "2021-01-14 23:10:55,813 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.2323924242302595, 0.44794991648693], [0.8930082023143768, 0.1069918], [2.0, 1.2451124978365313], [5.064869077788814, 6.245180322479091, 6.529781956788135, 4.7802674434797705, 1.4649128789993204, 0.28460163430904384]]\n",
      "2021-01-14 23:10:55,818 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:10:55,820 : INFO : built Dictionary(400 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1788 corpus positions)\n",
      "2021-01-14 23:10:56,228 : INFO : token count processed\n",
      "2021-01-14 23:10:56,260 : INFO : frequencies processed\n",
      "2021-01-14 23:11:05,955 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:11:05,956 : INFO : entropies processed\n",
      "2021-01-14 23:11:05,957 : INFO : extropies processed\n",
      "2021-01-14 23:11:05,964 : INFO : token count processed\n",
      "2021-01-14 23:11:05,971 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:11:05,976 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:11:05,977 : INFO : vocab #32006\n",
      "2021-01-14 23:11:05,984 : INFO : diff #set()\n",
      "2021-01-14 23:11:25,044 : INFO : alphabet #32006\n",
      "2021-01-14 23:11:34,495 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.1896556113004733, 0.4566928218479449], [0.8403117209672928, 0.15968828], [3.625, 1.3785939957689282], [5.064869077788814, 7.2691387000368, 7.504201973371211, 4.829805804454404, 2.439332895582397, 0.23506327333441135]]\n",
      "2021-01-14 23:11:34,500 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:11:34,502 : INFO : built Dictionary(326 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1625 corpus positions)\n",
      "2021-01-14 23:11:34,806 : INFO : token count processed\n",
      "2021-01-14 23:11:34,852 : INFO : frequencies processed\n",
      "2021-01-14 23:11:44,565 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:11:44,567 : INFO : entropies processed\n",
      "2021-01-14 23:11:44,567 : INFO : extropies processed\n",
      "2021-01-14 23:11:44,575 : INFO : token count processed\n",
      "2021-01-14 23:11:44,580 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:11:44,585 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:11:44,586 : INFO : vocab #32006\n",
      "2021-01-14 23:11:44,593 : INFO : diff #set()\n",
      "2021-01-14 23:12:03,492 : INFO : alphabet #32006\n",
      "2021-01-14 23:12:13,038 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.1506189910446882, 0.46498240932683216], [0.7962749302387238, 0.20372507], [3.6168746059562222, 1.3781755222681256], [5.064869077788814, 7.08857858466988, 7.284365050399797, 4.869082612058897, 2.219495972610983, 0.19578646572991776]]\n",
      "2021-01-14 23:12:13,041 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:12:13,043 : INFO : built Dictionary(159 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 548 corpus positions)\n",
      "2021-01-14 23:12:13,156 : INFO : token count processed\n",
      "2021-01-14 23:12:13,190 : INFO : frequencies processed\n",
      "2021-01-14 23:12:22,620 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:12:22,621 : INFO : entropies processed\n",
      "2021-01-14 23:12:22,622 : INFO : extropies processed\n",
      "2021-01-14 23:12:22,629 : INFO : token count processed\n",
      "2021-01-14 23:12:22,633 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:12:22,638 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:12:22,639 : INFO : vocab #32006\n",
      "2021-01-14 23:12:22,647 : INFO : diff #set()\n",
      "2021-01-14 23:12:41,541 : INFO : alphabet #32006\n",
      "2021-01-14 23:12:51,248 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.2417274679359986, 0.44608455501538696], [0.9083129465579987, 0.09168705], [2.0, 1.2451124978365313], [5.064869077788814, 6.0479231618016716, 6.3926780621325445, 4.720114177457942, 1.3278089843437302, 0.34475490033087297]]\n",
      "2021-01-14 23:12:51,252 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:12:51,254 : INFO : built Dictionary(161 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 667 corpus positions)\n",
      "2021-01-14 23:12:51,363 : INFO : token count processed\n",
      "2021-01-14 23:12:51,396 : INFO : frequencies processed\n",
      "2021-01-14 23:13:00,806 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:13:00,807 : INFO : entropies processed\n",
      "2021-01-14 23:13:00,808 : INFO : extropies processed\n",
      "2021-01-14 23:13:00,815 : INFO : token count processed\n",
      "2021-01-14 23:13:00,822 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:13:00,827 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:13:00,828 : INFO : vocab #32006\n",
      "2021-01-14 23:13:00,835 : INFO : diff #set()\n",
      "2021-01-14 23:13:19,724 : INFO : alphabet #32006\n",
      "2021-01-14 23:13:29,097 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.2359401065413742, 0.447239171154201], [0.8994073495268822, 0.10059265], [2.0, 1.2451124978365313], [5.064869077788814, 6.036583168403119, 6.336937687491108, 4.764514558700824, 1.272068609702294, 0.30035451908798905]]\n",
      "2021-01-14 23:13:29,111 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 23:13:29,112 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:13:29,115 : INFO : built Dictionary(569 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 7025 corpus positions)\n",
      "2021-01-14 23:13:29,825 : INFO : token count processed\n",
      "2021-01-14 23:13:29,869 : INFO : frequencies processed\n",
      "2021-01-14 23:13:39,267 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:13:39,268 : INFO : entropies processed\n",
      "2021-01-14 23:13:39,269 : INFO : extropies processed\n",
      "2021-01-14 23:13:39,280 : INFO : token count processed\n",
      "2021-01-14 23:13:39,285 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:13:39,288 : INFO : alphabet_target #32010\n",
      "2021-01-14 23:13:39,289 : INFO : vocab #32006\n",
      "2021-01-14 23:13:39,296 : INFO : diff #set()\n",
      "2021-01-14 23:13:58,362 : INFO : alphabet #32006\n",
      "2021-01-14 23:14:07,966 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.2171927490901457, 0.45102077859958867], [0.8947857096791267, 0.10521429], [3.4182958340544896, 1.369895090630202], [5.064869077788814, 7.29352035514053, 7.466983898905943, 4.891405534023401, 2.402114821117129, 0.17346354376541306]]\n",
      "2021-01-14 23:14:07,974 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:14:07,976 : INFO : built Dictionary(378 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 3268 corpus positions)\n",
      "2021-01-14 23:14:08,340 : INFO : token count processed\n",
      "2021-01-14 23:14:08,371 : INFO : frequencies processed\n",
      "2021-01-14 23:14:17,798 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:14:17,799 : INFO : entropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 23:14:17,800 : INFO : extropies processed\n",
      "2021-01-14 23:14:17,815 : INFO : token count processed\n",
      "2021-01-14 23:14:17,819 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:14:17,823 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:14:17,824 : INFO : vocab #32006\n",
      "2021-01-14 23:14:17,830 : INFO : diff #set()\n",
      "2021-01-14 23:14:36,977 : INFO : alphabet #32006\n",
      "2021-01-14 23:14:46,385 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.2200552166026974, 0.45043924697074805], [0.8742055147886276, 0.12579449], [3.3927474104487847, 1.3672090515720436], [5.064869077788814, 6.8153433747477745, 6.987921593740088, 4.8922908587965015, 1.923052515951274, 0.17257821899231374]]\n",
      "2021-01-14 23:14:46,388 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:14:46,390 : INFO : built Dictionary(138 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 289 corpus positions)\n",
      "2021-01-14 23:14:46,478 : INFO : token count processed\n",
      "2021-01-14 23:14:46,513 : INFO : frequencies processed\n",
      "2021-01-14 23:14:55,930 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:14:55,931 : INFO : entropies processed\n",
      "2021-01-14 23:14:55,932 : INFO : extropies processed\n",
      "2021-01-14 23:14:55,939 : INFO : token count processed\n",
      "2021-01-14 23:14:55,944 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:14:55,948 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:14:55,949 : INFO : vocab #32006\n",
      "2021-01-14 23:14:55,957 : INFO : diff #set()\n",
      "2021-01-14 23:15:14,961 : INFO : alphabet #32006\n",
      "2021-01-14 23:15:24,416 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.2397930179838883, 0.4464698264396471], [0.9053287506103516, 0.09467125], [1.0, 1.0], [5.064869077788814, 6.150121915859574, 6.5702520216388045, 4.644738972009584, 1.5053829438499902, 0.42013010577923016]]\n",
      "2021-01-14 23:15:24,421 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:15:24,422 : INFO : built Dictionary(297 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1000 corpus positions)\n",
      "2021-01-14 23:15:24,689 : INFO : token count processed\n",
      "2021-01-14 23:15:24,721 : INFO : frequencies processed\n",
      "2021-01-14 23:15:34,159 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:15:34,161 : INFO : entropies processed\n",
      "2021-01-14 23:15:34,162 : INFO : extropies processed\n",
      "2021-01-14 23:15:34,173 : INFO : token count processed\n",
      "2021-01-14 23:15:34,178 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:15:34,183 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:15:34,184 : INFO : vocab #32006\n",
      "2021-01-14 23:15:34,191 : INFO : diff #set()\n",
      "2021-01-14 23:15:53,165 : INFO : alphabet #32006\n",
      "2021-01-14 23:16:02,570 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.1917128210491474, 0.4562641557762625], [0.8361227214336395, 0.16387728], [3.0, 1.3485155455967714], [5.064869077788814, 7.0391145208191315, 7.349312305720366, 4.754671292887581, 2.2844432279315514, 0.3101977849012343]]\n",
      "2021-01-14 23:16:02,580 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 23:16:02,581 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:16:02,583 : INFO : built Dictionary(597 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 4375 corpus positions)\n",
      "2021-01-14 23:16:03,431 : INFO : token count processed\n",
      "2021-01-14 23:16:03,463 : INFO : frequencies processed\n",
      "2021-01-14 23:16:13,004 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:16:13,005 : INFO : entropies processed\n",
      "2021-01-14 23:16:13,006 : INFO : extropies processed\n",
      "2021-01-14 23:16:13,022 : INFO : token count processed\n",
      "2021-01-14 23:16:13,026 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:16:13,031 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:16:13,032 : INFO : vocab #32006\n",
      "2021-01-14 23:16:13,038 : INFO : diff #set()\n",
      "2021-01-14 23:16:31,898 : INFO : alphabet #32006\n",
      "2021-01-14 23:16:41,317 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.1810111287472078, 0.45850293325848773], [0.8320430517196655, 0.16795695], [3.6402239289418516, 1.3797477693995936], [5.064869077788814, 7.482466367279176, 7.769282597998655, 4.778052847069335, 2.7044135202098403, 0.28681623071947904]]\n",
      "2021-01-14 23:16:41,321 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:16:41,322 : INFO : built Dictionary(185 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 597 corpus positions)\n",
      "2021-01-14 23:16:41,459 : INFO : token count processed\n",
      "2021-01-14 23:16:41,494 : INFO : frequencies processed\n",
      "2021-01-14 23:16:51,047 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:16:51,048 : INFO : entropies processed\n",
      "2021-01-14 23:16:51,049 : INFO : extropies processed\n",
      "2021-01-14 23:16:51,056 : INFO : token count processed\n",
      "2021-01-14 23:16:51,063 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:16:51,067 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:16:51,068 : INFO : vocab #32006\n",
      "2021-01-14 23:16:51,075 : INFO : diff #set()\n",
      "2021-01-14 23:17:09,914 : INFO : alphabet #32006\n",
      "2021-01-14 23:17:19,436 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.235691464443496, 0.44728891079293803], [0.8987036049365997, 0.101296395], [2.0, 1.2451124978365313], [5.064869077788814, 6.372162341197667, 6.697374987371301, 4.739656431615181, 1.6325059095824868, 0.3252126461736342]]\n",
      "2021-01-14 23:17:19,441 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:17:19,443 : INFO : built Dictionary(331 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 2018 corpus positions)\n",
      "2021-01-14 23:17:19,756 : INFO : token count processed\n",
      "2021-01-14 23:17:19,791 : INFO : frequencies processed\n",
      "2021-01-14 23:17:29,200 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:17:29,202 : INFO : entropies processed\n",
      "2021-01-14 23:17:29,202 : INFO : extropies processed\n",
      "2021-01-14 23:17:29,210 : INFO : token count processed\n",
      "2021-01-14 23:17:29,216 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:17:29,220 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:17:29,221 : INFO : vocab #32006\n",
      "2021-01-14 23:17:29,228 : INFO : diff #set()\n",
      "2021-01-14 23:17:48,151 : INFO : alphabet #32006\n",
      "2021-01-14 23:17:57,869 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.2204645414110684, 0.45035621211249627], [0.8905567228794098, 0.10944328], [2.75, 1.3226647836567116], [5.064869077788814, 6.798155919669889, 7.032975735950956, 4.830049261507748, 1.9681066581621414, 0.23481981628106663]]\n",
      "2021-01-14 23:17:57,873 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:17:57,875 : INFO : built Dictionary(188 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 734 corpus positions)\n",
      "2021-01-14 23:17:58,020 : INFO : token count processed\n",
      "2021-01-14 23:17:58,047 : INFO : frequencies processed\n",
      "2021-01-14 23:18:07,347 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:18:07,348 : INFO : entropies processed\n",
      "2021-01-14 23:18:07,349 : INFO : extropies processed\n",
      "2021-01-14 23:18:07,356 : INFO : token count processed\n",
      "2021-01-14 23:18:07,360 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:18:07,367 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:18:07,368 : INFO : vocab #32006\n",
      "2021-01-14 23:18:07,375 : INFO : diff #set()\n",
      "2021-01-14 23:18:26,291 : INFO : alphabet #32006\n",
      "2021-01-14 23:18:35,852 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.2312293316839535, 0.4481834232814069], [0.893544152379036, 0.10645585], [2.521640636343318, 1.2998438251349493], [5.064869077788814, 6.271631856729336, 6.570183794437747, 4.766317140080402, 1.5053147166489325, 0.29855193770841115]]\n",
      "2021-01-14 23:18:35,859 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:18:35,861 : INFO : built Dictionary(359 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 3249 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 23:18:36,205 : INFO : token count processed\n",
      "2021-01-14 23:18:36,238 : INFO : frequencies processed\n",
      "2021-01-14 23:18:45,656 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:18:45,658 : INFO : entropies processed\n",
      "2021-01-14 23:18:45,658 : INFO : extropies processed\n",
      "2021-01-14 23:18:45,666 : INFO : token count processed\n",
      "2021-01-14 23:18:45,671 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:18:45,675 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:18:45,676 : INFO : vocab #32006\n",
      "2021-01-14 23:18:45,685 : INFO : diff #set()\n",
      "2021-01-14 23:19:04,874 : INFO : alphabet #32006\n",
      "2021-01-14 23:19:14,295 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.190873626387816, 0.45643892370403005], [0.8401778340339661, 0.15982217], [3.625, 1.3785939957689282], [5.064869077788814, 6.873598627629562, 7.050159638585837, 4.888308066832538, 1.985290560797023, 0.1765610109562754]]\n",
      "2021-01-14 23:19:14,298 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:19:14,300 : INFO : built Dictionary(117 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 197 corpus positions)\n",
      "2021-01-14 23:19:14,369 : INFO : token count processed\n",
      "2021-01-14 23:19:14,404 : INFO : frequencies processed\n",
      "2021-01-14 23:19:23,823 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:19:23,824 : INFO : entropies processed\n",
      "2021-01-14 23:19:23,825 : INFO : extropies processed\n",
      "2021-01-14 23:19:23,832 : INFO : token count processed\n",
      "2021-01-14 23:19:23,836 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:19:23,841 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:19:23,842 : INFO : vocab #32006\n",
      "2021-01-14 23:19:23,849 : INFO : diff #set()\n",
      "2021-01-14 23:19:42,860 : INFO : alphabet #32006\n",
      "2021-01-14 23:19:52,275 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.199537503118683, 0.4546410318451578], [0.881737731397152, 0.11826227], [2.0, 1.2451124978365313], [5.064869077788814, 6.049830202851529, 6.571256736990168, 4.543442543650176, 1.506387659201354, 0.521426534138639]]\n",
      "2021-01-14 23:19:52,280 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:19:52,281 : INFO : built Dictionary(263 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1277 corpus positions)\n",
      "2021-01-14 23:19:52,507 : INFO : token count processed\n",
      "2021-01-14 23:19:52,577 : INFO : frequencies processed\n",
      "2021-01-14 23:20:01,982 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:20:01,983 : INFO : entropies processed\n",
      "2021-01-14 23:20:01,984 : INFO : extropies processed\n",
      "2021-01-14 23:20:01,992 : INFO : token count processed\n",
      "2021-01-14 23:20:01,996 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:20:02,000 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:20:02,001 : INFO : vocab #32006\n",
      "2021-01-14 23:20:02,008 : INFO : diff #set()\n",
      "2021-01-14 23:20:21,147 : INFO : alphabet #32006\n",
      "2021-01-14 23:20:30,563 : INFO : Computed distances or similarities ('270', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.2077675981597578, 0.4529462253334684], [0.8620950728654861, 0.13790493], [2.5216406363433186, 1.2998438251349493], [5.064869077788814, 6.778844940588858, 7.039022896997208, 4.804691121380463, 1.9741538192083938, 0.2601779564083504]]\n",
      "2021-01-14 23:20:30,566 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:20:30,567 : INFO : built Dictionary(162 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 310 corpus positions)\n",
      "2021-01-14 23:20:30,676 : INFO : token count processed\n",
      "2021-01-14 23:20:30,708 : INFO : frequencies processed\n",
      "2021-01-14 23:20:40,130 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:20:40,131 : INFO : entropies processed\n",
      "2021-01-14 23:20:40,132 : INFO : extropies processed\n",
      "2021-01-14 23:20:40,145 : INFO : token count processed\n",
      "2021-01-14 23:20:40,150 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:20:40,156 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:20:40,157 : INFO : vocab #32006\n",
      "2021-01-14 23:20:40,164 : INFO : diff #set()\n",
      "2021-01-14 23:20:59,181 : INFO : alphabet #32006\n",
      "2021-01-14 23:21:08,610 : INFO : Computed distances or similarities ('270', 'sacp-python-common/setup.py')[[1.1932192838431996, 0.45595076031234333], [0.8673251420259476, 0.13267486], [2.584962500721156, 1.315172029168969], [5.064869077788814, 6.469677430851302, 6.8970165536807375, 4.637529954959378, 1.8321474758919232, 0.4273391228294354]]\n",
      "2021-01-14 23:21:08,614 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:21:08,616 : INFO : built Dictionary(232 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1212 corpus positions)\n",
      "2021-01-14 23:21:08,802 : INFO : token count processed\n",
      "2021-01-14 23:21:08,835 : INFO : frequencies processed\n",
      "2021-01-14 23:21:18,379 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:21:18,380 : INFO : entropies processed\n",
      "2021-01-14 23:21:18,381 : INFO : extropies processed\n",
      "2021-01-14 23:21:18,395 : INFO : token count processed\n",
      "2021-01-14 23:21:18,399 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:21:18,404 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:21:18,405 : INFO : vocab #32006\n",
      "2021-01-14 23:21:18,411 : INFO : diff #set()\n",
      "2021-01-14 23:21:37,303 : INFO : alphabet #32006\n",
      "2021-01-14 23:21:46,855 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.2206809565617294, 0.4503123229139118], [0.8822652846574783, 0.117734715], [2.521640636343318, 1.2998438251349493], [5.064869077788814, 6.459180448028249, 6.70651364104333, 4.817535884773733, 1.6416445632545154, 0.2473331930150806]]\n",
      "2021-01-14 23:21:46,859 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:21:46,860 : INFO : built Dictionary(144 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 362 corpus positions)\n",
      "2021-01-14 23:21:46,964 : INFO : token count processed\n",
      "2021-01-14 23:21:46,996 : INFO : frequencies processed\n",
      "2021-01-14 23:21:56,408 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:21:56,409 : INFO : entropies processed\n",
      "2021-01-14 23:21:56,410 : INFO : extropies processed\n",
      "2021-01-14 23:21:56,417 : INFO : token count processed\n",
      "2021-01-14 23:21:56,425 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:21:56,429 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:21:56,430 : INFO : vocab #32006\n",
      "2021-01-14 23:21:56,437 : INFO : diff #set()\n",
      "2021-01-14 23:22:15,215 : INFO : alphabet #32006\n",
      "2021-01-14 23:22:24,772 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.2313657971297873, 0.4481560133646859], [0.9035016521811485, 0.09649835], [2.0, 1.2451124978365313], [5.064869077788814, 6.097125733496388, 6.4988175799017895, 4.663177231383413, 1.4339485021129752, 0.40169184640540134]]\n",
      "2021-01-14 23:22:24,775 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:22:24,776 : INFO : built Dictionary(136 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 358 corpus positions)\n",
      "2021-01-14 23:22:24,870 : INFO : token count processed\n",
      "2021-01-14 23:22:24,904 : INFO : frequencies processed\n",
      "2021-01-14 23:22:34,335 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:22:34,336 : INFO : entropies processed\n",
      "2021-01-14 23:22:34,337 : INFO : extropies processed\n",
      "2021-01-14 23:22:34,344 : INFO : token count processed\n",
      "2021-01-14 23:22:34,351 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:22:34,355 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:22:34,356 : INFO : vocab #32006\n",
      "2021-01-14 23:22:34,363 : INFO : diff #set()\n",
      "2021-01-14 23:22:53,280 : INFO : alphabet #32006\n",
      "2021-01-14 23:23:02,842 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.2289336989600965, 0.4486450182284684], [0.8980218917131424, 0.10197811], [2.0, 1.2451124978365313], [5.064869077788814, 6.0695858597523715, 6.468741365221441, 4.665713572319745, 1.4038722874326268, 0.39915550546906964]]\n",
      "2021-01-14 23:23:02,845 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 23:23:02,846 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 23:23:02,847 : INFO : built Dictionary(138 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 454 corpus positions)\n",
      "2021-01-14 23:23:02,954 : INFO : token count processed\n",
      "2021-01-14 23:23:02,988 : INFO : frequencies processed\n",
      "2021-01-14 23:23:12,589 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:23:12,590 : INFO : entropies processed\n",
      "2021-01-14 23:23:12,591 : INFO : extropies processed\n",
      "2021-01-14 23:23:12,598 : INFO : token count processed\n",
      "2021-01-14 23:23:12,606 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:23:12,610 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:23:12,611 : INFO : vocab #32006\n",
      "2021-01-14 23:23:12,617 : INFO : diff #set()\n",
      "2021-01-14 23:23:31,618 : INFO : alphabet #32006\n",
      "2021-01-14 23:23:41,049 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.2157023675796976, 0.45132415555088334], [0.8907241225242615, 0.10927588], [2.0, 1.2451124978365313], [5.064869077788814, 6.104787343210121, 6.435992433253502, 4.733663987745433, 1.3711233554646878, 0.3312050900433814]]\n",
      "2021-01-14 23:23:41,066 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 23:23:41,067 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:23:41,071 : INFO : built Dictionary(421 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 9139 corpus positions)\n",
      "2021-01-14 23:23:41,498 : INFO : token count processed\n",
      "2021-01-14 23:23:41,530 : INFO : frequencies processed\n",
      "2021-01-14 23:23:50,963 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:23:50,964 : INFO : entropies processed\n",
      "2021-01-14 23:23:50,965 : INFO : extropies processed\n",
      "2021-01-14 23:23:50,975 : INFO : token count processed\n",
      "2021-01-14 23:23:50,980 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:23:50,986 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:23:50,987 : INFO : vocab #32006\n",
      "2021-01-14 23:23:50,993 : INFO : diff #set()\n",
      "2021-01-14 23:24:10,548 : INFO : alphabet #32006\n",
      "2021-01-14 23:24:21,566 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.2158010343408814, 0.4513040586685451], [0.896414153277874, 0.10358585], [3.169925001442312, 1.3594000115384994], [5.064869077788814, 6.89087415148015, 7.0631092250641645, 4.8926340042048, 1.9982401472753502, 0.17223507358401413]]\n",
      "2021-01-14 23:24:21,573 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:24:21,575 : INFO : built Dictionary(279 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 2299 corpus positions)\n",
      "2021-01-14 23:24:21,822 : INFO : token count processed\n",
      "2021-01-14 23:24:21,894 : INFO : frequencies processed\n",
      "2021-01-14 23:24:32,653 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:24:32,654 : INFO : entropies processed\n",
      "2021-01-14 23:24:32,655 : INFO : extropies processed\n",
      "2021-01-14 23:24:32,668 : INFO : token count processed\n",
      "2021-01-14 23:24:32,672 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:24:32,677 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:24:32,678 : INFO : vocab #32006\n",
      "2021-01-14 23:24:32,684 : INFO : diff #set()\n",
      "2021-01-14 23:24:51,687 : INFO : alphabet #32006\n",
      "2021-01-14 23:25:01,133 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.196051409206192, 0.4553627459757285], [0.846335843205452, 0.15366416], [3.095795255000934, 1.3487605247277434], [5.064869077788814, 6.655493573668506, 6.835958906419133, 4.884403745038188, 1.771089828630319, 0.18046533275062693]]\n",
      "2021-01-14 23:25:01,138 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:25:01,140 : INFO : built Dictionary(258 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1306 corpus positions)\n",
      "2021-01-14 23:25:01,353 : INFO : token count processed\n",
      "2021-01-14 23:25:01,385 : INFO : frequencies processed\n",
      "2021-01-14 23:25:10,806 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:25:10,807 : INFO : entropies processed\n",
      "2021-01-14 23:25:10,808 : INFO : extropies processed\n",
      "2021-01-14 23:25:10,815 : INFO : token count processed\n",
      "2021-01-14 23:25:10,820 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:25:10,824 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:25:10,825 : INFO : vocab #32006\n",
      "2021-01-14 23:25:10,833 : INFO : diff #set()\n",
      "2021-01-14 23:25:29,987 : INFO : alphabet #32006\n",
      "2021-01-14 23:25:39,394 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.2268729331478163, 0.4490601978741738], [0.8908455520868301, 0.10915445], [2.807354922057604, 1.3343545280186873], [5.064869077788814, 6.6236746347295465, 6.823246759803363, 4.865296952714997, 1.7583776820145482, 0.19957212507381605]]\n",
      "2021-01-14 23:25:39,399 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:25:39,400 : INFO : built Dictionary(258 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1245 corpus positions)\n",
      "2021-01-14 23:25:39,614 : INFO : token count processed\n",
      "2021-01-14 23:25:39,676 : INFO : frequencies processed\n",
      "2021-01-14 23:25:49,105 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:25:49,106 : INFO : entropies processed\n",
      "2021-01-14 23:25:49,107 : INFO : extropies processed\n",
      "2021-01-14 23:25:49,114 : INFO : token count processed\n",
      "2021-01-14 23:25:49,119 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:25:49,124 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:25:49,125 : INFO : vocab #32006\n",
      "2021-01-14 23:25:49,133 : INFO : diff #set()\n",
      "2021-01-14 23:26:08,290 : INFO : alphabet #32006\n",
      "2021-01-14 23:26:17,743 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.2041436699556072, 0.45369093386736475], [0.8241887241601944, 0.17581128], [2.94770277922009, 1.3393100707180505], [5.064869077788814, 6.75472436518627, 6.944739843670664, 4.874853599304419, 1.8798707658818499, 0.19001547848439415]]\n",
      "2021-01-14 23:26:17,748 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:26:17,750 : INFO : built Dictionary(214 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1040 corpus positions)\n",
      "2021-01-14 23:26:17,911 : INFO : token count processed\n",
      "2021-01-14 23:26:17,954 : INFO : frequencies processed\n",
      "2021-01-14 23:26:27,397 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:26:27,398 : INFO : entropies processed\n",
      "2021-01-14 23:26:27,399 : INFO : extropies processed\n",
      "2021-01-14 23:26:27,406 : INFO : token count processed\n",
      "2021-01-14 23:26:27,411 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:26:27,416 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:26:27,417 : INFO : vocab #32006\n",
      "2021-01-14 23:26:27,424 : INFO : diff #set()\n",
      "2021-01-14 23:26:46,271 : INFO : alphabet #32006\n",
      "2021-01-14 23:26:55,680 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.2139263683558619, 0.451686205238449], [0.8427797257900238, 0.15722027], [2.0, 1.2451124978365313], [5.064869077788814, 6.597313085495733, 6.805581441566013, 4.856600721718535, 1.7407123637771988, 0.20826835607028027]]\n",
      "2021-01-14 23:26:55,684 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:26:55,686 : INFO : built Dictionary(236 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1006 corpus positions)\n",
      "2021-01-14 23:26:55,879 : INFO : token count processed\n",
      "2021-01-14 23:26:55,912 : INFO : frequencies processed\n",
      "2021-01-14 23:27:05,446 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:27:05,447 : INFO : entropies processed\n",
      "2021-01-14 23:27:05,448 : INFO : extropies processed\n",
      "2021-01-14 23:27:05,462 : INFO : token count processed\n",
      "2021-01-14 23:27:05,467 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:27:05,473 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:27:05,474 : INFO : vocab #32006\n",
      "2021-01-14 23:27:05,481 : INFO : diff #set()\n",
      "2021-01-14 23:27:24,383 : INFO : alphabet #32006\n",
      "2021-01-14 23:27:34,307 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.2148293549897624, 0.4515020526286199], [0.8623951077461243, 0.13760489], [2.807354922057604, 1.3343545280186873], [5.064869077788814, 6.659481538516613, 6.876964996105382, 4.847385620200045, 1.812095918316568, 0.2174834575887692]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 23:27:34,311 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:27:34,313 : INFO : built Dictionary(261 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1168 corpus positions)\n",
      "2021-01-14 23:27:34,525 : INFO : token count processed\n",
      "2021-01-14 23:27:34,558 : INFO : frequencies processed\n",
      "2021-01-14 23:27:43,980 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:27:43,981 : INFO : entropies processed\n",
      "2021-01-14 23:27:43,982 : INFO : extropies processed\n",
      "2021-01-14 23:27:43,989 : INFO : token count processed\n",
      "2021-01-14 23:27:43,993 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:27:43,997 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:27:43,998 : INFO : vocab #32006\n",
      "2021-01-14 23:27:44,006 : INFO : diff #set()\n",
      "2021-01-14 23:28:02,908 : INFO : alphabet #32006\n",
      "2021-01-14 23:28:12,453 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.224267038962731, 0.44958630527849835], [0.8736429214477539, 0.12635708], [2.75, 1.3226647836567114], [5.064869077788814, 6.774682571479102, 6.955691227867222, 4.883860421400694, 1.8908221500784075, 0.18100865638812014]]\n",
      "2021-01-14 23:28:12,468 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:28:12,472 : INFO : built Dictionary(436 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 7905 corpus positions)\n",
      "2021-01-14 23:28:12,948 : INFO : token count processed\n",
      "2021-01-14 23:28:12,980 : INFO : frequencies processed\n",
      "2021-01-14 23:28:22,394 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:28:22,395 : INFO : entropies processed\n",
      "2021-01-14 23:28:22,396 : INFO : extropies processed\n",
      "2021-01-14 23:28:22,410 : INFO : token count processed\n",
      "2021-01-14 23:28:22,415 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:28:22,419 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:28:22,420 : INFO : vocab #32006\n",
      "2021-01-14 23:28:22,426 : INFO : diff #set()\n",
      "2021-01-14 23:28:41,440 : INFO : alphabet #32006\n",
      "2021-01-14 23:28:50,865 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.2192510929883498, 0.4506024591626729], [0.9019216671586037, 0.09807833], [3.321928094887362, 1.3680278410054498], [5.064869077788814, 6.839453716525233, 7.004769334973242, 4.899553459340806, 1.9399002571844273, 0.1653156184480089]]\n",
      "2021-01-14 23:28:50,871 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:28:50,873 : INFO : built Dictionary(336 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 2347 corpus positions)\n",
      "2021-01-14 23:28:51,185 : INFO : token count processed\n",
      "2021-01-14 23:28:51,224 : INFO : frequencies processed\n",
      "2021-01-14 23:29:00,670 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:29:00,671 : INFO : entropies processed\n",
      "2021-01-14 23:29:00,671 : INFO : extropies processed\n",
      "2021-01-14 23:29:00,679 : INFO : token count processed\n",
      "2021-01-14 23:29:00,683 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:29:00,687 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:29:00,688 : INFO : vocab #32006\n",
      "2021-01-14 23:29:00,694 : INFO : diff #set()\n",
      "2021-01-14 23:29:19,673 : INFO : alphabet #32006\n",
      "2021-01-14 23:29:29,096 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.2017286628564903, 0.45418857321983297], [0.8323126286268234, 0.16768737], [3.2776134368191157, 1.3618978811135465], [5.064869077788814, 6.86432793886027, 7.016867036250636, 4.912329980398448, 1.9519979584618214, 0.15253909739036597]]\n",
      "2021-01-14 23:29:29,100 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:29:29,101 : INFO : built Dictionary(176 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 549 corpus positions)\n",
      "2021-01-14 23:29:29,225 : INFO : token count processed\n",
      "2021-01-14 23:29:29,260 : INFO : frequencies processed\n",
      "2021-01-14 23:29:38,679 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:29:38,680 : INFO : entropies processed\n",
      "2021-01-14 23:29:38,681 : INFO : extropies processed\n",
      "2021-01-14 23:29:38,688 : INFO : token count processed\n",
      "2021-01-14 23:29:38,695 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:29:38,699 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:29:38,700 : INFO : vocab #32006\n",
      "2021-01-14 23:29:38,707 : INFO : diff #set()\n",
      "2021-01-14 23:29:57,732 : INFO : alphabet #32006\n",
      "2021-01-14 23:30:07,176 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.211979113801867, 0.45208383468017355], [0.8340412825345993, 0.16595872], [1.584962500721156, 1.1699250014423124], [5.064869077788814, 6.431978396403875, 6.7245140198440385, 4.772333454348651, 1.6596449420552242, 0.2925356234401635]]\n",
      "2021-01-14 23:30:07,180 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:30:07,181 : INFO : built Dictionary(233 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 703 corpus positions)\n",
      "2021-01-14 23:30:07,380 : INFO : token count processed\n",
      "2021-01-14 23:30:07,412 : INFO : frequencies processed\n",
      "2021-01-14 23:30:16,830 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:30:16,831 : INFO : entropies processed\n",
      "2021-01-14 23:30:16,831 : INFO : extropies processed\n",
      "2021-01-14 23:30:16,838 : INFO : token count processed\n",
      "2021-01-14 23:30:16,844 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:30:16,848 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:30:16,849 : INFO : vocab #32006\n",
      "2021-01-14 23:30:16,856 : INFO : diff #set()\n",
      "2021-01-14 23:30:35,759 : INFO : alphabet #32006\n",
      "2021-01-14 23:30:45,126 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/test_auth_utility.py')[[1.2227566000274348, 0.44989181450981064], [0.8692130446434021, 0.13078696], [2.321928094887362, 1.2877123795494492], [5.064869077788814, 6.911818353685893, 7.145949003083999, 4.83073842839071, 2.0810799252951844, 0.23413064939810546]]\n",
      "2021-01-14 23:30:45,140 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:30:45,143 : INFO : built Dictionary(323 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 7233 corpus positions)\n",
      "2021-01-14 23:30:45,459 : INFO : token count processed\n",
      "2021-01-14 23:30:45,517 : INFO : frequencies processed\n",
      "2021-01-14 23:30:55,045 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:30:55,046 : INFO : entropies processed\n",
      "2021-01-14 23:30:55,047 : INFO : extropies processed\n",
      "2021-01-14 23:30:55,056 : INFO : token count processed\n",
      "2021-01-14 23:30:55,060 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:30:55,064 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:30:55,065 : INFO : vocab #32006\n",
      "2021-01-14 23:30:55,074 : INFO : diff #set()\n",
      "2021-01-14 23:31:14,104 : INFO : alphabet #32006\n",
      "2021-01-14 23:31:23,663 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.2306076355603797, 0.44830833718041013], [0.8861123621463776, 0.11388764], [2.5216406363433186, 1.2998438251349493], [5.064869077788814, 6.363791471162389, 6.420188242480903, 5.0084723064703, 1.355319164692089, 0.05639677131851428]]\n",
      "2021-01-14 23:31:23,667 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:31:23,669 : INFO : built Dictionary(222 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1303 corpus positions)\n",
      "2021-01-14 23:31:23,843 : INFO : token count processed\n",
      "2021-01-14 23:31:23,873 : INFO : frequencies processed\n",
      "2021-01-14 23:31:33,386 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:31:33,387 : INFO : entropies processed\n",
      "2021-01-14 23:31:33,388 : INFO : extropies processed\n",
      "2021-01-14 23:31:33,395 : INFO : token count processed\n",
      "2021-01-14 23:31:33,400 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:31:33,404 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:31:33,405 : INFO : vocab #32006\n",
      "2021-01-14 23:31:33,412 : INFO : diff #set()\n",
      "2021-01-14 23:31:52,270 : INFO : alphabet #32006\n",
      "2021-01-14 23:32:01,667 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.221497619024619, 0.4501467800082832], [0.8874140679836273, 0.11258593], [2.9219280948873623, 1.3359016564230495], [5.064869077788814, 6.29000629755059, 6.579021883504678, 4.7758534918347255, 1.514152805715864, 0.28901558595408794]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 23:32:01,672 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:32:01,673 : INFO : built Dictionary(233 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1256 corpus positions)\n",
      "2021-01-14 23:32:01,864 : INFO : token count processed\n",
      "2021-01-14 23:32:01,897 : INFO : frequencies processed\n",
      "2021-01-14 23:32:11,395 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:32:11,396 : INFO : entropies processed\n",
      "2021-01-14 23:32:11,397 : INFO : extropies processed\n",
      "2021-01-14 23:32:11,404 : INFO : token count processed\n",
      "2021-01-14 23:32:11,408 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:32:11,413 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:32:11,414 : INFO : vocab #32006\n",
      "2021-01-14 23:32:11,421 : INFO : diff #set()\n",
      "2021-01-14 23:32:30,294 : INFO : alphabet #32006\n",
      "2021-01-14 23:32:39,827 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.220667871970391, 0.45031497623852396], [0.8874080553650856, 0.112591945], [2.75, 1.3226647836567116], [5.064869077788814, 6.361621244785958, 6.639979491376656, 4.7865108311981155, 1.5751104135878418, 0.27835824659069797]]\n",
      "2021-01-14 23:32:39,831 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:32:39,833 : INFO : built Dictionary(241 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1279 corpus positions)\n",
      "2021-01-14 23:32:40,033 : INFO : token count processed\n",
      "2021-01-14 23:32:40,066 : INFO : frequencies processed\n",
      "2021-01-14 23:32:49,461 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:32:49,462 : INFO : entropies processed\n",
      "2021-01-14 23:32:49,463 : INFO : extropies processed\n",
      "2021-01-14 23:32:49,475 : INFO : token count processed\n",
      "2021-01-14 23:32:49,480 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:32:49,486 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:32:49,487 : INFO : vocab #32006\n",
      "2021-01-14 23:32:49,494 : INFO : diff #set()\n",
      "2021-01-14 23:33:08,493 : INFO : alphabet #32006\n",
      "2021-01-14 23:33:17,917 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.1921589784887154, 0.4561712949712272], [0.8283727467060089, 0.17162725], [2.75, 1.3226647836567116], [5.064869077788814, 6.620594433343389, 6.896614581310292, 4.788848929821912, 1.8317455035214776, 0.27602014796690266]]\n",
      "2021-01-14 23:33:17,922 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:33:17,923 : INFO : built Dictionary(213 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1372 corpus positions)\n",
      "2021-01-14 23:33:18,088 : INFO : token count processed\n",
      "2021-01-14 23:33:18,120 : INFO : frequencies processed\n",
      "2021-01-14 23:33:27,518 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:33:27,519 : INFO : entropies processed\n",
      "2021-01-14 23:33:27,520 : INFO : extropies processed\n",
      "2021-01-14 23:33:27,531 : INFO : token count processed\n",
      "2021-01-14 23:33:27,536 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:33:27,541 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:33:27,542 : INFO : vocab #32006\n",
      "2021-01-14 23:33:27,550 : INFO : diff #set()\n",
      "2021-01-14 23:33:46,521 : INFO : alphabet #32006\n",
      "2021-01-14 23:33:55,931 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.197990210241757, 0.45496108005413266], [0.8416782021522522, 0.1583218], [2.75, 1.3226647836567116], [5.064869077788814, 6.207411496248084, 6.4097506946878084, 4.86252987934909, 1.344881616898994, 0.20233919843972448]]\n",
      "2021-01-14 23:33:55,934 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:33:55,937 : INFO : built Dictionary(153 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 276 corpus positions)\n",
      "2021-01-14 23:33:56,046 : INFO : token count processed\n",
      "2021-01-14 23:33:56,115 : INFO : frequencies processed\n",
      "2021-01-14 23:34:05,551 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:34:05,553 : INFO : entropies processed\n",
      "2021-01-14 23:34:05,554 : INFO : extropies processed\n",
      "2021-01-14 23:34:05,560 : INFO : token count processed\n",
      "2021-01-14 23:34:05,565 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:34:05,569 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:34:05,570 : INFO : vocab #32006\n",
      "2021-01-14 23:34:05,577 : INFO : diff #set()\n",
      "2021-01-14 23:34:24,550 : INFO : alphabet #32006\n",
      "2021-01-14 23:34:33,968 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.1791608969162786, 0.45889222838712634], [0.7949582785367966, 0.20504172], [2.807354922057604, 1.3343545280186873], [5.064869077788814, 6.5805228788529595, 6.9128044927547805, 4.732587463886993, 1.8479354149659661, 0.332281613901821]]\n",
      "2021-01-14 23:34:33,973 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 23:34:33,974 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:34:33,975 : INFO : built Dictionary(236 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1237 corpus positions)\n",
      "2021-01-14 23:34:34,168 : INFO : token count processed\n",
      "2021-01-14 23:34:34,201 : INFO : frequencies processed\n",
      "2021-01-14 23:34:43,597 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:34:43,598 : INFO : entropies processed\n",
      "2021-01-14 23:34:43,599 : INFO : extropies processed\n",
      "2021-01-14 23:34:43,606 : INFO : token count processed\n",
      "2021-01-14 23:34:43,610 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:34:43,616 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:34:43,617 : INFO : vocab #32006\n",
      "2021-01-14 23:34:43,625 : INFO : diff #set()\n",
      "2021-01-14 23:35:02,503 : INFO : alphabet #32006\n",
      "2021-01-14 23:35:11,926 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.229567965889528, 0.4485173878074766], [0.8950439319014549, 0.10495607], [2.725480556997868, 1.3192201298976014], [5.064869077788814, 6.422089779976135, 6.59573156723121, 4.89122729053374, 1.530862489442396, 0.17364178725507529]]\n",
      "2021-01-14 23:35:11,931 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:35:11,933 : INFO : built Dictionary(248 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1519 corpus positions)\n",
      "2021-01-14 23:35:12,137 : INFO : token count processed\n",
      "2021-01-14 23:35:12,168 : INFO : frequencies processed\n",
      "2021-01-14 23:35:21,593 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:35:21,594 : INFO : entropies processed\n",
      "2021-01-14 23:35:21,595 : INFO : extropies processed\n",
      "2021-01-14 23:35:21,603 : INFO : token count processed\n",
      "2021-01-14 23:35:21,610 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:35:21,614 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:35:21,615 : INFO : vocab #32006\n",
      "2021-01-14 23:35:21,622 : INFO : diff #set()\n",
      "2021-01-14 23:35:40,906 : INFO : alphabet #32006\n",
      "2021-01-14 23:35:50,351 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.2063879121479133, 0.4532294591056305], [0.8607101887464523, 0.13928981], [2.75, 1.3226647836567116], [5.064869077788814, 6.485445644653597, 6.786275812327242, 4.76403891011517, 1.7214067345384274, 0.3008301676736451]]\n",
      "2021-01-14 23:35:50,356 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:35:50,357 : INFO : built Dictionary(224 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1405 corpus positions)\n",
      "2021-01-14 23:35:50,547 : INFO : token count processed\n",
      "2021-01-14 23:35:50,613 : INFO : frequencies processed\n",
      "2021-01-14 23:36:00,151 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:36:00,152 : INFO : entropies processed\n",
      "2021-01-14 23:36:00,153 : INFO : extropies processed\n",
      "2021-01-14 23:36:00,160 : INFO : token count processed\n",
      "2021-01-14 23:36:00,167 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:36:00,172 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:36:00,173 : INFO : vocab #32006\n",
      "2021-01-14 23:36:00,180 : INFO : diff #set()\n",
      "2021-01-14 23:36:19,088 : INFO : alphabet #32006\n",
      "2021-01-14 23:36:28,521 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.203185821044864, 0.4538881788580813], [0.847661167383194, 0.15233883], [2.584962500721156, 1.315172029168969], [5.064869077788814, 6.2276600107346916, 6.428177753729369, 4.864351334794136, 1.363308675940555, 0.20051774299467784]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 23:36:28,525 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:36:28,527 : INFO : built Dictionary(213 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1603 corpus positions)\n",
      "2021-01-14 23:36:28,689 : INFO : token count processed\n",
      "2021-01-14 23:36:28,755 : INFO : frequencies processed\n",
      "2021-01-14 23:36:38,302 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:36:38,303 : INFO : entropies processed\n",
      "2021-01-14 23:36:38,303 : INFO : extropies processed\n",
      "2021-01-14 23:36:38,311 : INFO : token count processed\n",
      "2021-01-14 23:36:38,316 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:36:38,321 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:36:38,322 : INFO : vocab #32006\n",
      "2021-01-14 23:36:38,329 : INFO : diff #set()\n",
      "2021-01-14 23:36:57,206 : INFO : alphabet #32006\n",
      "2021-01-14 23:37:06,637 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.2240464649651932, 0.44963089384719773], [0.8872439190745354, 0.11275608], [2.807354922057604, 1.3343545280186873], [5.064869077788814, 6.253918170574241, 6.497126134583329, 4.821661113779726, 1.4322570567945148, 0.2432079640090885]]\n",
      "2021-01-14 23:37:06,641 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:37:06,643 : INFO : built Dictionary(186 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 700 corpus positions)\n",
      "2021-01-14 23:37:06,797 : INFO : token count processed\n",
      "2021-01-14 23:37:06,841 : INFO : frequencies processed\n",
      "2021-01-14 23:37:16,406 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:37:16,407 : INFO : entropies processed\n",
      "2021-01-14 23:37:16,408 : INFO : extropies processed\n",
      "2021-01-14 23:37:16,415 : INFO : token count processed\n",
      "2021-01-14 23:37:16,422 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:37:16,426 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:37:16,427 : INFO : vocab #32006\n",
      "2021-01-14 23:37:16,434 : INFO : diff #set()\n",
      "2021-01-14 23:37:35,360 : INFO : alphabet #32006\n",
      "2021-01-14 23:37:44,925 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.1873516520598677, 0.4571738609373953], [0.8083095848560333, 0.19169042], [2.584962500721156, 1.315172029168969], [5.064869077788814, 6.374522245625576, 6.6856093588550465, 4.753781964559344, 1.6207402810662321, 0.3110871132294708]]\n",
      "2021-01-14 23:37:44,930 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:37:44,935 : INFO : built Dictionary(305 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1982 corpus positions)\n",
      "2021-01-14 23:37:45,231 : INFO : token count processed\n",
      "2021-01-14 23:37:45,263 : INFO : frequencies processed\n",
      "2021-01-14 23:37:54,680 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:37:54,681 : INFO : entropies processed\n",
      "2021-01-14 23:37:54,682 : INFO : extropies processed\n",
      "2021-01-14 23:37:54,689 : INFO : token count processed\n",
      "2021-01-14 23:37:54,694 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:37:54,698 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:37:54,699 : INFO : vocab #32006\n",
      "2021-01-14 23:37:54,706 : INFO : diff #set()\n",
      "2021-01-14 23:38:13,622 : INFO : alphabet #32006\n",
      "2021-01-14 23:38:23,191 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.2093026720710278, 0.45263150796019613], [0.878666989505291, 0.12133301], [3.121928094887362, 1.3519647487142497], [5.064869077788814, 6.731238669067808, 7.040835760303336, 4.755271986553286, 1.975966682514522, 0.3095970912355286]]\n",
      "2021-01-14 23:38:23,195 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:38:23,197 : INFO : built Dictionary(232 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1374 corpus positions)\n",
      "2021-01-14 23:38:23,388 : INFO : token count processed\n",
      "2021-01-14 23:38:23,422 : INFO : frequencies processed\n",
      "2021-01-14 23:38:32,849 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:38:32,850 : INFO : entropies processed\n",
      "2021-01-14 23:38:32,850 : INFO : extropies processed\n",
      "2021-01-14 23:38:32,858 : INFO : token count processed\n",
      "2021-01-14 23:38:32,862 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:38:32,866 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:38:32,867 : INFO : vocab #32006\n",
      "2021-01-14 23:38:32,874 : INFO : diff #set()\n",
      "2021-01-14 23:38:51,919 : INFO : alphabet #32006\n",
      "2021-01-14 23:39:01,270 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.202918449548252, 0.45394326794306344], [0.8622191846370697, 0.13778082], [2.725480556997868, 1.3192201298976014], [5.064869077788814, 6.503741451859337, 6.751398192863376, 4.817212336784775, 1.6865291150745616, 0.24765674100403867]]\n",
      "2021-01-14 23:39:01,275 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:39:01,277 : INFO : built Dictionary(253 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 1696 corpus positions)\n",
      "2021-01-14 23:39:01,493 : INFO : token count processed\n",
      "2021-01-14 23:39:01,525 : INFO : frequencies processed\n",
      "2021-01-14 23:39:10,951 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:39:10,952 : INFO : entropies processed\n",
      "2021-01-14 23:39:10,953 : INFO : extropies processed\n",
      "2021-01-14 23:39:10,960 : INFO : token count processed\n",
      "2021-01-14 23:39:10,965 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:39:10,969 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:39:10,970 : INFO : vocab #32006\n",
      "2021-01-14 23:39:10,977 : INFO : diff #set()\n",
      "2021-01-14 23:39:29,990 : INFO : alphabet #32006\n",
      "2021-01-14 23:39:39,579 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.222285330921353, 0.44998722085134], [0.8845494464039803, 0.11545055], [2.94770277922009, 1.3393100707180505], [5.064869077788814, 6.334729224484471, 6.522619981000024, 4.8769783212732625, 1.4577509032112097, 0.18789075651555276]]\n",
      "2021-01-14 23:39:39,584 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:39:39,586 : INFO : built Dictionary(250 unique tokens: ['-', '.', '2:', '76', 'act']...) from 2 documents (total 2045 corpus positions)\n",
      "2021-01-14 23:39:39,785 : INFO : token count processed\n",
      "2021-01-14 23:39:39,829 : INFO : frequencies processed\n",
      "2021-01-14 23:39:49,267 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:39:49,268 : INFO : entropies processed\n",
      "2021-01-14 23:39:49,269 : INFO : extropies processed\n",
      "2021-01-14 23:39:49,279 : INFO : token count processed\n",
      "2021-01-14 23:39:49,284 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:39:49,288 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:39:49,289 : INFO : vocab #32006\n",
      "2021-01-14 23:39:49,295 : INFO : diff #set()\n",
      "2021-01-14 23:40:08,312 : INFO : alphabet #32006\n",
      "2021-01-14 23:40:17,754 : INFO : Computed distances or similarities ('270', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.207830211648525, 0.4529333798966942], [0.8833096921443939, 0.11669031], [2.94770277922009, 1.3393100707180505], [5.064869077788814, 6.21319712067992, 6.455987330350755, 4.8220788681179805, 1.3911182525619408, 0.24279020967083476]]\n",
      "2021-01-14 23:40:17,759 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 23:40:17,760 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:40:17,762 : INFO : built Dictionary(318 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1497 corpus positions)\n",
      "2021-01-14 23:40:18,276 : INFO : token count processed\n",
      "2021-01-14 23:40:18,309 : INFO : frequencies processed\n",
      "2021-01-14 23:40:27,744 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:40:27,745 : INFO : entropies processed\n",
      "2021-01-14 23:40:27,745 : INFO : extropies processed\n",
      "2021-01-14 23:40:27,752 : INFO : token count processed\n",
      "2021-01-14 23:40:27,758 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:40:27,763 : INFO : alphabet_target #32010\n",
      "2021-01-14 23:40:27,764 : INFO : vocab #32006\n",
      "2021-01-14 23:40:27,772 : INFO : diff #set()\n",
      "2021-01-14 23:40:46,776 : INFO : alphabet #32006\n",
      "2021-01-14 23:40:56,207 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.1189704637054505, 0.47192729541463113], [0.7047125995159149, 0.2952874], [3.501765340192027, 1.3357006876404753], [5.90184948466379, 6.905617163738059, 7.207837817255084, 5.599628831146766, 1.3059883325912933, 0.30222065351702465]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 23:40:56,213 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:40:56,217 : INFO : built Dictionary(394 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 2387 corpus positions)\n",
      "2021-01-14 23:40:57,016 : INFO : token count processed\n",
      "2021-01-14 23:40:57,052 : INFO : frequencies processed\n",
      "2021-01-14 23:41:06,474 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:41:06,475 : INFO : entropies processed\n",
      "2021-01-14 23:41:06,476 : INFO : extropies processed\n",
      "2021-01-14 23:41:06,484 : INFO : token count processed\n",
      "2021-01-14 23:41:06,489 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:41:06,493 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:41:06,495 : INFO : vocab #32006\n",
      "2021-01-14 23:41:06,501 : INFO : diff #set()\n",
      "2021-01-14 23:41:25,489 : INFO : alphabet #32006\n",
      "2021-01-14 23:41:34,910 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.0827613831841092, 0.4801318135019423], [0.7028188705444336, 0.29718113], [4.749127952571454, 1.4098172573588799], [5.90184948466379, 7.1219284286457345, 7.422585798525003, 5.601192114784522, 1.5207363138612129, 0.3006573698792687]]\n",
      "2021-01-14 23:41:34,916 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:41:34,918 : INFO : built Dictionary(323 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 2370 corpus positions)\n",
      "2021-01-14 23:41:35,451 : INFO : token count processed\n",
      "2021-01-14 23:41:35,483 : INFO : frequencies processed\n",
      "2021-01-14 23:41:45,021 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:41:45,022 : INFO : entropies processed\n",
      "2021-01-14 23:41:45,023 : INFO : extropies processed\n",
      "2021-01-14 23:41:45,030 : INFO : token count processed\n",
      "2021-01-14 23:41:45,035 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:41:45,039 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:41:45,040 : INFO : vocab #32006\n",
      "2021-01-14 23:41:45,047 : INFO : diff #set()\n",
      "2021-01-14 23:42:03,939 : INFO : alphabet #32006\n",
      "2021-01-14 23:42:13,492 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.1008205275505976, 0.47600448819201446], [0.718580424785614, 0.28141958], [4.214262795507957, 1.3942030218892532], [5.90184948466379, 6.41099024988467, 6.624052171771433, 5.688787562777028, 0.722202687107643, 0.2130619218867631]]\n",
      "2021-01-14 23:42:13,496 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:42:13,497 : INFO : built Dictionary(210 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 769 corpus positions)\n",
      "2021-01-14 23:42:13,754 : INFO : token count processed\n",
      "2021-01-14 23:42:13,803 : INFO : frequencies processed\n",
      "2021-01-14 23:42:23,232 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:42:23,233 : INFO : entropies processed\n",
      "2021-01-14 23:42:23,234 : INFO : extropies processed\n",
      "2021-01-14 23:42:23,241 : INFO : token count processed\n",
      "2021-01-14 23:42:23,245 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:42:23,252 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:42:23,253 : INFO : vocab #32006\n",
      "2021-01-14 23:42:23,260 : INFO : diff #set()\n",
      "2021-01-14 23:42:42,142 : INFO : alphabet #32006\n",
      "2021-01-14 23:42:51,694 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.1239615931518872, 0.47081830633106403], [0.7445273697376251, 0.25547263], [3.558518613048906, 1.3730602943991015], [5.90184948466379, 6.077866832717642, 6.5839311260576014, 5.39578519132383, 0.6820816413938111, 0.5060642933399597]]\n",
      "2021-01-14 23:42:51,698 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:42:51,700 : INFO : built Dictionary(187 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 534 corpus positions)\n",
      "2021-01-14 23:42:51,914 : INFO : token count processed\n",
      "2021-01-14 23:42:51,952 : INFO : frequencies processed\n",
      "2021-01-14 23:43:01,382 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:43:01,383 : INFO : entropies processed\n",
      "2021-01-14 23:43:01,384 : INFO : extropies processed\n",
      "2021-01-14 23:43:01,398 : INFO : token count processed\n",
      "2021-01-14 23:43:01,403 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:43:01,409 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:43:01,410 : INFO : vocab #32006\n",
      "2021-01-14 23:43:01,417 : INFO : diff #set()\n",
      "2021-01-14 23:43:20,211 : INFO : alphabet #32006\n",
      "2021-01-14 23:43:29,768 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.121391921233257, 0.47138861517802744], [0.7254703640937805, 0.27452964], [3.4613201402110083, 1.368539624592205], [5.90184948466379, 5.977547459003844, 6.569310425730374, 5.31008651793726, 0.6674609410665839, 0.5917629667265301]]\n",
      "2021-01-14 23:43:29,774 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:43:29,776 : INFO : built Dictionary(287 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 2245 corpus positions)\n",
      "2021-01-14 23:43:30,217 : INFO : token count processed\n",
      "2021-01-14 23:43:30,249 : INFO : frequencies processed\n",
      "2021-01-14 23:43:39,673 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:43:39,674 : INFO : entropies processed\n",
      "2021-01-14 23:43:39,675 : INFO : extropies processed\n",
      "2021-01-14 23:43:39,683 : INFO : token count processed\n",
      "2021-01-14 23:43:39,687 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:43:39,691 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:43:39,692 : INFO : vocab #32006\n",
      "2021-01-14 23:43:39,698 : INFO : diff #set()\n",
      "2021-01-14 23:43:58,707 : INFO : alphabet #32006\n",
      "2021-01-14 23:44:08,134 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.1623715702641824, 0.462455210636083], [0.7692833244800568, 0.23071668], [3.943465189601647, 1.3891873816887357], [5.90184948466379, 6.4614394051846435, 6.73339685775853, 5.629892032089904, 0.8315473730947396, 0.27195745257388637]]\n",
      "2021-01-14 23:44:08,138 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 23:44:08,139 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:44:08,141 : INFO : built Dictionary(255 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1469 corpus positions)\n",
      "2021-01-14 23:44:08,501 : INFO : token count processed\n",
      "2021-01-14 23:44:08,550 : INFO : frequencies processed\n",
      "2021-01-14 23:44:17,972 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:44:17,974 : INFO : entropies processed\n",
      "2021-01-14 23:44:17,974 : INFO : extropies processed\n",
      "2021-01-14 23:44:17,982 : INFO : token count processed\n",
      "2021-01-14 23:44:17,986 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:44:17,990 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:44:17,991 : INFO : vocab #32006\n",
      "2021-01-14 23:44:17,999 : INFO : diff #set()\n",
      "2021-01-14 23:44:37,023 : INFO : alphabet #32006\n",
      "2021-01-14 23:44:46,625 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.1522505194623427, 0.4646299261899175], [0.7587158232927322, 0.24128418], [3.350209029099897, 1.3638222121783115], [5.90184948466379, 6.327195724598159, 6.680166749662172, 5.548878459599777, 0.7783172649983818, 0.35297102506401323]]\n",
      "2021-01-14 23:44:46,638 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 23:44:46,639 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:44:46,641 : INFO : built Dictionary(452 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 6361 corpus positions)\n",
      "2021-01-14 23:44:47,583 : INFO : token count processed\n",
      "2021-01-14 23:44:47,616 : INFO : frequencies processed\n",
      "2021-01-14 23:44:57,032 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:44:57,033 : INFO : entropies processed\n",
      "2021-01-14 23:44:57,034 : INFO : extropies processed\n",
      "2021-01-14 23:44:57,043 : INFO : token count processed\n",
      "2021-01-14 23:44:57,049 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:44:57,054 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:44:57,055 : INFO : vocab #32006\n",
      "2021-01-14 23:44:57,062 : INFO : diff #set()\n",
      "2021-01-14 23:45:16,065 : INFO : alphabet #32006\n",
      "2021-01-14 23:45:25,511 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.0923446149107117, 0.47793274247161904], [0.7254151701927185, 0.27458483], [4.620631000600122, 1.4040924714254563], [5.90184948466379, 6.9079058562486315, 7.068942910924201, 5.74081242998822, 1.1670934262604105, 0.16103705467556928]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 23:45:25,518 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:45:25,520 : INFO : built Dictionary(356 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 2755 corpus positions)\n",
      "2021-01-14 23:45:26,162 : INFO : token count processed\n",
      "2021-01-14 23:45:26,210 : INFO : frequencies processed\n",
      "2021-01-14 23:45:35,617 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:45:35,618 : INFO : entropies processed\n",
      "2021-01-14 23:45:35,618 : INFO : extropies processed\n",
      "2021-01-14 23:45:35,626 : INFO : token count processed\n",
      "2021-01-14 23:45:35,632 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:45:35,636 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:45:35,637 : INFO : vocab #32006\n",
      "2021-01-14 23:45:35,644 : INFO : diff #set()\n",
      "2021-01-14 23:45:54,641 : INFO : alphabet #32006\n",
      "2021-01-14 23:46:04,070 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.088690492653384, 0.47876887624917674], [0.7153773307800293, 0.28462267], [4.587472353123262, 1.4072840578757722], [5.90184948466379, 6.61034830706307, 6.867653529772328, 5.6445442619545325, 0.9658040451085377, 0.2573052227092578]]\n",
      "2021-01-14 23:46:04,074 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:46:04,076 : INFO : built Dictionary(256 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 802 corpus positions)\n",
      "2021-01-14 23:46:04,454 : INFO : token count processed\n",
      "2021-01-14 23:46:04,520 : INFO : frequencies processed\n",
      "2021-01-14 23:46:13,926 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:46:13,927 : INFO : entropies processed\n",
      "2021-01-14 23:46:13,928 : INFO : extropies processed\n",
      "2021-01-14 23:46:13,935 : INFO : token count processed\n",
      "2021-01-14 23:46:13,942 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:46:13,947 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:46:13,948 : INFO : vocab #32006\n",
      "2021-01-14 23:46:13,954 : INFO : diff #set()\n",
      "2021-01-14 23:46:33,146 : INFO : alphabet #32006\n",
      "2021-01-14 23:46:42,589 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.1138562274089723, 0.47306907018257105], [0.7162984013557434, 0.2837016], [3.6464393446710153, 1.3752020230999098], [5.90184948466379, 6.616715366949855, 7.0434691079500995, 5.4750957436635455, 1.1416196232863092, 0.4267537410002449]]\n",
      "2021-01-14 23:46:42,596 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:46:42,598 : INFO : built Dictionary(465 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 2836 corpus positions)\n",
      "2021-01-14 23:46:43,622 : INFO : token count processed\n",
      "2021-01-14 23:46:43,654 : INFO : frequencies processed\n",
      "2021-01-14 23:46:53,204 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:46:53,205 : INFO : entropies processed\n",
      "2021-01-14 23:46:53,206 : INFO : extropies processed\n",
      "2021-01-14 23:46:53,214 : INFO : token count processed\n",
      "2021-01-14 23:46:53,222 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:46:53,226 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:46:53,227 : INFO : vocab #32006\n",
      "2021-01-14 23:46:53,234 : INFO : diff #set()\n",
      "2021-01-14 23:47:12,137 : INFO : alphabet #32006\n",
      "2021-01-14 23:47:21,510 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.0970242694505203, 0.47686620253662027], [0.7169270515441895, 0.28307295], [4.42511855035714, 1.4030404692024767], [5.90184948466379, 7.32185870753746, 7.607945320415203, 5.615762871786048, 1.7060958357514124, 0.2860866128777424]]\n",
      "2021-01-14 23:47:21,513 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:47:21,515 : INFO : built Dictionary(119 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 201 corpus positions)\n",
      "2021-01-14 23:47:21,597 : INFO : token count processed\n",
      "2021-01-14 23:47:21,624 : INFO : frequencies processed\n",
      "2021-01-14 23:47:30,978 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:47:30,979 : INFO : entropies processed\n",
      "2021-01-14 23:47:30,980 : INFO : extropies processed\n",
      "2021-01-14 23:47:30,987 : INFO : token count processed\n",
      "2021-01-14 23:47:30,992 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:47:30,997 : INFO : alphabet_target #32008\n",
      "2021-01-14 23:47:30,998 : INFO : vocab #32006\n",
      "2021-01-14 23:47:31,005 : INFO : diff #set()\n",
      "2021-01-14 23:47:50,136 : INFO : alphabet #32006\n",
      "2021-01-14 23:47:59,690 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/fireException.py')[[1.1603571346577608, 0.46288642926551027], [0.8054850250482559, 0.19451497], [2.0, 1.2451124978365313], [5.90184948466379, 5.176618657501385, 6.489639046943776, 4.588829095221398, 0.587789562279986, 1.313020389442391]]\n",
      "2021-01-14 23:47:59,694 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:47:59,695 : INFO : built Dictionary(211 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 594 corpus positions)\n",
      "2021-01-14 23:47:59,963 : INFO : token count processed\n",
      "2021-01-14 23:48:00,004 : INFO : frequencies processed\n",
      "2021-01-14 23:48:09,418 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:48:09,419 : INFO : entropies processed\n",
      "2021-01-14 23:48:09,420 : INFO : extropies processed\n",
      "2021-01-14 23:48:09,427 : INFO : token count processed\n",
      "2021-01-14 23:48:09,434 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:48:09,439 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:48:09,440 : INFO : vocab #32006\n",
      "2021-01-14 23:48:09,446 : INFO : diff #set()\n",
      "2021-01-14 23:48:28,309 : INFO : alphabet #32006\n",
      "2021-01-14 23:48:37,840 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.1691472167989452, 0.4610106645853758], [0.7720131874084473, 0.22798681], [2.692380602454975, 1.2874125465846893], [5.90184948466379, 6.468846789852156, 7.031750640846694, 5.338945633669253, 1.1299011561829033, 0.5629038509945374]]\n",
      "2021-01-14 23:48:37,846 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:48:37,848 : INFO : built Dictionary(413 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 2643 corpus positions)\n",
      "2021-01-14 23:48:38,726 : INFO : token count processed\n",
      "2021-01-14 23:48:38,757 : INFO : frequencies processed\n",
      "2021-01-14 23:48:48,179 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:48:48,180 : INFO : entropies processed\n",
      "2021-01-14 23:48:48,181 : INFO : extropies processed\n",
      "2021-01-14 23:48:48,188 : INFO : token count processed\n",
      "2021-01-14 23:48:48,193 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:48:48,197 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:48:48,198 : INFO : vocab #32006\n",
      "2021-01-14 23:48:48,205 : INFO : diff #set()\n",
      "2021-01-14 23:49:07,169 : INFO : alphabet #32006\n",
      "2021-01-14 23:49:16,749 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.1502987042040576, 0.46505166842397105], [0.7647906243801117, 0.23520938], [4.2336188853070205, 1.393149009541834], [5.90184948466379, 6.957796704012729, 7.215542486452886, 5.644103702223633, 1.3136930017890958, 0.257745782440157]]\n",
      "2021-01-14 23:49:16,756 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:49:16,759 : INFO : built Dictionary(330 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 3140 corpus positions)\n",
      "2021-01-14 23:49:17,297 : INFO : token count processed\n",
      "2021-01-14 23:49:17,331 : INFO : frequencies processed\n",
      "2021-01-14 23:49:26,748 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:49:26,749 : INFO : entropies processed\n",
      "2021-01-14 23:49:26,750 : INFO : extropies processed\n",
      "2021-01-14 23:49:26,757 : INFO : token count processed\n",
      "2021-01-14 23:49:26,765 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:49:26,770 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:49:26,771 : INFO : vocab #32006\n",
      "2021-01-14 23:49:26,778 : INFO : diff #set()\n",
      "2021-01-14 23:49:45,942 : INFO : alphabet #32006\n",
      "2021-01-14 23:49:55,370 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.1563865745647255, 0.46373874322689734], [0.7659832537174225, 0.23401675], [3.4042715900718337, 1.3346249595371238], [5.90184948466379, 6.441859572014148, 6.714054132985481, 5.629654923692456, 0.8122046483216909, 0.2721945609713332]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 23:49:55,375 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:49:55,377 : INFO : built Dictionary(346 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1616 corpus positions)\n",
      "2021-01-14 23:49:55,985 : INFO : token count processed\n",
      "2021-01-14 23:49:56,017 : INFO : frequencies processed\n",
      "2021-01-14 23:50:05,429 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:50:05,430 : INFO : entropies processed\n",
      "2021-01-14 23:50:05,431 : INFO : extropies processed\n",
      "2021-01-14 23:50:05,438 : INFO : token count processed\n",
      "2021-01-14 23:50:05,442 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:50:05,447 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:50:05,448 : INFO : vocab #32006\n",
      "2021-01-14 23:50:05,454 : INFO : diff #set()\n",
      "2021-01-14 23:50:24,598 : INFO : alphabet #32006\n",
      "2021-01-14 23:50:34,029 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.1066934077085546, 0.4746775189692635], [0.6853804588317871, 0.31461954], [4.42511855035714, 1.4030404692024767], [5.90184948466379, 6.998955278238291, 7.297794817163503, 5.603009945738577, 1.3959453324997124, 0.2988395389252121]]\n",
      "2021-01-14 23:50:34,034 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:50:34,036 : INFO : built Dictionary(269 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1435 corpus positions)\n",
      "2021-01-14 23:50:34,454 : INFO : token count processed\n",
      "2021-01-14 23:50:34,485 : INFO : frequencies processed\n",
      "2021-01-14 23:50:43,907 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:50:43,908 : INFO : entropies processed\n",
      "2021-01-14 23:50:43,909 : INFO : extropies processed\n",
      "2021-01-14 23:50:43,916 : INFO : token count processed\n",
      "2021-01-14 23:50:43,922 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:50:43,927 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:50:43,928 : INFO : vocab #32006\n",
      "2021-01-14 23:50:43,936 : INFO : diff #set()\n",
      "2021-01-14 23:51:02,920 : INFO : alphabet #32006\n",
      "2021-01-14 23:51:12,351 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.1369500972178057, 0.46795664592352726], [0.7475247383117676, 0.25247526], [3.940072687348654, 1.3885832848042485], [5.90184948466379, 6.492983191376071, 6.8224452375504185, 5.572387438489443, 0.9205957528866282, 0.3294620461743474]]\n",
      "2021-01-14 23:51:12,358 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:51:12,361 : INFO : built Dictionary(481 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 3374 corpus positions)\n",
      "2021-01-14 23:51:13,364 : INFO : token count processed\n",
      "2021-01-14 23:51:13,399 : INFO : frequencies processed\n",
      "2021-01-14 23:51:22,922 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:51:22,923 : INFO : entropies processed\n",
      "2021-01-14 23:51:22,924 : INFO : extropies processed\n",
      "2021-01-14 23:51:22,932 : INFO : token count processed\n",
      "2021-01-14 23:51:22,939 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:51:22,943 : INFO : alphabet_target #32008\n",
      "2021-01-14 23:51:22,944 : INFO : vocab #32006\n",
      "2021-01-14 23:51:22,952 : INFO : diff #set()\n",
      "2021-01-14 23:51:41,845 : INFO : alphabet #32006\n",
      "2021-01-14 23:51:51,201 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.1289944081280423, 0.46970532011836924], [0.7112661898136139, 0.2887338], [3.573660689688185, 1.3691943681464476], [5.90184948466379, 6.560342487747443, 6.8675121188035, 5.594679853607733, 0.9656626341397097, 0.3071696310560572]]\n",
      "2021-01-14 23:51:51,208 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 23:51:51,209 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:51:51,211 : INFO : built Dictionary(480 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 3579 corpus positions)\n",
      "2021-01-14 23:51:52,278 : INFO : token count processed\n",
      "2021-01-14 23:51:52,308 : INFO : frequencies processed\n",
      "2021-01-14 23:52:01,830 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:52:01,832 : INFO : entropies processed\n",
      "2021-01-14 23:52:01,832 : INFO : extropies processed\n",
      "2021-01-14 23:52:01,840 : INFO : token count processed\n",
      "2021-01-14 23:52:01,847 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:52:01,852 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:52:01,853 : INFO : vocab #32006\n",
      "2021-01-14 23:52:01,860 : INFO : diff #set()\n",
      "2021-01-14 23:52:20,769 : INFO : alphabet #32006\n",
      "2021-01-14 23:52:30,197 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.0974622815916195, 0.4767666187737922], [0.7054567337036133, 0.29454327], [4.6277218710815085, 1.4007064847197466], [5.90184948466379, 7.046173750105238, 7.301816289503099, 5.646206945265929, 1.399966804839309, 0.2556425393978614]]\n",
      "2021-01-14 23:52:30,208 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-14 23:52:30,209 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:52:30,212 : INFO : built Dictionary(532 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 5688 corpus positions)\n",
      "2021-01-14 23:52:31,547 : INFO : token count processed\n",
      "2021-01-14 23:52:31,590 : INFO : frequencies processed\n",
      "2021-01-14 23:52:41,137 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:52:41,138 : INFO : entropies processed\n",
      "2021-01-14 23:52:41,138 : INFO : extropies processed\n",
      "2021-01-14 23:52:41,147 : INFO : token count processed\n",
      "2021-01-14 23:52:41,153 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:52:41,158 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:52:41,159 : INFO : vocab #32006\n",
      "2021-01-14 23:52:41,166 : INFO : diff #set()\n",
      "2021-01-14 23:53:00,040 : INFO : alphabet #32006\n",
      "2021-01-14 23:53:09,578 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.0848460192307376, 0.4796517300443023], [0.6991488635540009, 0.30085114], [4.635803679712058, 1.3961236959354808], [5.90184948466379, 7.009229588004272, 7.168158793239858, 5.742920279428205, 1.2663093085760675, 0.1589292052355855]]\n",
      "2021-01-14 23:53:09,590 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:53:09,597 : INFO : built Dictionary(608 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 6629 corpus positions)\n",
      "2021-01-14 23:53:11,235 : INFO : token count processed\n",
      "2021-01-14 23:53:11,268 : INFO : frequencies processed\n",
      "2021-01-14 23:53:20,683 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:53:20,684 : INFO : entropies processed\n",
      "2021-01-14 23:53:20,685 : INFO : extropies processed\n",
      "2021-01-14 23:53:20,703 : INFO : token count processed\n",
      "2021-01-14 23:53:20,707 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:53:20,711 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:53:20,712 : INFO : vocab #32006\n",
      "2021-01-14 23:53:20,719 : INFO : diff #set()\n",
      "2021-01-14 23:53:39,699 : INFO : alphabet #32006\n",
      "2021-01-14 23:53:49,131 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.1354647130876045, 0.46828214667810164], [0.7355263531208038, 0.26447365], [4.788069956104106, 1.401661109877872], [5.90184948466379, 7.376088004590871, 7.583611582992867, 5.694325906261795, 1.6817620983290764, 0.20752357840199576]]\n",
      "2021-01-14 23:53:49,135 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:53:49,136 : INFO : built Dictionary(188 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 415 corpus positions)\n",
      "2021-01-14 23:53:49,367 : INFO : token count processed\n",
      "2021-01-14 23:53:49,400 : INFO : frequencies processed\n",
      "2021-01-14 23:53:58,811 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:53:58,812 : INFO : entropies processed\n",
      "2021-01-14 23:53:58,813 : INFO : extropies processed\n",
      "2021-01-14 23:53:58,820 : INFO : token count processed\n",
      "2021-01-14 23:53:58,824 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:53:58,828 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:53:58,829 : INFO : vocab #32006\n",
      "2021-01-14 23:53:58,837 : INFO : diff #set()\n",
      "2021-01-14 23:54:17,862 : INFO : alphabet #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 23:54:27,301 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.1531451938343298, 0.4644368632749731], [0.7217923402786255, 0.27820766], [2.3301249653149703, 1.2131230198581628], [5.90184948466379, 6.2993628166120885, 6.936104270831198, 5.265108030444681, 1.0342547861674074, 0.6367414542191092]]\n",
      "2021-01-14 23:54:27,304 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:54:27,306 : INFO : built Dictionary(91 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 131 corpus positions)\n",
      "2021-01-14 23:54:27,340 : INFO : token count processed\n",
      "2021-01-14 23:54:27,372 : INFO : frequencies processed\n",
      "2021-01-14 23:54:36,805 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:54:36,806 : INFO : entropies processed\n",
      "2021-01-14 23:54:36,807 : INFO : extropies processed\n",
      "2021-01-14 23:54:36,814 : INFO : token count processed\n",
      "2021-01-14 23:54:36,818 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:54:36,824 : INFO : alphabet_target #32008\n",
      "2021-01-14 23:54:36,825 : INFO : vocab #32006\n",
      "2021-01-14 23:54:36,833 : INFO : diff #set()\n",
      "2021-01-14 23:54:55,817 : INFO : alphabet #32006\n",
      "2021-01-14 23:55:05,245 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.254645159764502, 0.44352877244082617], [0.8687881231307983, 0.13121188], [0.0, 0.0], [5.90184948466379, 3.8936606896881862, 6.144342097936217, 3.6511680764157592, 0.242492613272427, 2.250681408248031]]\n",
      "2021-01-14 23:55:05,268 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:55:05,273 : INFO : built Dictionary(769 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 12573 corpus positions)\n",
      "2021-01-14 23:55:07,791 : INFO : token count processed\n",
      "2021-01-14 23:55:07,824 : INFO : frequencies processed\n",
      "2021-01-14 23:55:17,248 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:55:17,249 : INFO : entropies processed\n",
      "2021-01-14 23:55:17,250 : INFO : extropies processed\n",
      "2021-01-14 23:55:17,261 : INFO : token count processed\n",
      "2021-01-14 23:55:17,268 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:55:17,274 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:55:17,276 : INFO : vocab #32006\n",
      "2021-01-14 23:55:17,283 : INFO : diff #set()\n",
      "2021-01-14 23:55:36,294 : INFO : alphabet #32006\n",
      "2021-01-14 23:55:45,748 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.1067456728590166, 0.47466574294320146], [0.6891121566295624, 0.31088784], [4.680831882119565, 1.397497223313146], [5.90184948466379, 7.434393313070278, 7.664087840858489, 5.6721549568755805, 1.7622383561946986, 0.22969452778821076]]\n",
      "2021-01-14 23:55:45,757 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:55:45,760 : INFO : built Dictionary(522 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 4200 corpus positions)\n",
      "2021-01-14 23:55:46,947 : INFO : token count processed\n",
      "2021-01-14 23:55:46,988 : INFO : frequencies processed\n",
      "2021-01-14 23:55:56,522 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:55:56,523 : INFO : entropies processed\n",
      "2021-01-14 23:55:56,524 : INFO : extropies processed\n",
      "2021-01-14 23:55:56,532 : INFO : token count processed\n",
      "2021-01-14 23:55:56,536 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:55:56,541 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:55:56,542 : INFO : vocab #32006\n",
      "2021-01-14 23:55:56,553 : INFO : diff #set()\n",
      "2021-01-14 23:56:15,343 : INFO : alphabet #32006\n",
      "2021-01-14 23:56:24,772 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.0973904013028617, 0.4767829581840451], [0.6972797811031342, 0.30272022], [4.626480015040089, 1.3981382292398512], [5.90184948466379, 7.2991514951718255, 7.545508390198922, 5.655492589636694, 1.6436589055351316, 0.24635689502709646]]\n",
      "2021-01-14 23:56:24,780 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:56:24,782 : INFO : built Dictionary(492 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 3617 corpus positions)\n",
      "2021-01-14 23:56:25,861 : INFO : token count processed\n",
      "2021-01-14 23:56:25,893 : INFO : frequencies processed\n",
      "2021-01-14 23:56:35,426 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:56:35,427 : INFO : entropies processed\n",
      "2021-01-14 23:56:35,428 : INFO : extropies processed\n",
      "2021-01-14 23:56:35,436 : INFO : token count processed\n",
      "2021-01-14 23:56:35,440 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:56:35,445 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:56:35,446 : INFO : vocab #32006\n",
      "2021-01-14 23:56:35,456 : INFO : diff #set()\n",
      "2021-01-14 23:56:54,347 : INFO : alphabet #32006\n",
      "2021-01-14 23:57:03,772 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.1069164372812061, 0.4746272715449569], [0.7046985328197479, 0.29530147], [4.507462287781333, 1.3988881263528667], [5.90184948466379, 7.170319527000998, 7.424063455443174, 5.648105556221613, 1.5222139707793838, 0.2537439284421765]]\n",
      "2021-01-14 23:57:03,776 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:57:03,778 : INFO : built Dictionary(216 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 653 corpus positions)\n",
      "2021-01-14 23:57:04,058 : INFO : token count processed\n",
      "2021-01-14 23:57:04,095 : INFO : frequencies processed\n",
      "2021-01-14 23:57:13,632 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:57:13,633 : INFO : entropies processed\n",
      "2021-01-14 23:57:13,634 : INFO : extropies processed\n",
      "2021-01-14 23:57:13,648 : INFO : token count processed\n",
      "2021-01-14 23:57:13,653 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:57:13,659 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:57:13,660 : INFO : vocab #32006\n",
      "2021-01-14 23:57:13,667 : INFO : diff #set()\n",
      "2021-01-14 23:57:32,553 : INFO : alphabet #32006\n",
      "2021-01-14 23:57:42,001 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.1414809434008724, 0.4669665649286172], [0.7356295585632324, 0.26437044], [3.4414460711655215, 1.3543914481050605], [5.90184948466379, 6.353654804387375, 6.889908137278294, 5.365596151772872, 0.9880586526145034, 0.5362533328909187]]\n",
      "2021-01-14 23:57:42,005 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:57:42,007 : INFO : built Dictionary(214 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 781 corpus positions)\n",
      "2021-01-14 23:57:42,283 : INFO : token count processed\n",
      "2021-01-14 23:57:42,329 : INFO : frequencies processed\n",
      "2021-01-14 23:57:51,877 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:57:51,878 : INFO : entropies processed\n",
      "2021-01-14 23:57:51,879 : INFO : extropies processed\n",
      "2021-01-14 23:57:51,886 : INFO : token count processed\n",
      "2021-01-14 23:57:51,894 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:57:51,898 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:57:51,899 : INFO : vocab #32006\n",
      "2021-01-14 23:57:51,906 : INFO : diff #set()\n",
      "2021-01-14 23:58:10,812 : INFO : alphabet #32006\n",
      "2021-01-14 23:58:20,357 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.145408784955116, 0.4661116366319536], [0.7500082701444626, 0.24999173], [3.125, 1.3407118537600056], [5.90184948466379, 6.245180322479091, 6.753147007102498, 5.393882800040384, 0.851297522438708, 0.5079666846234074]]\n",
      "2021-01-14 23:58:20,362 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:58:20,365 : INFO : built Dictionary(418 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1853 corpus positions)\n",
      "2021-01-14 23:58:21,240 : INFO : token count processed\n",
      "2021-01-14 23:58:21,272 : INFO : frequencies processed\n",
      "2021-01-14 23:58:30,699 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:58:30,700 : INFO : entropies processed\n",
      "2021-01-14 23:58:30,701 : INFO : extropies processed\n",
      "2021-01-14 23:58:30,708 : INFO : token count processed\n",
      "2021-01-14 23:58:30,712 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:58:30,716 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:58:30,718 : INFO : vocab #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-14 23:58:30,724 : INFO : diff #set()\n",
      "2021-01-14 23:58:50,123 : INFO : alphabet #32006\n",
      "2021-01-14 23:58:59,566 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.114600804783371, 0.47290249664992656], [0.7170478105545044, 0.2829522], [4.912113890972228, 1.4139931548225333], [5.90184948466379, 7.2691387000368, 7.575733072991026, 5.595255111709564, 1.6738835883272358, 0.3065943729542262]]\n",
      "2021-01-14 23:58:59,572 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:58:59,574 : INFO : built Dictionary(358 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1690 corpus positions)\n",
      "2021-01-14 23:59:00,267 : INFO : token count processed\n",
      "2021-01-14 23:59:00,299 : INFO : frequencies processed\n",
      "2021-01-14 23:59:09,720 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:59:09,722 : INFO : entropies processed\n",
      "2021-01-14 23:59:09,722 : INFO : extropies processed\n",
      "2021-01-14 23:59:09,730 : INFO : token count processed\n",
      "2021-01-14 23:59:09,734 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:59:09,738 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:59:09,739 : INFO : vocab #32006\n",
      "2021-01-14 23:59:09,746 : INFO : diff #set()\n",
      "2021-01-14 23:59:28,755 : INFO : alphabet #32006\n",
      "2021-01-14 23:59:38,192 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.1118017916202771, 0.47352928857625], [0.7051029801368713, 0.29489702], [3.8205843997621054, 1.3550598095089945], [5.90184948466379, 7.08857858466988, 7.396388575296923, 5.594039494036748, 1.4945390906331326, 0.3078099906270433]]\n",
      "2021-01-14 23:59:38,196 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-14 23:59:38,197 : INFO : built Dictionary(192 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 613 corpus positions)\n",
      "2021-01-14 23:59:38,429 : INFO : token count processed\n",
      "2021-01-14 23:59:38,462 : INFO : frequencies processed\n",
      "2021-01-14 23:59:47,868 : INFO : scalar_distribution processed\n",
      "2021-01-14 23:59:47,869 : INFO : entropies processed\n",
      "2021-01-14 23:59:47,871 : INFO : extropies processed\n",
      "2021-01-14 23:59:47,883 : INFO : token count processed\n",
      "2021-01-14 23:59:47,888 : INFO : alphabet_source #32006\n",
      "2021-01-14 23:59:47,893 : INFO : alphabet_target #32009\n",
      "2021-01-14 23:59:47,894 : INFO : vocab #32006\n",
      "2021-01-14 23:59:47,903 : INFO : diff #set()\n",
      "2021-01-15 00:00:06,902 : INFO : alphabet #32006\n",
      "2021-01-15 00:00:16,237 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.1455051997434489, 0.46609069049078794], [0.7420228123664856, 0.2579772], [3.1916116417513782, 1.3373725452773135], [5.90184948466379, 6.0479231618016716, 6.643355507866765, 5.306417138598697, 0.7415060232029749, 0.5954323460650937]]\n",
      "2021-01-15 00:00:16,241 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:00:16,242 : INFO : built Dictionary(195 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 732 corpus positions)\n",
      "2021-01-15 00:00:16,487 : INFO : token count processed\n",
      "2021-01-15 00:00:16,519 : INFO : frequencies processed\n",
      "2021-01-15 00:00:25,922 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:00:25,924 : INFO : entropies processed\n",
      "2021-01-15 00:00:25,924 : INFO : extropies processed\n",
      "2021-01-15 00:00:25,931 : INFO : token count processed\n",
      "2021-01-15 00:00:25,936 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:00:25,940 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:00:25,941 : INFO : vocab #32006\n",
      "2021-01-15 00:00:25,948 : INFO : diff #set()\n",
      "2021-01-15 00:00:44,965 : INFO : alphabet #32006\n",
      "2021-01-15 00:00:54,404 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.1473886088674758, 0.4656818965466134], [0.756046786904335, 0.24395321], [3.1462863706621045, 1.344862508173919], [5.90184948466379, 6.036583168403119, 6.583499945731646, 5.354932707335264, 0.6816504610678553, 0.5469167773285264]]\n",
      "2021-01-15 00:00:54,417 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 00:00:54,418 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:00:54,422 : INFO : built Dictionary(599 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 7090 corpus positions)\n",
      "2021-01-15 00:00:56,065 : INFO : token count processed\n",
      "2021-01-15 00:00:56,111 : INFO : frequencies processed\n",
      "2021-01-15 00:01:05,529 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:01:05,530 : INFO : entropies processed\n",
      "2021-01-15 00:01:05,531 : INFO : extropies processed\n",
      "2021-01-15 00:01:05,540 : INFO : token count processed\n",
      "2021-01-15 00:01:05,546 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:01:05,551 : INFO : alphabet_target #32010\n",
      "2021-01-15 00:01:05,552 : INFO : vocab #32006\n",
      "2021-01-15 00:01:05,559 : INFO : diff #set()\n",
      "2021-01-15 00:01:24,581 : INFO : alphabet #32006\n",
      "2021-01-15 00:01:34,003 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.1235797488985442, 0.47090296491981465], [0.7223994135856628, 0.2776006], [3.872575561512011, 1.3629151298252458], [5.90184948466379, 7.29352035514053, 7.500213802230256, 5.6951560375740655, 1.5983643175664657, 0.2066934470897257]]\n",
      "2021-01-15 00:01:34,010 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:01:34,013 : INFO : built Dictionary(401 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 3333 corpus positions)\n",
      "2021-01-15 00:01:34,770 : INFO : token count processed\n",
      "2021-01-15 00:01:34,802 : INFO : frequencies processed\n",
      "2021-01-15 00:01:44,333 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:01:44,335 : INFO : entropies processed\n",
      "2021-01-15 00:01:44,335 : INFO : extropies processed\n",
      "2021-01-15 00:01:44,343 : INFO : token count processed\n",
      "2021-01-15 00:01:44,347 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:01:44,352 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:01:44,353 : INFO : vocab #32006\n",
      "2021-01-15 00:01:44,360 : INFO : diff #set()\n",
      "2021-01-15 00:02:03,273 : INFO : alphabet #32006\n",
      "2021-01-15 00:02:12,712 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.0948122537753848, 0.4773697491017372], [0.6907758414745331, 0.30922416], [4.543465189601648, 1.4042378986452497], [5.90184948466379, 6.8153433747477745, 7.041850263561015, 5.67534259585055, 1.1400007788972246, 0.22650688881324044]]\n",
      "2021-01-15 00:02:12,715 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:02:12,717 : INFO : built Dictionary(171 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 354 corpus positions)\n",
      "2021-01-15 00:02:12,907 : INFO : token count processed\n",
      "2021-01-15 00:02:12,939 : INFO : frequencies processed\n",
      "2021-01-15 00:02:22,857 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:02:22,858 : INFO : entropies processed\n",
      "2021-01-15 00:02:22,859 : INFO : extropies processed\n",
      "2021-01-15 00:02:22,866 : INFO : token count processed\n",
      "2021-01-15 00:02:22,874 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:02:22,878 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:02:22,879 : INFO : vocab #32006\n",
      "2021-01-15 00:02:22,886 : INFO : diff #set()\n",
      "2021-01-15 00:02:41,786 : INFO : alphabet #32006\n",
      "2021-01-15 00:02:51,358 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.139383513620857, 0.46742437418689986], [0.7738570719957352, 0.22614293], [3.0850551027564768, 1.3486914941048562], [5.90184948466379, 6.150121915859574, 6.846775641467868, 5.205195759055496, 0.9449261568040779, 0.6966537256082939]]\n",
      "2021-01-15 00:02:51,363 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:02:51,364 : INFO : built Dictionary(323 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1065 corpus positions)\n",
      "2021-01-15 00:02:51,930 : INFO : token count processed\n",
      "2021-01-15 00:02:51,961 : INFO : frequencies processed\n",
      "2021-01-15 00:03:01,375 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:03:01,376 : INFO : entropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 00:03:01,377 : INFO : extropies processed\n",
      "2021-01-15 00:03:01,384 : INFO : token count processed\n",
      "2021-01-15 00:03:01,388 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:03:01,393 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:03:01,394 : INFO : vocab #32006\n",
      "2021-01-15 00:03:01,401 : INFO : diff #set()\n",
      "2021-01-15 00:03:20,672 : INFO : alphabet #32006\n",
      "2021-01-15 00:03:30,601 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.1142386395381614, 0.4729835039900898], [0.7182893753051758, 0.28171062], [4.071928094887363, 1.3812290137117262], [5.90184948466379, 7.0391145208191315, 7.45176035768141, 5.489203647801512, 1.5499108730176197, 0.4126458368622785]]\n",
      "2021-01-15 00:03:30,610 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 00:03:30,611 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:03:30,614 : INFO : built Dictionary(624 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 4440 corpus positions)\n",
      "2021-01-15 00:03:32,398 : INFO : token count processed\n",
      "2021-01-15 00:03:32,430 : INFO : frequencies processed\n",
      "2021-01-15 00:03:41,839 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:03:41,841 : INFO : entropies processed\n",
      "2021-01-15 00:03:41,842 : INFO : extropies processed\n",
      "2021-01-15 00:03:41,856 : INFO : token count processed\n",
      "2021-01-15 00:03:41,860 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:03:41,865 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:03:41,866 : INFO : vocab #32006\n",
      "2021-01-15 00:03:41,872 : INFO : diff #set()\n",
      "2021-01-15 00:04:00,761 : INFO : alphabet #32006\n",
      "2021-01-15 00:04:10,468 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.1079628912662243, 0.47439165278631346], [0.7083511352539062, 0.29164886], [4.230003326102139, 1.3821288522201518], [5.90184948466379, 7.482466367279176, 7.810441970276573, 5.573873881666392, 1.9085924856127825, 0.32797560299739725]]\n",
      "2021-01-15 00:04:10,472 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:04:10,473 : INFO : built Dictionary(217 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 662 corpus positions)\n",
      "2021-01-15 00:04:10,750 : INFO : token count processed\n",
      "2021-01-15 00:04:10,782 : INFO : frequencies processed\n",
      "2021-01-15 00:04:20,062 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:04:20,063 : INFO : entropies processed\n",
      "2021-01-15 00:04:20,064 : INFO : extropies processed\n",
      "2021-01-15 00:04:20,070 : INFO : token count processed\n",
      "2021-01-15 00:04:20,074 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:04:20,078 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:04:20,079 : INFO : vocab #32006\n",
      "2021-01-15 00:04:20,084 : INFO : diff #set()\n",
      "2021-01-15 00:04:38,877 : INFO : alphabet #32006\n",
      "2021-01-15 00:04:48,309 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.1632683207677939, 0.4622635067503216], [0.7446373403072357, 0.25536266], [3.3027227528624894, 1.342089957691207], [5.90184948466379, 6.372162341197667, 6.913891397011659, 5.360120428849799, 1.0120419123478683, 0.5417290558139918]]\n",
      "2021-01-15 00:04:48,315 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:04:48,317 : INFO : built Dictionary(353 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 2083 corpus positions)\n",
      "2021-01-15 00:04:48,932 : INFO : token count processed\n",
      "2021-01-15 00:04:48,964 : INFO : frequencies processed\n",
      "2021-01-15 00:04:58,382 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:04:58,384 : INFO : entropies processed\n",
      "2021-01-15 00:04:58,384 : INFO : extropies processed\n",
      "2021-01-15 00:04:58,392 : INFO : token count processed\n",
      "2021-01-15 00:04:58,396 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:04:58,401 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:04:58,402 : INFO : vocab #32006\n",
      "2021-01-15 00:04:58,409 : INFO : diff #set()\n",
      "2021-01-15 00:05:17,414 : INFO : alphabet #32006\n",
      "2021-01-15 00:05:26,841 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.1169468483061986, 0.4723784164917108], [0.7039673626422882, 0.29603264], [4.338391841917422, 1.3965130604827538], [5.90184948466379, 6.798155919669889, 7.105882561970629, 5.594122842363051, 1.204033077306839, 0.3077266423007403]]\n",
      "2021-01-15 00:05:26,845 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:05:26,847 : INFO : built Dictionary(223 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 799 corpus positions)\n",
      "2021-01-15 00:05:27,146 : INFO : token count processed\n",
      "2021-01-15 00:05:27,178 : INFO : frequencies processed\n",
      "2021-01-15 00:05:36,591 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:05:36,592 : INFO : entropies processed\n",
      "2021-01-15 00:05:36,593 : INFO : extropies processed\n",
      "2021-01-15 00:05:36,600 : INFO : token count processed\n",
      "2021-01-15 00:05:36,605 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:05:36,609 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:05:36,610 : INFO : vocab #32006\n",
      "2021-01-15 00:05:36,617 : INFO : diff #set()\n",
      "2021-01-15 00:05:55,588 : INFO : alphabet #32006\n",
      "2021-01-15 00:06:05,033 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.1004431592907882, 0.47609000775705296], [0.6939774751663208, 0.30602252], [3.0438561897747243, 1.3150125288115007], [5.90184948466379, 6.271631856729336, 6.756214246069012, 5.417267095324114, 0.8543647614052219, 0.48458238933967657]]\n",
      "2021-01-15 00:06:05,040 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:06:05,044 : INFO : built Dictionary(385 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 3314 corpus positions)\n",
      "2021-01-15 00:06:05,837 : INFO : token count processed\n",
      "2021-01-15 00:06:05,904 : INFO : frequencies processed\n",
      "2021-01-15 00:06:15,315 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:06:15,316 : INFO : entropies processed\n",
      "2021-01-15 00:06:15,317 : INFO : extropies processed\n",
      "2021-01-15 00:06:15,325 : INFO : token count processed\n",
      "2021-01-15 00:06:15,331 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:06:15,336 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:06:15,338 : INFO : vocab #32006\n",
      "2021-01-15 00:06:15,345 : INFO : diff #set()\n",
      "2021-01-15 00:06:34,323 : INFO : alphabet #32006\n",
      "2021-01-15 00:06:43,765 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.1069908111438047, 0.4746105178584705], [0.7165697515010834, 0.28343025], [4.53421902118034, 1.4057643172189322], [5.90184948466379, 6.873598627629562, 7.114456034693086, 5.660992077600266, 1.212606550029296, 0.24085740706352432]]\n",
      "2021-01-15 00:06:43,768 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:06:43,770 : INFO : built Dictionary(151 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 262 corpus positions)\n",
      "2021-01-15 00:06:43,928 : INFO : token count processed\n",
      "2021-01-15 00:06:43,998 : INFO : frequencies processed\n",
      "2021-01-15 00:06:53,523 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:06:53,524 : INFO : entropies processed\n",
      "2021-01-15 00:06:53,525 : INFO : extropies processed\n",
      "2021-01-15 00:06:53,538 : INFO : token count processed\n",
      "2021-01-15 00:06:53,543 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:06:53,548 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:06:53,549 : INFO : vocab #32006\n",
      "2021-01-15 00:06:53,555 : INFO : diff #set()\n",
      "2021-01-15 00:07:12,467 : INFO : alphabet #32006\n",
      "2021-01-15 00:07:22,019 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.0929830904228663, 0.4777869465720146], [0.6880501806735992, 0.31194982], [3.1556390622295662, 1.3476547441220017], [5.90184948466379, 6.049830202851529, 6.809838208095206, 5.141841479420114, 0.9079887234314157, 0.7600080052436766]]\n",
      "2021-01-15 00:07:22,024 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:07:22,025 : INFO : built Dictionary(297 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1342 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 00:07:22,523 : INFO : token count processed\n",
      "2021-01-15 00:07:22,566 : INFO : frequencies processed\n",
      "2021-01-15 00:07:31,990 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:07:31,991 : INFO : entropies processed\n",
      "2021-01-15 00:07:31,992 : INFO : extropies processed\n",
      "2021-01-15 00:07:31,999 : INFO : token count processed\n",
      "2021-01-15 00:07:32,004 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:07:32,010 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:07:32,011 : INFO : vocab #32006\n",
      "2021-01-15 00:07:32,018 : INFO : diff #set()\n",
      "2021-01-15 00:07:50,916 : INFO : alphabet #32006\n",
      "2021-01-15 00:08:00,626 : INFO : Computed distances or similarities ('269', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.1323195641189614, 0.4689728579276921], [0.733307421207428, 0.26669258], [3.43162356584743, 1.3655629065374053], [5.90184948466379, 6.778844940588858, 7.178882062874448, 5.5018123623782005, 1.2770325782106573, 0.40003712228558985]]\n",
      "2021-01-15 00:08:00,630 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:08:00,631 : INFO : built Dictionary(194 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 375 corpus positions)\n",
      "2021-01-15 00:08:00,866 : INFO : token count processed\n",
      "2021-01-15 00:08:00,905 : INFO : frequencies processed\n",
      "2021-01-15 00:08:10,327 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:08:10,328 : INFO : entropies processed\n",
      "2021-01-15 00:08:10,329 : INFO : extropies processed\n",
      "2021-01-15 00:08:10,336 : INFO : token count processed\n",
      "2021-01-15 00:08:10,340 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:08:10,344 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:08:10,345 : INFO : vocab #32006\n",
      "2021-01-15 00:08:10,353 : INFO : diff #set()\n",
      "2021-01-15 00:08:29,257 : INFO : alphabet #32006\n",
      "2021-01-15 00:08:38,813 : INFO : Computed distances or similarities ('269', 'sacp-python-common/setup.py')[[1.0779859159188105, 0.4812352154744206], [0.687590628862381, 0.31240937], [3.5883543471732335, 1.3701080897057147], [5.90184948466379, 6.469677430851302, 7.078332650503398, 5.293194265011694, 1.176483165839608, 0.6086552196520962]]\n",
      "2021-01-15 00:08:38,817 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:08:38,819 : INFO : built Dictionary(255 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1277 corpus positions)\n",
      "2021-01-15 00:08:39,184 : INFO : token count processed\n",
      "2021-01-15 00:08:39,216 : INFO : frequencies processed\n",
      "2021-01-15 00:08:48,631 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:08:48,632 : INFO : entropies processed\n",
      "2021-01-15 00:08:48,633 : INFO : extropies processed\n",
      "2021-01-15 00:08:48,640 : INFO : token count processed\n",
      "2021-01-15 00:08:48,644 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:08:48,649 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:08:48,650 : INFO : vocab #32006\n",
      "2021-01-15 00:08:48,657 : INFO : diff #set()\n",
      "2021-01-15 00:09:07,833 : INFO : alphabet #32006\n",
      "2021-01-15 00:09:17,404 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.012525410647295, 0.49688813602525733], [0.5855062007904053, 0.4144938], [4.052389108859964, 1.3750172261534908], [5.90184948466379, 6.459180448028249, 6.7833921481215125, 5.577637784570527, 0.8815426634577221, 0.3242117000932634]]\n",
      "2021-01-15 00:09:17,407 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:09:17,408 : INFO : built Dictionary(175 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 427 corpus positions)\n",
      "2021-01-15 00:09:17,611 : INFO : token count processed\n",
      "2021-01-15 00:09:17,643 : INFO : frequencies processed\n",
      "2021-01-15 00:09:27,060 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:09:27,061 : INFO : entropies processed\n",
      "2021-01-15 00:09:27,062 : INFO : extropies processed\n",
      "2021-01-15 00:09:27,069 : INFO : token count processed\n",
      "2021-01-15 00:09:27,075 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:09:27,080 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:09:27,081 : INFO : vocab #32006\n",
      "2021-01-15 00:09:27,088 : INFO : diff #set()\n",
      "2021-01-15 00:09:46,092 : INFO : alphabet #32006\n",
      "2021-01-15 00:09:55,529 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.0934196966409908, 0.47768729873161886], [0.6645677089691162, 0.3354323], [3.321323501517743, 1.3342813686154507], [5.90184948466379, 6.097125733496388, 6.726260446038672, 5.272714772121508, 0.8244109613748813, 0.6291347125422835]]\n",
      "2021-01-15 00:09:55,532 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:09:55,534 : INFO : built Dictionary(166 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 423 corpus positions)\n",
      "2021-01-15 00:09:55,726 : INFO : token count processed\n",
      "2021-01-15 00:09:55,769 : INFO : frequencies processed\n",
      "2021-01-15 00:10:05,416 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:10:05,417 : INFO : entropies processed\n",
      "2021-01-15 00:10:05,417 : INFO : extropies processed\n",
      "2021-01-15 00:10:05,424 : INFO : token count processed\n",
      "2021-01-15 00:10:05,428 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:10:05,432 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:10:05,434 : INFO : vocab #32006\n",
      "2021-01-15 00:10:05,440 : INFO : diff #set()\n",
      "2021-01-15 00:10:24,859 : INFO : alphabet #32006\n",
      "2021-01-15 00:10:34,513 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.0755885350584795, 0.48179105979298786], [0.6572665274143219, 0.34273347], [3.4577968115985946, 1.3468874626322176], [5.90184948466379, 6.0695858597523715, 6.6852652909753765, 5.2861700534407845, 0.7834158063115861, 0.615679431223005]]\n",
      "2021-01-15 00:10:34,516 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 00:10:34,517 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:10:34,519 : INFO : built Dictionary(168 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 519 corpus positions)\n",
      "2021-01-15 00:10:34,712 : INFO : token count processed\n",
      "2021-01-15 00:10:34,745 : INFO : frequencies processed\n",
      "2021-01-15 00:10:44,316 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:10:44,317 : INFO : entropies processed\n",
      "2021-01-15 00:10:44,320 : INFO : extropies processed\n",
      "2021-01-15 00:10:44,328 : INFO : token count processed\n",
      "2021-01-15 00:10:44,333 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:10:44,338 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:10:44,340 : INFO : vocab #32006\n",
      "2021-01-15 00:10:44,347 : INFO : diff #set()\n",
      "2021-01-15 00:11:03,706 : INFO : alphabet #32006\n",
      "2021-01-15 00:11:13,198 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.0368162210152978, 0.4909623115145495], [0.5846920907497406, 0.4153079], [3.248320381093429, 1.3144127377751067], [5.90184948466379, 6.104787343210121, 6.627195228395724, 5.379441599478188, 0.7253457437319337, 0.5224078851856033]]\n",
      "2021-01-15 00:11:13,215 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 00:11:13,216 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:11:13,219 : INFO : built Dictionary(440 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 9204 corpus positions)\n",
      "2021-01-15 00:11:14,189 : INFO : token count processed\n",
      "2021-01-15 00:11:14,254 : INFO : frequencies processed\n",
      "2021-01-15 00:11:23,707 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:11:23,708 : INFO : entropies processed\n",
      "2021-01-15 00:11:23,710 : INFO : extropies processed\n",
      "2021-01-15 00:11:23,720 : INFO : token count processed\n",
      "2021-01-15 00:11:23,725 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:11:23,730 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:11:23,730 : INFO : vocab #32006\n",
      "2021-01-15 00:11:23,738 : INFO : diff #set()\n",
      "2021-01-15 00:11:42,831 : INFO : alphabet #32006\n",
      "2021-01-15 00:11:52,270 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.0496975424583215, 0.48787685952954885], [0.619345098733902, 0.3806549], [4.443841116991255, 1.389876608876928], [5.90184948466379, 6.89087415148015, 7.081045067778534, 5.711678568365405, 1.179195583114744, 0.190170916298384]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 00:11:52,276 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:11:52,278 : INFO : built Dictionary(306 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 2364 corpus positions)\n",
      "2021-01-15 00:11:52,777 : INFO : token count processed\n",
      "2021-01-15 00:11:52,820 : INFO : frequencies processed\n",
      "2021-01-15 00:12:02,340 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:12:02,341 : INFO : entropies processed\n",
      "2021-01-15 00:12:02,342 : INFO : extropies processed\n",
      "2021-01-15 00:12:02,350 : INFO : token count processed\n",
      "2021-01-15 00:12:02,354 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:12:02,359 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:12:02,360 : INFO : vocab #32006\n",
      "2021-01-15 00:12:02,367 : INFO : diff #set()\n",
      "2021-01-15 00:12:21,340 : INFO : alphabet #32006\n",
      "2021-01-15 00:12:30,887 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.0529103534821862, 0.48711333074227064], [0.5720716416835785, 0.42792836], [3.9894075153087605, 1.3723403998254118], [5.90184948466379, 6.655493573668506, 6.9056568725750145, 5.651686185757282, 1.0038073879112241, 0.2501632989065081]]\n",
      "2021-01-15 00:12:30,891 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:12:30,894 : INFO : built Dictionary(287 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1371 corpus positions)\n",
      "2021-01-15 00:12:31,322 : INFO : token count processed\n",
      "2021-01-15 00:12:31,355 : INFO : frequencies processed\n",
      "2021-01-15 00:12:40,965 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:12:40,966 : INFO : entropies processed\n",
      "2021-01-15 00:12:40,967 : INFO : extropies processed\n",
      "2021-01-15 00:12:40,974 : INFO : token count processed\n",
      "2021-01-15 00:12:40,978 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:12:40,982 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:12:40,983 : INFO : vocab #32006\n",
      "2021-01-15 00:12:40,989 : INFO : diff #set()\n",
      "2021-01-15 00:12:59,992 : INFO : alphabet #32006\n",
      "2021-01-15 00:13:09,598 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.1080234972425078, 0.47437801395861745], [0.7032661437988281, 0.29673386], [3.9261085007312415, 1.3842564126270234], [5.90184948466379, 6.6236746347295465, 6.942048609949893, 5.583475509443444, 1.0401991252861027, 0.3183739752203465]]\n",
      "2021-01-15 00:13:09,603 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:13:09,604 : INFO : built Dictionary(289 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1310 corpus positions)\n",
      "2021-01-15 00:13:10,059 : INFO : token count processed\n",
      "2021-01-15 00:13:10,091 : INFO : frequencies processed\n",
      "2021-01-15 00:13:19,559 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:13:19,561 : INFO : entropies processed\n",
      "2021-01-15 00:13:19,567 : INFO : extropies processed\n",
      "2021-01-15 00:13:19,574 : INFO : token count processed\n",
      "2021-01-15 00:13:19,578 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:13:19,584 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:13:19,585 : INFO : vocab #32006\n",
      "2021-01-15 00:13:19,594 : INFO : diff #set()\n",
      "2021-01-15 00:13:39,095 : INFO : alphabet #32006\n",
      "2021-01-15 00:13:48,857 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.0829757043751975, 0.48008241185893075], [0.6143442988395691, 0.3856557], [3.4754441557452296, 1.3285584086669733], [5.90184948466379, 6.75472436518627, 7.058632449787412, 5.597941400062648, 1.1567829651236217, 0.30390808460114194]]\n",
      "2021-01-15 00:13:48,861 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:13:48,862 : INFO : built Dictionary(243 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1105 corpus positions)\n",
      "2021-01-15 00:13:49,205 : INFO : token count processed\n",
      "2021-01-15 00:13:49,237 : INFO : frequencies processed\n",
      "2021-01-15 00:13:58,717 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:13:58,719 : INFO : entropies processed\n",
      "2021-01-15 00:13:58,720 : INFO : extropies processed\n",
      "2021-01-15 00:13:58,729 : INFO : token count processed\n",
      "2021-01-15 00:13:58,733 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:13:58,739 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:13:58,740 : INFO : vocab #32006\n",
      "2021-01-15 00:13:58,747 : INFO : diff #set()\n",
      "2021-01-15 00:14:17,891 : INFO : alphabet #32006\n",
      "2021-01-15 00:14:27,379 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.1193299355314594, 0.47184724909254505], [0.6799906492233276, 0.32000935], [3.4043473322310227, 1.3293635124158163], [5.90184948466379, 6.597313085495733, 6.948700657797268, 5.550461912362255, 1.0468511731334775, 0.35138757230153495]]\n",
      "2021-01-15 00:14:27,383 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:14:27,385 : INFO : built Dictionary(263 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1071 corpus positions)\n",
      "2021-01-15 00:14:27,813 : INFO : token count processed\n",
      "2021-01-15 00:14:27,845 : INFO : frequencies processed\n",
      "2021-01-15 00:14:37,336 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:14:37,337 : INFO : entropies processed\n",
      "2021-01-15 00:14:37,338 : INFO : extropies processed\n",
      "2021-01-15 00:14:37,352 : INFO : token count processed\n",
      "2021-01-15 00:14:37,356 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:14:37,360 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:14:37,361 : INFO : vocab #32006\n",
      "2021-01-15 00:14:37,368 : INFO : diff #set()\n",
      "2021-01-15 00:14:56,676 : INFO : alphabet #32006\n",
      "2021-01-15 00:15:06,220 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.0939308845061222, 0.4775706817256586], [0.6401044428348541, 0.35989556], [3.801712212208019, 1.356059067342199], [5.90184948466379, 6.659481538516613, 6.993864876942912, 5.567466146237492, 1.092015392279122, 0.3343833384262993]]\n",
      "2021-01-15 00:15:06,224 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:15:06,226 : INFO : built Dictionary(291 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1233 corpus positions)\n",
      "2021-01-15 00:15:06,738 : INFO : token count processed\n",
      "2021-01-15 00:15:06,818 : INFO : frequencies processed\n",
      "2021-01-15 00:15:16,318 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:15:16,319 : INFO : entropies processed\n",
      "2021-01-15 00:15:16,320 : INFO : extropies processed\n",
      "2021-01-15 00:15:16,328 : INFO : token count processed\n",
      "2021-01-15 00:15:16,333 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:15:16,337 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:15:16,338 : INFO : vocab #32006\n",
      "2021-01-15 00:15:16,344 : INFO : diff #set()\n",
      "2021-01-15 00:15:35,805 : INFO : alphabet #32006\n",
      "2021-01-15 00:15:45,394 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.0791713837131216, 0.48096083268236123], [0.6252042949199677, 0.3747957], [3.5061791510257163, 1.3335195258790433], [5.90184948466379, 6.774682571479102, 7.063852894157533, 5.61267916198536, 1.1620034094937424, 0.289170322678431]]\n",
      "2021-01-15 00:15:45,409 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:15:45,413 : INFO : built Dictionary(456 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 7970 corpus positions)\n",
      "2021-01-15 00:15:46,400 : INFO : token count processed\n",
      "2021-01-15 00:15:46,432 : INFO : frequencies processed\n",
      "2021-01-15 00:15:56,068 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:15:56,069 : INFO : entropies processed\n",
      "2021-01-15 00:15:56,070 : INFO : extropies processed\n",
      "2021-01-15 00:15:56,083 : INFO : token count processed\n",
      "2021-01-15 00:15:56,090 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:15:56,094 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:15:56,095 : INFO : vocab #32006\n",
      "2021-01-15 00:15:56,105 : INFO : diff #set()\n",
      "2021-01-15 00:16:15,454 : INFO : alphabet #32006\n",
      "2021-01-15 00:16:25,117 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.0679619036770889, 0.4835679023979493], [0.6593715846538544, 0.34062842], [4.429637079768335, 1.3886020884268255], [5.90184948466379, 6.839453716525233, 7.026653936703949, 5.714649264485074, 1.1248044520401583, 0.18720022017871596]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 00:16:25,123 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:16:25,125 : INFO : built Dictionary(365 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 2412 corpus positions)\n",
      "2021-01-15 00:16:25,786 : INFO : token count processed\n",
      "2021-01-15 00:16:25,818 : INFO : frequencies processed\n",
      "2021-01-15 00:16:35,414 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:16:35,415 : INFO : entropies processed\n",
      "2021-01-15 00:16:35,416 : INFO : extropies processed\n",
      "2021-01-15 00:16:35,424 : INFO : token count processed\n",
      "2021-01-15 00:16:35,428 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:16:35,432 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:16:35,434 : INFO : vocab #32006\n",
      "2021-01-15 00:16:35,442 : INFO : diff #set()\n",
      "2021-01-15 00:16:54,415 : INFO : alphabet #32006\n",
      "2021-01-15 00:17:03,833 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.0621238887801638, 0.484936916467974], [0.5773239433765411, 0.42267606], [3.8492546773698617, 1.3601702361324248], [5.90184948466379, 6.86432793886027, 7.0874208483932915, 5.678756575130768, 1.1855713637295011, 0.22309290953302163]]\n",
      "2021-01-15 00:17:03,837 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:17:03,839 : INFO : built Dictionary(206 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 614 corpus positions)\n",
      "2021-01-15 00:17:04,108 : INFO : token count processed\n",
      "2021-01-15 00:17:04,155 : INFO : frequencies processed\n",
      "2021-01-15 00:17:14,118 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:17:14,119 : INFO : entropies processed\n",
      "2021-01-15 00:17:14,120 : INFO : extropies processed\n",
      "2021-01-15 00:17:14,127 : INFO : token count processed\n",
      "2021-01-15 00:17:14,132 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:17:14,136 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:17:14,137 : INFO : vocab #32006\n",
      "2021-01-15 00:17:14,148 : INFO : diff #set()\n",
      "2021-01-15 00:17:33,032 : INFO : alphabet #32006\n",
      "2021-01-15 00:17:42,403 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.1082141571020556, 0.47433511279261914], [0.6655980348587036, 0.33440197], [3.2201755214643453, 1.318259091895678], [5.90184948466379, 6.431978396403875, 6.917450728683522, 5.416377152384144, 1.0156012440197317, 0.48547233227964703]]\n",
      "2021-01-15 00:17:42,407 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:17:42,409 : INFO : built Dictionary(257 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 768 corpus positions)\n",
      "2021-01-15 00:17:42,797 : INFO : token count processed\n",
      "2021-01-15 00:17:42,834 : INFO : frequencies processed\n",
      "2021-01-15 00:17:52,299 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:17:52,300 : INFO : entropies processed\n",
      "2021-01-15 00:17:52,302 : INFO : extropies processed\n",
      "2021-01-15 00:17:52,316 : INFO : token count processed\n",
      "2021-01-15 00:17:52,320 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:17:52,325 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:17:52,327 : INFO : vocab #32006\n",
      "2021-01-15 00:17:52,335 : INFO : diff #set()\n",
      "2021-01-15 00:18:11,201 : INFO : alphabet #32006\n",
      "2021-01-15 00:18:20,741 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/test_auth_utility.py')[[1.0894632899325953, 0.4785918014535968], [0.6803428530693054, 0.31965715], [4.195816471537619, 1.3955728918158388], [5.90184948466379, 6.911818353685893, 7.263660320827448, 5.550007517522236, 1.3618108361636576, 0.3518419671415547]]\n",
      "2021-01-15 00:18:20,754 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:18:20,757 : INFO : built Dictionary(351 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 7298 corpus positions)\n",
      "2021-01-15 00:18:21,386 : INFO : token count processed\n",
      "2021-01-15 00:18:21,418 : INFO : frequencies processed\n",
      "2021-01-15 00:18:30,828 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:18:30,829 : INFO : entropies processed\n",
      "2021-01-15 00:18:30,830 : INFO : extropies processed\n",
      "2021-01-15 00:18:30,839 : INFO : token count processed\n",
      "2021-01-15 00:18:30,843 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:18:30,847 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:18:30,848 : INFO : vocab #32006\n",
      "2021-01-15 00:18:30,857 : INFO : diff #set()\n",
      "2021-01-15 00:18:50,316 : INFO : alphabet #32006\n",
      "2021-01-15 00:18:59,762 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.0882809370062323, 0.4788627728573742], [0.6812903583049774, 0.31870964], [3.6204648653114306, 1.3385189355193012], [5.90184948466379, 6.363791471162389, 6.460411149717984, 5.805229806108194, 0.558561665054194, 0.09661967855559528]]\n",
      "2021-01-15 00:18:59,767 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:18:59,769 : INFO : built Dictionary(249 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1368 corpus positions)\n",
      "2021-01-15 00:19:00,147 : INFO : token count processed\n",
      "2021-01-15 00:19:00,209 : INFO : frequencies processed\n",
      "2021-01-15 00:19:09,638 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:19:09,640 : INFO : entropies processed\n",
      "2021-01-15 00:19:09,641 : INFO : extropies processed\n",
      "2021-01-15 00:19:09,648 : INFO : token count processed\n",
      "2021-01-15 00:19:09,655 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:19:09,659 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:19:09,660 : INFO : vocab #32006\n",
      "2021-01-15 00:19:09,670 : INFO : diff #set()\n",
      "2021-01-15 00:19:28,678 : INFO : alphabet #32006\n",
      "2021-01-15 00:19:38,097 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.0816657948756123, 0.480384508628463], [0.7145346403121948, 0.28546536], [4.1761085007312415, 1.393181266678285], [5.90184948466379, 6.29000629755059, 6.708524935403669, 5.483330846810712, 0.8066754507398786, 0.4185186378530785]]\n",
      "2021-01-15 00:19:38,102 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:19:38,103 : INFO : built Dictionary(258 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1321 corpus positions)\n",
      "2021-01-15 00:19:38,496 : INFO : token count processed\n",
      "2021-01-15 00:19:38,532 : INFO : frequencies processed\n",
      "2021-01-15 00:19:47,955 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:19:47,956 : INFO : entropies processed\n",
      "2021-01-15 00:19:47,956 : INFO : extropies processed\n",
      "2021-01-15 00:19:47,964 : INFO : token count processed\n",
      "2021-01-15 00:19:47,969 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:19:47,974 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:19:47,975 : INFO : vocab #32006\n",
      "2021-01-15 00:19:47,982 : INFO : diff #set()\n",
      "2021-01-15 00:20:07,126 : INFO : alphabet #32006\n",
      "2021-01-15 00:20:16,546 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.0744492822675058, 0.48205565137120926], [0.6532588601112366, 0.34674114], [3.948043294019837, 1.3653713027465362], [5.90184948466379, 6.361621244785958, 6.747109208408867, 5.516361521040881, 0.8452597237450767, 0.38548796362290894]]\n",
      "2021-01-15 00:20:16,551 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:20:16,553 : INFO : built Dictionary(272 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1344 corpus positions)\n",
      "2021-01-15 00:20:16,960 : INFO : token count processed\n",
      "2021-01-15 00:20:16,992 : INFO : frequencies processed\n",
      "2021-01-15 00:20:26,413 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:20:26,414 : INFO : entropies processed\n",
      "2021-01-15 00:20:26,415 : INFO : extropies processed\n",
      "2021-01-15 00:20:26,422 : INFO : token count processed\n",
      "2021-01-15 00:20:26,426 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:20:26,430 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:20:26,431 : INFO : vocab #32006\n",
      "2021-01-15 00:20:26,438 : INFO : diff #set()\n",
      "2021-01-15 00:20:45,429 : INFO : alphabet #32006\n",
      "2021-01-15 00:20:54,849 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.0912494801159782, 0.4781830238372808], [0.6890458166599274, 0.31095418], [3.3717656687748434, 1.3090152184086015], [5.90184948466379, 6.620594433343389, 7.024977734254495, 5.497466183752685, 1.123128249590705, 0.40438330091110597]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 00:20:54,854 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:20:54,856 : INFO : built Dictionary(243 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1437 corpus positions)\n",
      "2021-01-15 00:20:55,204 : INFO : token count processed\n",
      "2021-01-15 00:20:55,236 : INFO : frequencies processed\n",
      "2021-01-15 00:21:04,645 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:21:04,646 : INFO : entropies processed\n",
      "2021-01-15 00:21:04,647 : INFO : extropies processed\n",
      "2021-01-15 00:21:04,654 : INFO : token count processed\n",
      "2021-01-15 00:21:04,658 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:21:04,663 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:21:04,664 : INFO : vocab #32006\n",
      "2021-01-15 00:21:04,670 : INFO : diff #set()\n",
      "2021-01-15 00:21:23,654 : INFO : alphabet #32006\n",
      "2021-01-15 00:21:33,248 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.0496760118104793, 0.48788198439064506], [0.5517778992652893, 0.4482221], [3.4670078222369303, 1.324572535956481], [5.90184948466379, 6.207411496248084, 6.53556822416223, 5.573692756749644, 0.6337187394984394, 0.32815672791414574]]\n",
      "2021-01-15 00:21:33,251 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:21:33,252 : INFO : built Dictionary(189 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 341 corpus positions)\n",
      "2021-01-15 00:21:33,479 : INFO : token count processed\n",
      "2021-01-15 00:21:33,510 : INFO : frequencies processed\n",
      "2021-01-15 00:21:43,043 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:21:43,044 : INFO : entropies processed\n",
      "2021-01-15 00:21:43,045 : INFO : extropies processed\n",
      "2021-01-15 00:21:43,052 : INFO : token count processed\n",
      "2021-01-15 00:21:43,056 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:21:43,061 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:21:43,062 : INFO : vocab #32006\n",
      "2021-01-15 00:21:43,069 : INFO : diff #set()\n",
      "2021-01-15 00:22:01,716 : INFO : alphabet #32006\n",
      "2021-01-15 00:22:11,097 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.1277254424427932, 0.4699854502148184], [0.7284253239631653, 0.27157468], [3.3927474104487847, 1.3672090515720436], [5.90184948466379, 6.5805228788529595, 7.16812781342703, 5.314244550089719, 1.2662783287632395, 0.5876049345740704]]\n",
      "2021-01-15 00:22:11,101 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 00:22:11,102 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:22:11,104 : INFO : built Dictionary(269 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1302 corpus positions)\n",
      "2021-01-15 00:22:11,512 : INFO : token count processed\n",
      "2021-01-15 00:22:11,546 : INFO : frequencies processed\n",
      "2021-01-15 00:22:21,080 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:22:21,081 : INFO : entropies processed\n",
      "2021-01-15 00:22:21,082 : INFO : extropies processed\n",
      "2021-01-15 00:22:21,090 : INFO : token count processed\n",
      "2021-01-15 00:22:21,097 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:22:21,102 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:22:21,103 : INFO : vocab #32006\n",
      "2021-01-15 00:22:21,110 : INFO : diff #set()\n",
      "2021-01-15 00:22:40,010 : INFO : alphabet #32006\n",
      "2021-01-15 00:22:49,454 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.1344788939318153, 0.4684984249986894], [0.7191822528839111, 0.28081775], [3.621175542919471, 1.3730961658387517], [5.90184948466379, 6.422089779976135, 6.754965917199811, 5.568973347440115, 0.8531164325360203, 0.3328761372236757]]\n",
      "2021-01-15 00:22:49,459 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:22:49,460 : INFO : built Dictionary(278 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1584 corpus positions)\n",
      "2021-01-15 00:22:49,885 : INFO : token count processed\n",
      "2021-01-15 00:22:49,945 : INFO : frequencies processed\n",
      "2021-01-15 00:22:59,659 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:22:59,660 : INFO : entropies processed\n",
      "2021-01-15 00:22:59,661 : INFO : extropies processed\n",
      "2021-01-15 00:22:59,668 : INFO : token count processed\n",
      "2021-01-15 00:22:59,675 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:22:59,679 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:22:59,680 : INFO : vocab #32006\n",
      "2021-01-15 00:22:59,687 : INFO : diff #set()\n",
      "2021-01-15 00:23:18,544 : INFO : alphabet #32006\n",
      "2021-01-15 00:23:28,082 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.1011192962337981, 0.4759368027281812], [0.7134509980678558, 0.286549], [3.4685915424833182, 1.3173031677508968], [5.90184948466379, 6.485445644653597, 6.906320110029783, 5.480975019287604, 1.0044706253659923, 0.420874465376186]]\n",
      "2021-01-15 00:23:28,087 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:23:28,089 : INFO : built Dictionary(253 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1470 corpus positions)\n",
      "2021-01-15 00:23:28,449 : INFO : token count processed\n",
      "2021-01-15 00:23:28,494 : INFO : frequencies processed\n",
      "2021-01-15 00:23:37,914 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:23:37,915 : INFO : entropies processed\n",
      "2021-01-15 00:23:37,916 : INFO : extropies processed\n",
      "2021-01-15 00:23:37,923 : INFO : token count processed\n",
      "2021-01-15 00:23:37,928 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:23:37,933 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:23:37,934 : INFO : vocab #32006\n",
      "2021-01-15 00:23:37,941 : INFO : diff #set()\n",
      "2021-01-15 00:23:56,990 : INFO : alphabet #32006\n",
      "2021-01-15 00:24:06,552 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.0529297788332423, 0.48710872155029966], [0.5575630962848663, 0.4424369], [3.4670078222369303, 1.3245725359564808], [5.90184948466379, 6.2276600107346916, 6.549339790099363, 5.580169705299119, 0.6474903054355723, 0.32167977936467107]]\n",
      "2021-01-15 00:24:06,557 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:24:06,559 : INFO : built Dictionary(242 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1668 corpus positions)\n",
      "2021-01-15 00:24:06,918 : INFO : token count processed\n",
      "2021-01-15 00:24:06,950 : INFO : frequencies processed\n",
      "2021-01-15 00:24:16,369 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:24:16,371 : INFO : entropies processed\n",
      "2021-01-15 00:24:16,371 : INFO : extropies processed\n",
      "2021-01-15 00:24:16,379 : INFO : token count processed\n",
      "2021-01-15 00:24:16,383 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:24:16,387 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:24:16,388 : INFO : vocab #32006\n",
      "2021-01-15 00:24:16,395 : INFO : diff #set()\n",
      "2021-01-15 00:24:35,433 : INFO : alphabet #32006\n",
      "2021-01-15 00:24:44,868 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.0687184611065934, 0.4833910552840925], [0.6474641561508179, 0.35253584], [3.6536824788901066, 1.3456684491707118], [5.90184948466379, 6.253918170574241, 6.602520277206945, 5.553247378031085, 0.7006707925431543, 0.3486021066327041]]\n",
      "2021-01-15 00:24:44,872 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:24:44,873 : INFO : built Dictionary(217 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 765 corpus positions)\n",
      "2021-01-15 00:24:45,152 : INFO : token count processed\n",
      "2021-01-15 00:24:45,184 : INFO : frequencies processed\n",
      "2021-01-15 00:24:54,783 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:24:54,784 : INFO : entropies processed\n",
      "2021-01-15 00:24:54,785 : INFO : extropies processed\n",
      "2021-01-15 00:24:54,797 : INFO : token count processed\n",
      "2021-01-15 00:24:54,802 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:24:54,807 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:24:54,808 : INFO : vocab #32006\n",
      "2021-01-15 00:24:54,816 : INFO : diff #set()\n",
      "2021-01-15 00:25:13,846 : INFO : alphabet #32006\n",
      "2021-01-15 00:25:23,279 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.0598916217238228, 0.48546243377753473], [0.6400313079357147, 0.3599687], [3.315208074671593, 1.3125949272079123], [5.90184948466379, 6.374522245625576, 6.8494923983306695, 5.4268793319586965, 0.9476429136668791, 0.4749701527050938]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 00:25:23,284 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:25:23,286 : INFO : built Dictionary(335 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 2047 corpus positions)\n",
      "2021-01-15 00:25:23,897 : INFO : token count processed\n",
      "2021-01-15 00:25:23,957 : INFO : frequencies processed\n",
      "2021-01-15 00:25:33,395 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:25:33,397 : INFO : entropies processed\n",
      "2021-01-15 00:25:33,397 : INFO : extropies processed\n",
      "2021-01-15 00:25:33,405 : INFO : token count processed\n",
      "2021-01-15 00:25:33,411 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:25:33,416 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:25:33,417 : INFO : vocab #32006\n",
      "2021-01-15 00:25:33,424 : INFO : diff #set()\n",
      "2021-01-15 00:25:52,440 : INFO : alphabet #32006\n",
      "2021-01-15 00:26:01,861 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.11846833055425, 0.4720391546936047], [0.747276097536087, 0.2527239], [3.6776077224542876, 1.339731459144436], [5.90184948466379, 6.731238669067808, 7.135703289521985, 5.4973848642096135, 1.233853804858195, 0.40446462045417775]]\n",
      "2021-01-15 00:26:01,866 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:26:01,868 : INFO : built Dictionary(260 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1439 corpus positions)\n",
      "2021-01-15 00:26:02,261 : INFO : token count processed\n",
      "2021-01-15 00:26:02,321 : INFO : frequencies processed\n",
      "2021-01-15 00:26:11,759 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:26:11,760 : INFO : entropies processed\n",
      "2021-01-15 00:26:11,761 : INFO : extropies processed\n",
      "2021-01-15 00:26:11,768 : INFO : token count processed\n",
      "2021-01-15 00:26:11,775 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:26:11,780 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:26:11,781 : INFO : vocab #32006\n",
      "2021-01-15 00:26:11,787 : INFO : diff #set()\n",
      "2021-01-15 00:26:30,724 : INFO : alphabet #32006\n",
      "2021-01-15 00:26:40,163 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.080743508824941, 0.4805974382516422], [0.6781644523143768, 0.32183555], [3.707736532944161, 1.3467516928868868], [5.90184948466379, 6.503741451859337, 6.86821222858668, 5.537378707936448, 0.9663627439228897, 0.36447077672734274]]\n",
      "2021-01-15 00:26:40,168 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:26:40,170 : INFO : built Dictionary(279 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 1761 corpus positions)\n",
      "2021-01-15 00:26:40,633 : INFO : token count processed\n",
      "2021-01-15 00:26:40,665 : INFO : frequencies processed\n",
      "2021-01-15 00:26:50,090 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:26:50,091 : INFO : entropies processed\n",
      "2021-01-15 00:26:50,092 : INFO : extropies processed\n",
      "2021-01-15 00:26:50,100 : INFO : token count processed\n",
      "2021-01-15 00:26:50,105 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:26:50,111 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:26:50,112 : INFO : vocab #32006\n",
      "2021-01-15 00:26:50,120 : INFO : diff #set()\n",
      "2021-01-15 00:27:09,135 : INFO : alphabet #32006\n",
      "2021-01-15 00:27:18,567 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.0751068415673963, 0.481902897705579], [0.629414290189743, 0.3705857], [3.948043294019837, 1.3653713027465357], [5.90184948466379, 6.334729224484471, 6.619643498281288, 5.6169352108669734, 0.7177940136174978, 0.2849142737968169]]\n",
      "2021-01-15 00:27:18,573 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:27:18,575 : INFO : built Dictionary(279 unique tokens: ['#', \"'\", ',', '-', '--']...) from 2 documents (total 2110 corpus positions)\n",
      "2021-01-15 00:27:19,021 : INFO : token count processed\n",
      "2021-01-15 00:27:19,058 : INFO : frequencies processed\n",
      "2021-01-15 00:27:28,588 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:27:28,589 : INFO : entropies processed\n",
      "2021-01-15 00:27:28,590 : INFO : extropies processed\n",
      "2021-01-15 00:27:28,604 : INFO : token count processed\n",
      "2021-01-15 00:27:28,609 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:27:28,613 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:27:28,614 : INFO : vocab #32006\n",
      "2021-01-15 00:27:28,620 : INFO : diff #set()\n",
      "2021-01-15 00:27:47,492 : INFO : alphabet #32006\n",
      "2021-01-15 00:27:57,028 : INFO : Computed distances or similarities ('269', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.0673080259365257, 0.48372085216811517], [0.6718891263008118, 0.32811087], [3.726206441755229, 1.3518369466131777], [5.90184948466379, 6.21319712067992, 6.555148999584999, 5.559897605758712, 0.6532995149212084, 0.3419518789050784]]\n",
      "2021-01-15 00:27:57,033 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 00:27:57,034 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:27:57,035 : INFO : built Dictionary(292 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1447 corpus positions)\n",
      "2021-01-15 00:27:57,356 : INFO : token count processed\n",
      "2021-01-15 00:27:57,390 : INFO : frequencies processed\n",
      "2021-01-15 00:28:06,799 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:28:06,800 : INFO : entropies processed\n",
      "2021-01-15 00:28:06,801 : INFO : extropies processed\n",
      "2021-01-15 00:28:06,816 : INFO : token count processed\n",
      "2021-01-15 00:28:06,820 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:28:06,824 : INFO : alphabet_target #32010\n",
      "2021-01-15 00:28:06,825 : INFO : vocab #32006\n",
      "2021-01-15 00:28:06,831 : INFO : diff #set()\n",
      "2021-01-15 00:28:25,724 : INFO : alphabet #32006\n",
      "2021-01-15 00:28:35,288 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.205946193370285, 0.4533202137955059], [0.8859269991517067, 0.114073], [3.251629167387823, 1.3589504783379556], [5.215936788508252, 6.905617163738059, 7.134811394148439, 4.9867425580978715, 1.9188746056401875, 0.22919423041038023]]\n",
      "2021-01-15 00:28:35,294 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:28:35,296 : INFO : built Dictionary(381 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 2337 corpus positions)\n",
      "2021-01-15 00:28:35,780 : INFO : token count processed\n",
      "2021-01-15 00:28:35,844 : INFO : frequencies processed\n",
      "2021-01-15 00:28:45,278 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:28:45,280 : INFO : entropies processed\n",
      "2021-01-15 00:28:45,280 : INFO : extropies processed\n",
      "2021-01-15 00:28:45,288 : INFO : token count processed\n",
      "2021-01-15 00:28:45,292 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:28:45,297 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:28:45,297 : INFO : vocab #32006\n",
      "2021-01-15 00:28:45,304 : INFO : diff #set()\n",
      "2021-01-15 00:29:04,202 : INFO : alphabet #32006\n",
      "2021-01-15 00:29:13,767 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.2024301914888633, 0.45404390289618696], [0.8833302929997444, 0.11666971], [3.456564762130954, 1.3654661895235272], [5.215936788508252, 7.1219284286457345, 7.397801497259302, 4.940063719894685, 2.18186470875105, 0.2758730686135671]]\n",
      "2021-01-15 00:29:13,773 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:29:13,775 : INFO : built Dictionary(299 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 2320 corpus positions)\n",
      "2021-01-15 00:29:14,092 : INFO : token count processed\n",
      "2021-01-15 00:29:14,153 : INFO : frequencies processed\n",
      "2021-01-15 00:29:23,579 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:29:23,580 : INFO : entropies processed\n",
      "2021-01-15 00:29:23,581 : INFO : extropies processed\n",
      "2021-01-15 00:29:23,588 : INFO : token count processed\n",
      "2021-01-15 00:29:23,593 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:29:23,597 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:29:23,598 : INFO : vocab #32006\n",
      "2021-01-15 00:29:23,605 : INFO : diff #set()\n",
      "2021-01-15 00:29:42,614 : INFO : alphabet #32006\n",
      "2021-01-15 00:29:52,060 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2192426176683342, 0.4506041800200549], [0.8876960575580597, 0.11230394], [3.625, 1.3785939957689282], [5.215936788508252, 6.41099024988467, 6.556581036138412, 5.070346002254509, 1.34064424763016, 0.14559078625374156]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 00:29:52,064 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:29:52,066 : INFO : built Dictionary(182 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 719 corpus positions)\n",
      "2021-01-15 00:29:52,219 : INFO : token count processed\n",
      "2021-01-15 00:29:52,283 : INFO : frequencies processed\n",
      "2021-01-15 00:30:01,698 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:30:01,699 : INFO : entropies processed\n",
      "2021-01-15 00:30:01,700 : INFO : extropies processed\n",
      "2021-01-15 00:30:01,707 : INFO : token count processed\n",
      "2021-01-15 00:30:01,714 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:30:01,719 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:30:01,720 : INFO : vocab #32006\n",
      "2021-01-15 00:30:01,727 : INFO : diff #set()\n",
      "2021-01-15 00:30:20,738 : INFO : alphabet #32006\n",
      "2021-01-15 00:30:30,219 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.192530236252204, 0.45609405218937704], [0.8814261257648468, 0.118573874], [2.94770277922009, 1.3393100707180505], [5.215936788508252, 6.077866832717642, 6.4108830420486616, 4.882920579177233, 1.1949462535404098, 0.33301620933101983]]\n",
      "2021-01-15 00:30:30,223 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:30:30,224 : INFO : built Dictionary(160 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 484 corpus positions)\n",
      "2021-01-15 00:30:30,345 : INFO : token count processed\n",
      "2021-01-15 00:30:30,377 : INFO : frequencies processed\n",
      "2021-01-15 00:30:39,800 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:30:39,801 : INFO : entropies processed\n",
      "2021-01-15 00:30:39,802 : INFO : extropies processed\n",
      "2021-01-15 00:30:39,809 : INFO : token count processed\n",
      "2021-01-15 00:30:39,814 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:30:39,819 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:30:39,820 : INFO : vocab #32006\n",
      "2021-01-15 00:30:39,828 : INFO : diff #set()\n",
      "2021-01-15 00:30:58,801 : INFO : alphabet #32006\n",
      "2021-01-15 00:31:08,230 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.2199362650316776, 0.45046338300425504], [0.8985203802585602, 0.10147962], [2.5216406363433186, 1.2998438251349493], [5.215936788508252, 5.977547459003844, 6.372502848898151, 4.820981398613945, 1.156566060389899, 0.39495538989430656]]\n",
      "2021-01-15 00:31:08,236 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:31:08,238 : INFO : built Dictionary(261 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 2195 corpus positions)\n",
      "2021-01-15 00:31:08,507 : INFO : token count processed\n",
      "2021-01-15 00:31:08,541 : INFO : frequencies processed\n",
      "2021-01-15 00:31:17,967 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:31:17,968 : INFO : entropies processed\n",
      "2021-01-15 00:31:17,968 : INFO : extropies processed\n",
      "2021-01-15 00:31:17,976 : INFO : token count processed\n",
      "2021-01-15 00:31:17,983 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:31:17,987 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:31:17,988 : INFO : vocab #32006\n",
      "2021-01-15 00:31:17,995 : INFO : diff #set()\n",
      "2021-01-15 00:31:37,001 : INFO : alphabet #32006\n",
      "2021-01-15 00:31:46,436 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.2117566844603138, 0.45212929931486023], [0.8763748109340668, 0.12362519], [3.1820058147602133, 1.3495612861500508], [5.215936788508252, 6.4614394051846435, 6.641921235396882, 5.035454958296013, 1.4259844468886307, 0.18048183021223885]]\n",
      "2021-01-15 00:31:46,440 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 00:31:46,441 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:31:46,443 : INFO : built Dictionary(227 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1419 corpus positions)\n",
      "2021-01-15 00:31:46,647 : INFO : token count processed\n",
      "2021-01-15 00:31:46,683 : INFO : frequencies processed\n",
      "2021-01-15 00:31:56,118 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:31:56,119 : INFO : entropies processed\n",
      "2021-01-15 00:31:56,120 : INFO : extropies processed\n",
      "2021-01-15 00:31:56,127 : INFO : token count processed\n",
      "2021-01-15 00:31:56,132 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:31:56,137 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:31:56,138 : INFO : vocab #32006\n",
      "2021-01-15 00:31:56,145 : INFO : diff #set()\n",
      "2021-01-15 00:32:15,165 : INFO : alphabet #32006\n",
      "2021-01-15 00:32:24,593 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.2154464846849728, 0.4513762832516334], [0.9018912091851234, 0.09810879], [2.584962500721156, 1.315172029168969], [5.215936788508252, 6.327195724598159, 6.564220744477666, 4.978911768628745, 1.3482839559694142, 0.23702501987950697]]\n",
      "2021-01-15 00:32:24,605 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 00:32:24,606 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:32:24,609 : INFO : built Dictionary(433 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 6311 corpus positions)\n",
      "2021-01-15 00:32:25,139 : INFO : token count processed\n",
      "2021-01-15 00:32:25,170 : INFO : frequencies processed\n",
      "2021-01-15 00:32:34,710 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:32:34,711 : INFO : entropies processed\n",
      "2021-01-15 00:32:34,712 : INFO : extropies processed\n",
      "2021-01-15 00:32:34,721 : INFO : token count processed\n",
      "2021-01-15 00:32:34,727 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:32:34,732 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:32:34,733 : INFO : vocab #32006\n",
      "2021-01-15 00:32:34,741 : INFO : diff #set()\n",
      "2021-01-15 00:32:53,814 : INFO : alphabet #32006\n",
      "2021-01-15 00:33:03,357 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.208090231178022, 0.4528800435236278], [0.8910030499100685, 0.10899695], [3.9321380397593746, 1.3913834784291286], [5.215936788508252, 6.9079058562486315, 7.044406283758628, 5.0794363609982565, 1.828469495250376, 0.13650042750999614]]\n",
      "2021-01-15 00:33:03,363 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:33:03,365 : INFO : built Dictionary(338 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 2705 corpus positions)\n",
      "2021-01-15 00:33:03,717 : INFO : token count processed\n",
      "2021-01-15 00:33:03,763 : INFO : frequencies processed\n",
      "2021-01-15 00:33:13,195 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:33:13,196 : INFO : entropies processed\n",
      "2021-01-15 00:33:13,197 : INFO : extropies processed\n",
      "2021-01-15 00:33:13,207 : INFO : token count processed\n",
      "2021-01-15 00:33:13,213 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:33:13,218 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:33:13,219 : INFO : vocab #32006\n",
      "2021-01-15 00:33:13,225 : INFO : diff #set()\n",
      "2021-01-15 00:33:32,325 : INFO : alphabet #32006\n",
      "2021-01-15 00:33:41,895 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.2097081991156937, 0.4525484407399092], [0.8960030153393745, 0.103996985], [3.625, 1.3785939957689282], [5.215936788508252, 6.61034830706307, 6.828699747433037, 4.997585348138285, 1.612762958924785, 0.21835144036996645]]\n",
      "2021-01-15 00:33:41,899 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:33:41,900 : INFO : built Dictionary(229 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 752 corpus positions)\n",
      "2021-01-15 00:33:42,113 : INFO : token count processed\n",
      "2021-01-15 00:33:42,181 : INFO : frequencies processed\n",
      "2021-01-15 00:33:51,615 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:33:51,616 : INFO : entropies processed\n",
      "2021-01-15 00:33:51,617 : INFO : extropies processed\n",
      "2021-01-15 00:33:51,630 : INFO : token count processed\n",
      "2021-01-15 00:33:51,635 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:33:51,640 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:33:51,641 : INFO : vocab #32006\n",
      "2021-01-15 00:33:51,648 : INFO : diff #set()\n",
      "2021-01-15 00:34:10,573 : INFO : alphabet #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 00:34:20,133 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.223035637048251, 0.44983534376794837], [0.8873077183961868, 0.11269228], [2.9219280948873623, 1.3359016564230495], [5.215936788508252, 6.616715366949855, 6.913913752751449, 4.918738402706658, 1.697976964243197, 0.2971983858015941]]\n",
      "2021-01-15 00:34:20,140 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:34:20,142 : INFO : built Dictionary(444 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 2786 corpus positions)\n",
      "2021-01-15 00:34:20,722 : INFO : token count processed\n",
      "2021-01-15 00:34:20,789 : INFO : frequencies processed\n",
      "2021-01-15 00:34:30,222 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:34:30,223 : INFO : entropies processed\n",
      "2021-01-15 00:34:30,224 : INFO : extropies processed\n",
      "2021-01-15 00:34:30,232 : INFO : token count processed\n",
      "2021-01-15 00:34:30,239 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:34:30,246 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:34:30,247 : INFO : vocab #32006\n",
      "2021-01-15 00:34:30,254 : INFO : diff #set()\n",
      "2021-01-15 00:34:49,208 : INFO : alphabet #32006\n",
      "2021-01-15 00:34:58,902 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.1966742869852078, 0.45523362563342734], [0.8747598528862, 0.12524015], [3.6644977792004623, 1.381962919072374], [5.215936788508252, 7.32185870753746, 7.571634596352076, 4.966160899693636, 2.355697807843824, 0.24977588881461532]]\n",
      "2021-01-15 00:34:58,905 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:34:58,907 : INFO : built Dictionary(89 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 151 corpus positions)\n",
      "2021-01-15 00:34:58,954 : INFO : token count processed\n",
      "2021-01-15 00:34:58,986 : INFO : frequencies processed\n",
      "2021-01-15 00:35:08,422 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:35:08,423 : INFO : entropies processed\n",
      "2021-01-15 00:35:08,424 : INFO : extropies processed\n",
      "2021-01-15 00:35:08,431 : INFO : token count processed\n",
      "2021-01-15 00:35:08,435 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:35:08,440 : INFO : alphabet_target #32008\n",
      "2021-01-15 00:35:08,441 : INFO : vocab #32006\n",
      "2021-01-15 00:35:08,447 : INFO : diff #set()\n",
      "2021-01-15 00:35:27,462 : INFO : alphabet #32006\n",
      "2021-01-15 00:35:36,904 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2416015188263956, 0.4461096192170481], [0.9306736662983894, 0.06932633], [0.0, 0.0], [5.215936788508252, 5.176618657501385, 6.1788814288357505, 4.213674017173887, 0.9629446403274988, 1.0022627713343653]]\n",
      "2021-01-15 00:35:36,907 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:35:36,909 : INFO : built Dictionary(178 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 544 corpus positions)\n",
      "2021-01-15 00:35:37,061 : INFO : token count processed\n",
      "2021-01-15 00:35:37,123 : INFO : frequencies processed\n",
      "2021-01-15 00:35:46,546 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:35:46,547 : INFO : entropies processed\n",
      "2021-01-15 00:35:46,548 : INFO : extropies processed\n",
      "2021-01-15 00:35:46,555 : INFO : token count processed\n",
      "2021-01-15 00:35:46,560 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:35:46,565 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:35:46,566 : INFO : vocab #32006\n",
      "2021-01-15 00:35:46,573 : INFO : diff #set()\n",
      "2021-01-15 00:36:05,632 : INFO : alphabet #32006\n",
      "2021-01-15 00:36:15,085 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.2034712934422842, 0.4538293750302462], [0.8926317393779755, 0.10736826], [3.0, 1.3485155455967714], [5.215936788508252, 6.468846789852156, 6.846640968653386, 4.838142609707022, 1.630704180145134, 0.37779417880122956]]\n",
      "2021-01-15 00:36:15,091 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:36:15,093 : INFO : built Dictionary(389 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 2593 corpus positions)\n",
      "2021-01-15 00:36:15,575 : INFO : token count processed\n",
      "2021-01-15 00:36:15,645 : INFO : frequencies processed\n",
      "2021-01-15 00:36:25,070 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:36:25,071 : INFO : entropies processed\n",
      "2021-01-15 00:36:25,072 : INFO : extropies processed\n",
      "2021-01-15 00:36:25,087 : INFO : token count processed\n",
      "2021-01-15 00:36:25,094 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:36:25,100 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:36:25,100 : INFO : vocab #32006\n",
      "2021-01-15 00:36:25,107 : INFO : diff #set()\n",
      "2021-01-15 00:36:44,180 : INFO : alphabet #32006\n",
      "2021-01-15 00:36:53,649 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.1732550966018906, 0.4601392637079759], [0.8490538746118546, 0.15094613], [3.75, 1.3846096858033596], [5.215936788508252, 6.957796704012729, 7.151557143761236, 5.022176348759745, 1.9356203552529845, 0.1937604397485071]]\n",
      "2021-01-15 00:36:53,656 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:36:53,658 : INFO : built Dictionary(304 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 3090 corpus positions)\n",
      "2021-01-15 00:36:53,980 : INFO : token count processed\n",
      "2021-01-15 00:36:54,012 : INFO : frequencies processed\n",
      "2021-01-15 00:37:03,601 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:37:03,602 : INFO : entropies processed\n",
      "2021-01-15 00:37:03,603 : INFO : extropies processed\n",
      "2021-01-15 00:37:03,611 : INFO : token count processed\n",
      "2021-01-15 00:37:03,616 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:37:03,620 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:37:03,621 : INFO : vocab #32006\n",
      "2021-01-15 00:37:03,628 : INFO : diff #set()\n",
      "2021-01-15 00:37:22,859 : INFO : alphabet #32006\n",
      "2021-01-15 00:37:32,341 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.1997536696492408, 0.45459635494525796], [0.8721152245998383, 0.12788478], [2.94770277922009, 1.3393100707180505], [5.215936788508252, 6.441859572014148, 6.652819789289968, 5.004976571232433, 1.436883000781716, 0.2109602172758196]]\n",
      "2021-01-15 00:37:32,346 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:37:32,348 : INFO : built Dictionary(326 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1566 corpus positions)\n",
      "2021-01-15 00:37:32,690 : INFO : token count processed\n",
      "2021-01-15 00:37:32,737 : INFO : frequencies processed\n",
      "2021-01-15 00:37:42,491 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:37:42,492 : INFO : entropies processed\n",
      "2021-01-15 00:37:42,493 : INFO : extropies processed\n",
      "2021-01-15 00:37:42,500 : INFO : token count processed\n",
      "2021-01-15 00:37:42,505 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:37:42,509 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:37:42,510 : INFO : vocab #32006\n",
      "2021-01-15 00:37:42,520 : INFO : diff #set()\n",
      "2021-01-15 00:38:01,398 : INFO : alphabet #32006\n",
      "2021-01-15 00:38:10,827 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.210686682771729, 0.452348135895139], [0.8815844729542732, 0.11841553], [3.5465935642949384, 1.3764678303056375], [5.215936788508252, 6.998955278238291, 7.2464097789069655, 4.968482287839578, 2.0304729903987138, 0.2474545006686748]]\n",
      "2021-01-15 00:38:10,832 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:38:10,833 : INFO : built Dictionary(241 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1385 corpus positions)\n",
      "2021-01-15 00:38:11,058 : INFO : token count processed\n",
      "2021-01-15 00:38:11,090 : INFO : frequencies processed\n",
      "2021-01-15 00:38:20,637 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:38:20,638 : INFO : entropies processed\n",
      "2021-01-15 00:38:20,639 : INFO : extropies processed\n",
      "2021-01-15 00:38:20,646 : INFO : token count processed\n",
      "2021-01-15 00:38:20,651 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:38:20,655 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:38:20,656 : INFO : vocab #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 00:38:20,663 : INFO : diff #set()\n",
      "2021-01-15 00:38:39,564 : INFO : alphabet #32006\n",
      "2021-01-15 00:38:49,095 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.1977023013788428, 0.4550206819971013], [0.8847865760326385, 0.115213424], [3.5465935642949384, 1.3764678303056375], [5.215936788508252, 6.492983191376071, 6.711026759708179, 4.9978932201761435, 1.4950899711999277, 0.21804356833210825]]\n",
      "2021-01-15 00:38:49,102 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:38:49,106 : INFO : built Dictionary(454 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 3324 corpus positions)\n",
      "2021-01-15 00:38:49,685 : INFO : token count processed\n",
      "2021-01-15 00:38:49,711 : INFO : frequencies processed\n",
      "2021-01-15 00:38:58,930 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:38:58,931 : INFO : entropies processed\n",
      "2021-01-15 00:38:58,932 : INFO : extropies processed\n",
      "2021-01-15 00:38:58,940 : INFO : token count processed\n",
      "2021-01-15 00:38:58,944 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:38:58,948 : INFO : alphabet_target #32008\n",
      "2021-01-15 00:38:58,949 : INFO : vocab #32006\n",
      "2021-01-15 00:38:58,957 : INFO : diff #set()\n",
      "2021-01-15 00:39:18,260 : INFO : alphabet #32006\n",
      "2021-01-15 00:39:27,700 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.222228962774006, 0.4499986350424041], [0.8757092580199242, 0.12429074], [2.94770277922009, 1.3393100707180505], [5.215936788508252, 6.560342487747443, 6.809089610691018, 4.967189665564677, 1.5931528221827662, 0.24874712294357515]]\n",
      "2021-01-15 00:39:27,708 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 00:39:27,709 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:39:27,711 : INFO : built Dictionary(464 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 3529 corpus positions)\n",
      "2021-01-15 00:39:28,312 : INFO : token count processed\n",
      "2021-01-15 00:39:28,375 : INFO : frequencies processed\n",
      "2021-01-15 00:39:37,964 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:39:37,965 : INFO : entropies processed\n",
      "2021-01-15 00:39:37,966 : INFO : extropies processed\n",
      "2021-01-15 00:39:37,979 : INFO : token count processed\n",
      "2021-01-15 00:39:37,984 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:39:37,989 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:39:37,990 : INFO : vocab #32006\n",
      "2021-01-15 00:39:37,996 : INFO : diff #set()\n",
      "2021-01-15 00:39:57,055 : INFO : alphabet #32006\n",
      "2021-01-15 00:40:06,502 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.2096035311897633, 0.45256987775610086], [0.8917417675256729, 0.10825823], [3.8365916681089787, 1.3877044860218604], [5.215936788508252, 7.046173750105238, 7.277069793335759, 4.985040745277731, 2.061133004827507, 0.23089604323052093]]\n",
      "2021-01-15 00:40:06,513 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 00:40:06,514 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:40:06,521 : INFO : built Dictionary(518 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 5638 corpus positions)\n",
      "2021-01-15 00:40:07,264 : INFO : token count processed\n",
      "2021-01-15 00:40:07,296 : INFO : frequencies processed\n",
      "2021-01-15 00:40:16,699 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:40:16,700 : INFO : entropies processed\n",
      "2021-01-15 00:40:16,701 : INFO : extropies processed\n",
      "2021-01-15 00:40:16,710 : INFO : token count processed\n",
      "2021-01-15 00:40:16,715 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:40:16,721 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:40:16,722 : INFO : vocab #32006\n",
      "2021-01-15 00:40:16,729 : INFO : diff #set()\n",
      "2021-01-15 00:40:35,711 : INFO : alphabet #32006\n",
      "2021-01-15 00:40:45,143 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.1914506501282567, 0.45631874025612856], [0.8824176043272018, 0.117582396], [3.852168723603281, 1.3887904911367783], [5.215936788508252, 7.009229588004272, 7.146881252029742, 5.0782851244827825, 1.9309444635214899, 0.13765166402546924]]\n",
      "2021-01-15 00:40:45,155 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:40:45,159 : INFO : built Dictionary(597 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 6579 corpus positions)\n",
      "2021-01-15 00:40:46,037 : INFO : token count processed\n",
      "2021-01-15 00:40:46,070 : INFO : frequencies processed\n",
      "2021-01-15 00:40:55,474 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:40:55,475 : INFO : entropies processed\n",
      "2021-01-15 00:40:55,476 : INFO : extropies processed\n",
      "2021-01-15 00:40:55,485 : INFO : token count processed\n",
      "2021-01-15 00:40:55,489 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:40:55,494 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:40:55,495 : INFO : vocab #32006\n",
      "2021-01-15 00:40:55,503 : INFO : diff #set()\n",
      "2021-01-15 00:41:14,533 : INFO : alphabet #32006\n",
      "2021-01-15 00:41:23,990 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.197990941953959, 0.4549609285973785], [0.8605728894472122, 0.13942711], [3.638147696204827, 1.3665197591180607], [5.215936788508252, 7.376088004590871, 7.562022342894728, 5.030002450204394, 2.3460855543864767, 0.18593433830385742]]\n",
      "2021-01-15 00:41:23,993 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:41:23,995 : INFO : built Dictionary(158 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 365 corpus positions)\n",
      "2021-01-15 00:41:24,118 : INFO : token count processed\n",
      "2021-01-15 00:41:24,154 : INFO : frequencies processed\n",
      "2021-01-15 00:41:33,673 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:41:33,674 : INFO : entropies processed\n",
      "2021-01-15 00:41:33,675 : INFO : extropies processed\n",
      "2021-01-15 00:41:33,689 : INFO : token count processed\n",
      "2021-01-15 00:41:33,693 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:41:33,699 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:41:33,700 : INFO : vocab #32006\n",
      "2021-01-15 00:41:33,708 : INFO : diff #set()\n",
      "2021-01-15 00:41:52,565 : INFO : alphabet #32006\n",
      "2021-01-15 00:42:01,979 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.2257937387183016, 0.4492779284103112], [0.9091053903102875, 0.09089461], [2.0, 1.2451124978365313], [5.215936788508252, 6.2993628166120885, 6.755294792658343, 4.760004812461998, 1.539358004150091, 0.45593197604625413]]\n",
      "2021-01-15 00:42:01,982 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:42:01,983 : INFO : built Dictionary(57 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 81 corpus positions)\n",
      "2021-01-15 00:42:02,005 : INFO : token count processed\n",
      "2021-01-15 00:42:02,037 : INFO : frequencies processed\n",
      "2021-01-15 00:42:11,566 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:42:11,567 : INFO : entropies processed\n",
      "2021-01-15 00:42:11,568 : INFO : extropies processed\n",
      "2021-01-15 00:42:11,575 : INFO : token count processed\n",
      "2021-01-15 00:42:11,579 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:42:11,585 : INFO : alphabet_target #32008\n",
      "2021-01-15 00:42:11,586 : INFO : vocab #32006\n",
      "2021-01-15 00:42:11,594 : INFO : diff #set()\n",
      "2021-01-15 00:42:30,623 : INFO : alphabet #32006\n",
      "2021-01-15 00:42:40,035 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2423061596315292, 0.4459694300462193], [0.9192395135760307, 0.08076049], [1.0, 1.0], [5.215936788508252, 3.8936606896881862, 5.596511090190685, 3.5130863880057532, 0.380574301682433, 1.7028504005024985]]\n",
      "2021-01-15 00:42:40,058 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:42:40,063 : INFO : built Dictionary(750 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 12523 corpus positions)\n",
      "2021-01-15 00:42:41,560 : INFO : token count processed\n",
      "2021-01-15 00:42:41,600 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 00:42:51,118 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:42:51,119 : INFO : entropies processed\n",
      "2021-01-15 00:42:51,120 : INFO : extropies processed\n",
      "2021-01-15 00:42:51,131 : INFO : token count processed\n",
      "2021-01-15 00:42:51,136 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:42:51,142 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:42:51,143 : INFO : vocab #32006\n",
      "2021-01-15 00:42:51,152 : INFO : diff #set()\n",
      "2021-01-15 00:43:10,006 : INFO : alphabet #32006\n",
      "2021-01-15 00:43:19,811 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.1579336274892424, 0.4634062824089269], [0.85208959877491, 0.1479104], [4.293660689688185, 1.4013179594871903], [5.215936788508252, 7.434393313070278, 7.651110258896991, 4.999219842681538, 2.4351734703887393, 0.21671694582671286]]\n",
      "2021-01-15 00:43:19,820 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:43:19,823 : INFO : built Dictionary(503 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 4150 corpus positions)\n",
      "2021-01-15 00:43:20,475 : INFO : token count processed\n",
      "2021-01-15 00:43:20,507 : INFO : frequencies processed\n",
      "2021-01-15 00:43:30,031 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:43:30,033 : INFO : entropies processed\n",
      "2021-01-15 00:43:30,033 : INFO : extropies processed\n",
      "2021-01-15 00:43:30,041 : INFO : token count processed\n",
      "2021-01-15 00:43:30,046 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:43:30,050 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:43:30,051 : INFO : vocab #32006\n",
      "2021-01-15 00:43:30,059 : INFO : diff #set()\n",
      "2021-01-15 00:43:48,970 : INFO : alphabet #32006\n",
      "2021-01-15 00:43:58,413 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.1830374621016293, 0.45807734285846435], [0.8695866763591766, 0.13041332], [4.20184123230257, 1.4009137160862843], [5.215936788508252, 7.2991514951718255, 7.519329636977472, 4.995758646702606, 2.30339284846922, 0.2201781418056461]]\n",
      "2021-01-15 00:43:58,421 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:43:58,424 : INFO : built Dictionary(472 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 3567 corpus positions)\n",
      "2021-01-15 00:43:59,021 : INFO : token count processed\n",
      "2021-01-15 00:43:59,092 : INFO : frequencies processed\n",
      "2021-01-15 00:44:08,518 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:44:08,519 : INFO : entropies processed\n",
      "2021-01-15 00:44:08,520 : INFO : extropies processed\n",
      "2021-01-15 00:44:08,535 : INFO : token count processed\n",
      "2021-01-15 00:44:08,540 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:44:08,545 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:44:08,546 : INFO : vocab #32006\n",
      "2021-01-15 00:44:08,553 : INFO : diff #set()\n",
      "2021-01-15 00:44:27,559 : INFO : alphabet #32006\n",
      "2021-01-15 00:44:36,967 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.210774906595007, 0.4523300843595067], [0.8891283869743347, 0.11087161], [3.94770277922009, 1.392421898435754], [5.215936788508252, 7.170319527000998, 7.395649400166968, 4.9906069153422825, 2.179712611658716, 0.22532987316597008]]\n",
      "2021-01-15 00:44:36,971 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:44:36,972 : INFO : built Dictionary(190 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 603 corpus positions)\n",
      "2021-01-15 00:44:37,135 : INFO : token count processed\n",
      "2021-01-15 00:44:37,169 : INFO : frequencies processed\n",
      "2021-01-15 00:44:46,601 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:44:46,602 : INFO : entropies processed\n",
      "2021-01-15 00:44:46,603 : INFO : extropies processed\n",
      "2021-01-15 00:44:46,610 : INFO : token count processed\n",
      "2021-01-15 00:44:46,615 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:44:46,619 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:44:46,620 : INFO : vocab #32006\n",
      "2021-01-15 00:44:46,627 : INFO : diff #set()\n",
      "2021-01-15 00:45:05,816 : INFO : alphabet #32006\n",
      "2021-01-15 00:45:15,408 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.2217131608374, 0.4501031085502882], [0.9078640937805176, 0.092135906], [2.584962500721156, 1.315172029168969], [5.215936788508252, 6.353654804387375, 6.743612041365143, 4.825979551530484, 1.5276752528568913, 0.38995723697776796]]\n",
      "2021-01-15 00:45:15,412 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:45:15,413 : INFO : built Dictionary(185 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 731 corpus positions)\n",
      "2021-01-15 00:45:15,562 : INFO : token count processed\n",
      "2021-01-15 00:45:15,595 : INFO : frequencies processed\n",
      "2021-01-15 00:45:25,016 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:45:25,017 : INFO : entropies processed\n",
      "2021-01-15 00:45:25,017 : INFO : extropies processed\n",
      "2021-01-15 00:45:25,025 : INFO : token count processed\n",
      "2021-01-15 00:45:25,029 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:45:25,035 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:45:25,036 : INFO : vocab #32006\n",
      "2021-01-15 00:45:25,043 : INFO : diff #set()\n",
      "2021-01-15 00:45:44,223 : INFO : alphabet #32006\n",
      "2021-01-15 00:45:53,656 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.2163764583705652, 0.45118688940378826], [0.9129607826471329, 0.08703922], [2.584962500721156, 1.315172029168969], [5.215936788508252, 6.245180322479091, 6.587417769110826, 4.873699341876516, 1.3714809806025743, 0.34223744663173505]]\n",
      "2021-01-15 00:45:53,662 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:45:53,664 : INFO : built Dictionary(404 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1803 corpus positions)\n",
      "2021-01-15 00:45:54,189 : INFO : token count processed\n",
      "2021-01-15 00:45:54,258 : INFO : frequencies processed\n",
      "2021-01-15 00:46:03,676 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:46:03,677 : INFO : entropies processed\n",
      "2021-01-15 00:46:03,678 : INFO : extropies processed\n",
      "2021-01-15 00:46:03,685 : INFO : token count processed\n",
      "2021-01-15 00:46:03,693 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:46:03,697 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:46:03,698 : INFO : vocab #32006\n",
      "2021-01-15 00:46:03,705 : INFO : diff #set()\n",
      "2021-01-15 00:46:22,725 : INFO : alphabet #32006\n",
      "2021-01-15 00:46:32,169 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.1885613524806649, 0.45692116369803004], [0.8775858283042908, 0.12241417], [3.94770277922009, 1.392421898435754], [5.215936788508252, 7.2691387000368, 7.5301492808732124, 4.954926207671839, 2.3142124923649607, 0.2610105808364125]]\n",
      "2021-01-15 00:46:32,175 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:46:32,176 : INFO : built Dictionary(333 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1640 corpus positions)\n",
      "2021-01-15 00:46:32,550 : INFO : token count processed\n",
      "2021-01-15 00:46:32,615 : INFO : frequencies processed\n",
      "2021-01-15 00:46:42,033 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:46:42,034 : INFO : entropies processed\n",
      "2021-01-15 00:46:42,035 : INFO : extropies processed\n",
      "2021-01-15 00:46:42,043 : INFO : token count processed\n",
      "2021-01-15 00:46:42,048 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:46:42,053 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:46:42,054 : INFO : vocab #32006\n",
      "2021-01-15 00:46:42,061 : INFO : diff #set()\n",
      "2021-01-15 00:47:01,047 : INFO : alphabet #32006\n",
      "2021-01-15 00:47:10,460 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.1636937917266312, 0.4621726067818489], [0.8580406755208969, 0.14195932], [3.640223928941852, 1.3797477693995936], [5.215936788508252, 7.08857858466988, 7.322255297502107, 4.982260075676024, 2.106318508993855, 0.23367671283222702]]\n",
      "2021-01-15 00:47:10,464 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:47:10,466 : INFO : built Dictionary(165 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 563 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 00:47:10,613 : INFO : token count processed\n",
      "2021-01-15 00:47:10,677 : INFO : frequencies processed\n",
      "2021-01-15 00:47:20,489 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:47:20,490 : INFO : entropies processed\n",
      "2021-01-15 00:47:20,491 : INFO : extropies processed\n",
      "2021-01-15 00:47:20,504 : INFO : token count processed\n",
      "2021-01-15 00:47:20,509 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:47:20,513 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:47:20,514 : INFO : vocab #32006\n",
      "2021-01-15 00:47:20,521 : INFO : diff #set()\n",
      "2021-01-15 00:47:39,422 : INFO : alphabet #32006\n",
      "2021-01-15 00:47:48,855 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.2323748653841031, 0.44795343985739583], [0.9185666963458061, 0.0814333], [2.321928094887362, 1.2877123795494492], [5.215936788508252, 6.0479231618016716, 6.466054147380751, 4.797805802929172, 1.2501173588724992, 0.41813098557907935]]\n",
      "2021-01-15 00:47:48,859 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:47:48,860 : INFO : built Dictionary(167 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 682 corpus positions)\n",
      "2021-01-15 00:47:48,989 : INFO : token count processed\n",
      "2021-01-15 00:47:49,021 : INFO : frequencies processed\n",
      "2021-01-15 00:47:58,551 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:47:58,552 : INFO : entropies processed\n",
      "2021-01-15 00:47:58,553 : INFO : extropies processed\n",
      "2021-01-15 00:47:58,563 : INFO : token count processed\n",
      "2021-01-15 00:47:58,568 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:47:58,572 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:47:58,573 : INFO : vocab #32006\n",
      "2021-01-15 00:47:58,579 : INFO : diff #set()\n",
      "2021-01-15 00:48:17,332 : INFO : alphabet #32006\n",
      "2021-01-15 00:48:26,882 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.2243855948385982, 0.4495623431119011], [0.9203359708189964, 0.07966403], [2.321928094887362, 1.2877123795494492], [5.215936788508252, 6.036583168403119, 6.404502506083764, 4.848017450827607, 1.1885657175755124, 0.3679193376806449]]\n",
      "2021-01-15 00:48:26,904 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 00:48:26,905 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:48:26,910 : INFO : built Dictionary(571 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 7040 corpus positions)\n",
      "2021-01-15 00:48:27,780 : INFO : token count processed\n",
      "2021-01-15 00:48:27,812 : INFO : frequencies processed\n",
      "2021-01-15 00:48:37,223 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:48:37,224 : INFO : entropies processed\n",
      "2021-01-15 00:48:37,225 : INFO : extropies processed\n",
      "2021-01-15 00:48:37,240 : INFO : token count processed\n",
      "2021-01-15 00:48:37,245 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:48:37,249 : INFO : alphabet_target #32010\n",
      "2021-01-15 00:48:37,250 : INFO : vocab #32006\n",
      "2021-01-15 00:48:37,257 : INFO : diff #set()\n",
      "2021-01-15 00:48:56,124 : INFO : alphabet #32006\n",
      "2021-01-15 00:49:05,665 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.1851120798721186, 0.45764242905953084], [0.8584915995597839, 0.1415084], [3.94770277922009, 1.392421898435754], [5.215936788508252, 7.29352035514053, 7.474339692136722, 5.035117451512059, 2.25840290362847, 0.18081933699619146]]\n",
      "2021-01-15 00:49:05,672 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:49:05,674 : INFO : built Dictionary(382 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 3283 corpus positions)\n",
      "2021-01-15 00:49:06,103 : INFO : token count processed\n",
      "2021-01-15 00:49:06,165 : INFO : frequencies processed\n",
      "2021-01-15 00:49:15,574 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:49:15,575 : INFO : entropies processed\n",
      "2021-01-15 00:49:15,576 : INFO : extropies processed\n",
      "2021-01-15 00:49:15,584 : INFO : token count processed\n",
      "2021-01-15 00:49:15,588 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:49:15,592 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:49:15,593 : INFO : vocab #32006\n",
      "2021-01-15 00:49:15,600 : INFO : diff #set()\n",
      "2021-01-15 00:49:34,736 : INFO : alphabet #32006\n",
      "2021-01-15 00:49:44,272 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.2064354616723618, 0.453219691838189], [0.8927919641137123, 0.107208036], [3.7345216647797517, 1.383483006702452], [5.215936788508252, 6.8153433747477745, 7.005783538686577, 5.025496624569449, 1.7898467501783255, 0.1904401639388027]]\n",
      "2021-01-15 00:49:44,275 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:49:44,276 : INFO : built Dictionary(143 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 304 corpus positions)\n",
      "2021-01-15 00:49:44,378 : INFO : token count processed\n",
      "2021-01-15 00:49:44,410 : INFO : frequencies processed\n",
      "2021-01-15 00:49:53,818 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:49:53,819 : INFO : entropies processed\n",
      "2021-01-15 00:49:53,820 : INFO : extropies processed\n",
      "2021-01-15 00:49:53,827 : INFO : token count processed\n",
      "2021-01-15 00:49:53,832 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:49:53,837 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:49:53,838 : INFO : vocab #32006\n",
      "2021-01-15 00:49:53,845 : INFO : diff #set()\n",
      "2021-01-15 00:50:12,851 : INFO : alphabet #32006\n",
      "2021-01-15 00:50:22,301 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.2166282806426683, 0.45113563186610134], [0.9104397371411324, 0.08956026], [2.0, 1.2451124978365313], [5.215936788508252, 6.150121915859574, 6.638817741489495, 4.727240962878332, 1.4228809529812434, 0.48869582562992075]]\n",
      "2021-01-15 00:50:22,305 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:50:22,307 : INFO : built Dictionary(303 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1015 corpus positions)\n",
      "2021-01-15 00:50:22,619 : INFO : token count processed\n",
      "2021-01-15 00:50:22,688 : INFO : frequencies processed\n",
      "2021-01-15 00:50:32,109 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:50:32,110 : INFO : entropies processed\n",
      "2021-01-15 00:50:32,111 : INFO : extropies processed\n",
      "2021-01-15 00:50:32,119 : INFO : token count processed\n",
      "2021-01-15 00:50:32,126 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:50:32,130 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:50:32,131 : INFO : vocab #32006\n",
      "2021-01-15 00:50:32,138 : INFO : diff #set()\n",
      "2021-01-15 00:50:51,307 : INFO : alphabet #32006\n",
      "2021-01-15 00:51:00,746 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.1926463420460898, 0.45606990093388244], [0.8734717816114426, 0.12652822], [3.169925001442312, 1.3594000115384994], [5.215936788508252, 7.0391145208191315, 7.387878384075359, 4.867172925252024, 2.171941595567107, 0.3487638632562273]]\n",
      "2021-01-15 00:51:00,754 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 00:51:00,755 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:51:00,758 : INFO : built Dictionary(599 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 4390 corpus positions)\n",
      "2021-01-15 00:51:01,756 : INFO : token count processed\n",
      "2021-01-15 00:51:01,788 : INFO : frequencies processed\n",
      "2021-01-15 00:51:11,199 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:51:11,200 : INFO : entropies processed\n",
      "2021-01-15 00:51:11,201 : INFO : extropies processed\n",
      "2021-01-15 00:51:11,209 : INFO : token count processed\n",
      "2021-01-15 00:51:11,215 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:51:11,220 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:51:11,221 : INFO : vocab #32006\n",
      "2021-01-15 00:51:11,228 : INFO : diff #set()\n",
      "2021-01-15 00:51:30,208 : INFO : alphabet #32006\n",
      "2021-01-15 00:51:39,636 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.171077108900808, 0.4606008676063509], [0.8538022637367249, 0.14619774], [3.934560814286289, 1.3795921449622113], [5.215936788508252, 7.482466367279176, 7.777954274700859, 4.920448881086569, 2.562017486192607, 0.2954879074216832]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 00:51:39,640 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:51:39,642 : INFO : built Dictionary(190 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 612 corpus positions)\n",
      "2021-01-15 00:51:39,799 : INFO : token count processed\n",
      "2021-01-15 00:51:39,867 : INFO : frequencies processed\n",
      "2021-01-15 00:51:49,268 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:51:49,269 : INFO : entropies processed\n",
      "2021-01-15 00:51:49,269 : INFO : extropies processed\n",
      "2021-01-15 00:51:49,276 : INFO : token count processed\n",
      "2021-01-15 00:51:49,281 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:51:49,285 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:51:49,286 : INFO : vocab #32006\n",
      "2021-01-15 00:51:49,292 : INFO : diff #set()\n",
      "2021-01-15 00:52:08,287 : INFO : alphabet #32006\n",
      "2021-01-15 00:52:17,628 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.223786447962716, 0.4496834670955625], [0.9078728333115578, 0.09212717], [2.584962500721156, 1.315172029168969], [5.215936788508252, 6.372162341197667, 6.755867884047991, 4.832231245657928, 1.539931095539739, 0.38370554285032377]]\n",
      "2021-01-15 00:52:17,633 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:52:17,635 : INFO : built Dictionary(334 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 2033 corpus positions)\n",
      "2021-01-15 00:52:17,973 : INFO : token count processed\n",
      "2021-01-15 00:52:18,008 : INFO : frequencies processed\n",
      "2021-01-15 00:52:27,542 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:52:27,543 : INFO : entropies processed\n",
      "2021-01-15 00:52:27,544 : INFO : extropies processed\n",
      "2021-01-15 00:52:27,551 : INFO : token count processed\n",
      "2021-01-15 00:52:27,555 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:52:27,560 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:52:27,561 : INFO : vocab #32006\n",
      "2021-01-15 00:52:27,569 : INFO : diff #set()\n",
      "2021-01-15 00:52:46,496 : INFO : alphabet #32006\n",
      "2021-01-15 00:52:56,047 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.2222276512593477, 0.4499989006226679], [0.9004854932427406, 0.09951451], [3.4182958340544896, 1.369895090630202], [5.215936788508252, 6.798155919669889, 7.0608679127661205, 4.953224795412019, 1.8449311242578688, 0.2627119930962314]]\n",
      "2021-01-15 00:52:56,051 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:52:56,052 : INFO : built Dictionary(192 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 749 corpus positions)\n",
      "2021-01-15 00:52:56,204 : INFO : token count processed\n",
      "2021-01-15 00:52:56,236 : INFO : frequencies processed\n",
      "2021-01-15 00:53:05,659 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:53:05,661 : INFO : entropies processed\n",
      "2021-01-15 00:53:05,661 : INFO : extropies processed\n",
      "2021-01-15 00:53:05,668 : INFO : token count processed\n",
      "2021-01-15 00:53:05,675 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:53:05,679 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:53:05,680 : INFO : vocab #32006\n",
      "2021-01-15 00:53:05,687 : INFO : diff #set()\n",
      "2021-01-15 00:53:24,962 : INFO : alphabet #32006\n",
      "2021-01-15 00:53:34,495 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.2115789773564631, 0.45216562928054077], [0.9057287275791168, 0.09427127], [3.169925001442312, 1.3594000115384994], [5.215936788508252, 6.271631856729336, 6.622637485293764, 4.864931159943823, 1.4067006967855127, 0.35100562856442874]]\n",
      "2021-01-15 00:53:34,502 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:53:34,504 : INFO : built Dictionary(364 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 3264 corpus positions)\n",
      "2021-01-15 00:53:34,931 : INFO : token count processed\n",
      "2021-01-15 00:53:34,993 : INFO : frequencies processed\n",
      "2021-01-15 00:53:44,380 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:53:44,381 : INFO : entropies processed\n",
      "2021-01-15 00:53:44,382 : INFO : extropies processed\n",
      "2021-01-15 00:53:44,389 : INFO : token count processed\n",
      "2021-01-15 00:53:44,394 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:53:44,398 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:53:44,399 : INFO : vocab #32006\n",
      "2021-01-15 00:53:44,406 : INFO : diff #set()\n",
      "2021-01-15 00:54:03,259 : INFO : alphabet #32006\n",
      "2021-01-15 00:54:12,783 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.1751559376635095, 0.45973715386776887], [0.8759000897407532, 0.12409991], [3.852168723603281, 1.3887904911367785], [5.215936788508252, 6.873598627629562, 7.069361379019588, 5.020174037118226, 1.8534245905113362, 0.19576275139002597]]\n",
      "2021-01-15 00:54:12,786 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:54:12,787 : INFO : built Dictionary(124 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 212 corpus positions)\n",
      "2021-01-15 00:54:12,877 : INFO : token count processed\n",
      "2021-01-15 00:54:12,941 : INFO : frequencies processed\n",
      "2021-01-15 00:54:22,347 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:54:22,349 : INFO : entropies processed\n",
      "2021-01-15 00:54:22,349 : INFO : extropies processed\n",
      "2021-01-15 00:54:22,356 : INFO : token count processed\n",
      "2021-01-15 00:54:22,363 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:54:22,368 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:54:22,369 : INFO : vocab #32006\n",
      "2021-01-15 00:54:22,377 : INFO : diff #set()\n",
      "2021-01-15 00:54:41,257 : INFO : alphabet #32006\n",
      "2021-01-15 00:54:50,789 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.1951895969613275, 0.45554151740890236], [0.8924631178379059, 0.10753688], [2.0, 1.2451124978365313], [5.215936788508252, 6.049830202851529, 6.651558703564731, 4.6142082877950505, 1.4356219150564788, 0.6017285007132012]]\n",
      "2021-01-15 00:54:50,794 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:54:50,796 : INFO : built Dictionary(266 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1292 corpus positions)\n",
      "2021-01-15 00:54:51,061 : INFO : token count processed\n",
      "2021-01-15 00:54:51,093 : INFO : frequencies processed\n",
      "2021-01-15 00:55:00,502 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:55:00,503 : INFO : entropies processed\n",
      "2021-01-15 00:55:00,504 : INFO : extropies processed\n",
      "2021-01-15 00:55:00,518 : INFO : token count processed\n",
      "2021-01-15 00:55:00,522 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:55:00,527 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:55:00,528 : INFO : vocab #32006\n",
      "2021-01-15 00:55:00,536 : INFO : diff #set()\n",
      "2021-01-15 00:55:19,516 : INFO : alphabet #32006\n",
      "2021-01-15 00:55:29,093 : INFO : Computed distances or similarities ('268', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.1952154101572598, 0.4555361607671852], [0.8809455707669258, 0.11905443], [3.2776134368191157, 1.3618978811135465], [5.215936788508252, 6.778844940588858, 7.070181519852131, 4.9246002092449785, 1.8542447313438792, 0.29133657926327317]]\n",
      "2021-01-15 00:55:29,097 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:55:29,098 : INFO : built Dictionary(172 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 325 corpus positions)\n",
      "2021-01-15 00:55:29,242 : INFO : token count processed\n",
      "2021-01-15 00:55:29,278 : INFO : frequencies processed\n",
      "2021-01-15 00:55:38,691 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:55:38,692 : INFO : entropies processed\n",
      "2021-01-15 00:55:38,693 : INFO : extropies processed\n",
      "2021-01-15 00:55:38,700 : INFO : token count processed\n",
      "2021-01-15 00:55:38,707 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:55:38,711 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:55:38,712 : INFO : vocab #32006\n",
      "2021-01-15 00:55:38,719 : INFO : diff #set()\n",
      "2021-01-15 00:55:57,708 : INFO : alphabet #32006\n",
      "2021-01-15 00:56:07,157 : INFO : Computed distances or similarities ('268', 'sacp-python-common/setup.py')[[1.2108717086753038, 0.4523102792785628], [0.8897135630249977, 0.11028644], [1.584962500721156, 1.1699250014423124], [5.215936788508252, 6.469677430851302, 6.991792996375034, 4.6938212229845195, 1.7758562078667826, 0.5221155655237322]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 00:56:07,162 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:56:07,163 : INFO : built Dictionary(234 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1227 corpus positions)\n",
      "2021-01-15 00:56:07,377 : INFO : token count processed\n",
      "2021-01-15 00:56:07,426 : INFO : frequencies processed\n",
      "2021-01-15 00:56:16,858 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:56:16,859 : INFO : entropies processed\n",
      "2021-01-15 00:56:16,860 : INFO : extropies processed\n",
      "2021-01-15 00:56:16,867 : INFO : token count processed\n",
      "2021-01-15 00:56:16,871 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:56:16,874 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:56:16,875 : INFO : vocab #32006\n",
      "2021-01-15 00:56:16,881 : INFO : diff #set()\n",
      "2021-01-15 00:56:35,608 : INFO : alphabet #32006\n",
      "2021-01-15 00:56:45,061 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.2261588113617479, 0.44920425034200373], [0.8974698707461357, 0.10253013], [3.3927474104487847, 1.3672090515720436], [5.215936788508252, 6.459180448028249, 6.744536743886346, 4.930580492650154, 1.5285999553780947, 0.2853562958580973]]\n",
      "2021-01-15 00:56:45,064 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:56:45,066 : INFO : built Dictionary(149 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 377 corpus positions)\n",
      "2021-01-15 00:56:45,176 : INFO : token count processed\n",
      "2021-01-15 00:56:45,208 : INFO : frequencies processed\n",
      "2021-01-15 00:56:54,617 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:56:54,618 : INFO : entropies processed\n",
      "2021-01-15 00:56:54,619 : INFO : extropies processed\n",
      "2021-01-15 00:56:54,626 : INFO : token count processed\n",
      "2021-01-15 00:56:54,631 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:56:54,636 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:56:54,637 : INFO : vocab #32006\n",
      "2021-01-15 00:56:54,644 : INFO : diff #set()\n",
      "2021-01-15 00:57:13,628 : INFO : alphabet #32006\n",
      "2021-01-15 00:57:23,050 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.2219848085437421, 0.45004808140672486], [0.895738311111927, 0.10426169], [2.5216406363433186, 1.2998438251349493], [5.215936788508252, 6.097125733496388, 6.57484834510086, 4.738214176903779, 1.3589115565926084, 0.47772261160447194]]\n",
      "2021-01-15 00:57:23,053 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:57:23,055 : INFO : built Dictionary(142 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 373 corpus positions)\n",
      "2021-01-15 00:57:23,159 : INFO : token count processed\n",
      "2021-01-15 00:57:23,228 : INFO : frequencies processed\n",
      "2021-01-15 00:57:32,740 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:57:32,741 : INFO : entropies processed\n",
      "2021-01-15 00:57:32,742 : INFO : extropies processed\n",
      "2021-01-15 00:57:32,749 : INFO : token count processed\n",
      "2021-01-15 00:57:32,753 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:57:32,757 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:57:32,758 : INFO : vocab #32006\n",
      "2021-01-15 00:57:32,765 : INFO : diff #set()\n",
      "2021-01-15 00:57:51,623 : INFO : alphabet #32006\n",
      "2021-01-15 00:58:01,042 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.2268753917245465, 0.44905970209027973], [0.9012097343802452, 0.098790266], [2.2516291673878226, 1.2667563532600834], [5.215936788508252, 6.0695858597523715, 6.552182389203166, 4.733340259057458, 1.3362456006949142, 0.48259652945079434]]\n",
      "2021-01-15 00:58:01,046 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 00:58:01,047 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:58:01,048 : INFO : built Dictionary(144 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 469 corpus positions)\n",
      "2021-01-15 00:58:01,156 : INFO : token count processed\n",
      "2021-01-15 00:58:01,224 : INFO : frequencies processed\n",
      "2021-01-15 00:58:10,742 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:58:10,743 : INFO : entropies processed\n",
      "2021-01-15 00:58:10,744 : INFO : extropies processed\n",
      "2021-01-15 00:58:10,751 : INFO : token count processed\n",
      "2021-01-15 00:58:10,755 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:58:10,759 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:58:10,760 : INFO : vocab #32006\n",
      "2021-01-15 00:58:10,767 : INFO : diff #set()\n",
      "2021-01-15 00:58:29,962 : INFO : alphabet #32006\n",
      "2021-01-15 00:58:39,498 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.201363606121486, 0.45426389226170083], [0.8804213479161263, 0.11957865], [2.2516291673878226, 1.2667563532600834], [5.215936788508252, 6.104787343210121, 6.509094924320806, 4.811629207397566, 1.2931581358125541, 0.4043075811106851]]\n",
      "2021-01-15 00:58:39,515 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 00:58:39,516 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:58:39,519 : INFO : built Dictionary(424 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 9154 corpus positions)\n",
      "2021-01-15 00:58:40,041 : INFO : token count processed\n",
      "2021-01-15 00:58:40,107 : INFO : frequencies processed\n",
      "2021-01-15 00:58:49,510 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:58:49,512 : INFO : entropies processed\n",
      "2021-01-15 00:58:49,512 : INFO : extropies processed\n",
      "2021-01-15 00:58:49,522 : INFO : token count processed\n",
      "2021-01-15 00:58:49,527 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:58:49,532 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:58:49,533 : INFO : vocab #32006\n",
      "2021-01-15 00:58:49,540 : INFO : diff #set()\n",
      "2021-01-15 00:59:08,594 : INFO : alphabet #32006\n",
      "2021-01-15 00:59:18,149 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.2163726930912075, 0.45118765590153764], [0.8929858133196831, 0.10701419], [3.4138338639736006, 1.3468073701051007], [5.215936788508252, 6.89087415148015, 7.070891273854844, 5.035919666133559, 1.8549544853465925, 0.1800171223746938]]\n",
      "2021-01-15 00:59:18,155 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:59:18,158 : INFO : built Dictionary(282 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 2314 corpus positions)\n",
      "2021-01-15 00:59:18,439 : INFO : token count processed\n",
      "2021-01-15 00:59:18,485 : INFO : frequencies processed\n",
      "2021-01-15 00:59:28,084 : INFO : scalar_distribution processed\n",
      "2021-01-15 00:59:28,085 : INFO : entropies processed\n",
      "2021-01-15 00:59:28,086 : INFO : extropies processed\n",
      "2021-01-15 00:59:28,094 : INFO : token count processed\n",
      "2021-01-15 00:59:28,098 : INFO : alphabet_source #32006\n",
      "2021-01-15 00:59:28,104 : INFO : alphabet_target #32009\n",
      "2021-01-15 00:59:28,105 : INFO : vocab #32006\n",
      "2021-01-15 00:59:28,113 : INFO : diff #set()\n",
      "2021-01-15 00:59:47,324 : INFO : alphabet #32006\n",
      "2021-01-15 00:59:56,782 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.185536689550798, 0.4575535175323614], [0.8678800761699677, 0.13211992], [3.625, 1.3785939957689286], [5.215936788508252, 6.655493573668506, 6.858611773629838, 5.01281858854692, 1.6426749851215865, 0.20311819996133185]]\n",
      "2021-01-15 00:59:56,786 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 00:59:56,788 : INFO : built Dictionary(261 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1321 corpus positions)\n",
      "2021-01-15 00:59:57,032 : INFO : token count processed\n",
      "2021-01-15 00:59:57,064 : INFO : frequencies processed\n",
      "2021-01-15 01:00:06,493 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:00:06,494 : INFO : entropies processed\n",
      "2021-01-15 01:00:06,495 : INFO : extropies processed\n",
      "2021-01-15 01:00:06,508 : INFO : token count processed\n",
      "2021-01-15 01:00:06,512 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:00:06,516 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:00:06,517 : INFO : vocab #32006\n",
      "2021-01-15 01:00:06,525 : INFO : diff #set()\n",
      "2021-01-15 01:00:25,524 : INFO : alphabet #32006\n",
      "2021-01-15 01:00:34,942 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.2051281368139843, 0.4534883861419596], [0.8889240100979805, 0.11107599], [3.4182958340544896, 1.369895090630202], [5.215936788508252, 6.6236746347295465, 6.851333175890236, 4.988278247347562, 1.6353963873819843, 0.2276585411606895]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 01:00:34,946 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:00:34,948 : INFO : built Dictionary(259 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1260 corpus positions)\n",
      "2021-01-15 01:00:35,196 : INFO : token count processed\n",
      "2021-01-15 01:00:35,257 : INFO : frequencies processed\n",
      "2021-01-15 01:00:44,855 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:00:44,856 : INFO : entropies processed\n",
      "2021-01-15 01:00:44,857 : INFO : extropies processed\n",
      "2021-01-15 01:00:44,864 : INFO : token count processed\n",
      "2021-01-15 01:00:44,868 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:00:44,873 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:00:44,876 : INFO : vocab #32006\n",
      "2021-01-15 01:00:44,882 : INFO : diff #set()\n",
      "2021-01-15 01:01:03,774 : INFO : alphabet #32006\n",
      "2021-01-15 01:01:13,196 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.1369715610401783, 0.4679519457494542], [0.8334954231977463, 0.16650458], [3.773557262275185, 1.3866235995561977], [5.215936788508252, 6.75472436518627, 6.962564761144626, 5.008096392549896, 1.7466279726363743, 0.2078403959583559]]\n",
      "2021-01-15 01:01:13,200 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:01:13,202 : INFO : built Dictionary(216 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1055 corpus positions)\n",
      "2021-01-15 01:01:13,397 : INFO : token count processed\n",
      "2021-01-15 01:01:13,432 : INFO : frequencies processed\n",
      "2021-01-15 01:01:22,861 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:01:22,862 : INFO : entropies processed\n",
      "2021-01-15 01:01:22,863 : INFO : extropies processed\n",
      "2021-01-15 01:01:22,870 : INFO : token count processed\n",
      "2021-01-15 01:01:22,875 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:01:22,879 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:01:22,880 : INFO : vocab #32006\n",
      "2021-01-15 01:01:22,888 : INFO : diff #set()\n",
      "2021-01-15 01:01:41,910 : INFO : alphabet #32006\n",
      "2021-01-15 01:01:51,362 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.158636773384235, 0.4632553342599806], [0.8508890122175217, 0.14911099], [3.121928094887362, 1.3519647487142497], [5.215936788508252, 6.597313085495733, 6.832423083154019, 4.9808267908499655, 1.6164862946457674, 0.23510999765828622]]\n",
      "2021-01-15 01:01:51,366 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:01:51,368 : INFO : built Dictionary(240 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1021 corpus positions)\n",
      "2021-01-15 01:01:51,586 : INFO : token count processed\n",
      "2021-01-15 01:01:51,618 : INFO : frequencies processed\n",
      "2021-01-15 01:02:01,161 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:02:01,162 : INFO : entropies processed\n",
      "2021-01-15 01:02:01,163 : INFO : extropies processed\n",
      "2021-01-15 01:02:01,170 : INFO : token count processed\n",
      "2021-01-15 01:02:01,175 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:02:01,179 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:02:01,180 : INFO : vocab #32006\n",
      "2021-01-15 01:02:01,190 : INFO : diff #set()\n",
      "2021-01-15 01:02:20,086 : INFO : alphabet #32006\n",
      "2021-01-15 01:02:29,527 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.2033261927972898, 0.4538592620870286], [0.883534349501133, 0.11646565], [3.2776134368191157, 1.3618978811135465], [5.215936788508252, 6.659481538516613, 6.916658672593936, 4.958759654430928, 1.7007218840856844, 0.257177134077323]]\n",
      "2021-01-15 01:02:29,531 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:02:29,533 : INFO : built Dictionary(264 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1183 corpus positions)\n",
      "2021-01-15 01:02:29,790 : INFO : token count processed\n",
      "2021-01-15 01:02:29,822 : INFO : frequencies processed\n",
      "2021-01-15 01:02:39,349 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:02:39,350 : INFO : entropies processed\n",
      "2021-01-15 01:02:39,351 : INFO : extropies processed\n",
      "2021-01-15 01:02:39,358 : INFO : token count processed\n",
      "2021-01-15 01:02:39,362 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:02:39,367 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:02:39,368 : INFO : vocab #32006\n",
      "2021-01-15 01:02:39,377 : INFO : diff #set()\n",
      "2021-01-15 01:02:58,268 : INFO : alphabet #32006\n",
      "2021-01-15 01:03:07,684 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.2176640063655244, 0.45092493593692573], [0.8955840989947319, 0.1044159], [3.4182958340544896, 1.369895090630202], [5.215936788508252, 6.774682571479102, 6.991692897283892, 4.99892646270346, 1.7757561087756404, 0.2170103258047904]]\n",
      "2021-01-15 01:03:07,699 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:03:07,703 : INFO : built Dictionary(440 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 7920 corpus positions)\n",
      "2021-01-15 01:03:08,292 : INFO : token count processed\n",
      "2021-01-15 01:03:08,363 : INFO : frequencies processed\n",
      "2021-01-15 01:03:17,883 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:03:17,884 : INFO : entropies processed\n",
      "2021-01-15 01:03:17,885 : INFO : extropies processed\n",
      "2021-01-15 01:03:17,894 : INFO : token count processed\n",
      "2021-01-15 01:03:17,899 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:03:17,903 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:03:17,904 : INFO : vocab #32006\n",
      "2021-01-15 01:03:17,911 : INFO : diff #set()\n",
      "2021-01-15 01:03:36,779 : INFO : alphabet #32006\n",
      "2021-01-15 01:03:46,777 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.2028065522977311, 0.4539663271642748], [0.8847125321626663, 0.11528747], [3.4138338639736006, 1.3468073701051007], [5.215936788508252, 6.839453716525233, 7.012776855741192, 5.042613649292292, 1.7968400672329405, 0.17332313921595954]]\n",
      "2021-01-15 01:03:46,783 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:03:46,785 : INFO : built Dictionary(340 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 2362 corpus positions)\n",
      "2021-01-15 01:03:47,152 : INFO : token count processed\n",
      "2021-01-15 01:03:47,184 : INFO : frequencies processed\n",
      "2021-01-15 01:03:56,600 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:03:56,601 : INFO : entropies processed\n",
      "2021-01-15 01:03:56,602 : INFO : extropies processed\n",
      "2021-01-15 01:03:56,609 : INFO : token count processed\n",
      "2021-01-15 01:03:56,613 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:03:56,618 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:03:56,619 : INFO : vocab #32006\n",
      "2021-01-15 01:03:56,625 : INFO : diff #set()\n",
      "2021-01-15 01:04:15,803 : INFO : alphabet #32006\n",
      "2021-01-15 01:04:25,233 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.1727457754899824, 0.4602471265992852], [0.8540428578853607, 0.14595714], [3.5724694587701356, 1.3725136963533666], [5.215936788508252, 6.86432793886027, 7.035381630634889, 5.044883096733633, 1.8194448421266376, 0.1710536917746195]]\n",
      "2021-01-15 01:04:25,237 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:04:25,238 : INFO : built Dictionary(182 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 564 corpus positions)\n",
      "2021-01-15 01:04:25,381 : INFO : token count processed\n",
      "2021-01-15 01:04:25,415 : INFO : frequencies processed\n",
      "2021-01-15 01:04:34,827 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:04:34,828 : INFO : entropies processed\n",
      "2021-01-15 01:04:34,829 : INFO : extropies processed\n",
      "2021-01-15 01:04:34,836 : INFO : token count processed\n",
      "2021-01-15 01:04:34,840 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:04:34,845 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:04:34,846 : INFO : vocab #32006\n",
      "2021-01-15 01:04:34,852 : INFO : diff #set()\n",
      "2021-01-15 01:04:53,875 : INFO : alphabet #32006\n",
      "2021-01-15 01:05:03,168 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.1744295465784518, 0.459890733904687], [0.8477861136198044, 0.15221389], [1.9219280948873623, 1.2148067842293933], [5.215936788508252, 6.431978396403875, 6.776074481314927, 4.871840703597199, 1.5601376928066752, 0.3440960849110519]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 01:05:03,172 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:05:03,173 : INFO : built Dictionary(237 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 718 corpus positions)\n",
      "2021-01-15 01:05:03,392 : INFO : token count processed\n",
      "2021-01-15 01:05:03,438 : INFO : frequencies processed\n",
      "2021-01-15 01:05:12,849 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:05:12,850 : INFO : entropies processed\n",
      "2021-01-15 01:05:12,851 : INFO : extropies processed\n",
      "2021-01-15 01:05:12,858 : INFO : token count processed\n",
      "2021-01-15 01:05:12,863 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:05:12,868 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:05:12,869 : INFO : vocab #32006\n",
      "2021-01-15 01:05:12,877 : INFO : diff #set()\n",
      "2021-01-15 01:05:32,039 : INFO : alphabet #32006\n",
      "2021-01-15 01:05:41,471 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/test_auth_utility.py')[[1.2199825182858737, 0.45045399761622223], [0.888113759458065, 0.11188624], [2.9219280948873623, 1.3359016564230495], [5.215936788508252, 6.911818353685893, 7.182656617063307, 4.945098525130838, 1.9667198285550551, 0.27083826337741357]]\n",
      "2021-01-15 01:05:41,485 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:05:41,488 : INFO : built Dictionary(325 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 7248 corpus positions)\n",
      "2021-01-15 01:05:41,858 : INFO : token count processed\n",
      "2021-01-15 01:05:41,890 : INFO : frequencies processed\n",
      "2021-01-15 01:05:51,299 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:05:51,300 : INFO : entropies processed\n",
      "2021-01-15 01:05:51,301 : INFO : extropies processed\n",
      "2021-01-15 01:05:51,310 : INFO : token count processed\n",
      "2021-01-15 01:05:51,315 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:05:51,319 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:05:51,319 : INFO : vocab #32006\n",
      "2021-01-15 01:05:51,326 : INFO : diff #set()\n",
      "2021-01-15 01:06:10,338 : INFO : alphabet #32006\n",
      "2021-01-15 01:06:19,779 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.216910902464489, 0.45107811905671213], [0.8808471783995628, 0.11915282], [3.378783493486176, 1.3660934553878117], [5.215936788508252, 6.363791471162389, 6.428906707952938, 5.150821551717704, 1.212969919444686, 0.06511523679054854]]\n",
      "2021-01-15 01:06:19,784 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:06:19,785 : INFO : built Dictionary(227 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1318 corpus positions)\n",
      "2021-01-15 01:06:20,001 : INFO : token count processed\n",
      "2021-01-15 01:06:20,033 : INFO : frequencies processed\n",
      "2021-01-15 01:06:29,452 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:06:29,453 : INFO : entropies processed\n",
      "2021-01-15 01:06:29,454 : INFO : extropies processed\n",
      "2021-01-15 01:06:29,461 : INFO : token count processed\n",
      "2021-01-15 01:06:29,465 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:06:29,470 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:06:29,471 : INFO : vocab #32006\n",
      "2021-01-15 01:06:29,478 : INFO : diff #set()\n",
      "2021-01-15 01:06:48,489 : INFO : alphabet #32006\n",
      "2021-01-15 01:06:57,932 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.229839378177607, 0.44846279502753944], [0.9081559106707573, 0.09184409], [3.2516291673878226, 1.3589504783379556], [5.215936788508252, 6.29000629755059, 6.621423130528152, 4.88451995553069, 1.4054863420199002, 0.3314168329775615]]\n",
      "2021-01-15 01:06:57,936 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:06:57,938 : INFO : built Dictionary(239 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1271 corpus positions)\n",
      "2021-01-15 01:06:58,155 : INFO : token count processed\n",
      "2021-01-15 01:06:58,200 : INFO : frequencies processed\n",
      "2021-01-15 01:07:08,006 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:07:08,008 : INFO : entropies processed\n",
      "2021-01-15 01:07:08,009 : INFO : extropies processed\n",
      "2021-01-15 01:07:08,016 : INFO : token count processed\n",
      "2021-01-15 01:07:08,022 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:07:08,027 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:07:08,028 : INFO : vocab #32006\n",
      "2021-01-15 01:07:08,034 : INFO : diff #set()\n",
      "2021-01-15 01:07:26,922 : INFO : alphabet #32006\n",
      "2021-01-15 01:07:36,732 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.2231147851491784, 0.4498193285745687], [0.8976006284356117, 0.10239937], [2.94770277922009, 1.3393100707180505], [5.215936788508252, 6.361621244785958, 6.685545894953162, 4.892012138341048, 1.46960910644491, 0.3239246501672035]]\n",
      "2021-01-15 01:07:36,736 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:07:36,738 : INFO : built Dictionary(246 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1294 corpus positions)\n",
      "2021-01-15 01:07:36,963 : INFO : token count processed\n",
      "2021-01-15 01:07:36,995 : INFO : frequencies processed\n",
      "2021-01-15 01:07:46,537 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:07:46,538 : INFO : entropies processed\n",
      "2021-01-15 01:07:46,539 : INFO : extropies processed\n",
      "2021-01-15 01:07:46,546 : INFO : token count processed\n",
      "2021-01-15 01:07:46,554 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:07:46,558 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:07:46,559 : INFO : vocab #32006\n",
      "2021-01-15 01:07:46,566 : INFO : diff #set()\n",
      "2021-01-15 01:08:05,484 : INFO : alphabet #32006\n",
      "2021-01-15 01:08:14,924 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.1682587200311023, 0.46119957492233943], [0.8395031839609146, 0.16049682], [3.0957952550009336, 1.3487605247277434], [5.215936788508252, 6.620594433343389, 6.930470847975995, 4.906060373875645, 1.7145340594677432, 0.3098764146326056]]\n",
      "2021-01-15 01:08:14,929 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:08:14,931 : INFO : built Dictionary(220 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1387 corpus positions)\n",
      "2021-01-15 01:08:15,135 : INFO : token count processed\n",
      "2021-01-15 01:08:15,167 : INFO : frequencies processed\n",
      "2021-01-15 01:08:24,709 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:08:24,711 : INFO : entropies processed\n",
      "2021-01-15 01:08:24,711 : INFO : extropies processed\n",
      "2021-01-15 01:08:24,718 : INFO : token count processed\n",
      "2021-01-15 01:08:24,723 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:08:24,728 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:08:24,729 : INFO : vocab #32006\n",
      "2021-01-15 01:08:24,737 : INFO : diff #set()\n",
      "2021-01-15 01:08:43,621 : INFO : alphabet #32006\n",
      "2021-01-15 01:08:53,172 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.1700056423871077, 0.46082829485178356], [0.851518303155899, 0.1484817], [2.75, 1.3226647836567114], [5.215936788508252, 6.207411496248084, 6.451279310609053, 4.972068974147284, 1.235342522100801, 0.24386781436096872]]\n",
      "2021-01-15 01:08:53,176 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:08:53,177 : INFO : built Dictionary(159 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 291 corpus positions)\n",
      "2021-01-15 01:08:53,314 : INFO : token count processed\n",
      "2021-01-15 01:08:53,346 : INFO : frequencies processed\n",
      "2021-01-15 01:09:02,749 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:09:02,750 : INFO : entropies processed\n",
      "2021-01-15 01:09:02,751 : INFO : extropies processed\n",
      "2021-01-15 01:09:02,757 : INFO : token count processed\n",
      "2021-01-15 01:09:02,764 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:09:02,768 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:09:02,769 : INFO : vocab #32006\n",
      "2021-01-15 01:09:02,775 : INFO : diff #set()\n",
      "2021-01-15 01:09:21,585 : INFO : alphabet #32006\n",
      "2021-01-15 01:09:31,153 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.1685975719277053, 0.46112751067552016], [0.8353850543498993, 0.16461495], [2.94770277922009, 1.3393100707180505], [5.215936788508252, 6.5805228788529595, 6.966335073686475, 4.8301245936747375, 1.750398285178223, 0.38581219483351514]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 01:09:31,158 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:09:31,159 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:09:31,161 : INFO : built Dictionary(240 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1252 corpus positions)\n",
      "2021-01-15 01:09:31,386 : INFO : token count processed\n",
      "2021-01-15 01:09:31,419 : INFO : frequencies processed\n",
      "2021-01-15 01:09:40,851 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:09:40,852 : INFO : entropies processed\n",
      "2021-01-15 01:09:40,853 : INFO : extropies processed\n",
      "2021-01-15 01:09:40,860 : INFO : token count processed\n",
      "2021-01-15 01:09:40,865 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:09:40,871 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:09:40,872 : INFO : vocab #32006\n",
      "2021-01-15 01:09:40,880 : INFO : diff #set()\n",
      "2021-01-15 01:10:00,085 : INFO : alphabet #32006\n",
      "2021-01-15 01:10:09,659 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.19216422703519, 0.4561702027919952], [0.8764025568962097, 0.12359744], [3.2516291673878226, 1.3589504783379556], [5.215936788508252, 6.422089779976135, 6.629119568233586, 5.008907000250801, 1.413182779725334, 0.20702978825745078]]\n",
      "2021-01-15 01:10:09,664 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:10:09,666 : INFO : built Dictionary(253 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1534 corpus positions)\n",
      "2021-01-15 01:10:09,901 : INFO : token count processed\n",
      "2021-01-15 01:10:09,945 : INFO : frequencies processed\n",
      "2021-01-15 01:10:19,368 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:10:19,369 : INFO : entropies processed\n",
      "2021-01-15 01:10:19,370 : INFO : extropies processed\n",
      "2021-01-15 01:10:19,377 : INFO : token count processed\n",
      "2021-01-15 01:10:19,382 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:10:19,387 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:10:19,388 : INFO : vocab #32006\n",
      "2021-01-15 01:10:19,395 : INFO : diff #set()\n",
      "2021-01-15 01:10:38,430 : INFO : alphabet #32006\n",
      "2021-01-15 01:10:47,855 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.1866199265205397, 0.45732684856268124], [0.8664681166410446, 0.13353188], [3.121928094887362, 1.3519647487142497], [5.215936788508252, 6.485445644653597, 6.818956492719677, 4.882425940442172, 1.603019704211425, 0.33351084806608]]\n",
      "2021-01-15 01:10:47,859 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:10:47,861 : INFO : built Dictionary(230 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1420 corpus positions)\n",
      "2021-01-15 01:10:48,067 : INFO : token count processed\n",
      "2021-01-15 01:10:48,100 : INFO : frequencies processed\n",
      "2021-01-15 01:10:57,689 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:10:57,690 : INFO : entropies processed\n",
      "2021-01-15 01:10:57,691 : INFO : extropies processed\n",
      "2021-01-15 01:10:57,698 : INFO : token count processed\n",
      "2021-01-15 01:10:57,706 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:10:57,710 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:10:57,711 : INFO : vocab #32006\n",
      "2021-01-15 01:10:57,718 : INFO : diff #set()\n",
      "2021-01-15 01:11:16,728 : INFO : alphabet #32006\n",
      "2021-01-15 01:11:26,156 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.1731295841531164, 0.4601658397604057], [0.8543300032615662, 0.14567], [2.75, 1.3226647836567114], [5.215936788508252, 6.2276600107346916, 6.46691908234202, 4.976677716900923, 1.2509822938337685, 0.23925907160732862]]\n",
      "2021-01-15 01:11:26,161 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:11:26,163 : INFO : built Dictionary(220 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1618 corpus positions)\n",
      "2021-01-15 01:11:26,361 : INFO : token count processed\n",
      "2021-01-15 01:11:26,394 : INFO : frequencies processed\n",
      "2021-01-15 01:11:35,816 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:11:35,817 : INFO : entropies processed\n",
      "2021-01-15 01:11:35,818 : INFO : extropies processed\n",
      "2021-01-15 01:11:35,825 : INFO : token count processed\n",
      "2021-01-15 01:11:35,830 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:11:35,835 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:11:35,836 : INFO : vocab #32006\n",
      "2021-01-15 01:11:35,843 : INFO : diff #set()\n",
      "2021-01-15 01:11:54,854 : INFO : alphabet #32006\n",
      "2021-01-15 01:12:04,301 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.2092241528095728, 0.45264759518777375], [0.8928008079528809, 0.10719919], [2.75, 1.3226647836567114], [5.215936788508252, 6.253918170574241, 6.53560234701253, 4.9342526120699635, 1.319665558504278, 0.28168417643828914]]\n",
      "2021-01-15 01:12:04,305 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:12:04,306 : INFO : built Dictionary(193 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 715 corpus positions)\n",
      "2021-01-15 01:12:04,471 : INFO : token count processed\n",
      "2021-01-15 01:12:04,504 : INFO : frequencies processed\n",
      "2021-01-15 01:12:13,930 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:12:13,931 : INFO : entropies processed\n",
      "2021-01-15 01:12:13,932 : INFO : extropies processed\n",
      "2021-01-15 01:12:13,939 : INFO : token count processed\n",
      "2021-01-15 01:12:13,946 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:12:13,951 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:12:13,952 : INFO : vocab #32006\n",
      "2021-01-15 01:12:13,958 : INFO : diff #set()\n",
      "2021-01-15 01:12:32,979 : INFO : alphabet #32006\n",
      "2021-01-15 01:12:42,558 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.1592749510456466, 0.4631184182985783], [0.8330598026514053, 0.1669402], [2.5, 1.2968140217166513], [5.215936788508252, 6.374522245625576, 6.734790806405064, 4.8556682277287635, 1.5188540178968122, 0.3602685607794882]]\n",
      "2021-01-15 01:12:42,563 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:12:42,565 : INFO : built Dictionary(309 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1997 corpus positions)\n",
      "2021-01-15 01:12:42,911 : INFO : token count processed\n",
      "2021-01-15 01:12:42,950 : INFO : frequencies processed\n",
      "2021-01-15 01:12:52,476 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:12:52,477 : INFO : entropies processed\n",
      "2021-01-15 01:12:52,478 : INFO : extropies processed\n",
      "2021-01-15 01:12:52,486 : INFO : token count processed\n",
      "2021-01-15 01:12:52,490 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:12:52,494 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:12:52,495 : INFO : vocab #32006\n",
      "2021-01-15 01:12:52,504 : INFO : diff #set()\n",
      "2021-01-15 01:13:11,348 : INFO : alphabet #32006\n",
      "2021-01-15 01:13:20,723 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.1709085361641944, 0.46063663362202834], [0.8214786797761917, 0.17852132], [3.521640636343319, 1.3740281872300928], [5.215936788508252, 6.731238669067808, 7.061216982292032, 4.885958475284027, 1.84528019378378, 0.329978313224224]]\n",
      "2021-01-15 01:13:20,728 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:13:20,730 : INFO : built Dictionary(234 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1389 corpus positions)\n",
      "2021-01-15 01:13:20,960 : INFO : token count processed\n",
      "2021-01-15 01:13:21,021 : INFO : frequencies processed\n",
      "2021-01-15 01:13:30,443 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:13:30,444 : INFO : entropies processed\n",
      "2021-01-15 01:13:30,445 : INFO : extropies processed\n",
      "2021-01-15 01:13:30,452 : INFO : token count processed\n",
      "2021-01-15 01:13:30,456 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:13:30,460 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:13:30,461 : INFO : vocab #32006\n",
      "2021-01-15 01:13:30,468 : INFO : diff #set()\n",
      "2021-01-15 01:13:49,370 : INFO : alphabet #32006\n",
      "2021-01-15 01:13:59,092 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.1654568553689881, 0.46179631680059624], [0.8539678901433945, 0.14603211], [3.521640636343319, 1.3740281872300928], [5.215936788508252, 6.503741451859337, 6.779452222344388, 4.940226018023201, 1.5635154338361366, 0.275710770485051]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 01:13:59,097 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:13:59,099 : INFO : built Dictionary(259 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 1711 corpus positions)\n",
      "2021-01-15 01:13:59,355 : INFO : token count processed\n",
      "2021-01-15 01:13:59,424 : INFO : frequencies processed\n",
      "2021-01-15 01:14:08,874 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:14:08,875 : INFO : entropies processed\n",
      "2021-01-15 01:14:08,876 : INFO : extropies processed\n",
      "2021-01-15 01:14:08,883 : INFO : token count processed\n",
      "2021-01-15 01:14:08,887 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:14:08,892 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:14:08,893 : INFO : vocab #32006\n",
      "2021-01-15 01:14:08,899 : INFO : diff #set()\n",
      "2021-01-15 01:14:27,810 : INFO : alphabet #32006\n",
      "2021-01-15 01:14:37,380 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.2056287037212576, 0.45338546706108596], [0.8815535455942154, 0.118446454], [3.121928094887362, 1.3519647487142497], [5.215936788508252, 6.334729224484471, 6.558775641531831, 4.991890371460892, 1.3428388530235793, 0.22404641704735972]]\n",
      "2021-01-15 01:14:37,385 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:14:37,387 : INFO : built Dictionary(255 unique tokens: ['-', '2', '26', '63', '64']...) from 2 documents (total 2060 corpus positions)\n",
      "2021-01-15 01:14:37,629 : INFO : token count processed\n",
      "2021-01-15 01:14:37,689 : INFO : frequencies processed\n",
      "2021-01-15 01:14:47,116 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:14:47,117 : INFO : entropies processed\n",
      "2021-01-15 01:14:47,118 : INFO : extropies processed\n",
      "2021-01-15 01:14:47,126 : INFO : token count processed\n",
      "2021-01-15 01:14:47,130 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:14:47,135 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:14:47,136 : INFO : vocab #32006\n",
      "2021-01-15 01:14:47,143 : INFO : diff #set()\n",
      "2021-01-15 01:15:06,184 : INFO : alphabet #32006\n",
      "2021-01-15 01:15:15,633 : INFO : Computed distances or similarities ('268', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.177239012195892, 0.4592973001119582], [0.8544030487537384, 0.14559695], [3.251629167387823, 1.3589504783379556], [5.215936788508252, 6.21319712067992, 6.483473200948562, 4.94566070823961, 1.2675364124403101, 0.27027608026864147]]\n",
      "2021-01-15 01:15:15,638 : INFO : Removed 1 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:15:15,639 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:15:15,640 : INFO : built Dictionary(277 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 1416 corpus positions)\n",
      "2021-01-15 01:15:15,791 : INFO : token count processed\n",
      "2021-01-15 01:15:15,854 : INFO : frequencies processed\n",
      "2021-01-15 01:15:25,285 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:15:25,286 : INFO : entropies processed\n",
      "2021-01-15 01:15:25,287 : INFO : extropies processed\n",
      "2021-01-15 01:15:25,294 : INFO : token count processed\n",
      "2021-01-15 01:15:25,300 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:15:25,305 : INFO : alphabet_target #32010\n",
      "2021-01-15 01:15:25,306 : INFO : vocab #32006\n",
      "2021-01-15 01:15:25,313 : INFO : diff #set()\n",
      "2021-01-15 01:15:44,286 : INFO : alphabet #32006\n",
      "2021-01-15 01:15:53,704 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.2092712095713758, 0.45263795394048145], [0.8726595640182495, 0.12734044], [2.0, 1.2451124978365313], [4.392747410448784, 6.905617163738059, 7.059714686379992, 4.238649887806851, 2.666967275931208, 0.15409752264193344]]\n",
      "2021-01-15 01:15:53,709 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:15:53,710 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:15:53,712 : INFO : built Dictionary(365 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 2306 corpus positions)\n",
      "2021-01-15 01:15:53,944 : INFO : token count processed\n",
      "2021-01-15 01:15:53,979 : INFO : frequencies processed\n",
      "2021-01-15 01:16:03,385 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:16:03,386 : INFO : entropies processed\n",
      "2021-01-15 01:16:03,387 : INFO : extropies processed\n",
      "2021-01-15 01:16:03,400 : INFO : token count processed\n",
      "2021-01-15 01:16:03,404 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:16:03,408 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:16:03,410 : INFO : vocab #32006\n",
      "2021-01-15 01:16:03,416 : INFO : diff #set()\n",
      "2021-01-15 01:16:22,417 : INFO : alphabet #32006\n",
      "2021-01-15 01:16:31,842 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.2106180186877142, 0.4523621862964948], [0.869701161980629, 0.13029884], [2.725480556997868, 1.3192201298976014], [4.392747410448784, 7.1219284286457345, 7.34555553778339, 4.169120301311128, 2.952808127334606, 0.22362710913765582]]\n",
      "2021-01-15 01:16:31,848 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:16:31,849 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:16:31,854 : INFO : built Dictionary(284 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 2289 corpus positions)\n",
      "2021-01-15 01:16:31,999 : INFO : token count processed\n",
      "2021-01-15 01:16:32,029 : INFO : frequencies processed\n",
      "2021-01-15 01:16:41,431 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:16:41,432 : INFO : entropies processed\n",
      "2021-01-15 01:16:41,433 : INFO : extropies processed\n",
      "2021-01-15 01:16:41,441 : INFO : token count processed\n",
      "2021-01-15 01:16:41,445 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:16:41,449 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:16:41,450 : INFO : vocab #32006\n",
      "2021-01-15 01:16:41,457 : INFO : diff #set()\n",
      "2021-01-15 01:17:00,468 : INFO : alphabet #32006\n",
      "2021-01-15 01:17:09,894 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2352303634564967, 0.44738118108490105], [0.8981899991631508, 0.10181], [2.807354922057604, 1.3343545280186873], [4.392747410448784, 6.41099024988467, 6.496594062570125, 4.307143597763329, 2.103846652121341, 0.0856038126854548]]\n",
      "2021-01-15 01:17:09,897 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:17:09,898 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:17:09,900 : INFO : built Dictionary(165 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 688 corpus positions)\n",
      "2021-01-15 01:17:09,973 : INFO : token count processed\n",
      "2021-01-15 01:17:10,005 : INFO : frequencies processed\n",
      "2021-01-15 01:17:19,426 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:17:19,427 : INFO : entropies processed\n",
      "2021-01-15 01:17:19,428 : INFO : extropies processed\n",
      "2021-01-15 01:17:19,435 : INFO : token count processed\n",
      "2021-01-15 01:17:19,439 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:17:19,443 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:17:19,444 : INFO : vocab #32006\n",
      "2021-01-15 01:17:19,451 : INFO : diff #set()\n",
      "2021-01-15 01:17:38,319 : INFO : alphabet #32006\n",
      "2021-01-15 01:17:47,736 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.1979226123833033, 0.4549750725370883], [0.8621764481067657, 0.13782355], [1.9219280948873623, 1.2148067842293933], [4.392747410448784, 6.077866832717642, 6.258404502883416, 4.2122097402830105, 1.865657092434632, 0.1805376701657746]]\n",
      "2021-01-15 01:17:47,739 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:17:47,740 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:17:47,741 : INFO : built Dictionary(142 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 453 corpus positions)\n",
      "2021-01-15 01:17:47,804 : INFO : token count processed\n",
      "2021-01-15 01:17:47,836 : INFO : frequencies processed\n",
      "2021-01-15 01:17:57,528 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:17:57,529 : INFO : entropies processed\n",
      "2021-01-15 01:17:57,533 : INFO : extropies processed\n",
      "2021-01-15 01:17:57,542 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 01:17:57,548 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:17:57,553 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:17:57,553 : INFO : vocab #32006\n",
      "2021-01-15 01:17:57,560 : INFO : diff #set()\n",
      "2021-01-15 01:18:16,410 : INFO : alphabet #32006\n",
      "2021-01-15 01:18:25,950 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.2358071243307838, 0.4472657722205432], [0.9056177139282227, 0.094382286], [1.584962500721156, 1.1699250014423124], [4.392747410448784, 5.977547459003844, 6.181210284535722, 4.189084584916906, 1.788462874086938, 0.20366282553187798]]\n",
      "2021-01-15 01:18:25,955 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:18:25,956 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:18:25,958 : INFO : built Dictionary(244 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 2164 corpus positions)\n",
      "2021-01-15 01:18:26,079 : INFO : token count processed\n",
      "2021-01-15 01:18:26,112 : INFO : frequencies processed\n",
      "2021-01-15 01:18:35,531 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:18:35,532 : INFO : entropies processed\n",
      "2021-01-15 01:18:35,533 : INFO : extropies processed\n",
      "2021-01-15 01:18:35,540 : INFO : token count processed\n",
      "2021-01-15 01:18:35,545 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:18:35,549 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:18:35,550 : INFO : vocab #32006\n",
      "2021-01-15 01:18:35,559 : INFO : diff #set()\n",
      "2021-01-15 01:18:54,401 : INFO : alphabet #32006\n",
      "2021-01-15 01:19:03,943 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.2360822754069312, 0.4472107359368143], [0.8966640010476112, 0.103336], [2.584962500721156, 1.315172029168969], [4.392747410448784, 6.4614394051846435, 6.581162675350155, 4.2730241402832725, 2.188415264901371, 0.11972327016551176]]\n",
      "2021-01-15 01:19:03,948 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:19:03,949 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:19:03,951 : INFO : built Dictionary(210 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 1388 corpus positions)\n",
      "2021-01-15 01:19:04,064 : INFO : token count processed\n",
      "2021-01-15 01:19:04,098 : INFO : frequencies processed\n",
      "2021-01-15 01:19:13,508 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:19:13,510 : INFO : entropies processed\n",
      "2021-01-15 01:19:13,510 : INFO : extropies processed\n",
      "2021-01-15 01:19:13,518 : INFO : token count processed\n",
      "2021-01-15 01:19:13,523 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:19:13,528 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:19:13,529 : INFO : vocab #32006\n",
      "2021-01-15 01:19:13,536 : INFO : diff #set()\n",
      "2021-01-15 01:19:32,453 : INFO : alphabet #32006\n",
      "2021-01-15 01:19:42,287 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.2386638147648428, 0.44669503004632405], [0.9155816957354546, 0.084418304], [1.0, 1.0], [4.392747410448784, 6.327195724598159, 6.470159579771088, 4.249783555275855, 2.0774121693223035, 0.14296385517292887]]\n",
      "2021-01-15 01:19:42,299 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:19:42,300 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:19:42,303 : INFO : built Dictionary(418 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 6280 corpus positions)\n",
      "2021-01-15 01:19:42,574 : INFO : token count processed\n",
      "2021-01-15 01:19:42,606 : INFO : frequencies processed\n",
      "2021-01-15 01:19:52,038 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:19:52,039 : INFO : entropies processed\n",
      "2021-01-15 01:19:52,040 : INFO : extropies processed\n",
      "2021-01-15 01:19:52,049 : INFO : token count processed\n",
      "2021-01-15 01:19:52,055 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:19:52,061 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:19:52,062 : INFO : vocab #32006\n",
      "2021-01-15 01:19:52,069 : INFO : diff #set()\n",
      "2021-01-15 01:20:11,087 : INFO : alphabet #32006\n",
      "2021-01-15 01:20:20,525 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.2324397917313783, 0.4479404119671445], [0.8944754600524902, 0.10552454], [3.238901256602631, 1.3579502728384498], [4.392747410448784, 6.9079058562486315, 7.018332772971068, 4.282320493726349, 2.6255853625222834, 0.1104269167224361]]\n",
      "2021-01-15 01:20:20,532 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:20:20,533 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:20:20,534 : INFO : built Dictionary(320 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 2674 corpus positions)\n",
      "2021-01-15 01:20:20,715 : INFO : token count processed\n",
      "2021-01-15 01:20:20,748 : INFO : frequencies processed\n",
      "2021-01-15 01:20:30,732 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:20:30,733 : INFO : entropies processed\n",
      "2021-01-15 01:20:30,734 : INFO : extropies processed\n",
      "2021-01-15 01:20:30,742 : INFO : token count processed\n",
      "2021-01-15 01:20:30,746 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:20:30,751 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:20:30,752 : INFO : vocab #32006\n",
      "2021-01-15 01:20:30,759 : INFO : diff #set()\n",
      "2021-01-15 01:20:49,793 : INFO : alphabet #32006\n",
      "2021-01-15 01:20:59,217 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.228193187741729, 0.44879411960391946], [0.8957915306091309, 0.10420847], [3.2776134368191157, 1.3618978811135465], [4.392747410448784, 6.61034830706307, 6.772579239767905, 4.230516477743949, 2.379831829319121, 0.16223093270483524]]\n",
      "2021-01-15 01:20:59,221 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:20:59,222 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:20:59,224 : INFO : built Dictionary(212 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 721 corpus positions)\n",
      "2021-01-15 01:20:59,344 : INFO : token count processed\n",
      "2021-01-15 01:20:59,378 : INFO : frequencies processed\n",
      "2021-01-15 01:21:08,787 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:21:08,788 : INFO : entropies processed\n",
      "2021-01-15 01:21:08,789 : INFO : extropies processed\n",
      "2021-01-15 01:21:08,796 : INFO : token count processed\n",
      "2021-01-15 01:21:08,800 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:21:08,807 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:21:08,808 : INFO : vocab #32006\n",
      "2021-01-15 01:21:08,815 : INFO : diff #set()\n",
      "2021-01-15 01:21:27,833 : INFO : alphabet #32006\n",
      "2021-01-15 01:21:37,251 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.2388590109255844, 0.44665608469315005], [0.8891919553279877, 0.110808045], [1.9219280948873623, 1.2148067842293933], [4.392747410448784, 6.616715366949855, 6.79020656571252, 4.219256211686119, 2.3974591552637357, 0.17349119876266528]]\n",
      "2021-01-15 01:21:37,257 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:21:37,258 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:21:37,261 : INFO : built Dictionary(424 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 2755 corpus positions)\n",
      "2021-01-15 01:21:37,521 : INFO : token count processed\n",
      "2021-01-15 01:21:37,554 : INFO : frequencies processed\n",
      "2021-01-15 01:21:46,856 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:21:46,858 : INFO : entropies processed\n",
      "2021-01-15 01:21:46,858 : INFO : extropies processed\n",
      "2021-01-15 01:21:46,866 : INFO : token count processed\n",
      "2021-01-15 01:21:46,870 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:21:46,875 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:21:46,876 : INFO : vocab #32006\n",
      "2021-01-15 01:21:46,882 : INFO : diff #set()\n",
      "2021-01-15 01:22:05,897 : INFO : alphabet #32006\n",
      "2021-01-15 01:22:15,330 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.2037321315247844, 0.4537756588901256], [0.8326077163219452, 0.16739228], [3.5, 1.372578305734497], [4.392747410448784, 7.32185870753746, 7.5197892931359185, 4.194816824850326, 3.1270418826871342, 0.1979305855984581]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 01:22:15,333 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:22:15,334 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:22:15,335 : INFO : built Dictionary(68 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 120 corpus positions)\n",
      "2021-01-15 01:22:15,361 : INFO : token count processed\n",
      "2021-01-15 01:22:15,393 : INFO : frequencies processed\n",
      "2021-01-15 01:22:24,956 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:22:24,958 : INFO : entropies processed\n",
      "2021-01-15 01:22:24,959 : INFO : extropies processed\n",
      "2021-01-15 01:22:24,973 : INFO : token count processed\n",
      "2021-01-15 01:22:24,980 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:22:24,985 : INFO : alphabet_target #32008\n",
      "2021-01-15 01:22:24,987 : INFO : vocab #32006\n",
      "2021-01-15 01:22:24,994 : INFO : diff #set()\n",
      "2021-01-15 01:22:43,860 : INFO : alphabet #32006\n",
      "2021-01-15 01:22:53,279 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2304215349547394, 0.44834574286886647], [0.9220399856567383, 0.077960014], [0.0, 0.0], [4.392747410448784, 5.176618657501385, 5.799846021826149, 3.769520046124021, 1.4070986113773651, 0.6232273643247641]]\n",
      "2021-01-15 01:22:53,283 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:22:53,284 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:22:53,285 : INFO : built Dictionary(160 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 513 corpus positions)\n",
      "2021-01-15 01:22:53,359 : INFO : token count processed\n",
      "2021-01-15 01:22:53,392 : INFO : frequencies processed\n",
      "2021-01-15 01:23:03,093 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:23:03,094 : INFO : entropies processed\n",
      "2021-01-15 01:23:03,095 : INFO : extropies processed\n",
      "2021-01-15 01:23:03,102 : INFO : token count processed\n",
      "2021-01-15 01:23:03,106 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:23:03,110 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:23:03,111 : INFO : vocab #32006\n",
      "2021-01-15 01:23:03,119 : INFO : diff #set()\n",
      "2021-01-15 01:23:22,023 : INFO : alphabet #32006\n",
      "2021-01-15 01:23:31,471 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.2123737329510347, 0.4520031968857825], [0.8936621323227882, 0.10633787], [2.321928094887362, 1.2877123795494492], [4.392747410448784, 6.468846789852156, 6.690139309217525, 4.171454891083416, 2.2973918987687405, 0.22129251936536853]]\n",
      "2021-01-15 01:23:31,477 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:23:31,478 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:23:31,483 : INFO : built Dictionary(373 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 2562 corpus positions)\n",
      "2021-01-15 01:23:31,730 : INFO : token count processed\n",
      "2021-01-15 01:23:31,764 : INFO : frequencies processed\n",
      "2021-01-15 01:23:41,306 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:23:41,307 : INFO : entropies processed\n",
      "2021-01-15 01:23:41,308 : INFO : extropies processed\n",
      "2021-01-15 01:23:41,315 : INFO : token count processed\n",
      "2021-01-15 01:23:41,321 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:23:41,325 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:23:41,326 : INFO : vocab #32006\n",
      "2021-01-15 01:23:41,333 : INFO : diff #set()\n",
      "2021-01-15 01:24:00,222 : INFO : alphabet #32006\n",
      "2021-01-15 01:24:09,778 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.153031343945904, 0.4644614221766413], [0.8291966468095779, 0.17080335], [3.121928094887362, 1.3519647487142497], [4.392747410448784, 6.957796704012729, 7.100349823855676, 4.250194290605838, 2.7076024134068915, 0.1425531198429466]]\n",
      "2021-01-15 01:24:09,785 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:24:09,786 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:24:09,787 : INFO : built Dictionary(287 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 3059 corpus positions)\n",
      "2021-01-15 01:24:09,945 : INFO : token count processed\n",
      "2021-01-15 01:24:09,978 : INFO : frequencies processed\n",
      "2021-01-15 01:24:19,404 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:24:19,405 : INFO : entropies processed\n",
      "2021-01-15 01:24:19,406 : INFO : extropies processed\n",
      "2021-01-15 01:24:19,414 : INFO : token count processed\n",
      "2021-01-15 01:24:19,418 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:24:19,423 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:24:19,424 : INFO : vocab #32006\n",
      "2021-01-15 01:24:19,432 : INFO : diff #set()\n",
      "2021-01-15 01:24:38,457 : INFO : alphabet #32006\n",
      "2021-01-15 01:24:47,892 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.216491821649509, 0.4511634061684929], [0.8825077712535858, 0.11749223], [2.0, 1.2451124978365313], [4.392747410448784, 6.441859572014148, 6.600724550592629, 4.233882431870304, 2.2079771401438446, 0.1588649785784808]]\n",
      "2021-01-15 01:24:47,897 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:24:47,898 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:24:47,901 : INFO : built Dictionary(311 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 1535 corpus positions)\n",
      "2021-01-15 01:24:48,082 : INFO : token count processed\n",
      "2021-01-15 01:24:48,117 : INFO : frequencies processed\n",
      "2021-01-15 01:24:57,811 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:24:57,812 : INFO : entropies processed\n",
      "2021-01-15 01:24:57,813 : INFO : extropies processed\n",
      "2021-01-15 01:24:57,824 : INFO : token count processed\n",
      "2021-01-15 01:24:57,829 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:24:57,834 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:24:57,835 : INFO : vocab #32006\n",
      "2021-01-15 01:24:57,846 : INFO : diff #set()\n",
      "2021-01-15 01:25:16,854 : INFO : alphabet #32006\n",
      "2021-01-15 01:25:26,278 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.2260220005528613, 0.44923185833367196], [0.8808445632457733, 0.11915544], [2.5, 1.2968140217166515], [4.392747410448784, 6.998955278238291, 7.1745942104516915, 4.217108478235384, 2.7818468000029073, 0.17563893221340088]]\n",
      "2021-01-15 01:25:26,282 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:25:26,283 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:25:26,285 : INFO : built Dictionary(226 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 1354 corpus positions)\n",
      "2021-01-15 01:25:26,410 : INFO : token count processed\n",
      "2021-01-15 01:25:26,445 : INFO : frequencies processed\n",
      "2021-01-15 01:25:35,904 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:25:35,905 : INFO : entropies processed\n",
      "2021-01-15 01:25:35,906 : INFO : extropies processed\n",
      "2021-01-15 01:25:35,920 : INFO : token count processed\n",
      "2021-01-15 01:25:35,925 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:25:35,930 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:25:35,931 : INFO : vocab #32006\n",
      "2021-01-15 01:25:35,940 : INFO : diff #set()\n",
      "2021-01-15 01:25:54,842 : INFO : alphabet #32006\n",
      "2021-01-15 01:26:04,277 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.2189876380564524, 0.45065595808179953], [0.8840910717844963, 0.11590893], [2.584962500721156, 1.315172029168969], [4.392747410448784, 6.492983191376071, 6.628512386422239, 4.257218215402617, 2.2357649759734546, 0.13552919504616767]]\n",
      "2021-01-15 01:26:04,285 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:26:04,286 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:26:04,288 : INFO : built Dictionary(438 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 3293 corpus positions)\n",
      "2021-01-15 01:26:04,552 : INFO : token count processed\n",
      "2021-01-15 01:26:04,585 : INFO : frequencies processed\n",
      "2021-01-15 01:26:14,003 : INFO : scalar_distribution processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 01:26:14,004 : INFO : entropies processed\n",
      "2021-01-15 01:26:14,005 : INFO : extropies processed\n",
      "2021-01-15 01:26:14,013 : INFO : token count processed\n",
      "2021-01-15 01:26:14,017 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:26:14,021 : INFO : alphabet_target #32008\n",
      "2021-01-15 01:26:14,022 : INFO : vocab #32006\n",
      "2021-01-15 01:26:14,030 : INFO : diff #set()\n",
      "2021-01-15 01:26:33,043 : INFO : alphabet #32006\n",
      "2021-01-15 01:26:42,476 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.2037655529947804, 0.45376877710111324], [0.8647513538599014, 0.13524865], [1.584962500721156, 1.1699250014423124], [4.392747410448784, 6.560342487747443, 6.761250219808773, 4.191839678387454, 2.368502809359989, 0.2009077320613306]]\n",
      "2021-01-15 01:26:42,483 : INFO : Removed 1 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:26:42,485 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:26:42,487 : INFO : built Dictionary(447 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 3498 corpus positions)\n",
      "2021-01-15 01:26:42,775 : INFO : token count processed\n",
      "2021-01-15 01:26:42,807 : INFO : frequencies processed\n",
      "2021-01-15 01:26:52,335 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:26:52,336 : INFO : entropies processed\n",
      "2021-01-15 01:26:52,336 : INFO : extropies processed\n",
      "2021-01-15 01:26:52,344 : INFO : token count processed\n",
      "2021-01-15 01:26:52,348 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:26:52,353 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:26:52,354 : INFO : vocab #32006\n",
      "2021-01-15 01:26:52,365 : INFO : diff #set()\n",
      "2021-01-15 01:27:11,278 : INFO : alphabet #32006\n",
      "2021-01-15 01:27:20,697 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.2151765120938598, 0.45143129431919], [0.8827016651630402, 0.117298335], [3.521640636343319, 1.3740281872300928], [4.392747410448784, 7.046173750105238, 7.233980217776965, 4.204940942777057, 2.841232807328181, 0.18780646767172726]]\n",
      "2021-01-15 01:27:20,708 : INFO : Removed 1 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:27:20,709 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:27:20,712 : INFO : built Dictionary(501 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 5607 corpus positions)\n",
      "2021-01-15 01:27:21,070 : INFO : token count processed\n",
      "2021-01-15 01:27:21,103 : INFO : frequencies processed\n",
      "2021-01-15 01:27:30,635 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:27:30,636 : INFO : entropies processed\n",
      "2021-01-15 01:27:30,637 : INFO : extropies processed\n",
      "2021-01-15 01:27:30,645 : INFO : token count processed\n",
      "2021-01-15 01:27:30,650 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:27:30,654 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:27:30,655 : INFO : vocab #32006\n",
      "2021-01-15 01:27:30,665 : INFO : diff #set()\n",
      "2021-01-15 01:27:49,534 : INFO : alphabet #32006\n",
      "2021-01-15 01:27:58,956 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.2059256528779088, 0.4533244348898947], [0.8667847067117691, 0.1332153], [3.506890595608519, 1.3728719392429896], [4.392747410448784, 7.009229588004272, 7.1159084753110555, 4.286068523142001, 2.7231610648622713, 0.10667888730678321]]\n",
      "2021-01-15 01:27:58,969 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:27:58,970 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:27:58,974 : INFO : built Dictionary(581 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 6548 corpus positions)\n",
      "2021-01-15 01:27:59,400 : INFO : token count processed\n",
      "2021-01-15 01:27:59,432 : INFO : frequencies processed\n",
      "2021-01-15 01:28:08,961 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:28:08,962 : INFO : entropies processed\n",
      "2021-01-15 01:28:08,963 : INFO : extropies processed\n",
      "2021-01-15 01:28:08,972 : INFO : token count processed\n",
      "2021-01-15 01:28:08,976 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:28:08,982 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:28:08,983 : INFO : vocab #32006\n",
      "2021-01-15 01:28:08,993 : INFO : diff #set()\n",
      "2021-01-15 01:28:27,895 : INFO : alphabet #32006\n",
      "2021-01-15 01:28:37,501 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.2173849746102872, 0.45098167952353574], [0.8654755800962448, 0.13452442], [3.251629167387823, 1.3589504783379556], [4.392747410448784, 7.376088004590871, 7.5417251894555415, 4.227110225584114, 3.1489777790067572, 0.1656371848646705]]\n",
      "2021-01-15 01:28:37,505 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:28:37,506 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:28:37,508 : INFO : built Dictionary(138 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 334 corpus positions)\n",
      "2021-01-15 01:28:37,585 : INFO : token count processed\n",
      "2021-01-15 01:28:37,653 : INFO : frequencies processed\n",
      "2021-01-15 01:28:47,194 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:28:47,195 : INFO : entropies processed\n",
      "2021-01-15 01:28:47,196 : INFO : extropies processed\n",
      "2021-01-15 01:28:47,203 : INFO : token count processed\n",
      "2021-01-15 01:28:47,210 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:28:47,215 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:28:47,216 : INFO : vocab #32006\n",
      "2021-01-15 01:28:47,223 : INFO : diff #set()\n",
      "2021-01-15 01:29:06,165 : INFO : alphabet #32006\n",
      "2021-01-15 01:29:15,737 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.221825204820998, 0.4500804103896936], [0.9011291041970253, 0.098870896], [1.584962500721156, 1.1699250014423124], [4.392747410448784, 6.2993628166120885, 6.550517287267766, 4.141592939793107, 2.157769876818982, 0.25115447065567764]]\n",
      "2021-01-15 01:29:15,740 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:29:15,741 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:29:15,742 : INFO : built Dictionary(38 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 50 corpus positions)\n",
      "2021-01-15 01:29:15,755 : INFO : token count processed\n",
      "2021-01-15 01:29:15,783 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 01:29:15,784 : INFO : frequencies processed\n",
      "2021-01-15 01:29:15,785 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 01:29:15,790 : INFO : token count processed\n",
      "2021-01-15 01:29:15,797 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:29:15,801 : INFO : alphabet_target #32008\n",
      "2021-01-15 01:29:15,802 : INFO : vocab #32006\n",
      "2021-01-15 01:29:15,809 : INFO : diff #set()\n",
      "2021-01-15 01:29:34,722 : INFO : alphabet #32006\n",
      "2021-01-15 01:29:44,281 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2699262789092163, 0.44054294154457607], [0.9490979872643948, 0.050902013], [nan, nan], [4.392747410448784, 3.8936606896881862, 5.068691731826158, 3.2177163683108123, 0.6759443213773739, 1.175031042137972]]\n",
      "2021-01-15 01:29:44,303 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:29:44,305 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:29:44,315 : INFO : built Dictionary(737 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 12492 corpus positions)\n",
      "2021-01-15 01:29:44,945 : INFO : token count processed\n",
      "2021-01-15 01:29:45,001 : INFO : frequencies processed\n",
      "2021-01-15 01:29:54,611 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:29:54,612 : INFO : entropies processed\n",
      "2021-01-15 01:29:54,613 : INFO : extropies processed\n",
      "2021-01-15 01:29:54,624 : INFO : token count processed\n",
      "2021-01-15 01:29:54,627 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:29:54,633 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:29:54,635 : INFO : vocab #32006\n",
      "2021-01-15 01:29:54,643 : INFO : diff #set()\n",
      "2021-01-15 01:30:13,194 : INFO : alphabet #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 01:30:22,669 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.1843080754009485, 0.4578108789972044], [0.8357858657836914, 0.16421413], [3.6168746059562222, 1.3781755222681253], [4.392747410448784, 7.434393313070278, 7.638747517292185, 4.188393206226877, 3.2460001068434003, 0.20435420422190642]]\n",
      "2021-01-15 01:30:22,678 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:30:22,679 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:30:22,681 : INFO : built Dictionary(491 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 4119 corpus positions)\n",
      "2021-01-15 01:30:23,006 : INFO : token count processed\n",
      "2021-01-15 01:30:23,077 : INFO : frequencies processed\n",
      "2021-01-15 01:30:32,487 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:30:32,488 : INFO : entropies processed\n",
      "2021-01-15 01:30:32,489 : INFO : extropies processed\n",
      "2021-01-15 01:30:32,504 : INFO : token count processed\n",
      "2021-01-15 01:30:32,508 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:30:32,512 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:30:32,513 : INFO : vocab #32006\n",
      "2021-01-15 01:30:32,520 : INFO : diff #set()\n",
      "2021-01-15 01:30:51,410 : INFO : alphabet #32006\n",
      "2021-01-15 01:31:00,971 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.2155113438816874, 0.4513630691901354], [0.8588507026433945, 0.1411493], [3.238901256602631, 1.3579502728384498], [4.392747410448784, 7.2991514951718255, 7.490364978373804, 4.201533927246806, 3.0976175679250195, 0.19121348320197828]]\n",
      "2021-01-15 01:31:00,978 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:31:00,979 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:31:00,982 : INFO : built Dictionary(456 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 3536 corpus positions)\n",
      "2021-01-15 01:31:01,280 : INFO : token count processed\n",
      "2021-01-15 01:31:01,315 : INFO : frequencies processed\n",
      "2021-01-15 01:31:11,107 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:31:11,108 : INFO : entropies processed\n",
      "2021-01-15 01:31:11,109 : INFO : extropies processed\n",
      "2021-01-15 01:31:11,123 : INFO : token count processed\n",
      "2021-01-15 01:31:11,129 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:31:11,134 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:31:11,135 : INFO : vocab #32006\n",
      "2021-01-15 01:31:11,143 : INFO : diff #set()\n",
      "2021-01-15 01:31:30,121 : INFO : alphabet #32006\n",
      "2021-01-15 01:31:39,550 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.227550375249693, 0.448923629791271], [0.8822607770562172, 0.11773922], [3.392747410448785, 1.3672090515720434], [4.392747410448784, 7.170319527000998, 7.356328595082136, 4.206738342367647, 2.9635811846333517, 0.18600906808113837]]\n",
      "2021-01-15 01:31:39,554 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:31:39,557 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:31:39,559 : INFO : built Dictionary(172 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 572 corpus positions)\n",
      "2021-01-15 01:31:39,658 : INFO : token count processed\n",
      "2021-01-15 01:31:39,692 : INFO : frequencies processed\n",
      "2021-01-15 01:31:49,098 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:31:49,099 : INFO : entropies processed\n",
      "2021-01-15 01:31:49,100 : INFO : extropies processed\n",
      "2021-01-15 01:31:49,107 : INFO : token count processed\n",
      "2021-01-15 01:31:49,111 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:31:49,116 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:31:49,117 : INFO : vocab #32006\n",
      "2021-01-15 01:31:49,124 : INFO : diff #set()\n",
      "2021-01-15 01:32:08,135 : INFO : alphabet #32006\n",
      "2021-01-15 01:32:17,577 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.22999045544877, 0.4484324125946794], [0.9017948061227798, 0.098205194], [1.584962500721156, 1.1699250014423124], [4.392747410448784, 6.353654804387375, 6.5916997959229615, 4.154702418913198, 2.1989523854741773, 0.23804499153558645]]\n",
      "2021-01-15 01:32:17,581 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:32:17,582 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:32:17,583 : INFO : built Dictionary(167 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 700 corpus positions)\n",
      "2021-01-15 01:32:17,665 : INFO : token count processed\n",
      "2021-01-15 01:32:17,700 : INFO : frequencies processed\n",
      "2021-01-15 01:32:27,107 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:32:27,108 : INFO : entropies processed\n",
      "2021-01-15 01:32:27,109 : INFO : extropies processed\n",
      "2021-01-15 01:32:27,116 : INFO : token count processed\n",
      "2021-01-15 01:32:27,120 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:32:27,125 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:32:27,126 : INFO : vocab #32006\n",
      "2021-01-15 01:32:27,132 : INFO : diff #set()\n",
      "2021-01-15 01:32:46,304 : INFO : alphabet #32006\n",
      "2021-01-15 01:32:55,739 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.2315184356071078, 0.448125358967935], [0.9089170470833778, 0.09108295], [1.584962500721156, 1.1699250014423124], [4.392747410448784, 6.245180322479091, 6.442991716266693, 4.194936016661181, 2.050244305817909, 0.19781139378760226]]\n",
      "2021-01-15 01:32:55,745 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:32:55,746 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:32:55,748 : INFO : built Dictionary(392 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 1772 corpus positions)\n",
      "2021-01-15 01:32:56,004 : INFO : token count processed\n",
      "2021-01-15 01:32:56,036 : INFO : frequencies processed\n",
      "2021-01-15 01:33:05,444 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:33:05,445 : INFO : entropies processed\n",
      "2021-01-15 01:33:05,446 : INFO : extropies processed\n",
      "2021-01-15 01:33:05,454 : INFO : token count processed\n",
      "2021-01-15 01:33:05,459 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:33:05,463 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:33:05,464 : INFO : vocab #32006\n",
      "2021-01-15 01:33:05,470 : INFO : diff #set()\n",
      "2021-01-15 01:33:24,490 : INFO : alphabet #32006\n",
      "2021-01-15 01:33:33,912 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.220413082532286, 0.4503666492811071], [0.8629701733589172, 0.13702983], [2.725480556997868, 1.3192201298976014], [4.392747410448784, 7.2691387000368, 7.480132600175367, 4.181753510310217, 3.087385189726583, 0.21099390013856745]]\n",
      "2021-01-15 01:33:33,917 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:33:33,918 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:33:33,920 : INFO : built Dictionary(317 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 1609 corpus positions)\n",
      "2021-01-15 01:33:34,107 : INFO : token count processed\n",
      "2021-01-15 01:33:34,176 : INFO : frequencies processed\n",
      "2021-01-15 01:33:43,715 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:33:43,716 : INFO : entropies processed\n",
      "2021-01-15 01:33:43,718 : INFO : extropies processed\n",
      "2021-01-15 01:33:43,725 : INFO : token count processed\n",
      "2021-01-15 01:33:43,732 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:33:43,737 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:33:43,738 : INFO : vocab #32006\n",
      "2021-01-15 01:33:43,745 : INFO : diff #set()\n",
      "2021-01-15 01:34:02,627 : INFO : alphabet #32006\n",
      "2021-01-15 01:34:12,056 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.186177460848006, 0.4574194080347465], [0.8261980563402176, 0.17380194], [2.9219280948873623, 1.3359016564230495], [4.392747410448784, 7.08857858466988, 7.257815747088628, 4.223510248030035, 2.865068336639844, 0.16923716241874853]]\n",
      "2021-01-15 01:34:12,059 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 01:34:12,060 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:34:12,061 : INFO : built Dictionary(147 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 532 corpus positions)\n",
      "2021-01-15 01:34:12,131 : INFO : token count processed\n",
      "2021-01-15 01:34:12,164 : INFO : frequencies processed\n",
      "2021-01-15 01:34:21,698 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:34:21,699 : INFO : entropies processed\n",
      "2021-01-15 01:34:21,700 : INFO : extropies processed\n",
      "2021-01-15 01:34:21,711 : INFO : token count processed\n",
      "2021-01-15 01:34:21,716 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:34:21,720 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:34:21,721 : INFO : vocab #32006\n",
      "2021-01-15 01:34:21,728 : INFO : diff #set()\n",
      "2021-01-15 01:34:40,539 : INFO : alphabet #32006\n",
      "2021-01-15 01:34:49,984 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.2374267297547894, 0.4469420100785133], [0.9155380055308342, 0.084461994], [1.0, 1.0], [4.392747410448784, 6.0479231618016716, 6.290440379269414, 4.1502301929810415, 1.89769296882063, 0.2425172174677428]]\n",
      "2021-01-15 01:34:49,988 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:34:49,989 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:34:49,990 : INFO : built Dictionary(148 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 651 corpus positions)\n",
      "2021-01-15 01:34:50,062 : INFO : token count processed\n",
      "2021-01-15 01:34:50,121 : INFO : frequencies processed\n",
      "2021-01-15 01:34:59,690 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:34:59,691 : INFO : entropies processed\n",
      "2021-01-15 01:34:59,692 : INFO : extropies processed\n",
      "2021-01-15 01:34:59,699 : INFO : token count processed\n",
      "2021-01-15 01:34:59,703 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:34:59,708 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:34:59,709 : INFO : vocab #32006\n",
      "2021-01-15 01:34:59,715 : INFO : diff #set()\n",
      "2021-01-15 01:35:18,550 : INFO : alphabet #32006\n",
      "2021-01-15 01:35:28,098 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.236104913012382, 0.447206208519458], [0.9189421832561493, 0.08105782], [1.584962500721156, 1.1699250014423124], [4.392747410448784, 6.036583168403119, 6.241264369308119, 4.188066209543784, 1.848516958859335, 0.20468120090500008]]\n",
      "2021-01-15 01:35:28,115 : INFO : Removed 1 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:35:28,116 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:35:28,123 : INFO : built Dictionary(558 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 7009 corpus positions)\n",
      "2021-01-15 01:35:28,533 : INFO : token count processed\n",
      "2021-01-15 01:35:28,580 : INFO : frequencies processed\n",
      "2021-01-15 01:35:37,998 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:35:37,999 : INFO : entropies processed\n",
      "2021-01-15 01:35:38,000 : INFO : extropies processed\n",
      "2021-01-15 01:35:38,009 : INFO : token count processed\n",
      "2021-01-15 01:35:38,016 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:35:38,020 : INFO : alphabet_target #32010\n",
      "2021-01-15 01:35:38,021 : INFO : vocab #32006\n",
      "2021-01-15 01:35:38,029 : INFO : diff #set()\n",
      "2021-01-15 01:35:57,028 : INFO : alphabet #32006\n",
      "2021-01-15 01:36:06,452 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.1893641014866123, 0.4567536296593995], [0.8529747426509857, 0.14702526], [2.9219280948873623, 1.3359016564230495], [4.392747410448784, 7.29352035514053, 7.452623043173421, 4.233644722415892, 3.059875632724637, 0.15910268803289096]]\n",
      "2021-01-15 01:36:06,459 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:36:06,460 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:36:06,462 : INFO : built Dictionary(369 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 3252 corpus positions)\n",
      "2021-01-15 01:36:06,678 : INFO : token count processed\n",
      "2021-01-15 01:36:06,710 : INFO : frequencies processed\n",
      "2021-01-15 01:36:16,112 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:36:16,113 : INFO : entropies processed\n",
      "2021-01-15 01:36:16,114 : INFO : extropies processed\n",
      "2021-01-15 01:36:16,122 : INFO : token count processed\n",
      "2021-01-15 01:36:16,126 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:36:16,130 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:36:16,131 : INFO : vocab #32006\n",
      "2021-01-15 01:36:16,139 : INFO : diff #set()\n",
      "2021-01-15 01:36:35,252 : INFO : alphabet #32006\n",
      "2021-01-15 01:36:44,660 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.2285347008837924, 0.44872534387883656], [0.8915758579969406, 0.10842414], [2.521640636343318, 1.2998438251349491], [4.392747410448784, 6.8153433747477745, 6.963618154088085, 4.244472631108474, 2.5708707436393006, 0.1482747793403103]]\n",
      "2021-01-15 01:36:44,663 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:36:44,664 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:36:44,665 : INFO : built Dictionary(124 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 273 corpus positions)\n",
      "2021-01-15 01:36:44,718 : INFO : token count processed\n",
      "2021-01-15 01:36:44,749 : INFO : frequencies processed\n",
      "2021-01-15 01:36:54,171 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:36:54,172 : INFO : entropies processed\n",
      "2021-01-15 01:36:54,173 : INFO : extropies processed\n",
      "2021-01-15 01:36:54,180 : INFO : token count processed\n",
      "2021-01-15 01:36:54,184 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:36:54,188 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:36:54,189 : INFO : vocab #32006\n",
      "2021-01-15 01:36:54,196 : INFO : diff #set()\n",
      "2021-01-15 01:37:13,455 : INFO : alphabet #32006\n",
      "2021-01-15 01:37:22,861 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.243912626507726, 0.445650150628339], [0.915109671652317, 0.08489033], [1.0, 1.0], [4.392747410448784, 6.150121915859574, 6.418844468741799, 4.12402485756656, 2.026097058293015, 0.2687225528822248]]\n",
      "2021-01-15 01:37:22,865 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:37:22,866 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:37:22,868 : INFO : built Dictionary(284 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 984 corpus positions)\n",
      "2021-01-15 01:37:23,012 : INFO : token count processed\n",
      "2021-01-15 01:37:23,043 : INFO : frequencies processed\n",
      "2021-01-15 01:37:32,476 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:37:32,477 : INFO : entropies processed\n",
      "2021-01-15 01:37:32,478 : INFO : extropies processed\n",
      "2021-01-15 01:37:32,492 : INFO : token count processed\n",
      "2021-01-15 01:37:32,497 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:37:32,501 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:37:32,502 : INFO : vocab #32006\n",
      "2021-01-15 01:37:32,509 : INFO : diff #set()\n",
      "2021-01-15 01:37:51,540 : INFO : alphabet #32006\n",
      "2021-01-15 01:38:00,990 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.1552603876980552, 0.46398106034327424], [0.7925067096948624, 0.20749329], [2.725480556997868, 1.3192201298976014], [4.392747410448784, 7.0391145208191315, 7.278622873928475, 4.153239057339441, 2.8858754634796906, 0.23950835310934337]]\n",
      "2021-01-15 01:38:00,999 : INFO : Removed 1 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:38:01,000 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:38:01,003 : INFO : built Dictionary(582 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 4359 corpus positions)\n",
      "2021-01-15 01:38:01,465 : INFO : token count processed\n",
      "2021-01-15 01:38:01,500 : INFO : frequencies processed\n",
      "2021-01-15 01:38:11,031 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:38:11,032 : INFO : entropies processed\n",
      "2021-01-15 01:38:11,033 : INFO : extropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 01:38:11,041 : INFO : token count processed\n",
      "2021-01-15 01:38:11,045 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:38:11,050 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:38:11,051 : INFO : vocab #32006\n",
      "2021-01-15 01:38:11,060 : INFO : diff #set()\n",
      "2021-01-15 01:38:29,968 : INFO : alphabet #32006\n",
      "2021-01-15 01:38:39,304 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.1774604976087175, 0.45925058162855203], [0.79752017557621, 0.20247982], [3.7254805569978675, 1.3829870736079668], [4.392747410448784, 7.482466367279176, 7.746143708430956, 4.129070069297005, 3.3533962979821714, 0.2636773411517801]]\n",
      "2021-01-15 01:38:39,308 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:38:39,308 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:38:39,310 : INFO : built Dictionary(172 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 581 corpus positions)\n",
      "2021-01-15 01:38:39,393 : INFO : token count processed\n",
      "2021-01-15 01:38:39,427 : INFO : frequencies processed\n",
      "2021-01-15 01:38:48,953 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:38:48,954 : INFO : entropies processed\n",
      "2021-01-15 01:38:48,955 : INFO : extropies processed\n",
      "2021-01-15 01:38:48,962 : INFO : token count processed\n",
      "2021-01-15 01:38:48,970 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:38:48,974 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:38:48,975 : INFO : vocab #32006\n",
      "2021-01-15 01:38:48,982 : INFO : diff #set()\n",
      "2021-01-15 01:39:07,890 : INFO : alphabet #32006\n",
      "2021-01-15 01:39:17,322 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.23114645118952, 0.4482000719705589], [0.902298666536808, 0.09770133], [1.584962500721156, 1.1699250014423124], [4.392747410448784, 6.372162341197667, 6.6058699420656515, 4.1590398095808, 2.2131225316168672, 0.2337076008679846]]\n",
      "2021-01-15 01:39:17,327 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:39:17,328 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:39:17,330 : INFO : built Dictionary(317 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 2002 corpus positions)\n",
      "2021-01-15 01:39:17,503 : INFO : token count processed\n",
      "2021-01-15 01:39:17,536 : INFO : frequencies processed\n",
      "2021-01-15 01:39:27,054 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:39:27,055 : INFO : entropies processed\n",
      "2021-01-15 01:39:27,056 : INFO : extropies processed\n",
      "2021-01-15 01:39:27,064 : INFO : token count processed\n",
      "2021-01-15 01:39:27,071 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:39:27,075 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:39:27,076 : INFO : vocab #32006\n",
      "2021-01-15 01:39:27,083 : INFO : diff #set()\n",
      "2021-01-15 01:39:46,151 : INFO : alphabet #32006\n",
      "2021-01-15 01:39:55,585 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.2293698865794227, 0.44855723853627755], [0.892133317887783, 0.10786668], [2.75, 1.3226647836567116], [4.392747410448784, 6.798155919669889, 6.991831905896246, 4.199071424222426, 2.599084495447462, 0.1936759862263573]]\n",
      "2021-01-15 01:39:55,589 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:39:55,590 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:39:55,591 : INFO : built Dictionary(176 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 718 corpus positions)\n",
      "2021-01-15 01:39:55,673 : INFO : token count processed\n",
      "2021-01-15 01:39:55,705 : INFO : frequencies processed\n",
      "2021-01-15 01:40:05,230 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:40:05,231 : INFO : entropies processed\n",
      "2021-01-15 01:40:05,232 : INFO : extropies processed\n",
      "2021-01-15 01:40:05,239 : INFO : token count processed\n",
      "2021-01-15 01:40:05,246 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:40:05,251 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:40:05,252 : INFO : vocab #32006\n",
      "2021-01-15 01:40:05,258 : INFO : diff #set()\n",
      "2021-01-15 01:40:24,517 : INFO : alphabet #32006\n",
      "2021-01-15 01:40:34,061 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.2095944430632282, 0.45257173918923765], [0.8889142572879791, 0.11108574], [2.0, 1.2451124978365313], [4.392747410448784, 6.271631856729336, 6.485793933825135, 4.178585333352985, 2.0930465233763504, 0.21416207709579904]]\n",
      "2021-01-15 01:40:34,068 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:40:34,069 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:40:34,071 : INFO : built Dictionary(350 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 3233 corpus positions)\n",
      "2021-01-15 01:40:34,300 : INFO : token count processed\n",
      "2021-01-15 01:40:34,335 : INFO : frequencies processed\n",
      "2021-01-15 01:40:43,752 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:40:43,753 : INFO : entropies processed\n",
      "2021-01-15 01:40:43,754 : INFO : extropies processed\n",
      "2021-01-15 01:40:43,762 : INFO : token count processed\n",
      "2021-01-15 01:40:43,770 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:40:43,774 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:40:43,775 : INFO : vocab #32006\n",
      "2021-01-15 01:40:43,782 : INFO : diff #set()\n",
      "2021-01-15 01:41:02,786 : INFO : alphabet #32006\n",
      "2021-01-15 01:41:12,212 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.2200647511520182, 0.4504373124617595], [0.87104332447052, 0.12895668], [2.9139770731827523, 1.3356231683419404], [4.392747410448784, 6.873598627629562, 7.028602429218358, 4.237743608859988, 2.6358550187695737, 0.155003801588796]]\n",
      "2021-01-15 01:41:12,215 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:41:12,216 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:41:12,217 : INFO : built Dictionary(101 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 181 corpus positions)\n",
      "2021-01-15 01:41:12,260 : INFO : token count processed\n",
      "2021-01-15 01:41:12,292 : INFO : frequencies processed\n",
      "2021-01-15 01:41:21,682 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:41:21,683 : INFO : entropies processed\n",
      "2021-01-15 01:41:21,684 : INFO : extropies processed\n",
      "2021-01-15 01:41:21,691 : INFO : token count processed\n",
      "2021-01-15 01:41:21,699 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:41:21,703 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:41:21,704 : INFO : vocab #32006\n",
      "2021-01-15 01:41:21,711 : INFO : diff #set()\n",
      "2021-01-15 01:41:40,701 : INFO : alphabet #32006\n",
      "2021-01-15 01:41:50,121 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.1432668687936445, 0.46657745452056476], [0.8206644356250763, 0.17933556], [2.521640636343318, 1.2998438251349491], [4.392747410448784, 6.049830202851529, 6.343389390131784, 4.09918822316853, 1.9506419796829997, 0.29355918728025454]]\n",
      "2021-01-15 01:41:50,126 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:41:50,127 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:41:50,128 : INFO : built Dictionary(250 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 1261 corpus positions)\n",
      "2021-01-15 01:41:50,271 : INFO : token count processed\n",
      "2021-01-15 01:41:50,304 : INFO : frequencies processed\n",
      "2021-01-15 01:41:59,716 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:41:59,717 : INFO : entropies processed\n",
      "2021-01-15 01:41:59,718 : INFO : extropies processed\n",
      "2021-01-15 01:41:59,725 : INFO : token count processed\n",
      "2021-01-15 01:41:59,731 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:41:59,738 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:41:59,739 : INFO : vocab #32006\n",
      "2021-01-15 01:41:59,746 : INFO : diff #set()\n",
      "2021-01-15 01:42:19,136 : INFO : alphabet #32006\n",
      "2021-01-15 01:42:28,561 : INFO : Computed distances or similarities ('267', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.2176810320691502, 0.45092147407103705], [0.8621931970119476, 0.1378068], [2.235926350629032, 1.2653331222512112], [4.392747410448784, 6.778844940588858, 6.984534952465548, 4.187057398572094, 2.591787542016764, 0.20569001187669045]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 01:42:28,565 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:42:28,566 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:42:28,567 : INFO : built Dictionary(147 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 294 corpus positions)\n",
      "2021-01-15 01:42:28,642 : INFO : token count processed\n",
      "2021-01-15 01:42:28,677 : INFO : frequencies processed\n",
      "2021-01-15 01:42:38,075 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:42:38,076 : INFO : entropies processed\n",
      "2021-01-15 01:42:38,077 : INFO : extropies processed\n",
      "2021-01-15 01:42:38,084 : INFO : token count processed\n",
      "2021-01-15 01:42:38,090 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:42:38,095 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:42:38,096 : INFO : vocab #32006\n",
      "2021-01-15 01:42:38,104 : INFO : diff #set()\n",
      "2021-01-15 01:42:56,993 : INFO : alphabet #32006\n",
      "2021-01-15 01:43:06,570 : INFO : Computed distances or similarities ('267', 'sacp-python-common/setup.py')[[1.1820920522795553, 0.4582758087383779], [0.8369676023721695, 0.1630324], [2.75, 1.3226647836567116], [4.392747410448784, 6.469677430851302, 6.750273926654785, 4.1121509146453015, 2.3575265162060006, 0.2805964958034828]]\n",
      "2021-01-15 01:43:06,575 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:43:06,576 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:43:06,577 : INFO : built Dictionary(219 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 1196 corpus positions)\n",
      "2021-01-15 01:43:06,684 : INFO : token count processed\n",
      "2021-01-15 01:43:06,716 : INFO : frequencies processed\n",
      "2021-01-15 01:43:16,125 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:43:16,126 : INFO : entropies processed\n",
      "2021-01-15 01:43:16,127 : INFO : extropies processed\n",
      "2021-01-15 01:43:16,134 : INFO : token count processed\n",
      "2021-01-15 01:43:16,142 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:43:16,147 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:43:16,148 : INFO : vocab #32006\n",
      "2021-01-15 01:43:16,155 : INFO : diff #set()\n",
      "2021-01-15 01:43:35,176 : INFO : alphabet #32006\n",
      "2021-01-15 01:43:44,612 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.1950498157014133, 0.45557052639393375], [0.8619751185178757, 0.13802488], [2.2516291673878226, 1.2667563532600834], [4.392747410448784, 6.459180448028249, 6.644566142596375, 4.207361715880658, 2.2518187321475907, 0.18538569456812581]]\n",
      "2021-01-15 01:43:44,616 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:43:44,617 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:43:44,618 : INFO : built Dictionary(128 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 346 corpus positions)\n",
      "2021-01-15 01:43:44,680 : INFO : token count processed\n",
      "2021-01-15 01:43:44,744 : INFO : frequencies processed\n",
      "2021-01-15 01:43:54,283 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:43:54,284 : INFO : entropies processed\n",
      "2021-01-15 01:43:54,285 : INFO : extropies processed\n",
      "2021-01-15 01:43:54,292 : INFO : token count processed\n",
      "2021-01-15 01:43:54,296 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:43:54,300 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:43:54,301 : INFO : vocab #32006\n",
      "2021-01-15 01:43:54,308 : INFO : diff #set()\n",
      "2021-01-15 01:44:13,395 : INFO : alphabet #32006\n",
      "2021-01-15 01:44:22,828 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.2081389016833017, 0.4528700614067725], [0.8814654052257538, 0.118534595], [2.584962500721156, 1.315172029168969], [4.392747410448784, 6.097125733496388, 6.345335201549952, 4.144537942395219, 1.952587791101168, 0.24820946805356403]]\n",
      "2021-01-15 01:44:22,831 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:44:22,832 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:44:22,833 : INFO : built Dictionary(121 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 342 corpus positions)\n",
      "2021-01-15 01:44:22,893 : INFO : token count processed\n",
      "2021-01-15 01:44:22,961 : INFO : frequencies processed\n",
      "2021-01-15 01:44:32,879 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:44:32,880 : INFO : entropies processed\n",
      "2021-01-15 01:44:32,881 : INFO : extropies processed\n",
      "2021-01-15 01:44:32,888 : INFO : token count processed\n",
      "2021-01-15 01:44:32,893 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:44:32,899 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:44:32,900 : INFO : vocab #32006\n",
      "2021-01-15 01:44:32,907 : INFO : diff #set()\n",
      "2021-01-15 01:44:52,045 : INFO : alphabet #32006\n",
      "2021-01-15 01:45:01,600 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.2157438177682682, 0.45131571257511877], [0.8884678557515144, 0.111532144], [2.321928094887362, 1.2877123795494492], [4.392747410448784, 6.0695858597523715, 6.319593241713899, 4.142740028487258, 1.9268458312651147, 0.2500073819615274]]\n",
      "2021-01-15 01:45:01,604 : INFO : Removed 1 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:45:01,605 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:45:01,609 : INFO : built Dictionary(123 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 438 corpus positions)\n",
      "2021-01-15 01:45:01,668 : INFO : token count processed\n",
      "2021-01-15 01:45:01,700 : INFO : frequencies processed\n",
      "2021-01-15 01:45:11,104 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:45:11,105 : INFO : entropies processed\n",
      "2021-01-15 01:45:11,106 : INFO : extropies processed\n",
      "2021-01-15 01:45:11,113 : INFO : token count processed\n",
      "2021-01-15 01:45:11,119 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:45:11,124 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:45:11,125 : INFO : vocab #32006\n",
      "2021-01-15 01:45:11,133 : INFO : diff #set()\n",
      "2021-01-15 01:45:30,037 : INFO : alphabet #32006\n",
      "2021-01-15 01:45:39,598 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.1846298686005263, 0.45774344403731876], [0.856643944978714, 0.14335606], [2.2516291673878226, 1.2667563532600834], [4.392747410448784, 6.104787343210121, 6.301022529030484, 4.19651222462842, 1.9082751185816997, 0.19623518582036326]]\n",
      "2021-01-15 01:45:39,615 : INFO : Removed 1 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:45:39,616 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:45:39,624 : INFO : built Dictionary(406 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 9123 corpus positions)\n",
      "2021-01-15 01:45:39,885 : INFO : token count processed\n",
      "2021-01-15 01:45:39,954 : INFO : frequencies processed\n",
      "2021-01-15 01:45:49,375 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:45:49,376 : INFO : entropies processed\n",
      "2021-01-15 01:45:49,377 : INFO : extropies processed\n",
      "2021-01-15 01:45:49,396 : INFO : token count processed\n",
      "2021-01-15 01:45:49,400 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:45:49,404 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:45:49,405 : INFO : vocab #32006\n",
      "2021-01-15 01:45:49,412 : INFO : diff #set()\n",
      "2021-01-15 01:46:08,323 : INFO : alphabet #32006\n",
      "2021-01-15 01:46:17,873 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.180061207463767, 0.4587027174174512], [0.8385405242443085, 0.16145948], [3.2516291673878226, 1.3589504783379556], [4.392747410448784, 6.89087415148015, 7.04843147839984, 4.235190083529096, 2.6556840679510554, 0.15755732691968927]]\n",
      "2021-01-15 01:46:17,880 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:46:17,881 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:46:17,882 : INFO : built Dictionary(265 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 2283 corpus positions)\n",
      "2021-01-15 01:46:18,025 : INFO : token count processed\n",
      "2021-01-15 01:46:18,058 : INFO : frequencies processed\n",
      "2021-01-15 01:46:27,477 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:46:27,478 : INFO : entropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 01:46:27,479 : INFO : extropies processed\n",
      "2021-01-15 01:46:27,486 : INFO : token count processed\n",
      "2021-01-15 01:46:27,494 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:46:27,499 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:46:27,500 : INFO : vocab #32006\n",
      "2021-01-15 01:46:27,507 : INFO : diff #set()\n",
      "2021-01-15 01:46:46,509 : INFO : alphabet #32006\n",
      "2021-01-15 01:46:55,738 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.1640140229917053, 0.4621042143791288], [0.8129287362098694, 0.18707126], [3.121928094887362, 1.3519647487142497], [4.392747410448784, 6.655493573668506, 6.797946104869032, 4.250294879248258, 2.405198694420248, 0.1424525312005258]]\n",
      "2021-01-15 01:46:55,743 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:46:55,744 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:46:55,745 : INFO : built Dictionary(245 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 1290 corpus positions)\n",
      "2021-01-15 01:46:55,864 : INFO : token count processed\n",
      "2021-01-15 01:46:55,905 : INFO : frequencies processed\n",
      "2021-01-15 01:47:05,346 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:47:05,347 : INFO : entropies processed\n",
      "2021-01-15 01:47:05,348 : INFO : extropies processed\n",
      "2021-01-15 01:47:05,362 : INFO : token count processed\n",
      "2021-01-15 01:47:05,366 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:47:05,371 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:47:05,372 : INFO : vocab #32006\n",
      "2021-01-15 01:47:05,379 : INFO : diff #set()\n",
      "2021-01-15 01:47:24,358 : INFO : alphabet #32006\n",
      "2021-01-15 01:47:33,774 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.2103155872008369, 0.4524240817875283], [0.8729683309793472, 0.12703167], [2.521640636343318, 1.2998438251349493], [4.392747410448784, 6.6236746347295465, 6.763556857076231, 4.2528651881021, 2.3708094466274465, 0.13988222234668424]]\n",
      "2021-01-15 01:47:33,779 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:47:33,780 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:47:33,784 : INFO : built Dictionary(243 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 1229 corpus positions)\n",
      "2021-01-15 01:47:33,927 : INFO : token count processed\n",
      "2021-01-15 01:47:33,997 : INFO : frequencies processed\n",
      "2021-01-15 01:47:43,406 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:47:43,407 : INFO : entropies processed\n",
      "2021-01-15 01:47:43,408 : INFO : extropies processed\n",
      "2021-01-15 01:47:43,415 : INFO : token count processed\n",
      "2021-01-15 01:47:43,420 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:47:43,425 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:47:43,426 : INFO : vocab #32006\n",
      "2021-01-15 01:47:43,433 : INFO : diff #set()\n",
      "2021-01-15 01:48:02,443 : INFO : alphabet #32006\n",
      "2021-01-15 01:48:11,874 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.1130846260806084, 0.4732418132513793], [0.7684820294380188, 0.23151797], [3.121928094887362, 1.3519647487142497], [4.392747410448784, 6.75472436518627, 6.877469212123129, 4.270002563511925, 2.484721801674345, 0.12274484693685928]]\n",
      "2021-01-15 01:48:11,878 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:48:11,879 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:48:11,880 : INFO : built Dictionary(198 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 1024 corpus positions)\n",
      "2021-01-15 01:48:11,985 : INFO : token count processed\n",
      "2021-01-15 01:48:12,017 : INFO : frequencies processed\n",
      "2021-01-15 01:48:21,424 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:48:21,425 : INFO : entropies processed\n",
      "2021-01-15 01:48:21,426 : INFO : extropies processed\n",
      "2021-01-15 01:48:21,433 : INFO : token count processed\n",
      "2021-01-15 01:48:21,437 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:48:21,441 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:48:21,442 : INFO : vocab #32006\n",
      "2021-01-15 01:48:21,452 : INFO : diff #set()\n",
      "2021-01-15 01:48:40,452 : INFO : alphabet #32006\n",
      "2021-01-15 01:48:49,884 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.1662176628754641, 0.46163412714149304], [0.816237673163414, 0.18376233], [2.584962500721156, 1.315172029168969], [4.392747410448784, 6.597313085495733, 6.727760673779509, 4.262299822165009, 2.3350132633307243, 0.13044758828377567]]\n",
      "2021-01-15 01:48:49,889 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:48:49,890 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:48:49,891 : INFO : built Dictionary(224 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 990 corpus positions)\n",
      "2021-01-15 01:48:50,003 : INFO : token count processed\n",
      "2021-01-15 01:48:50,069 : INFO : frequencies processed\n",
      "2021-01-15 01:48:59,604 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:48:59,605 : INFO : entropies processed\n",
      "2021-01-15 01:48:59,606 : INFO : extropies processed\n",
      "2021-01-15 01:48:59,620 : INFO : token count processed\n",
      "2021-01-15 01:48:59,625 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:48:59,629 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:48:59,630 : INFO : vocab #32006\n",
      "2021-01-15 01:48:59,640 : INFO : diff #set()\n",
      "2021-01-15 01:49:18,512 : INFO : alphabet #32006\n",
      "2021-01-15 01:49:28,063 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.2021574021445214, 0.45410014698593865], [0.8601872026920319, 0.1398128], [2.2516291673878226, 1.2667563532600834], [4.392747410448784, 6.659481538516613, 6.812205796300063, 4.240023152665334, 2.4194583858512786, 0.15272425778344978]]\n",
      "2021-01-15 01:49:28,067 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:49:28,068 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:49:28,071 : INFO : built Dictionary(248 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 1152 corpus positions)\n",
      "2021-01-15 01:49:28,210 : INFO : token count processed\n",
      "2021-01-15 01:49:28,280 : INFO : frequencies processed\n",
      "2021-01-15 01:49:37,692 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:49:37,693 : INFO : entropies processed\n",
      "2021-01-15 01:49:37,694 : INFO : extropies processed\n",
      "2021-01-15 01:49:37,707 : INFO : token count processed\n",
      "2021-01-15 01:49:37,712 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:49:37,717 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:49:37,718 : INFO : vocab #32006\n",
      "2021-01-15 01:49:37,725 : INFO : diff #set()\n",
      "2021-01-15 01:49:56,598 : INFO : alphabet #32006\n",
      "2021-01-15 01:50:06,151 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.1930462115722578, 0.45598674333591505], [0.8563361167907715, 0.14366388], [2.584962500721156, 1.315172029168969], [4.392747410448784, 6.774682571479102, 6.895792314704954, 4.271637667222931, 2.5030449042561695, 0.1211097432258521]]\n",
      "2021-01-15 01:50:06,165 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:50:06,166 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:50:06,173 : INFO : built Dictionary(422 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 7889 corpus positions)\n",
      "2021-01-15 01:50:06,455 : INFO : token count processed\n",
      "2021-01-15 01:50:06,520 : INFO : frequencies processed\n",
      "2021-01-15 01:50:15,939 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:50:15,940 : INFO : entropies processed\n",
      "2021-01-15 01:50:15,940 : INFO : extropies processed\n",
      "2021-01-15 01:50:15,950 : INFO : token count processed\n",
      "2021-01-15 01:50:15,954 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:50:15,958 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:50:15,959 : INFO : vocab #32006\n",
      "2021-01-15 01:50:15,967 : INFO : diff #set()\n",
      "2021-01-15 01:50:34,838 : INFO : alphabet #32006\n",
      "2021-01-15 01:50:44,399 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.1829050940138963, 0.4581051199808296], [0.8398466408252716, 0.16015336], [3.2516291673878226, 1.3589504783379556], [4.392747410448784, 6.839453716525233, 6.988003519004885, 4.244197607969132, 2.595256108556101, 0.14854980247965255]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 01:50:44,405 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:50:44,406 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:50:44,408 : INFO : built Dictionary(326 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 2331 corpus positions)\n",
      "2021-01-15 01:50:44,610 : INFO : token count processed\n",
      "2021-01-15 01:50:44,657 : INFO : frequencies processed\n",
      "2021-01-15 01:50:54,114 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:50:54,115 : INFO : entropies processed\n",
      "2021-01-15 01:50:54,116 : INFO : extropies processed\n",
      "2021-01-15 01:50:54,124 : INFO : token count processed\n",
      "2021-01-15 01:50:54,131 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:50:54,136 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:50:54,137 : INFO : vocab #32006\n",
      "2021-01-15 01:50:54,144 : INFO : diff #set()\n",
      "2021-01-15 01:51:12,934 : INFO : alphabet #32006\n",
      "2021-01-15 01:51:22,496 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.1728890970429504, 0.4602167691673192], [0.8130490183830261, 0.18695098], [2.5, 1.2968140217166515], [4.392747410448784, 6.86432793886027, 6.98435877024469, 4.272716579064365, 2.5916113597959054, 0.12003083138441983]]\n",
      "2021-01-15 01:51:22,500 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:51:22,501 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:51:22,502 : INFO : built Dictionary(161 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 533 corpus positions)\n",
      "2021-01-15 01:51:22,577 : INFO : token count processed\n",
      "2021-01-15 01:51:22,608 : INFO : frequencies processed\n",
      "2021-01-15 01:51:32,030 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:51:32,032 : INFO : entropies processed\n",
      "2021-01-15 01:51:32,033 : INFO : extropies processed\n",
      "2021-01-15 01:51:32,039 : INFO : token count processed\n",
      "2021-01-15 01:51:32,044 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:51:32,048 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:51:32,049 : INFO : vocab #32006\n",
      "2021-01-15 01:51:32,056 : INFO : diff #set()\n",
      "2021-01-15 01:51:51,356 : INFO : alphabet #32006\n",
      "2021-01-15 01:52:00,791 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.193387168522852, 0.4559158612537408], [0.8239713758230209, 0.17602862], [1.9219280948873623, 1.2148067842293933], [4.392747410448784, 6.431978396403875, 6.613611595798754, 4.211114211053904, 2.2208641853499698, 0.18163319939487899]]\n",
      "2021-01-15 01:52:00,795 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:52:00,796 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:52:00,797 : INFO : built Dictionary(218 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 687 corpus positions)\n",
      "2021-01-15 01:52:00,906 : INFO : token count processed\n",
      "2021-01-15 01:52:00,939 : INFO : frequencies processed\n",
      "2021-01-15 01:52:10,360 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:52:10,361 : INFO : entropies processed\n",
      "2021-01-15 01:52:10,362 : INFO : extropies processed\n",
      "2021-01-15 01:52:10,369 : INFO : token count processed\n",
      "2021-01-15 01:52:10,375 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:52:10,380 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:52:10,381 : INFO : vocab #32006\n",
      "2021-01-15 01:52:10,388 : INFO : diff #set()\n",
      "2021-01-15 01:52:29,402 : INFO : alphabet #32006\n",
      "2021-01-15 01:52:38,827 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/test_auth_utility.py')[[1.2222968690565004, 0.44998488452380375], [0.8637145608663559, 0.13628544], [2.5, 1.2968140217166515], [4.392747410448784, 6.911818353685893, 7.060332608796839, 4.244233155337838, 2.667585198348055, 0.14851425511094618]]\n",
      "2021-01-15 01:52:38,840 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:52:38,841 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:52:38,844 : INFO : built Dictionary(311 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 7217 corpus positions)\n",
      "2021-01-15 01:52:39,033 : INFO : token count processed\n",
      "2021-01-15 01:52:39,104 : INFO : frequencies processed\n",
      "2021-01-15 01:52:48,520 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:52:48,521 : INFO : entropies processed\n",
      "2021-01-15 01:52:48,522 : INFO : extropies processed\n",
      "2021-01-15 01:52:48,536 : INFO : token count processed\n",
      "2021-01-15 01:52:48,541 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:52:48,545 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:52:48,546 : INFO : vocab #32006\n",
      "2021-01-15 01:52:48,553 : INFO : diff #set()\n",
      "2021-01-15 01:53:07,532 : INFO : alphabet #32006\n",
      "2021-01-15 01:53:16,956 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.2388527401489913, 0.44665733572698124], [0.8985373601317406, 0.10146264], [2.0, 1.2451124978365313], [4.392747410448784, 6.363791471162389, 6.4049167114097205, 4.351622170201454, 2.0121693009609363, 0.04112524024733144]]\n",
      "2021-01-15 01:53:16,961 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:53:16,962 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:53:16,963 : INFO : built Dictionary(213 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 1287 corpus positions)\n",
      "2021-01-15 01:53:17,074 : INFO : token count processed\n",
      "2021-01-15 01:53:17,133 : INFO : frequencies processed\n",
      "2021-01-15 01:53:26,542 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:53:26,543 : INFO : entropies processed\n",
      "2021-01-15 01:53:26,544 : INFO : extropies processed\n",
      "2021-01-15 01:53:26,551 : INFO : token count processed\n",
      "2021-01-15 01:53:26,556 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:53:26,561 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:53:26,562 : INFO : vocab #32006\n",
      "2021-01-15 01:53:26,569 : INFO : diff #set()\n",
      "2021-01-15 01:53:45,558 : INFO : alphabet #32006\n",
      "2021-01-15 01:53:55,373 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.2408779326282717, 0.4462536693496393], [0.8934901878237724, 0.10650981], [1.5, 1.1225562489182657], [4.392747410448784, 6.29000629755059, 6.530392738856159, 4.152360969143215, 2.137645328407375, 0.24038644130556897]]\n",
      "2021-01-15 01:53:55,378 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:53:55,379 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:53:55,380 : INFO : built Dictionary(221 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 1240 corpus positions)\n",
      "2021-01-15 01:53:55,494 : INFO : token count processed\n",
      "2021-01-15 01:53:55,553 : INFO : frequencies processed\n",
      "2021-01-15 01:54:05,106 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:54:05,107 : INFO : entropies processed\n",
      "2021-01-15 01:54:05,108 : INFO : extropies processed\n",
      "2021-01-15 01:54:05,115 : INFO : token count processed\n",
      "2021-01-15 01:54:05,121 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:54:05,126 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:54:05,127 : INFO : vocab #32006\n",
      "2021-01-15 01:54:05,134 : INFO : diff #set()\n",
      "2021-01-15 01:54:24,041 : INFO : alphabet #32006\n",
      "2021-01-15 01:54:33,627 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.1989663297541182, 0.45475912317030204], [0.8590493500232697, 0.14095065], [2.2516291673878226, 1.2667563532600834], [4.392747410448784, 6.361621244785958, 6.578710287031954, 4.175658368202789, 2.1859628765831696, 0.21708904224599568]]\n",
      "2021-01-15 01:54:33,632 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:54:33,633 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:54:33,634 : INFO : built Dictionary(230 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 1263 corpus positions)\n",
      "2021-01-15 01:54:33,755 : INFO : token count processed\n",
      "2021-01-15 01:54:33,787 : INFO : frequencies processed\n",
      "2021-01-15 01:54:43,320 : INFO : scalar_distribution processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 01:54:43,321 : INFO : entropies processed\n",
      "2021-01-15 01:54:43,322 : INFO : extropies processed\n",
      "2021-01-15 01:54:43,329 : INFO : token count processed\n",
      "2021-01-15 01:54:43,337 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:54:43,341 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:54:43,342 : INFO : vocab #32006\n",
      "2021-01-15 01:54:43,350 : INFO : diff #set()\n",
      "2021-01-15 01:55:02,255 : INFO : alphabet #32006\n",
      "2021-01-15 01:55:11,817 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.17772243827096, 0.45919534208132007], [0.7971308529376984, 0.20286915], [1.9219280948873623, 1.2148067842293933], [4.392747410448784, 6.620594433343389, 6.842149088407831, 4.171192755384341, 2.449401677959047, 0.22155465506444205]]\n",
      "2021-01-15 01:55:11,822 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:55:11,823 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:55:11,824 : INFO : built Dictionary(201 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 1356 corpus positions)\n",
      "2021-01-15 01:55:11,929 : INFO : token count processed\n",
      "2021-01-15 01:55:11,963 : INFO : frequencies processed\n",
      "2021-01-15 01:55:21,261 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:55:21,262 : INFO : entropies processed\n",
      "2021-01-15 01:55:21,263 : INFO : extropies processed\n",
      "2021-01-15 01:55:21,277 : INFO : token count processed\n",
      "2021-01-15 01:55:21,281 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:55:21,285 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:55:21,286 : INFO : vocab #32006\n",
      "2021-01-15 01:55:21,293 : INFO : diff #set()\n",
      "2021-01-15 01:55:40,144 : INFO : alphabet #32006\n",
      "2021-01-15 01:55:49,680 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.1433356339698444, 0.4665624851987459], [0.7801631540060043, 0.21983685], [2.2516291673878226, 1.2667563532600834], [4.392747410448784, 6.207411496248084, 6.344833095757894, 4.2553258109389756, 1.9520856853091093, 0.1374215995098096]]\n",
      "2021-01-15 01:55:49,683 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:55:49,684 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:55:49,685 : INFO : built Dictionary(144 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 260 corpus positions)\n",
      "2021-01-15 01:55:49,753 : INFO : token count processed\n",
      "2021-01-15 01:55:49,787 : INFO : frequencies processed\n",
      "2021-01-15 01:55:59,211 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:55:59,212 : INFO : entropies processed\n",
      "2021-01-15 01:55:59,213 : INFO : extropies processed\n",
      "2021-01-15 01:55:59,220 : INFO : token count processed\n",
      "2021-01-15 01:55:59,227 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:55:59,231 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:55:59,232 : INFO : vocab #32006\n",
      "2021-01-15 01:55:59,239 : INFO : diff #set()\n",
      "2021-01-15 01:56:18,203 : INFO : alphabet #32006\n",
      "2021-01-15 01:56:27,631 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.2049525733361546, 0.45352449394726535], [0.8092962205410004, 0.19070378], [0.9182958340544896, 0.9182958340544896], [4.392747410448784, 6.5805228788529595, 6.825435152255526, 4.1478351370462185, 2.432687741806742, 0.2449122734025666]]\n",
      "2021-01-15 01:56:27,636 : INFO : Removed 1 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:56:27,637 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:56:27,638 : INFO : built Dictionary(224 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 1221 corpus positions)\n",
      "2021-01-15 01:56:27,758 : INFO : token count processed\n",
      "2021-01-15 01:56:27,793 : INFO : frequencies processed\n",
      "2021-01-15 01:56:37,206 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:56:37,207 : INFO : entropies processed\n",
      "2021-01-15 01:56:37,208 : INFO : extropies processed\n",
      "2021-01-15 01:56:37,222 : INFO : token count processed\n",
      "2021-01-15 01:56:37,227 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:56:37,231 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:56:37,232 : INFO : vocab #32006\n",
      "2021-01-15 01:56:37,239 : INFO : diff #set()\n",
      "2021-01-15 01:56:56,253 : INFO : alphabet #32006\n",
      "2021-01-15 01:57:05,688 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.2312207286279582, 0.44818515136999854], [0.8871904239058495, 0.112809576], [2.2516291673878226, 1.2667563532600834], [4.392747410448784, 6.422089779976135, 6.540653832664084, 4.274183357760835, 2.1479064222153, 0.11856405268794923]]\n",
      "2021-01-15 01:57:05,693 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:57:05,694 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:57:05,695 : INFO : built Dictionary(237 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 1503 corpus positions)\n",
      "2021-01-15 01:57:05,817 : INFO : token count processed\n",
      "2021-01-15 01:57:05,877 : INFO : frequencies processed\n",
      "2021-01-15 01:57:15,297 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:57:15,298 : INFO : entropies processed\n",
      "2021-01-15 01:57:15,299 : INFO : extropies processed\n",
      "2021-01-15 01:57:15,310 : INFO : token count processed\n",
      "2021-01-15 01:57:15,314 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:57:15,319 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:57:15,319 : INFO : vocab #32006\n",
      "2021-01-15 01:57:15,326 : INFO : diff #set()\n",
      "2021-01-15 01:57:34,329 : INFO : alphabet #32006\n",
      "2021-01-15 01:57:43,760 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.1925566939615608, 0.4560885484758789], [0.8283894658088684, 0.17161053], [1.9219280948873623, 1.2148067842293933], [4.392747410448784, 6.485445644653597, 6.7348670100803965, 4.143326045021984, 2.342119599631612, 0.24942136542679982]]\n",
      "2021-01-15 01:57:43,764 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:57:43,765 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:57:43,770 : INFO : built Dictionary(211 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 1389 corpus positions)\n",
      "2021-01-15 01:57:43,879 : INFO : token count processed\n",
      "2021-01-15 01:57:43,944 : INFO : frequencies processed\n",
      "2021-01-15 01:57:53,367 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:57:53,368 : INFO : entropies processed\n",
      "2021-01-15 01:57:53,369 : INFO : extropies processed\n",
      "2021-01-15 01:57:53,376 : INFO : token count processed\n",
      "2021-01-15 01:57:53,383 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:57:53,388 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:57:53,389 : INFO : vocab #32006\n",
      "2021-01-15 01:57:53,397 : INFO : diff #set()\n",
      "2021-01-15 01:58:12,406 : INFO : alphabet #32006\n",
      "2021-01-15 01:58:21,841 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.145435418994108, 0.46610585019093803], [0.7843454033136368, 0.2156546], [2.2516291673878226, 1.2667563532600834], [4.392747410448784, 6.2276600107346916, 6.3621247550471, 4.258282666136376, 1.9693773445983158, 0.1344647443124085]]\n",
      "2021-01-15 01:58:21,846 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:58:21,847 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:58:21,849 : INFO : built Dictionary(202 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 1587 corpus positions)\n",
      "2021-01-15 01:58:21,955 : INFO : token count processed\n",
      "2021-01-15 01:58:21,990 : INFO : frequencies processed\n",
      "2021-01-15 01:58:31,527 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:58:31,528 : INFO : entropies processed\n",
      "2021-01-15 01:58:31,529 : INFO : extropies processed\n",
      "2021-01-15 01:58:31,536 : INFO : token count processed\n",
      "2021-01-15 01:58:31,543 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:58:31,547 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:58:31,548 : INFO : vocab #32006\n",
      "2021-01-15 01:58:31,555 : INFO : diff #set()\n",
      "2021-01-15 01:58:50,469 : INFO : alphabet #32006\n",
      "2021-01-15 01:59:00,064 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.2138861677314878, 0.4516944071359704], [0.8713548481464386, 0.12864515], [1.9219280948873623, 1.2148067842293933], [4.392747410448784, 6.253918170574241, 6.445298724622628, 4.201366856400398, 2.0525513141738436, 0.19138055404838727]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 01:59:00,068 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:59:00,069 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:59:00,070 : INFO : built Dictionary(174 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 684 corpus positions)\n",
      "2021-01-15 01:59:00,151 : INFO : token count processed\n",
      "2021-01-15 01:59:00,184 : INFO : frequencies processed\n",
      "2021-01-15 01:59:09,710 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:59:09,711 : INFO : entropies processed\n",
      "2021-01-15 01:59:09,712 : INFO : extropies processed\n",
      "2021-01-15 01:59:09,719 : INFO : token count processed\n",
      "2021-01-15 01:59:09,726 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:59:09,731 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:59:09,732 : INFO : vocab #32006\n",
      "2021-01-15 01:59:09,738 : INFO : diff #set()\n",
      "2021-01-15 01:59:28,475 : INFO : alphabet #32006\n",
      "2021-01-15 01:59:37,906 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.1606148630813415, 0.4628312139692768], [0.7904068827629089, 0.20959312], [1.9219280948873623, 1.2148067842293933], [4.392747410448784, 6.374522245625576, 6.592285197691517, 4.174984458382843, 2.1995377872427326, 0.2177629520659412]]\n",
      "2021-01-15 01:59:37,912 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 01:59:37,913 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 01:59:37,917 : INFO : built Dictionary(295 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 1966 corpus positions)\n",
      "2021-01-15 01:59:38,086 : INFO : token count processed\n",
      "2021-01-15 01:59:38,118 : INFO : frequencies processed\n",
      "2021-01-15 01:59:47,662 : INFO : scalar_distribution processed\n",
      "2021-01-15 01:59:47,663 : INFO : entropies processed\n",
      "2021-01-15 01:59:47,664 : INFO : extropies processed\n",
      "2021-01-15 01:59:47,678 : INFO : token count processed\n",
      "2021-01-15 01:59:47,684 : INFO : alphabet_source #32006\n",
      "2021-01-15 01:59:47,690 : INFO : alphabet_target #32009\n",
      "2021-01-15 01:59:47,691 : INFO : vocab #32006\n",
      "2021-01-15 01:59:47,698 : INFO : diff #set()\n",
      "2021-01-15 02:00:06,621 : INFO : alphabet #32006\n",
      "2021-01-15 02:00:16,162 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.1988946815802335, 0.4547739409153289], [0.8446054011583328, 0.1553946], [2.2516291673878226, 1.2667563532600834], [4.392747410448784, 6.731238669067808, 7.002103086841555, 4.121882992675036, 2.609355676392771, 0.2708644177737476]]\n",
      "2021-01-15 02:00:16,166 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 02:00:16,167 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:00:16,169 : INFO : built Dictionary(221 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 1358 corpus positions)\n",
      "2021-01-15 02:00:16,279 : INFO : token count processed\n",
      "2021-01-15 02:00:16,310 : INFO : frequencies processed\n",
      "2021-01-15 02:00:25,730 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:00:25,731 : INFO : entropies processed\n",
      "2021-01-15 02:00:25,732 : INFO : extropies processed\n",
      "2021-01-15 02:00:25,739 : INFO : token count processed\n",
      "2021-01-15 02:00:25,744 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:00:25,748 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:00:25,749 : INFO : vocab #32006\n",
      "2021-01-15 02:00:25,756 : INFO : diff #set()\n",
      "2021-01-15 02:00:44,769 : INFO : alphabet #32006\n",
      "2021-01-15 02:00:54,195 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.212239082686879, 0.45203070853691285], [0.8622594773769379, 0.13774052], [1.9219280948873623, 1.2148067842293933], [4.392747410448784, 6.503741451859337, 6.704742591517073, 4.1917462707910484, 2.311995181068289, 0.2010011396577358]]\n",
      "2021-01-15 02:00:54,200 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 02:00:54,201 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:00:54,202 : INFO : built Dictionary(241 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 1680 corpus positions)\n",
      "2021-01-15 02:00:54,329 : INFO : token count processed\n",
      "2021-01-15 02:00:54,361 : INFO : frequencies processed\n",
      "2021-01-15 02:01:03,774 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:01:03,775 : INFO : entropies processed\n",
      "2021-01-15 02:01:03,776 : INFO : extropies processed\n",
      "2021-01-15 02:01:03,783 : INFO : token count processed\n",
      "2021-01-15 02:01:03,788 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:01:03,793 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:01:03,794 : INFO : vocab #32006\n",
      "2021-01-15 02:01:03,800 : INFO : diff #set()\n",
      "2021-01-15 02:01:22,827 : INFO : alphabet #32006\n",
      "2021-01-15 02:01:32,254 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.2078546053606383, 0.4529283756149589], [0.8553979098796844, 0.14460209], [2.521640636343318, 1.2998438251349491], [4.392747410448784, 6.334729224484471, 6.472194626023459, 4.255282008909797, 2.0794472155746746, 0.13746540153898756]]\n",
      "2021-01-15 02:01:32,260 : INFO : Removed 1 and 0 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 02:01:32,261 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:01:32,262 : INFO : built Dictionary(238 unique tokens: ['-', '/', '38', ':', 'ec']...) from 2 documents (total 2029 corpus positions)\n",
      "2021-01-15 02:01:32,385 : INFO : token count processed\n",
      "2021-01-15 02:01:32,418 : INFO : frequencies processed\n",
      "2021-01-15 02:01:41,837 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:01:41,838 : INFO : entropies processed\n",
      "2021-01-15 02:01:41,839 : INFO : extropies processed\n",
      "2021-01-15 02:01:41,847 : INFO : token count processed\n",
      "2021-01-15 02:01:41,851 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:01:41,856 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:01:41,857 : INFO : vocab #32006\n",
      "2021-01-15 02:01:41,866 : INFO : diff #set()\n",
      "2021-01-15 02:02:00,851 : INFO : alphabet #32006\n",
      "2021-01-15 02:02:10,270 : INFO : Computed distances or similarities ('267', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.1886673381901955, 0.4568990373963811], [0.8376404792070389, 0.16235952], [2.521640636343318, 1.2998438251349491], [4.392747410448784, 6.21319712067992, 6.411009811659639, 4.194934719469066, 2.0182624012108548, 0.19781269097971865]]\n",
      "2021-01-15 02:02:10,275 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 02:02:10,276 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:02:10,278 : INFO : built Dictionary(301 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1471 corpus positions)\n",
      "2021-01-15 02:02:10,636 : INFO : token count processed\n",
      "2021-01-15 02:02:10,671 : INFO : frequencies processed\n",
      "2021-01-15 02:02:20,103 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:02:20,104 : INFO : entropies processed\n",
      "2021-01-15 02:02:20,105 : INFO : extropies processed\n",
      "2021-01-15 02:02:20,113 : INFO : token count processed\n",
      "2021-01-15 02:02:20,119 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:02:20,124 : INFO : alphabet_target #32010\n",
      "2021-01-15 02:02:20,125 : INFO : vocab #32006\n",
      "2021-01-15 02:02:20,132 : INFO : diff #set()\n",
      "2021-01-15 02:02:39,186 : INFO : alphabet #32006\n",
      "2021-01-15 02:02:48,629 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.1433876539221606, 0.4665511617416063], [0.7173500955104828, 0.2826499], [2.777776811456786, 1.2902865154177445], [5.362003250893972, 6.905617163738059, 7.162132773427571, 5.10548764120446, 1.8001295225335987, 0.2565156096895116]]\n",
      "2021-01-15 02:02:48,635 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:02:48,640 : INFO : built Dictionary(390 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 2361 corpus positions)\n",
      "2021-01-15 02:02:49,197 : INFO : token count processed\n",
      "2021-01-15 02:02:49,229 : INFO : frequencies processed\n",
      "2021-01-15 02:02:59,038 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:02:59,039 : INFO : entropies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 02:02:59,040 : INFO : extropies processed\n",
      "2021-01-15 02:02:59,047 : INFO : token count processed\n",
      "2021-01-15 02:02:59,053 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:02:59,057 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:02:59,058 : INFO : vocab #32006\n",
      "2021-01-15 02:02:59,065 : INFO : diff #set()\n",
      "2021-01-15 02:03:18,039 : INFO : alphabet #32006\n",
      "2021-01-15 02:03:27,459 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.1552034348955325, 0.4639933213768622], [0.7331791520118713, 0.26682085], [3.1403611636984845, 1.3338450692604933], [5.362003250893972, 7.1219284286457345, 7.421219132358889, 5.062712547180817, 2.059215881464917, 0.29929070371315447]]\n",
      "2021-01-15 02:03:27,465 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:03:27,467 : INFO : built Dictionary(307 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 2344 corpus positions)\n",
      "2021-01-15 02:03:27,845 : INFO : token count processed\n",
      "2021-01-15 02:03:27,879 : INFO : frequencies processed\n",
      "2021-01-15 02:03:37,333 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:03:37,334 : INFO : entropies processed\n",
      "2021-01-15 02:03:37,335 : INFO : extropies processed\n",
      "2021-01-15 02:03:37,342 : INFO : token count processed\n",
      "2021-01-15 02:03:37,348 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:03:37,352 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:03:37,353 : INFO : vocab #32006\n",
      "2021-01-15 02:03:37,360 : INFO : diff #set()\n",
      "2021-01-15 02:03:56,045 : INFO : alphabet #32006\n",
      "2021-01-15 02:04:05,510 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.145682698778945, 0.46605213369575815], [0.7302254140377045, 0.2697746], [3.3464393446710154, 1.3368123128978013], [5.362003250893972, 6.41099024988467, 6.584907501914485, 5.188085998864158, 1.2229042510205135, 0.17391725202981512]]\n",
      "2021-01-15 02:04:05,514 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:04:05,516 : INFO : built Dictionary(191 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 743 corpus positions)\n",
      "2021-01-15 02:04:05,711 : INFO : token count processed\n",
      "2021-01-15 02:04:05,746 : INFO : frequencies processed\n",
      "2021-01-15 02:04:15,279 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:04:15,280 : INFO : entropies processed\n",
      "2021-01-15 02:04:15,281 : INFO : extropies processed\n",
      "2021-01-15 02:04:15,288 : INFO : token count processed\n",
      "2021-01-15 02:04:15,295 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:04:15,299 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:04:15,300 : INFO : vocab #32006\n",
      "2021-01-15 02:04:15,307 : INFO : diff #set()\n",
      "2021-01-15 02:04:34,167 : INFO : alphabet #32006\n",
      "2021-01-15 02:04:43,601 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.1164552009812492, 0.47248814883318646], [0.7272099256515503, 0.27279007], [2.353533948577482, 1.2177935163324858], [5.362003250893972, 6.077866832717642, 6.465319251291195, 4.974550832320418, 1.103316000397223, 0.3874524185735533]]\n",
      "2021-01-15 02:04:43,605 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:04:43,607 : INFO : built Dictionary(164 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 508 corpus positions)\n",
      "2021-01-15 02:04:43,764 : INFO : token count processed\n",
      "2021-01-15 02:04:43,800 : INFO : frequencies processed\n",
      "2021-01-15 02:04:53,488 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:04:53,489 : INFO : entropies processed\n",
      "2021-01-15 02:04:53,490 : INFO : extropies processed\n",
      "2021-01-15 02:04:53,504 : INFO : token count processed\n",
      "2021-01-15 02:04:53,509 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:04:53,514 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:04:53,515 : INFO : vocab #32006\n",
      "2021-01-15 02:04:53,522 : INFO : diff #set()\n",
      "2021-01-15 02:05:12,417 : INFO : alphabet #32006\n",
      "2021-01-15 02:05:22,132 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.109112629402173, 0.47413304821158353], [0.7244068384170532, 0.27559316], [2.939829017466402, 1.3012558257668296], [5.362003250893972, 5.977547459003844, 6.396321051045491, 4.943229658852325, 1.034317800151519, 0.41877359204164666]]\n",
      "2021-01-15 02:05:22,138 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:05:22,140 : INFO : built Dictionary(267 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 2219 corpus positions)\n",
      "2021-01-15 02:05:22,445 : INFO : token count processed\n",
      "2021-01-15 02:05:22,505 : INFO : frequencies processed\n",
      "2021-01-15 02:05:31,951 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:05:31,952 : INFO : entropies processed\n",
      "2021-01-15 02:05:31,953 : INFO : extropies processed\n",
      "2021-01-15 02:05:31,960 : INFO : token count processed\n",
      "2021-01-15 02:05:31,964 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:05:31,969 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:05:31,970 : INFO : vocab #32006\n",
      "2021-01-15 02:05:31,976 : INFO : diff #set()\n",
      "2021-01-15 02:05:50,865 : INFO : alphabet #32006\n",
      "2021-01-15 02:06:00,429 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.142784622782732, 0.4666824604618209], [0.7178001403808594, 0.28219986], [3.221097250057956, 1.326818270405738], [5.362003250893972, 6.4614394051846435, 6.671565003556038, 5.151877652522577, 1.3095617526620664, 0.21012559837139477]]\n",
      "2021-01-15 02:06:00,434 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 02:06:00,435 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:06:00,436 : INFO : built Dictionary(234 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1443 corpus positions)\n",
      "2021-01-15 02:06:00,681 : INFO : token count processed\n",
      "2021-01-15 02:06:00,748 : INFO : frequencies processed\n",
      "2021-01-15 02:06:10,169 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:06:10,170 : INFO : entropies processed\n",
      "2021-01-15 02:06:10,171 : INFO : extropies processed\n",
      "2021-01-15 02:06:10,178 : INFO : token count processed\n",
      "2021-01-15 02:06:10,184 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:06:10,189 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:06:10,190 : INFO : vocab #32006\n",
      "2021-01-15 02:06:10,197 : INFO : diff #set()\n",
      "2021-01-15 02:06:29,201 : INFO : alphabet #32006\n",
      "2021-01-15 02:06:38,626 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.171567274314591, 0.46049690093788537], [0.7791444063186646, 0.2208556], [2.472905595320056, 1.2576515238575927], [5.362003250893972, 6.327195724598159, 6.595812893055658, 5.0933860824364725, 1.2338096421616864, 0.2686171684574994]]\n",
      "2021-01-15 02:06:38,638 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 02:06:38,639 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:06:38,642 : INFO : built Dictionary(440 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 6335 corpus positions)\n",
      "2021-01-15 02:06:39,289 : INFO : token count processed\n",
      "2021-01-15 02:06:39,360 : INFO : frequencies processed\n",
      "2021-01-15 02:06:48,789 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:06:48,790 : INFO : entropies processed\n",
      "2021-01-15 02:06:48,791 : INFO : extropies processed\n",
      "2021-01-15 02:06:48,799 : INFO : token count processed\n",
      "2021-01-15 02:06:48,807 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:06:48,812 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:06:48,813 : INFO : vocab #32006\n",
      "2021-01-15 02:06:48,821 : INFO : diff #set()\n",
      "2021-01-15 02:07:07,825 : INFO : alphabet #32006\n",
      "2021-01-15 02:07:17,255 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.1609231366161754, 0.4627651872735817], [0.7527489215135574, 0.24725108], [3.816232755328732, 1.3785152764802466], [5.362003250893972, 6.9079058562486315, 7.056275258940236, 5.213633848202367, 1.6942720080462639, 0.14836940269160426]]\n",
      "2021-01-15 02:07:17,262 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:07:17,264 : INFO : built Dictionary(344 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 2729 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 02:07:17,682 : INFO : token count processed\n",
      "2021-01-15 02:07:17,728 : INFO : frequencies processed\n",
      "2021-01-15 02:07:27,141 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:07:27,143 : INFO : entropies processed\n",
      "2021-01-15 02:07:27,143 : INFO : extropies processed\n",
      "2021-01-15 02:07:27,151 : INFO : token count processed\n",
      "2021-01-15 02:07:27,157 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:07:27,161 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:07:27,163 : INFO : vocab #32006\n",
      "2021-01-15 02:07:27,169 : INFO : diff #set()\n",
      "2021-01-15 02:07:46,190 : INFO : alphabet #32006\n",
      "2021-01-15 02:07:55,625 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.1772201827081228, 0.45930127230226014], [0.7909952104091644, 0.20900479], [3.601125010956985, 1.3652104012284374], [5.362003250893972, 6.61034830706307, 6.851402628338261, 5.120948929618781, 1.4893993774442889, 0.24105432127519055]]\n",
      "2021-01-15 02:07:55,629 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:07:55,631 : INFO : built Dictionary(233 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 776 corpus positions)\n",
      "2021-01-15 02:07:55,887 : INFO : token count processed\n",
      "2021-01-15 02:07:55,920 : INFO : frequencies processed\n",
      "2021-01-15 02:08:05,309 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:08:05,310 : INFO : entropies processed\n",
      "2021-01-15 02:08:05,311 : INFO : extropies processed\n",
      "2021-01-15 02:08:05,318 : INFO : token count processed\n",
      "2021-01-15 02:08:05,321 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:08:05,325 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:08:05,326 : INFO : vocab #32006\n",
      "2021-01-15 02:08:05,332 : INFO : diff #set()\n",
      "2021-01-15 02:08:24,443 : INFO : alphabet #32006\n",
      "2021-01-15 02:08:33,867 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.1668942555540054, 0.4614899861573226], [0.8033214062452316, 0.1966786], [3.236857303422074, 1.3349321432842867], [5.362003250893972, 6.616715366949855, 6.9425700948276585, 5.036148523016168, 1.5805668439336866, 0.3258547278778039]]\n",
      "2021-01-15 02:08:33,874 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:08:33,877 : INFO : built Dictionary(448 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 2810 corpus positions)\n",
      "2021-01-15 02:08:34,508 : INFO : token count processed\n",
      "2021-01-15 02:08:34,541 : INFO : frequencies processed\n",
      "2021-01-15 02:08:43,967 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:08:43,968 : INFO : entropies processed\n",
      "2021-01-15 02:08:43,969 : INFO : extropies processed\n",
      "2021-01-15 02:08:43,977 : INFO : token count processed\n",
      "2021-01-15 02:08:43,985 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:08:43,990 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:08:43,991 : INFO : vocab #32006\n",
      "2021-01-15 02:08:44,000 : INFO : diff #set()\n",
      "2021-01-15 02:09:03,001 : INFO : alphabet #32006\n",
      "2021-01-15 02:09:12,420 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.1172232114833882, 0.47231675648377713], [0.6887664198875427, 0.31123358], [3.7810760119146067, 1.3738228372872952], [5.362003250893972, 7.32185870753746, 7.5778176308521665, 5.106044327579266, 2.2158143799581946, 0.25595892331470615]]\n",
      "2021-01-15 02:09:12,424 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:09:12,425 : INFO : built Dictionary(95 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 175 corpus positions)\n",
      "2021-01-15 02:09:12,477 : INFO : token count processed\n",
      "2021-01-15 02:09:12,509 : INFO : frequencies processed\n",
      "2021-01-15 02:09:22,043 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:09:22,044 : INFO : entropies processed\n",
      "2021-01-15 02:09:22,045 : INFO : extropies processed\n",
      "2021-01-15 02:09:22,052 : INFO : token count processed\n",
      "2021-01-15 02:09:22,056 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:09:22,061 : INFO : alphabet_target #32008\n",
      "2021-01-15 02:09:22,062 : INFO : vocab #32006\n",
      "2021-01-15 02:09:22,068 : INFO : diff #set()\n",
      "2021-01-15 02:09:40,939 : INFO : alphabet #32006\n",
      "2021-01-15 02:09:50,481 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2036530528539837, 0.4537919427492841], [0.8517125397920609, 0.14828746], [1.5, 1.1225562489182657], [5.362003250893972, 5.176618657501385, 6.224816402615255, 4.313805505780102, 0.8628131517212827, 1.0481977451138693]]\n",
      "2021-01-15 02:09:50,485 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:09:50,486 : INFO : built Dictionary(186 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 568 corpus positions)\n",
      "2021-01-15 02:09:50,667 : INFO : token count processed\n",
      "2021-01-15 02:09:50,699 : INFO : frequencies processed\n",
      "2021-01-15 02:10:00,110 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:10:00,111 : INFO : entropies processed\n",
      "2021-01-15 02:10:00,112 : INFO : extropies processed\n",
      "2021-01-15 02:10:00,119 : INFO : token count processed\n",
      "2021-01-15 02:10:00,124 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:10:00,129 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:10:00,130 : INFO : vocab #32006\n",
      "2021-01-15 02:10:00,136 : INFO : diff #set()\n",
      "2021-01-15 02:10:19,010 : INFO : alphabet #32006\n",
      "2021-01-15 02:10:28,543 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.1569652281081426, 0.46361433507071054], [0.7728090882301331, 0.22719091], [2.6062389286533896, 1.2645273540141966], [5.362003250893972, 6.468846789852156, 6.886100790776407, 4.944749249969721, 1.524097539882435, 0.4172540009242507]]\n",
      "2021-01-15 02:10:28,549 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:10:28,551 : INFO : built Dictionary(397 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 2617 corpus positions)\n",
      "2021-01-15 02:10:29,117 : INFO : token count processed\n",
      "2021-01-15 02:10:29,149 : INFO : frequencies processed\n",
      "2021-01-15 02:10:38,559 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:10:38,560 : INFO : entropies processed\n",
      "2021-01-15 02:10:38,560 : INFO : extropies processed\n",
      "2021-01-15 02:10:38,568 : INFO : token count processed\n",
      "2021-01-15 02:10:38,574 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:10:38,579 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:10:38,580 : INFO : vocab #32006\n",
      "2021-01-15 02:10:38,588 : INFO : diff #set()\n",
      "2021-01-15 02:10:57,493 : INFO : alphabet #32006\n",
      "2021-01-15 02:11:07,035 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.149412114287659, 0.46524349302432955], [0.7360580861568451, 0.2639419], [3.483074189428569, 1.35567590267696], [5.362003250893972, 6.957796704012729, 7.1741268157671145, 5.1456731391395865, 1.8121235648731426, 0.21633011175438543]]\n",
      "2021-01-15 02:11:07,042 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:11:07,044 : INFO : built Dictionary(313 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 3114 corpus positions)\n",
      "2021-01-15 02:11:07,424 : INFO : token count processed\n",
      "2021-01-15 02:11:07,460 : INFO : frequencies processed\n",
      "2021-01-15 02:11:17,030 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:11:17,031 : INFO : entropies processed\n",
      "2021-01-15 02:11:17,032 : INFO : extropies processed\n",
      "2021-01-15 02:11:17,040 : INFO : token count processed\n",
      "2021-01-15 02:11:17,045 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:11:17,050 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:11:17,051 : INFO : vocab #32006\n",
      "2021-01-15 02:11:17,058 : INFO : diff #set()\n",
      "2021-01-15 02:11:36,050 : INFO : alphabet #32006\n",
      "2021-01-15 02:11:45,466 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.1353309658395987, 0.46831147770425663], [0.7119723856449127, 0.2880276], [2.4137995646056796, 1.2397034351567287], [5.362003250893972, 6.441859572014148, 6.678171650134946, 5.125691172773173, 1.3161683992409738, 0.2363120781207977]]\n",
      "2021-01-15 02:11:45,471 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 02:11:45,473 : INFO : built Dictionary(333 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1590 corpus positions)\n",
      "2021-01-15 02:11:45,865 : INFO : token count processed\n",
      "2021-01-15 02:11:45,911 : INFO : frequencies processed\n",
      "2021-01-15 02:11:55,326 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:11:55,328 : INFO : entropies processed\n",
      "2021-01-15 02:11:55,328 : INFO : extropies processed\n",
      "2021-01-15 02:11:55,336 : INFO : token count processed\n",
      "2021-01-15 02:11:55,340 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:11:55,344 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:11:55,345 : INFO : vocab #32006\n",
      "2021-01-15 02:11:55,352 : INFO : diff #set()\n",
      "2021-01-15 02:12:14,272 : INFO : alphabet #32006\n",
      "2021-01-15 02:12:23,646 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.1704171323898274, 0.460740926284022], [0.7642123252153397, 0.23578767], [3.3758145836939115, 1.3492844704371658], [5.362003250893972, 6.998955278238291, 7.2696551583018625, 5.091303370830399, 1.9076519074078906, 0.2706998800635718]]\n",
      "2021-01-15 02:12:23,651 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:12:23,653 : INFO : built Dictionary(250 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1409 corpus positions)\n",
      "2021-01-15 02:12:23,920 : INFO : token count processed\n",
      "2021-01-15 02:12:23,952 : INFO : frequencies processed\n",
      "2021-01-15 02:12:33,357 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:12:33,358 : INFO : entropies processed\n",
      "2021-01-15 02:12:33,359 : INFO : extropies processed\n",
      "2021-01-15 02:12:33,366 : INFO : token count processed\n",
      "2021-01-15 02:12:33,374 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:12:33,378 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:12:33,379 : INFO : vocab #32006\n",
      "2021-01-15 02:12:33,386 : INFO : diff #set()\n",
      "2021-01-15 02:12:52,370 : INFO : alphabet #32006\n",
      "2021-01-15 02:13:01,797 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.1311168114463284, 0.4692375352814792], [0.7147086262702942, 0.28529137], [3.0574760762899316, 1.3065633102011562], [5.362003250893972, 6.492983191376071, 6.749740620064657, 5.1052458222053865, 1.3877373691706847, 0.2567574286885854]]\n",
      "2021-01-15 02:13:01,805 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:13:01,807 : INFO : built Dictionary(462 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 3348 corpus positions)\n",
      "2021-01-15 02:13:02,461 : INFO : token count processed\n",
      "2021-01-15 02:13:02,493 : INFO : frequencies processed\n",
      "2021-01-15 02:13:11,898 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:13:11,899 : INFO : entropies processed\n",
      "2021-01-15 02:13:11,900 : INFO : extropies processed\n",
      "2021-01-15 02:13:11,908 : INFO : token count processed\n",
      "2021-01-15 02:13:11,915 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:13:11,919 : INFO : alphabet_target #32008\n",
      "2021-01-15 02:13:11,920 : INFO : vocab #32006\n",
      "2021-01-15 02:13:11,927 : INFO : diff #set()\n",
      "2021-01-15 02:13:30,917 : INFO : alphabet #32006\n",
      "2021-01-15 02:13:40,338 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.164814252257482, 0.4619333963443716], [0.7502073794603348, 0.24979262], [2.6601297526332566, 1.2849790309834181], [5.362003250893972, 6.560342487747443, 6.831534773710708, 5.090810964930706, 1.4695315228167365, 0.2711922859632656]]\n",
      "2021-01-15 02:13:40,346 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 02:13:40,346 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:13:40,351 : INFO : built Dictionary(470 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 3553 corpus positions)\n",
      "2021-01-15 02:13:41,020 : INFO : token count processed\n",
      "2021-01-15 02:13:41,052 : INFO : frequencies processed\n",
      "2021-01-15 02:13:50,461 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:13:50,462 : INFO : entropies processed\n",
      "2021-01-15 02:13:50,463 : INFO : extropies processed\n",
      "2021-01-15 02:13:50,471 : INFO : token count processed\n",
      "2021-01-15 02:13:50,475 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:13:50,480 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:13:50,481 : INFO : vocab #32006\n",
      "2021-01-15 02:13:50,487 : INFO : diff #set()\n",
      "2021-01-15 02:14:09,621 : INFO : alphabet #32006\n",
      "2021-01-15 02:14:19,038 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.1743845888053188, 0.4599002426472467], [0.7816916406154633, 0.21830836], [3.8001821971347023, 1.375987951484803], [5.362003250893972, 7.046173750105238, 7.294148057502742, 5.114028943496468, 1.93214480660877, 0.24797430739750403]]\n",
      "2021-01-15 02:14:19,049 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 02:14:19,050 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:14:19,053 : INFO : built Dictionary(521 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 5662 corpus positions)\n",
      "2021-01-15 02:14:19,961 : INFO : token count processed\n",
      "2021-01-15 02:14:19,993 : INFO : frequencies processed\n",
      "2021-01-15 02:14:29,519 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:14:29,520 : INFO : entropies processed\n",
      "2021-01-15 02:14:29,521 : INFO : extropies processed\n",
      "2021-01-15 02:14:29,530 : INFO : token count processed\n",
      "2021-01-15 02:14:29,534 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:14:29,538 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:14:29,539 : INFO : vocab #32006\n",
      "2021-01-15 02:14:29,547 : INFO : diff #set()\n",
      "2021-01-15 02:14:48,412 : INFO : alphabet #32006\n",
      "2021-01-15 02:14:57,960 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.1407255621899093, 0.46713133979538446], [0.7356173694133759, 0.26438263], [4.035273502696888, 1.3855801046867775], [5.362003250893972, 7.009229588004272, 7.1573205998763845, 5.21391223902186, 1.7953173489824126, 0.14809101187211215]]\n",
      "2021-01-15 02:14:57,972 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:14:57,976 : INFO : built Dictionary(600 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 6603 corpus positions)\n",
      "2021-01-15 02:14:58,982 : INFO : token count processed\n",
      "2021-01-15 02:14:59,014 : INFO : frequencies processed\n",
      "2021-01-15 02:15:08,428 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:15:08,429 : INFO : entropies processed\n",
      "2021-01-15 02:15:08,429 : INFO : extropies processed\n",
      "2021-01-15 02:15:08,438 : INFO : token count processed\n",
      "2021-01-15 02:15:08,443 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:15:08,447 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:15:08,448 : INFO : vocab #32006\n",
      "2021-01-15 02:15:08,456 : INFO : diff #set()\n",
      "2021-01-15 02:15:27,895 : INFO : alphabet #32006\n",
      "2021-01-15 02:15:37,438 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.1312417578380303, 0.46921002571497], [0.7071280479431152, 0.29287195], [4.050013373778756, 1.3870541264071081], [5.362003250893972, 7.376088004590871, 7.570293587647198, 5.167797667837645, 2.208290336753226, 0.19420558305632696]]\n",
      "2021-01-15 02:15:37,442 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:15:37,443 : INFO : built Dictionary(164 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 389 corpus positions)\n",
      "2021-01-15 02:15:37,599 : INFO : token count processed\n",
      "2021-01-15 02:15:37,666 : INFO : frequencies processed\n",
      "2021-01-15 02:15:47,092 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:15:47,093 : INFO : entropies processed\n",
      "2021-01-15 02:15:47,094 : INFO : extropies processed\n",
      "2021-01-15 02:15:47,101 : INFO : token count processed\n",
      "2021-01-15 02:15:47,109 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:15:47,113 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:15:47,114 : INFO : vocab #32006\n",
      "2021-01-15 02:15:47,121 : INFO : diff #set()\n",
      "2021-01-15 02:16:05,992 : INFO : alphabet #32006\n",
      "2021-01-15 02:16:15,518 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.1770761322564824, 0.45933166285899524], [0.7985980808734894, 0.20140192], [2.2221915755066783, 1.2138650192953737], [5.362003250893972, 6.2993628166120885, 6.789830288758717, 4.871535778747344, 1.4278270378647449, 0.4904674721466282]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 02:16:15,521 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:16:15,522 : INFO : built Dictionary(66 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 105 corpus positions)\n",
      "2021-01-15 02:16:15,546 : INFO : token count processed\n",
      "2021-01-15 02:16:15,578 : INFO : frequencies processed\n",
      "2021-01-15 02:16:24,872 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:16:24,873 : INFO : entropies processed\n",
      "2021-01-15 02:16:24,874 : INFO : extropies processed\n",
      "2021-01-15 02:16:24,881 : INFO : token count processed\n",
      "2021-01-15 02:16:24,885 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:16:24,890 : INFO : alphabet_target #32008\n",
      "2021-01-15 02:16:24,891 : INFO : vocab #32006\n",
      "2021-01-15 02:16:24,897 : INFO : diff #set()\n",
      "2021-01-15 02:16:43,886 : INFO : alphabet #32006\n",
      "2021-01-15 02:16:53,308 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2542424434493873, 0.4436080080498459], [0.8861399441957474, 0.113860056], [0.0, 0.0], [5.362003250893972, 3.8936606896881862, 5.705348585183591, 3.550315355398567, 0.34334533428961933, 1.811687895495405]]\n",
      "2021-01-15 02:16:53,331 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:16:53,336 : INFO : built Dictionary(750 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 12547 corpus positions)\n",
      "2021-01-15 02:16:54,906 : INFO : token count processed\n",
      "2021-01-15 02:16:54,969 : INFO : frequencies processed\n",
      "2021-01-15 02:17:04,389 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:17:04,391 : INFO : entropies processed\n",
      "2021-01-15 02:17:04,391 : INFO : extropies processed\n",
      "2021-01-15 02:17:04,402 : INFO : token count processed\n",
      "2021-01-15 02:17:04,407 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:17:04,412 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:17:04,413 : INFO : vocab #32006\n",
      "2021-01-15 02:17:04,421 : INFO : diff #set()\n",
      "2021-01-15 02:17:23,412 : INFO : alphabet #32006\n",
      "2021-01-15 02:17:32,809 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.123318490647551, 0.4709609059614173], [0.7093973457813263, 0.29060265], [4.59882024669601, 1.4061047857499671], [5.362003250893972, 7.434393313070278, 7.653915228606464, 5.142481335357787, 2.291911977712492, 0.21952191553618583]]\n",
      "2021-01-15 02:17:32,818 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:17:32,820 : INFO : built Dictionary(506 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 4174 corpus positions)\n",
      "2021-01-15 02:17:33,583 : INFO : token count processed\n",
      "2021-01-15 02:17:33,628 : INFO : frequencies processed\n",
      "2021-01-15 02:17:43,041 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:17:43,042 : INFO : entropies processed\n",
      "2021-01-15 02:17:43,043 : INFO : extropies processed\n",
      "2021-01-15 02:17:43,051 : INFO : token count processed\n",
      "2021-01-15 02:17:43,055 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:17:43,060 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:17:43,061 : INFO : vocab #32006\n",
      "2021-01-15 02:17:43,068 : INFO : diff #set()\n",
      "2021-01-15 02:18:02,072 : INFO : alphabet #32006\n",
      "2021-01-15 02:18:11,684 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.132742052188142, 0.4688799561925569], [0.7204161286354065, 0.27958387], [4.314121886703896, 1.3971915106575201], [5.362003250893972, 7.2991514951718255, 7.524651968962527, 5.136502777103271, 2.1626487180685547, 0.22550047379070115]]\n",
      "2021-01-15 02:18:11,692 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:18:11,694 : INFO : built Dictionary(481 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 3591 corpus positions)\n",
      "2021-01-15 02:18:12,405 : INFO : token count processed\n",
      "2021-01-15 02:18:12,475 : INFO : frequencies processed\n",
      "2021-01-15 02:18:21,898 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:18:21,899 : INFO : entropies processed\n",
      "2021-01-15 02:18:21,900 : INFO : extropies processed\n",
      "2021-01-15 02:18:21,908 : INFO : token count processed\n",
      "2021-01-15 02:18:21,912 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:18:21,916 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:18:21,917 : INFO : vocab #32006\n",
      "2021-01-15 02:18:21,924 : INFO : diff #set()\n",
      "2021-01-15 02:18:40,905 : INFO : alphabet #32006\n",
      "2021-01-15 02:18:50,321 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.1656993540125276, 0.4617446083396742], [0.7556321322917938, 0.24436787], [3.5780564832443083, 1.3622510528319187], [5.362003250893972, 7.170319527000998, 7.412774570484423, 5.119548207410546, 2.050771319590451, 0.2424550434834254]]\n",
      "2021-01-15 02:18:50,325 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:18:50,326 : INFO : built Dictionary(196 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 627 corpus positions)\n",
      "2021-01-15 02:18:50,511 : INFO : token count processed\n",
      "2021-01-15 02:18:50,543 : INFO : frequencies processed\n",
      "2021-01-15 02:19:00,072 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:19:00,074 : INFO : entropies processed\n",
      "2021-01-15 02:19:00,075 : INFO : extropies processed\n",
      "2021-01-15 02:19:00,087 : INFO : token count processed\n",
      "2021-01-15 02:19:00,092 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:19:00,097 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:19:00,098 : INFO : vocab #32006\n",
      "2021-01-15 02:19:00,105 : INFO : diff #set()\n",
      "2021-01-15 02:19:19,016 : INFO : alphabet #32006\n",
      "2021-01-15 02:19:28,838 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.1743649162131564, 0.4599044036001031], [0.7892183661460876, 0.21078163], [2.556656707462823, 1.2476381669990098], [5.362003250893972, 6.353654804387375, 6.781144294940246, 4.934513760341101, 1.419141044046274, 0.4274894905528708]]\n",
      "2021-01-15 02:19:28,842 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:19:28,843 : INFO : built Dictionary(191 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 755 corpus positions)\n",
      "2021-01-15 02:19:29,023 : INFO : token count processed\n",
      "2021-01-15 02:19:29,056 : INFO : frequencies processed\n",
      "2021-01-15 02:19:38,583 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:19:38,584 : INFO : entropies processed\n",
      "2021-01-15 02:19:38,585 : INFO : extropies processed\n",
      "2021-01-15 02:19:38,598 : INFO : token count processed\n",
      "2021-01-15 02:19:38,603 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:19:38,607 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:19:38,608 : INFO : vocab #32006\n",
      "2021-01-15 02:19:38,615 : INFO : diff #set()\n",
      "2021-01-15 02:19:57,475 : INFO : alphabet #32006\n",
      "2021-01-15 02:20:06,898 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.1855907893022872, 0.4575421917472635], [0.8153191953897476, 0.1846808], [2.751629167387823, 1.2995901901368234], [5.362003250893972, 6.245180322479091, 6.6392806740348895, 4.967902899338174, 1.2772774231409176, 0.3941003515557986]]\n",
      "2021-01-15 02:20:06,903 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:20:06,905 : INFO : built Dictionary(411 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1827 corpus positions)\n",
      "2021-01-15 02:20:07,519 : INFO : token count processed\n",
      "2021-01-15 02:20:07,551 : INFO : frequencies processed\n",
      "2021-01-15 02:20:17,084 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:20:17,085 : INFO : entropies processed\n",
      "2021-01-15 02:20:17,086 : INFO : extropies processed\n",
      "2021-01-15 02:20:17,094 : INFO : token count processed\n",
      "2021-01-15 02:20:17,101 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:20:17,106 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:20:17,107 : INFO : vocab #32006\n",
      "2021-01-15 02:20:17,115 : INFO : diff #set()\n",
      "2021-01-15 02:20:35,843 : INFO : alphabet #32006\n",
      "2021-01-15 02:20:45,496 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.154495289134475, 0.4641458280476119], [0.7581005990505219, 0.2418994], [3.775830987387494, 1.3743828557277293], [5.362003250893972, 7.2691387000368, 7.549040278070066, 5.082101672860706, 2.1870370271760944, 0.2799015780332663]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 02:20:45,501 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:20:45,503 : INFO : built Dictionary(341 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1664 corpus positions)\n",
      "2021-01-15 02:20:45,948 : INFO : token count processed\n",
      "2021-01-15 02:20:45,995 : INFO : frequencies processed\n",
      "2021-01-15 02:20:55,426 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:20:55,427 : INFO : entropies processed\n",
      "2021-01-15 02:20:55,428 : INFO : extropies processed\n",
      "2021-01-15 02:20:55,435 : INFO : token count processed\n",
      "2021-01-15 02:20:55,439 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:20:55,444 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:20:55,445 : INFO : vocab #32006\n",
      "2021-01-15 02:20:55,451 : INFO : diff #set()\n",
      "2021-01-15 02:21:14,641 : INFO : alphabet #32006\n",
      "2021-01-15 02:21:24,084 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.1156905501409347, 0.47265891504472907], [0.7113745212554932, 0.28862548], [3.4014228908019635, 1.354608536505665], [5.362003250893972, 7.08857858466988, 7.34161859624883, 5.108963239315022, 1.9796153453548584, 0.2530400115789506]]\n",
      "2021-01-15 02:21:24,087 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:21:24,089 : INFO : built Dictionary(170 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 587 corpus positions)\n",
      "2021-01-15 02:21:24,259 : INFO : token count processed\n",
      "2021-01-15 02:21:24,293 : INFO : frequencies processed\n",
      "2021-01-15 02:21:33,709 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:21:33,710 : INFO : entropies processed\n",
      "2021-01-15 02:21:33,711 : INFO : extropies processed\n",
      "2021-01-15 02:21:33,718 : INFO : token count processed\n",
      "2021-01-15 02:21:33,724 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:21:33,728 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:21:33,729 : INFO : vocab #32006\n",
      "2021-01-15 02:21:33,736 : INFO : diff #set()\n",
      "2021-01-15 02:21:52,700 : INFO : alphabet #32006\n",
      "2021-01-15 02:22:02,121 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.1821500143737842, 0.4582636360529833], [0.8062994927167892, 0.1937005], [2.556656707462823, 1.2476381669990098], [5.362003250893972, 6.0479231618016716, 6.511462000329187, 4.898464412366456, 1.1494587494352153, 0.46353883852751565]]\n",
      "2021-01-15 02:22:02,125 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:22:02,126 : INFO : built Dictionary(172 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 706 corpus positions)\n",
      "2021-01-15 02:22:02,277 : INFO : token count processed\n",
      "2021-01-15 02:22:02,309 : INFO : frequencies processed\n",
      "2021-01-15 02:22:11,737 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:22:11,738 : INFO : entropies processed\n",
      "2021-01-15 02:22:11,739 : INFO : extropies processed\n",
      "2021-01-15 02:22:11,753 : INFO : token count processed\n",
      "2021-01-15 02:22:11,758 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:22:11,763 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:22:11,764 : INFO : vocab #32006\n",
      "2021-01-15 02:22:11,773 : INFO : diff #set()\n",
      "2021-01-15 02:22:30,775 : INFO : alphabet #32006\n",
      "2021-01-15 02:22:40,219 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.1904481964726334, 0.45652757349401824], [0.826917827129364, 0.17308217], [2.8453509366224363, 1.3210203571681218], [5.362003250893972, 6.036583168403119, 6.461941530622971, 4.93664488867412, 1.099938279728999, 0.42535836221985157]]\n",
      "2021-01-15 02:22:40,233 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 02:22:40,234 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:22:40,237 : INFO : built Dictionary(578 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 7064 corpus positions)\n",
      "2021-01-15 02:22:41,277 : INFO : token count processed\n",
      "2021-01-15 02:22:41,323 : INFO : frequencies processed\n",
      "2021-01-15 02:22:50,751 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:22:50,752 : INFO : entropies processed\n",
      "2021-01-15 02:22:50,753 : INFO : extropies processed\n",
      "2021-01-15 02:22:50,762 : INFO : token count processed\n",
      "2021-01-15 02:22:50,766 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:22:50,771 : INFO : alphabet_target #32010\n",
      "2021-01-15 02:22:50,772 : INFO : vocab #32006\n",
      "2021-01-15 02:22:50,780 : INFO : diff #set()\n",
      "2021-01-15 02:23:09,795 : INFO : alphabet #32006\n",
      "2021-01-15 02:23:19,235 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.1263659495298561, 0.47028593559876275], [0.6932134330272675, 0.30678657], [3.8143719431796272, 1.375194245172978], [5.362003250893972, 7.29352035514053, 7.484992746932839, 5.170530859101664, 2.1229894960388673, 0.1914723917923089]]\n",
      "2021-01-15 02:23:19,243 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:23:19,245 : INFO : built Dictionary(389 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 3307 corpus positions)\n",
      "2021-01-15 02:23:19,745 : INFO : token count processed\n",
      "2021-01-15 02:23:19,793 : INFO : frequencies processed\n",
      "2021-01-15 02:23:29,342 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:23:29,344 : INFO : entropies processed\n",
      "2021-01-15 02:23:29,344 : INFO : extropies processed\n",
      "2021-01-15 02:23:29,352 : INFO : token count processed\n",
      "2021-01-15 02:23:29,359 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:23:29,363 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:23:29,364 : INFO : vocab #32006\n",
      "2021-01-15 02:23:29,371 : INFO : diff #set()\n",
      "2021-01-15 02:23:48,221 : INFO : alphabet #32006\n",
      "2021-01-15 02:23:57,636 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.1799444060379798, 0.45872729470999984], [0.7772671282291412, 0.22273287], [3.6011250109569843, 1.3652104012284378], [5.362003250893972, 6.8153433747477745, 7.026500464164686, 5.150846161477061, 1.6644972132707139, 0.21115708941691125]]\n",
      "2021-01-15 02:23:57,640 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:23:57,641 : INFO : built Dictionary(150 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 328 corpus positions)\n",
      "2021-01-15 02:23:57,766 : INFO : token count processed\n",
      "2021-01-15 02:23:57,798 : INFO : frequencies processed\n",
      "2021-01-15 02:24:07,339 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:24:07,340 : INFO : entropies processed\n",
      "2021-01-15 02:24:07,341 : INFO : extropies processed\n",
      "2021-01-15 02:24:07,348 : INFO : token count processed\n",
      "2021-01-15 02:24:07,352 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:24:07,359 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:24:07,360 : INFO : vocab #32006\n",
      "2021-01-15 02:24:07,367 : INFO : diff #set()\n",
      "2021-01-15 02:24:26,274 : INFO : alphabet #32006\n",
      "2021-01-15 02:24:35,720 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.1818891204819517, 0.4583184317721483], [0.8054524213075638, 0.19454758], [2.058813890331201, 1.2062416803425784], [5.362003250893972, 6.150121915859574, 6.680382339165473, 4.831742827588072, 1.3183790882715014, 0.530260423305899]]\n",
      "2021-01-15 02:24:35,725 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:24:35,726 : INFO : built Dictionary(307 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1039 corpus positions)\n",
      "2021-01-15 02:24:36,082 : INFO : token count processed\n",
      "2021-01-15 02:24:36,114 : INFO : frequencies processed\n",
      "2021-01-15 02:24:45,565 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:24:45,566 : INFO : entropies processed\n",
      "2021-01-15 02:24:45,567 : INFO : extropies processed\n",
      "2021-01-15 02:24:45,574 : INFO : token count processed\n",
      "2021-01-15 02:24:45,577 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:24:45,581 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:24:45,582 : INFO : vocab #32006\n",
      "2021-01-15 02:24:45,587 : INFO : diff #set()\n",
      "2021-01-15 02:25:04,458 : INFO : alphabet #32006\n",
      "2021-01-15 02:25:13,887 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.1303802519328636, 0.4693997698733427], [0.7211692035198212, 0.2788308], [3.3758145836939115, 1.3492844704371658], [5.362003250893972, 7.0391145208191315, 7.391363975185316, 5.009753796527788, 2.029360724291344, 0.35224945436618427]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 02:25:13,896 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 02:25:13,897 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:25:13,900 : INFO : built Dictionary(605 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 4414 corpus positions)\n",
      "2021-01-15 02:25:15,064 : INFO : token count processed\n",
      "2021-01-15 02:25:15,094 : INFO : frequencies processed\n",
      "2021-01-15 02:25:24,630 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:25:24,631 : INFO : entropies processed\n",
      "2021-01-15 02:25:24,632 : INFO : extropies processed\n",
      "2021-01-15 02:25:24,640 : INFO : token count processed\n",
      "2021-01-15 02:25:24,646 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:25:24,652 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:25:24,653 : INFO : vocab #32006\n",
      "2021-01-15 02:25:24,659 : INFO : diff #set()\n",
      "2021-01-15 02:25:43,572 : INFO : alphabet #32006\n",
      "2021-01-15 02:25:53,121 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.1101041617668672, 0.47391025434624207], [0.6860526204109192, 0.31394738], [4.057476076289931, 1.3869063270638977], [5.362003250893972, 7.482466367279176, 7.786316911822178, 5.058152706350969, 2.424313660928206, 0.3038505445430024]]\n",
      "2021-01-15 02:25:53,124 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:25:53,126 : INFO : built Dictionary(196 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 636 corpus positions)\n",
      "2021-01-15 02:25:53,311 : INFO : token count processed\n",
      "2021-01-15 02:25:53,341 : INFO : frequencies processed\n",
      "2021-01-15 02:26:02,766 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:26:02,768 : INFO : entropies processed\n",
      "2021-01-15 02:26:02,768 : INFO : extropies processed\n",
      "2021-01-15 02:26:02,775 : INFO : token count processed\n",
      "2021-01-15 02:26:02,782 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:26:02,787 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:26:02,788 : INFO : vocab #32006\n",
      "2021-01-15 02:26:02,795 : INFO : diff #set()\n",
      "2021-01-15 02:26:21,789 : INFO : alphabet #32006\n",
      "2021-01-15 02:26:31,207 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.1753370549400237, 0.45969887642426566], [0.7874975949525833, 0.2125024], [2.556656707462823, 1.2476381669990098], [5.362003250893972, 6.372162341197667, 6.793161649643288, 4.941003942448351, 1.4311583987493162, 0.42099930844562117]]\n",
      "2021-01-15 02:26:31,213 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:26:31,217 : INFO : built Dictionary(343 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 2057 corpus positions)\n",
      "2021-01-15 02:26:31,626 : INFO : token count processed\n",
      "2021-01-15 02:26:31,659 : INFO : frequencies processed\n",
      "2021-01-15 02:26:41,254 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:26:41,255 : INFO : entropies processed\n",
      "2021-01-15 02:26:41,256 : INFO : extropies processed\n",
      "2021-01-15 02:26:41,263 : INFO : token count processed\n",
      "2021-01-15 02:26:41,268 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:26:41,274 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:26:41,275 : INFO : vocab #32006\n",
      "2021-01-15 02:26:41,282 : INFO : diff #set()\n",
      "2021-01-15 02:27:00,296 : INFO : alphabet #32006\n",
      "2021-01-15 02:27:09,895 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.182088124185306, 0.4582766337053208], [0.7883561998605728, 0.2116438], [2.970950594454669, 1.3173982504938433], [5.362003250893972, 6.798155919669889, 7.08775497491569, 5.072404195648172, 1.725751724021718, 0.28959905524580076]]\n",
      "2021-01-15 02:27:09,899 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:27:09,900 : INFO : built Dictionary(199 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 773 corpus positions)\n",
      "2021-01-15 02:27:10,093 : INFO : token count processed\n",
      "2021-01-15 02:27:10,128 : INFO : frequencies processed\n",
      "2021-01-15 02:27:19,562 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:27:19,563 : INFO : entropies processed\n",
      "2021-01-15 02:27:19,564 : INFO : extropies processed\n",
      "2021-01-15 02:27:19,571 : INFO : token count processed\n",
      "2021-01-15 02:27:19,575 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:27:19,580 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:27:19,580 : INFO : vocab #32006\n",
      "2021-01-15 02:27:19,592 : INFO : diff #set()\n",
      "2021-01-15 02:27:38,613 : INFO : alphabet #32006\n",
      "2021-01-15 02:27:48,053 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.1776594947177297, 0.4592086147653772], [0.7981547117233276, 0.20184529], [2.939829017466402, 1.3012558257668299], [5.362003250893972, 6.271631856729336, 6.6698625316872295, 4.963772575936078, 1.3078592807932576, 0.3982306749578939]]\n",
      "2021-01-15 02:27:48,060 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:27:48,063 : INFO : built Dictionary(372 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 3288 corpus positions)\n",
      "2021-01-15 02:27:48,568 : INFO : token count processed\n",
      "2021-01-15 02:27:48,613 : INFO : frequencies processed\n",
      "2021-01-15 02:27:58,050 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:27:58,051 : INFO : entropies processed\n",
      "2021-01-15 02:27:58,052 : INFO : extropies processed\n",
      "2021-01-15 02:27:58,060 : INFO : token count processed\n",
      "2021-01-15 02:27:58,067 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:27:58,071 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:27:58,072 : INFO : vocab #32006\n",
      "2021-01-15 02:27:58,080 : INFO : diff #set()\n",
      "2021-01-15 02:28:17,100 : INFO : alphabet #32006\n",
      "2021-01-15 02:28:26,529 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.1447224105408444, 0.46626080610022874], [0.7604440748691559, 0.23955593], [3.622579761842491, 1.3687877799930959], [5.362003250893972, 6.873598627629562, 7.0874844666944945, 5.148117411829039, 1.7254812158005226, 0.21388583906493253]]\n",
      "2021-01-15 02:28:26,532 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:28:26,534 : INFO : built Dictionary(132 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 236 corpus positions)\n",
      "2021-01-15 02:28:26,629 : INFO : token count processed\n",
      "2021-01-15 02:28:26,662 : INFO : frequencies processed\n",
      "2021-01-15 02:28:36,092 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:28:36,093 : INFO : entropies processed\n",
      "2021-01-15 02:28:36,094 : INFO : extropies processed\n",
      "2021-01-15 02:28:36,101 : INFO : token count processed\n",
      "2021-01-15 02:28:36,109 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:28:36,113 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:28:36,114 : INFO : vocab #32006\n",
      "2021-01-15 02:28:36,121 : INFO : diff #set()\n",
      "2021-01-15 02:28:55,015 : INFO : alphabet #32006\n",
      "2021-01-15 02:29:04,450 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.1613957986605374, 0.46266398806721154], [0.7687875181436539, 0.23121248], [1.9182958340544893, 1.2183406773511978], [5.362003250893972, 6.049830202851529, 6.686273454350764, 4.725559999394737, 1.3242702034567921, 0.6364432514992346]]\n",
      "2021-01-15 02:29:04,455 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:29:04,457 : INFO : built Dictionary(272 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1316 corpus positions)\n",
      "2021-01-15 02:29:04,773 : INFO : token count processed\n",
      "2021-01-15 02:29:04,821 : INFO : frequencies processed\n",
      "2021-01-15 02:29:14,364 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:29:14,365 : INFO : entropies processed\n",
      "2021-01-15 02:29:14,365 : INFO : extropies processed\n",
      "2021-01-15 02:29:14,373 : INFO : token count processed\n",
      "2021-01-15 02:29:14,380 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:29:14,385 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:29:14,386 : INFO : vocab #32006\n",
      "2021-01-15 02:29:14,394 : INFO : diff #set()\n",
      "2021-01-15 02:29:33,274 : INFO : alphabet #32006\n",
      "2021-01-15 02:29:42,697 : INFO : Computed distances or similarities ('266', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.1458719466576115, 0.46601103181277426], [0.756993755698204, 0.24300624], [3.292878689342031, 1.3483557008036189], [5.362003250893972, 6.778844940588858, 7.08968373134109, 5.05116446014174, 1.727680480447118, 0.3108387907522321]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 02:29:42,700 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:29:42,702 : INFO : built Dictionary(176 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 349 corpus positions)\n",
      "2021-01-15 02:29:42,862 : INFO : token count processed\n",
      "2021-01-15 02:29:42,895 : INFO : frequencies processed\n",
      "2021-01-15 02:29:52,446 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:29:52,447 : INFO : entropies processed\n",
      "2021-01-15 02:29:52,448 : INFO : extropies processed\n",
      "2021-01-15 02:29:52,460 : INFO : token count processed\n",
      "2021-01-15 02:29:52,466 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:29:52,472 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:29:52,473 : INFO : vocab #32006\n",
      "2021-01-15 02:29:52,481 : INFO : diff #set()\n",
      "2021-01-15 02:30:11,370 : INFO : alphabet #32006\n",
      "2021-01-15 02:30:20,924 : INFO : Computed distances or similarities ('266', 'sacp-python-common/setup.py')[[1.1691083354545222, 0.461018928218012], [0.7843053936958313, 0.2156946], [2.725480556997868, 1.3192201298976014], [5.362003250893972, 6.469677430851302, 7.00414724688037, 4.827533434864904, 1.6421439959863982, 0.534469816029068]]\n",
      "2021-01-15 02:30:20,928 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:30:20,931 : INFO : built Dictionary(242 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1251 corpus positions)\n",
      "2021-01-15 02:30:21,198 : INFO : token count processed\n",
      "2021-01-15 02:30:21,257 : INFO : frequencies processed\n",
      "2021-01-15 02:30:30,672 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:30:30,673 : INFO : entropies processed\n",
      "2021-01-15 02:30:30,674 : INFO : extropies processed\n",
      "2021-01-15 02:30:30,681 : INFO : token count processed\n",
      "2021-01-15 02:30:30,688 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:30:30,693 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:30:30,694 : INFO : vocab #32006\n",
      "2021-01-15 02:30:30,701 : INFO : diff #set()\n",
      "2021-01-15 02:30:49,579 : INFO : alphabet #32006\n",
      "2021-01-15 02:30:59,125 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.1649095766937825, 0.46191305667703], [0.7613387256860733, 0.23866127], [3.140361163698485, 1.3338450692604933], [5.362003250893972, 6.459180448028249, 6.779141732671068, 5.042041966251153, 1.417138481777096, 0.3199612846428188]]\n",
      "2021-01-15 02:30:59,129 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:30:59,131 : INFO : built Dictionary(156 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 401 corpus positions)\n",
      "2021-01-15 02:30:59,283 : INFO : token count processed\n",
      "2021-01-15 02:30:59,356 : INFO : frequencies processed\n",
      "2021-01-15 02:31:08,780 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:31:08,781 : INFO : entropies processed\n",
      "2021-01-15 02:31:08,782 : INFO : extropies processed\n",
      "2021-01-15 02:31:08,789 : INFO : token count processed\n",
      "2021-01-15 02:31:08,795 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:31:08,800 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:31:08,801 : INFO : vocab #32006\n",
      "2021-01-15 02:31:08,808 : INFO : diff #set()\n",
      "2021-01-15 02:31:27,717 : INFO : alphabet #32006\n",
      "2021-01-15 02:31:37,281 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.1370278482243668, 0.4679396203614703], [0.740732878446579, 0.25926712], [2.353533948577482, 1.2177935163324858], [5.362003250893972, 6.097125733496388, 6.596916155783454, 4.862212828606907, 1.234912904889482, 0.4997904222870657]]\n",
      "2021-01-15 02:31:37,284 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:31:37,286 : INFO : built Dictionary(148 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 397 corpus positions)\n",
      "2021-01-15 02:31:37,420 : INFO : token count processed\n",
      "2021-01-15 02:31:37,454 : INFO : frequencies processed\n",
      "2021-01-15 02:31:47,254 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:31:47,255 : INFO : entropies processed\n",
      "2021-01-15 02:31:47,256 : INFO : extropies processed\n",
      "2021-01-15 02:31:47,263 : INFO : token count processed\n",
      "2021-01-15 02:31:47,269 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:31:47,275 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:31:47,276 : INFO : vocab #32006\n",
      "2021-01-15 02:31:47,283 : INFO : diff #set()\n",
      "2021-01-15 02:32:06,668 : INFO : alphabet #32006\n",
      "2021-01-15 02:32:16,100 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.1361174073263047, 0.4681390622866845], [0.7443320453166962, 0.25566795], [2.4137995646056805, 1.2397034351567289], [5.362003250893972, 6.0695858597523715, 6.5645431940171015, 4.867045916629241, 1.2025399431231296, 0.49495733426473]]\n",
      "2021-01-15 02:32:16,104 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 02:32:16,105 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:32:16,106 : INFO : built Dictionary(151 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 493 corpus positions)\n",
      "2021-01-15 02:32:16,231 : INFO : token count processed\n",
      "2021-01-15 02:32:16,263 : INFO : frequencies processed\n",
      "2021-01-15 02:32:25,685 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:32:25,686 : INFO : entropies processed\n",
      "2021-01-15 02:32:25,687 : INFO : extropies processed\n",
      "2021-01-15 02:32:25,694 : INFO : token count processed\n",
      "2021-01-15 02:32:25,699 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:32:25,704 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:32:25,705 : INFO : vocab #32006\n",
      "2021-01-15 02:32:25,712 : INFO : diff #set()\n",
      "2021-01-15 02:32:44,738 : INFO : alphabet #32006\n",
      "2021-01-15 02:32:54,173 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.1627789028548332, 0.46236811293101493], [0.7677251547574997, 0.23227485], [2.270942421748537, 1.2317687033144475], [5.362003250893972, 6.104787343210121, 6.556089488193864, 4.9107011059102295, 1.1940862372998922, 0.4513021449837433]]\n",
      "2021-01-15 02:32:54,190 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 02:32:54,191 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:32:54,194 : INFO : built Dictionary(429 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 9178 corpus positions)\n",
      "2021-01-15 02:32:54,792 : INFO : token count processed\n",
      "2021-01-15 02:32:54,838 : INFO : frequencies processed\n",
      "2021-01-15 02:33:04,138 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:33:04,139 : INFO : entropies processed\n",
      "2021-01-15 02:33:04,140 : INFO : extropies processed\n",
      "2021-01-15 02:33:04,150 : INFO : token count processed\n",
      "2021-01-15 02:33:04,154 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:33:04,158 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:33:04,159 : INFO : vocab #32006\n",
      "2021-01-15 02:33:04,168 : INFO : diff #set()\n",
      "2021-01-15 02:33:23,208 : INFO : alphabet #32006\n",
      "2021-01-15 02:33:32,651 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.1409611350895765, 0.46707994069129166], [0.7381637990474701, 0.2618362], [3.719375822415369, 1.3715742053681013], [5.362003250893972, 6.89087415148015, 7.07687237664036, 5.176005025733762, 1.7148691257463877, 0.18599822516020925]]\n",
      "2021-01-15 02:33:32,657 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:33:32,659 : INFO : built Dictionary(289 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 2338 corpus positions)\n",
      "2021-01-15 02:33:32,981 : INFO : token count processed\n",
      "2021-01-15 02:33:33,012 : INFO : frequencies processed\n",
      "2021-01-15 02:33:42,437 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:33:42,438 : INFO : entropies processed\n",
      "2021-01-15 02:33:42,439 : INFO : extropies processed\n",
      "2021-01-15 02:33:42,452 : INFO : token count processed\n",
      "2021-01-15 02:33:42,456 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:33:42,460 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:33:42,461 : INFO : vocab #32006\n",
      "2021-01-15 02:33:42,468 : INFO : diff #set()\n",
      "2021-01-15 02:34:01,446 : INFO : alphabet #32006\n",
      "2021-01-15 02:34:10,859 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.166398167135141, 0.4615956637936074], [0.7687641382217407, 0.23123586], [3.5296964395284127, 1.3633016379265088], [5.362003250893972, 6.655493573668506, 6.884560571964886, 5.132936252597593, 1.5225573210709138, 0.22906699829637933]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 02:34:10,864 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:34:10,866 : INFO : built Dictionary(268 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1345 corpus positions)\n",
      "2021-01-15 02:34:11,158 : INFO : token count processed\n",
      "2021-01-15 02:34:11,193 : INFO : frequencies processed\n",
      "2021-01-15 02:34:20,722 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:34:20,723 : INFO : entropies processed\n",
      "2021-01-15 02:34:20,724 : INFO : extropies processed\n",
      "2021-01-15 02:34:20,738 : INFO : token count processed\n",
      "2021-01-15 02:34:20,742 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:34:20,746 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:34:20,747 : INFO : vocab #32006\n",
      "2021-01-15 02:34:20,753 : INFO : diff #set()\n",
      "2021-01-15 02:34:39,632 : INFO : alphabet #32006\n",
      "2021-01-15 02:34:49,604 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.148396924275699, 0.46546333626740577], [0.7572666555643082, 0.24273334], [3.27280432733462, 1.3385495915153638], [5.362003250893972, 6.6236746347295465, 6.881220146065722, 5.104457739557796, 1.5192168951717502, 0.25754551133617554]]\n",
      "2021-01-15 02:34:49,608 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:34:49,610 : INFO : built Dictionary(267 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1284 corpus positions)\n",
      "2021-01-15 02:34:49,905 : INFO : token count processed\n",
      "2021-01-15 02:34:49,937 : INFO : frequencies processed\n",
      "2021-01-15 02:34:59,362 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:34:59,364 : INFO : entropies processed\n",
      "2021-01-15 02:34:59,364 : INFO : extropies processed\n",
      "2021-01-15 02:34:59,372 : INFO : token count processed\n",
      "2021-01-15 02:34:59,376 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:34:59,381 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:34:59,382 : INFO : vocab #32006\n",
      "2021-01-15 02:34:59,390 : INFO : diff #set()\n",
      "2021-01-15 02:35:18,264 : INFO : alphabet #32006\n",
      "2021-01-15 02:35:27,806 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.147855257975222, 0.4655807211807641], [0.7210007309913635, 0.27899927], [3.4906015629507228, 1.3546321911175774], [5.362003250893972, 6.75472436518627, 6.999634652776274, 5.117092963303968, 1.6376314018823024, 0.24491028759000422]]\n",
      "2021-01-15 02:35:27,810 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:35:27,811 : INFO : built Dictionary(225 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1079 corpus positions)\n",
      "2021-01-15 02:35:28,052 : INFO : token count processed\n",
      "2021-01-15 02:35:28,116 : INFO : frequencies processed\n",
      "2021-01-15 02:35:37,558 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:35:37,559 : INFO : entropies processed\n",
      "2021-01-15 02:35:37,560 : INFO : extropies processed\n",
      "2021-01-15 02:35:37,567 : INFO : token count processed\n",
      "2021-01-15 02:35:37,574 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:35:37,579 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:35:37,580 : INFO : vocab #32006\n",
      "2021-01-15 02:35:37,587 : INFO : diff #set()\n",
      "2021-01-15 02:35:56,465 : INFO : alphabet #32006\n",
      "2021-01-15 02:36:06,009 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.1593173192533894, 0.46310933140005667], [0.7287961542606354, 0.27120385], [2.6601297526332566, 1.2849790309834181], [5.362003250893972, 6.597313085495733, 6.873955600699754, 5.08536073568995, 1.5119523498057825, 0.2766425152040215]]\n",
      "2021-01-15 02:36:06,014 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:36:06,015 : INFO : built Dictionary(247 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1045 corpus positions)\n",
      "2021-01-15 02:36:06,271 : INFO : token count processed\n",
      "2021-01-15 02:36:06,333 : INFO : frequencies processed\n",
      "2021-01-15 02:36:15,755 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:36:15,756 : INFO : entropies processed\n",
      "2021-01-15 02:36:15,757 : INFO : extropies processed\n",
      "2021-01-15 02:36:15,765 : INFO : token count processed\n",
      "2021-01-15 02:36:15,772 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:36:15,777 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:36:15,778 : INFO : vocab #32006\n",
      "2021-01-15 02:36:15,785 : INFO : diff #set()\n",
      "2021-01-15 02:36:34,689 : INFO : alphabet #32006\n",
      "2021-01-15 02:36:44,259 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.173064325836689, 0.4601796587935668], [0.7674245685338974, 0.23257543], [3.1086949695628423, 1.325206121374507], [5.362003250893972, 6.659481538516613, 6.953140454419085, 5.068344334991501, 1.591137203525113, 0.29365891590247184]]\n",
      "2021-01-15 02:36:44,263 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:36:44,265 : INFO : built Dictionary(272 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1207 corpus positions)\n",
      "2021-01-15 02:36:44,579 : INFO : token count processed\n",
      "2021-01-15 02:36:44,611 : INFO : frequencies processed\n",
      "2021-01-15 02:36:54,038 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:36:54,040 : INFO : entropies processed\n",
      "2021-01-15 02:36:54,040 : INFO : extropies processed\n",
      "2021-01-15 02:36:54,047 : INFO : token count processed\n",
      "2021-01-15 02:36:54,052 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:36:54,056 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:36:54,057 : INFO : vocab #32006\n",
      "2021-01-15 02:36:54,064 : INFO : diff #set()\n",
      "2021-01-15 02:37:12,909 : INFO : alphabet #32006\n",
      "2021-01-15 02:37:22,373 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.1742021292179705, 0.4599388375908203], [0.7753662616014481, 0.22463374], [3.1086949695628423, 1.3252061213745068], [5.362003250893972, 6.774682571479102, 7.021162185428877, 5.115523636944197, 1.6591589345349051, 0.24647961394977536]]\n",
      "2021-01-15 02:37:22,388 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:37:22,392 : INFO : built Dictionary(446 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 7944 corpus positions)\n",
      "2021-01-15 02:37:23,075 : INFO : token count processed\n",
      "2021-01-15 02:37:23,107 : INFO : frequencies processed\n",
      "2021-01-15 02:37:32,705 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:37:32,706 : INFO : entropies processed\n",
      "2021-01-15 02:37:32,706 : INFO : extropies processed\n",
      "2021-01-15 02:37:32,716 : INFO : token count processed\n",
      "2021-01-15 02:37:32,720 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:37:32,724 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:37:32,725 : INFO : vocab #32006\n",
      "2021-01-15 02:37:32,732 : INFO : diff #set()\n",
      "2021-01-15 02:37:51,775 : INFO : alphabet #32006\n",
      "2021-01-15 02:38:01,219 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.142773796956236, 0.4666848182577547], [0.7322050631046295, 0.26779494], [3.6243797016511117, 1.3679541655632246], [5.362003250893972, 6.839453716525233, 7.021029682658669, 5.180427284760536, 1.6590264317646968, 0.18157596613343596]]\n",
      "2021-01-15 02:38:01,225 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:38:01,227 : INFO : built Dictionary(344 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 2386 corpus positions)\n",
      "2021-01-15 02:38:01,641 : INFO : token count processed\n",
      "2021-01-15 02:38:01,674 : INFO : frequencies processed\n",
      "2021-01-15 02:38:11,100 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:38:11,101 : INFO : entropies processed\n",
      "2021-01-15 02:38:11,102 : INFO : extropies processed\n",
      "2021-01-15 02:38:11,110 : INFO : token count processed\n",
      "2021-01-15 02:38:11,114 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:38:11,118 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:38:11,119 : INFO : vocab #32006\n",
      "2021-01-15 02:38:11,126 : INFO : diff #set()\n",
      "2021-01-15 02:38:30,134 : INFO : alphabet #32006\n",
      "2021-01-15 02:38:39,590 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.1393695932598946, 0.46742741560434903], [0.7167662680149078, 0.28323373], [3.819548827786958, 1.378018083789763], [5.362003250893972, 6.86432793886027, 7.0515220668889835, 5.174809122865257, 1.6895188159950116, 0.1871941280287137]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 02:38:39,594 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:38:39,595 : INFO : built Dictionary(187 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 588 corpus positions)\n",
      "2021-01-15 02:38:39,767 : INFO : token count processed\n",
      "2021-01-15 02:38:39,810 : INFO : frequencies processed\n",
      "2021-01-15 02:38:49,246 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:38:49,247 : INFO : entropies processed\n",
      "2021-01-15 02:38:49,247 : INFO : extropies processed\n",
      "2021-01-15 02:38:49,254 : INFO : token count processed\n",
      "2021-01-15 02:38:49,259 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:38:49,264 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:38:49,265 : INFO : vocab #32006\n",
      "2021-01-15 02:38:49,273 : INFO : diff #set()\n",
      "2021-01-15 02:39:08,577 : INFO : alphabet #32006\n",
      "2021-01-15 02:39:18,019 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.1633397626137925, 0.4622482410214562], [0.7339092791080475, 0.26609072], [2.6644977792004614, 1.3077785569772304], [5.362003250893972, 6.431978396403875, 6.814864165682145, 4.9791174816157024, 1.4528609147881735, 0.38288576927827034]]\n",
      "2021-01-15 02:39:18,023 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:39:18,025 : INFO : built Dictionary(242 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 742 corpus positions)\n",
      "2021-01-15 02:39:18,275 : INFO : token count processed\n",
      "2021-01-15 02:39:18,320 : INFO : frequencies processed\n",
      "2021-01-15 02:39:27,873 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:39:27,874 : INFO : entropies processed\n",
      "2021-01-15 02:39:27,875 : INFO : extropies processed\n",
      "2021-01-15 02:39:27,882 : INFO : token count processed\n",
      "2021-01-15 02:39:27,887 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:39:27,891 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:39:27,892 : INFO : vocab #32006\n",
      "2021-01-15 02:39:27,903 : INFO : diff #set()\n",
      "2021-01-15 02:39:46,773 : INFO : alphabet #32006\n",
      "2021-01-15 02:39:56,186 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/test_auth_utility.py')[[1.1701211097590387, 0.46080377519162324], [0.7724580019712448, 0.227542], [3.327819531114783, 1.3601165249282494], [5.362003250893972, 6.911818353685893, 7.207335468706454, 5.0664861358734115, 1.8453322178124818, 0.29551711502056044]]\n",
      "2021-01-15 02:39:56,200 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:39:56,202 : INFO : built Dictionary(332 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 7272 corpus positions)\n",
      "2021-01-15 02:39:56,639 : INFO : token count processed\n",
      "2021-01-15 02:39:56,671 : INFO : frequencies processed\n",
      "2021-01-15 02:40:06,210 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:40:06,212 : INFO : entropies processed\n",
      "2021-01-15 02:40:06,212 : INFO : extropies processed\n",
      "2021-01-15 02:40:06,222 : INFO : token count processed\n",
      "2021-01-15 02:40:06,228 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:40:06,233 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:40:06,234 : INFO : vocab #32006\n",
      "2021-01-15 02:40:06,243 : INFO : diff #set()\n",
      "2021-01-15 02:40:25,100 : INFO : alphabet #32006\n",
      "2021-01-15 02:40:34,630 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.1494095862424287, 0.4652440402241751], [0.74247145652771, 0.25752854], [3.261842390463366, 1.3420600764705342], [5.362003250893972, 6.363791471162389, 6.4415356021903465, 5.284259119866014, 1.0795323512963746, 0.07774413102795741]]\n",
      "2021-01-15 02:40:34,634 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:40:34,636 : INFO : built Dictionary(233 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1342 corpus positions)\n",
      "2021-01-15 02:40:34,882 : INFO : token count processed\n",
      "2021-01-15 02:40:34,917 : INFO : frequencies processed\n",
      "2021-01-15 02:40:44,357 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:40:44,358 : INFO : entropies processed\n",
      "2021-01-15 02:40:44,359 : INFO : extropies processed\n",
      "2021-01-15 02:40:44,370 : INFO : token count processed\n",
      "2021-01-15 02:40:44,376 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:40:44,381 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:40:44,382 : INFO : vocab #32006\n",
      "2021-01-15 02:40:44,389 : INFO : diff #set()\n",
      "2021-01-15 02:41:03,301 : INFO : alphabet #32006\n",
      "2021-01-15 02:41:12,857 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.1550997489573125, 0.46401564497598], [0.7587878853082657, 0.24121211], [3.292481250360578, 1.346670068407762], [5.362003250893972, 6.29000629755059, 6.648373584095632, 5.0036359643489305, 1.2863703332016598, 0.35836728654504135]]\n",
      "2021-01-15 02:41:12,862 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:41:12,867 : INFO : built Dictionary(244 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1295 corpus positions)\n",
      "2021-01-15 02:41:13,127 : INFO : token count processed\n",
      "2021-01-15 02:41:13,175 : INFO : frequencies processed\n",
      "2021-01-15 02:41:22,590 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:41:22,591 : INFO : entropies processed\n",
      "2021-01-15 02:41:22,592 : INFO : extropies processed\n",
      "2021-01-15 02:41:22,598 : INFO : token count processed\n",
      "2021-01-15 02:41:22,604 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:41:22,607 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:41:22,608 : INFO : vocab #32006\n",
      "2021-01-15 02:41:22,614 : INFO : diff #set()\n",
      "2021-01-15 02:41:41,771 : INFO : alphabet #32006\n",
      "2021-01-15 02:41:51,203 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.155500957461937, 0.46392927664364464], [0.7634423673152924, 0.23655763], [3.286629486786195, 1.356873199789472], [5.362003250893972, 6.361621244785958, 6.7177339647425125, 5.0058905309374175, 1.3557307138485406, 0.35611271995655436]]\n",
      "2021-01-15 02:41:51,208 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:41:51,209 : INFO : built Dictionary(253 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1318 corpus positions)\n",
      "2021-01-15 02:41:51,467 : INFO : token count processed\n",
      "2021-01-15 02:41:51,499 : INFO : frequencies processed\n",
      "2021-01-15 02:42:00,925 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:42:00,926 : INFO : entropies processed\n",
      "2021-01-15 02:42:00,927 : INFO : extropies processed\n",
      "2021-01-15 02:42:00,934 : INFO : token count processed\n",
      "2021-01-15 02:42:00,939 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:42:00,944 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:42:00,945 : INFO : vocab #32006\n",
      "2021-01-15 02:42:00,952 : INFO : diff #set()\n",
      "2021-01-15 02:42:19,985 : INFO : alphabet #32006\n",
      "2021-01-15 02:42:29,410 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.1268150361868072, 0.4701866325869655], [0.6894172132015228, 0.3105828], [3.010434089033337, 1.3244418533115332], [5.362003250893972, 6.620594433343389, 6.957686301810957, 5.024911382426405, 1.5956830509169855, 0.3370918684675681]]\n",
      "2021-01-15 02:42:29,415 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:42:29,416 : INFO : built Dictionary(226 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1411 corpus positions)\n",
      "2021-01-15 02:42:29,663 : INFO : token count processed\n",
      "2021-01-15 02:42:29,696 : INFO : frequencies processed\n",
      "2021-01-15 02:42:39,105 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:42:39,106 : INFO : entropies processed\n",
      "2021-01-15 02:42:39,107 : INFO : extropies processed\n",
      "2021-01-15 02:42:39,122 : INFO : token count processed\n",
      "2021-01-15 02:42:39,129 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:42:39,136 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:42:39,137 : INFO : vocab #32006\n",
      "2021-01-15 02:42:39,146 : INFO : diff #set()\n",
      "2021-01-15 02:42:58,117 : INFO : alphabet #32006\n",
      "2021-01-15 02:43:07,682 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.1631841870402915, 0.462281485779636], [0.7600800395011902, 0.23991996], [3.0062389286533895, 1.3374329493342525], [5.362003250893972, 6.207411496248084, 6.4973460247298265, 5.0720687224122285, 1.1353427738358546, 0.2899345284817425]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 02:43:07,685 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:43:07,687 : INFO : built Dictionary(164 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 315 corpus positions)\n",
      "2021-01-15 02:43:07,839 : INFO : token count processed\n",
      "2021-01-15 02:43:07,871 : INFO : frequencies processed\n",
      "2021-01-15 02:43:17,452 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:43:17,453 : INFO : entropies processed\n",
      "2021-01-15 02:43:17,454 : INFO : extropies processed\n",
      "2021-01-15 02:43:17,468 : INFO : token count processed\n",
      "2021-01-15 02:43:17,472 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:43:17,476 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:43:17,477 : INFO : vocab #32006\n",
      "2021-01-15 02:43:17,484 : INFO : diff #set()\n",
      "2021-01-15 02:43:36,667 : INFO : alphabet #32006\n",
      "2021-01-15 02:43:46,096 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.117957336079621, 0.4721530424456137], [0.6783588528633118, 0.32164115], [3.263933429485634, 1.3501699926082456], [5.362003250893972, 6.5805228788529595, 6.956621100797342, 4.985905028949588, 1.5946178499033703, 0.3760982219443827]]\n",
      "2021-01-15 02:43:46,101 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 02:43:46,102 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:43:46,104 : INFO : built Dictionary(249 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1276 corpus positions)\n",
      "2021-01-15 02:43:46,374 : INFO : token count processed\n",
      "2021-01-15 02:43:46,407 : INFO : frequencies processed\n",
      "2021-01-15 02:43:55,944 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:43:55,945 : INFO : entropies processed\n",
      "2021-01-15 02:43:55,946 : INFO : extropies processed\n",
      "2021-01-15 02:43:55,953 : INFO : token count processed\n",
      "2021-01-15 02:43:55,961 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:43:55,966 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:43:55,967 : INFO : vocab #32006\n",
      "2021-01-15 02:43:55,974 : INFO : diff #set()\n",
      "2021-01-15 02:44:14,877 : INFO : alphabet #32006\n",
      "2021-01-15 02:44:24,573 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.163893639223808, 0.46212992259578045], [0.7626347094774246, 0.23736529], [3.010570934268483, 1.339214111981318], [5.362003250893972, 6.422089779976135, 6.677155468559314, 5.106937562310793, 1.315152217665342, 0.255065688583179]]\n",
      "2021-01-15 02:44:24,578 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:44:24,581 : INFO : built Dictionary(259 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1558 corpus positions)\n",
      "2021-01-15 02:44:24,864 : INFO : token count processed\n",
      "2021-01-15 02:44:24,925 : INFO : frequencies processed\n",
      "2021-01-15 02:44:34,639 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:44:34,641 : INFO : entropies processed\n",
      "2021-01-15 02:44:34,641 : INFO : extropies processed\n",
      "2021-01-15 02:44:34,649 : INFO : token count processed\n",
      "2021-01-15 02:44:34,655 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:44:34,660 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:44:34,661 : INFO : vocab #32006\n",
      "2021-01-15 02:44:34,668 : INFO : diff #set()\n",
      "2021-01-15 02:44:53,570 : INFO : alphabet #32006\n",
      "2021-01-15 02:45:03,020 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.1292339677250722, 0.4696524736867811], [0.7120150625705719, 0.28798494], [3.1403611636984845, 1.3338450692604933], [5.362003250893972, 6.485445644653597, 6.841557862165433, 5.005891033382135, 1.4795546112714613, 0.3561122175118365]]\n",
      "2021-01-15 02:45:03,025 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:45:03,026 : INFO : built Dictionary(236 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1444 corpus positions)\n",
      "2021-01-15 02:45:03,287 : INFO : token count processed\n",
      "2021-01-15 02:45:03,349 : INFO : frequencies processed\n",
      "2021-01-15 02:45:12,907 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:45:12,908 : INFO : entropies processed\n",
      "2021-01-15 02:45:12,909 : INFO : extropies processed\n",
      "2021-01-15 02:45:12,917 : INFO : token count processed\n",
      "2021-01-15 02:45:12,923 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:45:12,928 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:45:12,929 : INFO : vocab #32006\n",
      "2021-01-15 02:45:12,936 : INFO : diff #set()\n",
      "2021-01-15 02:45:32,298 : INFO : alphabet #32006\n",
      "2021-01-15 02:45:41,759 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.1625307069494446, 0.46242117940172117], [0.7594022303819656, 0.24059777], [3.0306390622295662, 1.3416390540875702], [5.362003250893972, 6.2276600107346916, 6.509934677373252, 5.079728584255411, 1.1479314264792801, 0.2822746666385605]]\n",
      "2021-01-15 02:45:41,764 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:45:41,766 : INFO : built Dictionary(224 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1642 corpus positions)\n",
      "2021-01-15 02:45:42,000 : INFO : token count processed\n",
      "2021-01-15 02:45:42,031 : INFO : frequencies processed\n",
      "2021-01-15 02:45:51,440 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:45:51,441 : INFO : entropies processed\n",
      "2021-01-15 02:45:51,442 : INFO : extropies processed\n",
      "2021-01-15 02:45:51,449 : INFO : token count processed\n",
      "2021-01-15 02:45:51,453 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:45:51,458 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:45:51,459 : INFO : vocab #32006\n",
      "2021-01-15 02:45:51,467 : INFO : diff #set()\n",
      "2021-01-15 02:46:10,485 : INFO : alphabet #32006\n",
      "2021-01-15 02:46:19,932 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.1682347540559823, 0.4612046726625713], [0.7734231948852539, 0.2265768], [3.1403611636984845, 1.3338450692604933], [5.362003250893972, 6.253918170574241, 6.563841234152839, 5.052080187315372, 1.2018379832588675, 0.3099230635785988]]\n",
      "2021-01-15 02:46:19,936 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:46:19,939 : INFO : built Dictionary(197 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 739 corpus positions)\n",
      "2021-01-15 02:46:20,142 : INFO : token count processed\n",
      "2021-01-15 02:46:20,188 : INFO : frequencies processed\n",
      "2021-01-15 02:46:29,605 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:46:29,606 : INFO : entropies processed\n",
      "2021-01-15 02:46:29,607 : INFO : extropies processed\n",
      "2021-01-15 02:46:29,614 : INFO : token count processed\n",
      "2021-01-15 02:46:29,620 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:46:29,624 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:46:29,625 : INFO : vocab #32006\n",
      "2021-01-15 02:46:29,632 : INFO : diff #set()\n",
      "2021-01-15 02:46:48,646 : INFO : alphabet #32006\n",
      "2021-01-15 02:46:58,069 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.143222565293283, 0.466587099349226], [0.7389504015445709, 0.2610496], [3.189898095464287, 1.351714529210319], [5.362003250893972, 6.374522245625576, 6.779961800752419, 4.956563695767128, 1.4179585498584473, 0.4054395551268435]]\n",
      "2021-01-15 02:46:58,074 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:46:58,076 : INFO : built Dictionary(318 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 2021 corpus positions)\n",
      "2021-01-15 02:46:58,489 : INFO : token count processed\n",
      "2021-01-15 02:46:58,533 : INFO : frequencies processed\n",
      "2021-01-15 02:47:07,955 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:47:07,956 : INFO : entropies processed\n",
      "2021-01-15 02:47:07,957 : INFO : extropies processed\n",
      "2021-01-15 02:47:07,964 : INFO : token count processed\n",
      "2021-01-15 02:47:07,969 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:47:07,975 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:47:07,976 : INFO : vocab #32006\n",
      "2021-01-15 02:47:07,984 : INFO : diff #set()\n",
      "2021-01-15 02:47:27,002 : INFO : alphabet #32006\n",
      "2021-01-15 02:47:36,428 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.1367236851352545, 0.468006231669913], [0.7393935918807983, 0.2606064], [3.1403611636984845, 1.3338450692604933], [5.362003250893972, 6.731238669067808, 7.091047490599387, 5.002194429362393, 1.7290442397054155, 0.3598088215315798]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 02:47:36,433 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:47:36,435 : INFO : built Dictionary(241 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1413 corpus positions)\n",
      "2021-01-15 02:47:36,706 : INFO : token count processed\n",
      "2021-01-15 02:47:36,739 : INFO : frequencies processed\n",
      "2021-01-15 02:47:46,167 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:47:46,168 : INFO : entropies processed\n",
      "2021-01-15 02:47:46,169 : INFO : extropies processed\n",
      "2021-01-15 02:47:46,176 : INFO : token count processed\n",
      "2021-01-15 02:47:46,181 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:47:46,186 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:47:46,187 : INFO : vocab #32006\n",
      "2021-01-15 02:47:46,195 : INFO : diff #set()\n",
      "2021-01-15 02:48:05,233 : INFO : alphabet #32006\n",
      "2021-01-15 02:48:14,675 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.136362276061049, 0.46808540443045343], [0.7552985697984695, 0.24470143], [3.3758145836939115, 1.3492844704371658], [5.362003250893972, 6.503741451859337, 6.818009355334491, 5.0477353474188185, 1.4560061044405188, 0.31426790347515343]]\n",
      "2021-01-15 02:48:14,680 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:48:14,682 : INFO : built Dictionary(264 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 1735 corpus positions)\n",
      "2021-01-15 02:48:14,995 : INFO : token count processed\n",
      "2021-01-15 02:48:15,061 : INFO : frequencies processed\n",
      "2021-01-15 02:48:24,485 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:48:24,486 : INFO : entropies processed\n",
      "2021-01-15 02:48:24,487 : INFO : extropies processed\n",
      "2021-01-15 02:48:24,494 : INFO : token count processed\n",
      "2021-01-15 02:48:24,500 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:48:24,504 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:48:24,505 : INFO : vocab #32006\n",
      "2021-01-15 02:48:24,512 : INFO : diff #set()\n",
      "2021-01-15 02:48:43,507 : INFO : alphabet #32006\n",
      "2021-01-15 02:48:52,933 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.1554374222476989, 0.4639429517546354], [0.7609875649213791, 0.23901244], [3.261842390463366, 1.342060076470534], [5.362003250893972, 6.334729224484471, 6.58834682234237, 5.108385653036073, 1.226343571448398, 0.2536175978578985]]\n",
      "2021-01-15 02:48:52,939 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:48:52,941 : INFO : built Dictionary(263 unique tokens: ['\"', '.', '2', '65', '>']...) from 2 documents (total 2084 corpus positions)\n",
      "2021-01-15 02:48:53,237 : INFO : token count processed\n",
      "2021-01-15 02:48:53,270 : INFO : frequencies processed\n",
      "2021-01-15 02:49:02,785 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:49:02,786 : INFO : entropies processed\n",
      "2021-01-15 02:49:02,787 : INFO : extropies processed\n",
      "2021-01-15 02:49:02,795 : INFO : token count processed\n",
      "2021-01-15 02:49:02,799 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:49:02,803 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:49:02,804 : INFO : vocab #32006\n",
      "2021-01-15 02:49:02,810 : INFO : diff #set()\n",
      "2021-01-15 02:49:21,690 : INFO : alphabet #32006\n",
      "2021-01-15 02:49:31,113 : INFO : Computed distances or similarities ('266', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.137825471594837, 0.46776503193873487], [0.7517665177583694, 0.24823348], [3.010434089033337, 1.3244418533115332], [5.362003250893972, 6.21319712067992, 6.51755939406709, 5.057640977506802, 1.1555561431731185, 0.30436227338717003]]\n",
      "2021-01-15 02:49:31,118 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 02:49:31,119 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:49:31,120 : INFO : built Dictionary(278 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 1417 corpus positions)\n",
      "2021-01-15 02:49:31,278 : INFO : token count processed\n",
      "2021-01-15 02:49:31,312 : INFO : frequencies processed\n",
      "2021-01-15 02:49:40,809 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:49:40,810 : INFO : entropies processed\n",
      "2021-01-15 02:49:40,811 : INFO : extropies processed\n",
      "2021-01-15 02:49:40,818 : INFO : token count processed\n",
      "2021-01-15 02:49:40,823 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:49:40,827 : INFO : alphabet_target #32010\n",
      "2021-01-15 02:49:40,828 : INFO : vocab #32006\n",
      "2021-01-15 02:49:40,834 : INFO : diff #set()\n",
      "2021-01-15 02:49:59,664 : INFO : alphabet #32006\n",
      "2021-01-15 02:50:09,100 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.1945883451638417, 0.45566632220738545], [0.825949639081955, 0.17405036], [1.9219280948873623, 1.2148067842293933], [4.392747410448784, 6.905617163738059, 7.055091579050604, 4.243272995136239, 2.6623441686018197, 0.1494744153125449]]\n",
      "2021-01-15 02:50:09,106 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:50:09,108 : INFO : built Dictionary(367 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 2307 corpus positions)\n",
      "2021-01-15 02:50:09,345 : INFO : token count processed\n",
      "2021-01-15 02:50:09,405 : INFO : frequencies processed\n",
      "2021-01-15 02:50:18,954 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:50:18,955 : INFO : entropies processed\n",
      "2021-01-15 02:50:18,956 : INFO : extropies processed\n",
      "2021-01-15 02:50:18,963 : INFO : token count processed\n",
      "2021-01-15 02:50:18,967 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:50:18,972 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:50:18,973 : INFO : vocab #32006\n",
      "2021-01-15 02:50:18,979 : INFO : diff #set()\n",
      "2021-01-15 02:50:37,879 : INFO : alphabet #32006\n",
      "2021-01-15 02:50:47,427 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.1936200613356633, 0.45586745746258106], [0.8450790643692017, 0.15492094], [2.5, 1.2968140217166515], [4.392747410448784, 7.1219284286457345, 7.346660151000091, 4.168015688094428, 2.9539127405513064, 0.22473172235435612]]\n",
      "2021-01-15 02:50:47,433 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:50:47,438 : INFO : built Dictionary(287 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 2290 corpus positions)\n",
      "2021-01-15 02:50:47,605 : INFO : token count processed\n",
      "2021-01-15 02:50:47,669 : INFO : frequencies processed\n",
      "2021-01-15 02:50:57,102 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:50:57,103 : INFO : entropies processed\n",
      "2021-01-15 02:50:57,103 : INFO : extropies processed\n",
      "2021-01-15 02:50:57,111 : INFO : token count processed\n",
      "2021-01-15 02:50:57,115 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:50:57,119 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:50:57,120 : INFO : vocab #32006\n",
      "2021-01-15 02:50:57,127 : INFO : diff #set()\n",
      "2021-01-15 02:51:16,060 : INFO : alphabet #32006\n",
      "2021-01-15 02:51:25,607 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.2422576262413614, 0.4459790829996079], [0.894091933965683, 0.105908066], [2.321928094887362, 1.2877123795494492], [4.392747410448784, 6.41099024988467, 6.4981046574020915, 4.305633002931362, 2.1053572469533073, 0.08711440751742128]]\n",
      "2021-01-15 02:51:25,611 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:51:25,612 : INFO : built Dictionary(167 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 689 corpus positions)\n",
      "2021-01-15 02:51:25,697 : INFO : token count processed\n",
      "2021-01-15 02:51:25,757 : INFO : frequencies processed\n",
      "2021-01-15 02:51:35,173 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:51:35,174 : INFO : entropies processed\n",
      "2021-01-15 02:51:35,175 : INFO : extropies processed\n",
      "2021-01-15 02:51:35,182 : INFO : token count processed\n",
      "2021-01-15 02:51:35,190 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:51:35,197 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:51:35,198 : INFO : vocab #32006\n",
      "2021-01-15 02:51:35,205 : INFO : diff #set()\n",
      "2021-01-15 02:51:54,197 : INFO : alphabet #32006\n",
      "2021-01-15 02:52:03,619 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.239043771754851, 0.4466192276429906], [0.8899121209979057, 0.11008788], [1.584962500721156, 1.1699250014423124], [4.392747410448784, 6.077866832717642, 6.270561949558953, 4.200052293607474, 1.8778145391101688, 0.19269511684131135]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 02:52:03,622 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:52:03,623 : INFO : built Dictionary(141 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 454 corpus positions)\n",
      "2021-01-15 02:52:03,694 : INFO : token count processed\n",
      "2021-01-15 02:52:03,753 : INFO : frequencies processed\n",
      "2021-01-15 02:52:13,188 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:52:13,189 : INFO : entropies processed\n",
      "2021-01-15 02:52:13,190 : INFO : extropies processed\n",
      "2021-01-15 02:52:13,197 : INFO : token count processed\n",
      "2021-01-15 02:52:13,202 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:52:13,206 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:52:13,207 : INFO : vocab #32006\n",
      "2021-01-15 02:52:13,215 : INFO : diff #set()\n",
      "2021-01-15 02:52:32,198 : INFO : alphabet #32006\n",
      "2021-01-15 02:52:41,615 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.237186199336975, 0.4469900629175907], [0.8902659565210342, 0.10973404], [2.321928094887362, 1.2877123795494492], [4.392747410448784, 5.977547459003844, 6.167912674054772, 4.202382195397856, 1.775165263605988, 0.19036521505092807]]\n",
      "2021-01-15 02:52:41,620 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:52:41,622 : INFO : built Dictionary(246 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 2165 corpus positions)\n",
      "2021-01-15 02:52:41,760 : INFO : token count processed\n",
      "2021-01-15 02:52:41,795 : INFO : frequencies processed\n",
      "2021-01-15 02:52:51,229 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:52:51,230 : INFO : entropies processed\n",
      "2021-01-15 02:52:51,231 : INFO : extropies processed\n",
      "2021-01-15 02:52:51,239 : INFO : token count processed\n",
      "2021-01-15 02:52:51,244 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:52:51,249 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:52:51,251 : INFO : vocab #32006\n",
      "2021-01-15 02:52:51,257 : INFO : diff #set()\n",
      "2021-01-15 02:53:10,261 : INFO : alphabet #32006\n",
      "2021-01-15 02:53:19,688 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.2447305894972942, 0.44548775905617666], [0.8923139423131943, 0.10768606], [2.321928094887362, 1.2877123795494492], [4.392747410448784, 6.4614394051846435, 6.581204295302249, 4.272982520331179, 2.1884568848534647, 0.11976489011760538]]\n",
      "2021-01-15 02:53:19,692 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 02:53:19,693 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:53:19,695 : INFO : built Dictionary(209 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 1389 corpus positions)\n",
      "2021-01-15 02:53:19,813 : INFO : token count processed\n",
      "2021-01-15 02:53:19,847 : INFO : frequencies processed\n",
      "2021-01-15 02:53:29,271 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:53:29,272 : INFO : entropies processed\n",
      "2021-01-15 02:53:29,273 : INFO : extropies processed\n",
      "2021-01-15 02:53:29,280 : INFO : token count processed\n",
      "2021-01-15 02:53:29,286 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:53:29,292 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:53:29,293 : INFO : vocab #32006\n",
      "2021-01-15 02:53:29,300 : INFO : diff #set()\n",
      "2021-01-15 02:53:48,457 : INFO : alphabet #32006\n",
      "2021-01-15 02:53:57,656 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.2005793875329411, 0.4544257778952911], [0.8471917659044266, 0.15280823], [1.9219280948873623, 1.2148067842293933], [4.392747410448784, 6.327195724598159, 6.4588424041553365, 4.261100730891607, 2.0660949937065523, 0.13164667955717757]]\n",
      "2021-01-15 02:53:57,668 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 02:53:57,668 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:53:57,671 : INFO : built Dictionary(421 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 6281 corpus positions)\n",
      "2021-01-15 02:53:57,964 : INFO : token count processed\n",
      "2021-01-15 02:53:58,026 : INFO : frequencies processed\n",
      "2021-01-15 02:54:07,459 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:54:07,460 : INFO : entropies processed\n",
      "2021-01-15 02:54:07,461 : INFO : extropies processed\n",
      "2021-01-15 02:54:07,469 : INFO : token count processed\n",
      "2021-01-15 02:54:07,474 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:54:07,478 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:54:07,479 : INFO : vocab #32006\n",
      "2021-01-15 02:54:07,486 : INFO : diff #set()\n",
      "2021-01-15 02:54:26,551 : INFO : alphabet #32006\n",
      "2021-01-15 02:54:35,997 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.2343124367416505, 0.44756497952377833], [0.8780139088630676, 0.12198609], [2.9219280948873623, 1.3359016564230495], [4.392747410448784, 6.9079058562486315, 7.0201577929982495, 4.280495473699167, 2.6274103825494652, 0.11225193674961798]]\n",
      "2021-01-15 02:54:36,004 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:54:36,006 : INFO : built Dictionary(323 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 2675 corpus positions)\n",
      "2021-01-15 02:54:36,202 : INFO : token count processed\n",
      "2021-01-15 02:54:36,269 : INFO : frequencies processed\n",
      "2021-01-15 02:54:45,987 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:54:45,988 : INFO : entropies processed\n",
      "2021-01-15 02:54:45,989 : INFO : extropies processed\n",
      "2021-01-15 02:54:45,997 : INFO : token count processed\n",
      "2021-01-15 02:54:46,004 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:54:46,009 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:54:46,010 : INFO : vocab #32006\n",
      "2021-01-15 02:54:46,017 : INFO : diff #set()\n",
      "2021-01-15 02:55:05,105 : INFO : alphabet #32006\n",
      "2021-01-15 02:55:14,683 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.2072714623823033, 0.45304803556908313], [0.85093954205513, 0.14906046], [2.94770277922009, 1.3393100707180505], [4.392747410448784, 6.61034830706307, 6.774620598576904, 4.22847511893495, 2.38187318812812, 0.16427229151383393]]\n",
      "2021-01-15 02:55:14,687 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:55:14,688 : INFO : built Dictionary(213 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 722 corpus positions)\n",
      "2021-01-15 02:55:14,799 : INFO : token count processed\n",
      "2021-01-15 02:55:14,868 : INFO : frequencies processed\n",
      "2021-01-15 02:55:24,297 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:55:24,298 : INFO : entropies processed\n",
      "2021-01-15 02:55:24,299 : INFO : extropies processed\n",
      "2021-01-15 02:55:24,308 : INFO : token count processed\n",
      "2021-01-15 02:55:24,313 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:55:24,317 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:55:24,318 : INFO : vocab #32006\n",
      "2021-01-15 02:55:24,324 : INFO : diff #set()\n",
      "2021-01-15 02:55:43,593 : INFO : alphabet #32006\n",
      "2021-01-15 02:55:53,328 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.2239216405856108, 0.44965613075138583], [0.8817364647984505, 0.118263535], [1.9219280948873623, 1.2148067842293933], [4.392747410448784, 6.616715366949855, 6.7860743396912575, 4.223388437707381, 2.3933269292424733, 0.16935897274140288]]\n",
      "2021-01-15 02:55:53,334 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:55:53,336 : INFO : built Dictionary(429 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 2756 corpus positions)\n",
      "2021-01-15 02:55:53,635 : INFO : token count processed\n",
      "2021-01-15 02:55:53,705 : INFO : frequencies processed\n",
      "2021-01-15 02:56:03,136 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:56:03,137 : INFO : entropies processed\n",
      "2021-01-15 02:56:03,138 : INFO : extropies processed\n",
      "2021-01-15 02:56:03,146 : INFO : token count processed\n",
      "2021-01-15 02:56:03,150 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:56:03,154 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:56:03,155 : INFO : vocab #32006\n",
      "2021-01-15 02:56:03,163 : INFO : diff #set()\n",
      "2021-01-15 02:56:22,092 : INFO : alphabet #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 02:56:31,667 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.1945242515020444, 0.45567963047824556], [0.8242608308792114, 0.17573917], [2.94770277922009, 1.3393100707180505], [4.392747410448784, 7.32185870753746, 7.525529409040304, 4.18907670894594, 3.13278199859152, 0.20367070150284405]]\n",
      "2021-01-15 02:56:31,670 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:56:31,671 : INFO : built Dictionary(68 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 121 corpus positions)\n",
      "2021-01-15 02:56:31,698 : INFO : token count processed\n",
      "2021-01-15 02:56:31,730 : INFO : frequencies processed\n",
      "2021-01-15 02:56:41,186 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:56:41,187 : INFO : entropies processed\n",
      "2021-01-15 02:56:41,188 : INFO : extropies processed\n",
      "2021-01-15 02:56:41,195 : INFO : token count processed\n",
      "2021-01-15 02:56:41,199 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:56:41,204 : INFO : alphabet_target #32008\n",
      "2021-01-15 02:56:41,205 : INFO : vocab #32006\n",
      "2021-01-15 02:56:41,212 : INFO : diff #set()\n",
      "2021-01-15 02:57:00,400 : INFO : alphabet #32006\n",
      "2021-01-15 02:57:09,843 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/fireException.py')[[1.1976328892749477, 0.45503505379823667], [0.8769299611449242, 0.12307004], [1.0, 1.0], [4.392747410448784, 5.176618657501385, 5.7618486874217805, 3.80751738052839, 1.3691012769729962, 0.5852300299203952]]\n",
      "2021-01-15 02:57:09,847 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:57:09,848 : INFO : built Dictionary(163 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 514 corpus positions)\n",
      "2021-01-15 02:57:09,930 : INFO : token count processed\n",
      "2021-01-15 02:57:09,963 : INFO : frequencies processed\n",
      "2021-01-15 02:57:19,557 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:57:19,558 : INFO : entropies processed\n",
      "2021-01-15 02:57:19,559 : INFO : extropies processed\n",
      "2021-01-15 02:57:19,566 : INFO : token count processed\n",
      "2021-01-15 02:57:19,572 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:57:19,576 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:57:19,577 : INFO : vocab #32006\n",
      "2021-01-15 02:57:19,584 : INFO : diff #set()\n",
      "2021-01-15 02:57:38,616 : INFO : alphabet #32006\n",
      "2021-01-15 02:57:48,057 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.2170848818724327, 0.45104272198881845], [0.8792814761400223, 0.120718524], [1.584962500721156, 1.1699250014423124], [4.392747410448784, 6.468846789852156, 6.699757843117881, 4.161836357183059, 2.307010432669097, 0.23091105326572503]]\n",
      "2021-01-15 02:57:48,063 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:57:48,066 : INFO : built Dictionary(377 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 2563 corpus positions)\n",
      "2021-01-15 02:57:48,316 : INFO : token count processed\n",
      "2021-01-15 02:57:48,384 : INFO : frequencies processed\n",
      "2021-01-15 02:57:57,976 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:57:57,977 : INFO : entropies processed\n",
      "2021-01-15 02:57:57,978 : INFO : extropies processed\n",
      "2021-01-15 02:57:57,985 : INFO : token count processed\n",
      "2021-01-15 02:57:57,990 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:57:57,994 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:57:57,995 : INFO : vocab #32006\n",
      "2021-01-15 02:57:58,002 : INFO : diff #set()\n",
      "2021-01-15 02:58:16,893 : INFO : alphabet #32006\n",
      "2021-01-15 02:58:26,332 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.2281644688548765, 0.44879990412643617], [0.8733973056077957, 0.1266027], [2.521640636343318, 1.2998438251349491], [4.392747410448784, 6.957796704012729, 7.106943224357758, 4.243600890103755, 2.714195813908974, 0.1491465203450293]]\n",
      "2021-01-15 02:58:26,339 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:58:26,341 : INFO : built Dictionary(287 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 3060 corpus positions)\n",
      "2021-01-15 02:58:26,506 : INFO : token count processed\n",
      "2021-01-15 02:58:26,569 : INFO : frequencies processed\n",
      "2021-01-15 02:58:36,002 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:58:36,003 : INFO : entropies processed\n",
      "2021-01-15 02:58:36,003 : INFO : extropies processed\n",
      "2021-01-15 02:58:36,011 : INFO : token count processed\n",
      "2021-01-15 02:58:36,015 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:58:36,020 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:58:36,021 : INFO : vocab #32006\n",
      "2021-01-15 02:58:36,027 : INFO : diff #set()\n",
      "2021-01-15 02:58:55,020 : INFO : alphabet #32006\n",
      "2021-01-15 02:59:04,436 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.2018524942212832, 0.4541630298235143], [0.8659514337778091, 0.13404857], [2.321928094887362, 1.2877123795494492], [4.392747410448784, 6.441859572014148, 6.598544333214131, 4.236062649248802, 2.205796922765347, 0.1566847611999833]]\n",
      "2021-01-15 02:59:04,441 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:59:04,443 : INFO : built Dictionary(311 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 1536 corpus positions)\n",
      "2021-01-15 02:59:04,626 : INFO : token count processed\n",
      "2021-01-15 02:59:04,660 : INFO : frequencies processed\n",
      "2021-01-15 02:59:14,087 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:59:14,088 : INFO : entropies processed\n",
      "2021-01-15 02:59:14,089 : INFO : extropies processed\n",
      "2021-01-15 02:59:14,096 : INFO : token count processed\n",
      "2021-01-15 02:59:14,101 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:59:14,107 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:59:14,108 : INFO : vocab #32006\n",
      "2021-01-15 02:59:14,116 : INFO : diff #set()\n",
      "2021-01-15 02:59:33,130 : INFO : alphabet #32006\n",
      "2021-01-15 02:59:42,531 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.188344694900898, 0.45696640128500704], [0.8291476368904114, 0.17085236], [2.7219280948873625, 1.3198385641318495], [4.392747410448784, 6.998955278238291, 7.1656448228612915, 4.226057865825784, 2.772897412412507, 0.16668954462300078]]\n",
      "2021-01-15 02:59:42,535 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 02:59:42,537 : INFO : built Dictionary(227 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 1355 corpus positions)\n",
      "2021-01-15 02:59:42,666 : INFO : token count processed\n",
      "2021-01-15 02:59:42,729 : INFO : frequencies processed\n",
      "2021-01-15 02:59:52,479 : INFO : scalar_distribution processed\n",
      "2021-01-15 02:59:52,480 : INFO : entropies processed\n",
      "2021-01-15 02:59:52,481 : INFO : extropies processed\n",
      "2021-01-15 02:59:52,494 : INFO : token count processed\n",
      "2021-01-15 02:59:52,500 : INFO : alphabet_source #32006\n",
      "2021-01-15 02:59:52,505 : INFO : alphabet_target #32009\n",
      "2021-01-15 02:59:52,506 : INFO : vocab #32006\n",
      "2021-01-15 02:59:52,513 : INFO : diff #set()\n",
      "2021-01-15 03:00:11,796 : INFO : alphabet #32006\n",
      "2021-01-15 03:00:21,348 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.2045830333835281, 0.45360051531614565], [0.8497924357652664, 0.15020756], [2.503258334775646, 1.2991301890771523], [4.392747410448784, 6.492983191376071, 6.620611260702769, 4.2651193411220865, 2.2278638502539847, 0.1276280693266978]]\n",
      "2021-01-15 03:00:21,355 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:00:21,358 : INFO : built Dictionary(440 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 3294 corpus positions)\n",
      "2021-01-15 03:00:21,635 : INFO : token count processed\n",
      "2021-01-15 03:00:21,680 : INFO : frequencies processed\n",
      "2021-01-15 03:00:31,109 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:00:31,110 : INFO : entropies processed\n",
      "2021-01-15 03:00:31,111 : INFO : extropies processed\n",
      "2021-01-15 03:00:31,119 : INFO : token count processed\n",
      "2021-01-15 03:00:31,124 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:00:31,129 : INFO : alphabet_target #32008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 03:00:31,130 : INFO : vocab #32006\n",
      "2021-01-15 03:00:31,137 : INFO : diff #set()\n",
      "2021-01-15 03:00:50,030 : INFO : alphabet #32006\n",
      "2021-01-15 03:00:59,583 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.230499973375734, 0.44832997621002313], [0.8744784295558929, 0.12552157], [1.0, 1.0], [4.392747410448784, 6.560342487747443, 6.7633951905419005, 4.1896947076543265, 2.3706477800931163, 0.20305270279445775]]\n",
      "2021-01-15 03:00:59,590 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 03:00:59,591 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:00:59,593 : INFO : built Dictionary(450 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 3499 corpus positions)\n",
      "2021-01-15 03:00:59,911 : INFO : token count processed\n",
      "2021-01-15 03:00:59,973 : INFO : frequencies processed\n",
      "2021-01-15 03:01:09,407 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:01:09,408 : INFO : entropies processed\n",
      "2021-01-15 03:01:09,409 : INFO : extropies processed\n",
      "2021-01-15 03:01:09,417 : INFO : token count processed\n",
      "2021-01-15 03:01:09,423 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:01:09,428 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:01:09,429 : INFO : vocab #32006\n",
      "2021-01-15 03:01:09,436 : INFO : diff #set()\n",
      "2021-01-15 03:01:28,576 : INFO : alphabet #32006\n",
      "2021-01-15 03:01:38,110 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.2053366229718308, 0.45344551465908944], [0.8481417149305344, 0.15185829], [3.1219280948873624, 1.3519647487142497], [4.392747410448784, 7.046173750105238, 7.23848403956972, 4.2004371209843026, 2.8457366291209354, 0.1923102894644817]]\n",
      "2021-01-15 03:01:38,121 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 03:01:38,122 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:01:38,125 : INFO : built Dictionary(501 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 5608 corpus positions)\n",
      "2021-01-15 03:01:38,506 : INFO : token count processed\n",
      "2021-01-15 03:01:38,553 : INFO : frequencies processed\n",
      "2021-01-15 03:01:47,952 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:01:47,953 : INFO : entropies processed\n",
      "2021-01-15 03:01:47,954 : INFO : extropies processed\n",
      "2021-01-15 03:01:47,963 : INFO : token count processed\n",
      "2021-01-15 03:01:47,967 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:01:47,971 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:01:47,972 : INFO : vocab #32006\n",
      "2021-01-15 03:01:47,981 : INFO : diff #set()\n",
      "2021-01-15 03:02:07,005 : INFO : alphabet #32006\n",
      "2021-01-15 03:02:16,421 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.218244672932446, 0.4508068979956269], [0.8596081733703613, 0.14039183], [3.5068905956085192, 1.3728719392429896], [4.392747410448784, 7.009229588004272, 7.118358350158433, 4.283618648294624, 2.7256109397096484, 0.10912876215416034]]\n",
      "2021-01-15 03:02:16,434 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:02:16,437 : INFO : built Dictionary(583 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 6549 corpus positions)\n",
      "2021-01-15 03:02:16,864 : INFO : token count processed\n",
      "2021-01-15 03:02:16,890 : INFO : frequencies processed\n",
      "2021-01-15 03:02:26,178 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:02:26,179 : INFO : entropies processed\n",
      "2021-01-15 03:02:26,180 : INFO : extropies processed\n",
      "2021-01-15 03:02:26,195 : INFO : token count processed\n",
      "2021-01-15 03:02:26,200 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:02:26,204 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:02:26,205 : INFO : vocab #32006\n",
      "2021-01-15 03:02:26,212 : INFO : diff #set()\n",
      "2021-01-15 03:02:45,249 : INFO : alphabet #32006\n",
      "2021-01-15 03:02:54,686 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.2021077714969954, 0.45411038140072446], [0.8395795524120331, 0.16042045], [3.095795255000934, 1.3487605247277434], [4.392747410448784, 7.376088004590871, 7.542691951235771, 4.226143463803885, 3.1499445407869864, 0.16660394664489964]]\n",
      "2021-01-15 03:02:54,689 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:02:54,691 : INFO : built Dictionary(139 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 335 corpus positions)\n",
      "2021-01-15 03:02:54,762 : INFO : token count processed\n",
      "2021-01-15 03:02:54,795 : INFO : frequencies processed\n",
      "2021-01-15 03:03:04,228 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:03:04,229 : INFO : entropies processed\n",
      "2021-01-15 03:03:04,230 : INFO : extropies processed\n",
      "2021-01-15 03:03:04,236 : INFO : token count processed\n",
      "2021-01-15 03:03:04,244 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:03:04,249 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:03:04,250 : INFO : vocab #32006\n",
      "2021-01-15 03:03:04,257 : INFO : diff #set()\n",
      "2021-01-15 03:03:23,278 : INFO : alphabet #32006\n",
      "2021-01-15 03:03:32,708 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.211402201191699, 0.4522017747206327], [0.8675399422645569, 0.13246006], [1.584962500721156, 1.1699250014423124], [4.392747410448784, 6.2993628166120885, 6.544993785708257, 4.147116441352615, 2.152246375259473, 0.24563096909616888]]\n",
      "2021-01-15 03:03:32,711 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:03:32,712 : INFO : built Dictionary(38 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 51 corpus positions)\n",
      "2021-01-15 03:03:32,729 : INFO : token count processed\n",
      "2021-01-15 03:03:32,761 : INFO : frequencies processed\n",
      "2021-01-15 03:03:42,190 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:03:42,191 : INFO : entropies processed\n",
      "2021-01-15 03:03:42,192 : INFO : extropies processed\n",
      "2021-01-15 03:03:42,199 : INFO : token count processed\n",
      "2021-01-15 03:03:42,206 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:03:42,211 : INFO : alphabet_target #32008\n",
      "2021-01-15 03:03:42,212 : INFO : vocab #32006\n",
      "2021-01-15 03:03:42,218 : INFO : diff #set()\n",
      "2021-01-15 03:04:01,234 : INFO : alphabet #32006\n",
      "2021-01-15 03:04:10,649 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.254008870275174, 0.44365397722588285], [0.8931660056114197, 0.106833994], [0.0, 0.0], [4.392747410448784, 3.8936606896881862, 4.995025599694795, 3.2913825004421753, 0.602278189246011, 1.101364910006609]]\n",
      "2021-01-15 03:04:10,672 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:04:10,677 : INFO : built Dictionary(741 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 12493 corpus positions)\n",
      "2021-01-15 03:04:11,370 : INFO : token count processed\n",
      "2021-01-15 03:04:11,402 : INFO : frequencies processed\n",
      "2021-01-15 03:04:20,936 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:04:20,937 : INFO : entropies processed\n",
      "2021-01-15 03:04:20,938 : INFO : extropies processed\n",
      "2021-01-15 03:04:20,949 : INFO : token count processed\n",
      "2021-01-15 03:04:20,955 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:04:20,960 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:04:20,961 : INFO : vocab #32006\n",
      "2021-01-15 03:04:20,969 : INFO : diff #set()\n",
      "2021-01-15 03:04:40,242 : INFO : alphabet #32006\n",
      "2021-01-15 03:04:49,693 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.1782165234269966, 0.45909118273820454], [0.81874680519104, 0.1812532], [3.251629167387823, 1.3589504783379556], [4.392747410448784, 7.434393313070278, 7.640472532767779, 4.186668190751282, 3.247725122318995, 0.20607921969750098]]\n",
      "2021-01-15 03:04:49,702 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:04:49,708 : INFO : built Dictionary(495 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 4120 corpus positions)\n",
      "2021-01-15 03:04:50,043 : INFO : token count processed\n",
      "2021-01-15 03:04:50,091 : INFO : frequencies processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 03:04:59,629 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:04:59,630 : INFO : entropies processed\n",
      "2021-01-15 03:04:59,631 : INFO : extropies processed\n",
      "2021-01-15 03:04:59,639 : INFO : token count processed\n",
      "2021-01-15 03:04:59,643 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:04:59,648 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:04:59,648 : INFO : vocab #32006\n",
      "2021-01-15 03:04:59,655 : INFO : diff #set()\n",
      "2021-01-15 03:05:18,528 : INFO : alphabet #32006\n",
      "2021-01-15 03:05:27,961 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.2036515223034798, 0.4537922579313714], [0.8299861401319504, 0.17001386], [2.75, 1.3226647836567116], [4.392747410448784, 7.2991514951718255, 7.494508676285076, 4.197390229335534, 3.1017612658362914, 0.19535718111325018]]\n",
      "2021-01-15 03:05:27,969 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:05:27,971 : INFO : built Dictionary(462 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 3537 corpus positions)\n",
      "2021-01-15 03:05:28,283 : INFO : token count processed\n",
      "2021-01-15 03:05:28,318 : INFO : frequencies processed\n",
      "2021-01-15 03:05:37,868 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:05:37,869 : INFO : entropies processed\n",
      "2021-01-15 03:05:37,870 : INFO : extropies processed\n",
      "2021-01-15 03:05:37,878 : INFO : token count processed\n",
      "2021-01-15 03:05:37,883 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:05:37,888 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:05:37,889 : INFO : vocab #32006\n",
      "2021-01-15 03:05:37,897 : INFO : diff #set()\n",
      "2021-01-15 03:05:56,766 : INFO : alphabet #32006\n",
      "2021-01-15 03:06:06,297 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.212295386033786, 0.45201920426765657], [0.8477745205163956, 0.15222548], [2.5, 1.2968140217166515], [4.392747410448784, 7.170319527000998, 7.360736976980078, 4.202329960469704, 2.967989566531294, 0.19041744997908072]]\n",
      "2021-01-15 03:06:06,300 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:06:06,302 : INFO : built Dictionary(174 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 573 corpus positions)\n",
      "2021-01-15 03:06:06,389 : INFO : token count processed\n",
      "2021-01-15 03:06:06,425 : INFO : frequencies processed\n",
      "2021-01-15 03:06:15,854 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:06:15,855 : INFO : entropies processed\n",
      "2021-01-15 03:06:15,856 : INFO : extropies processed\n",
      "2021-01-15 03:06:15,863 : INFO : token count processed\n",
      "2021-01-15 03:06:15,868 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:06:15,873 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:06:15,874 : INFO : vocab #32006\n",
      "2021-01-15 03:06:15,882 : INFO : diff #set()\n",
      "2021-01-15 03:06:34,949 : INFO : alphabet #32006\n",
      "2021-01-15 03:06:44,367 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.2239026372232915, 0.4496599730861305], [0.8701508343219757, 0.12984917], [1.0, 1.0], [4.392747410448784, 6.353654804387375, 6.590789732030947, 4.155612482805212, 2.1980423215821627, 0.2371349276435719]]\n",
      "2021-01-15 03:06:44,370 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:06:44,372 : INFO : built Dictionary(169 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 701 corpus positions)\n",
      "2021-01-15 03:06:44,454 : INFO : token count processed\n",
      "2021-01-15 03:06:44,487 : INFO : frequencies processed\n",
      "2021-01-15 03:06:53,925 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:06:53,926 : INFO : entropies processed\n",
      "2021-01-15 03:06:53,927 : INFO : extropies processed\n",
      "2021-01-15 03:06:53,934 : INFO : token count processed\n",
      "2021-01-15 03:06:53,942 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:06:53,946 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:06:53,947 : INFO : vocab #32006\n",
      "2021-01-15 03:06:53,954 : INFO : diff #set()\n",
      "2021-01-15 03:07:12,955 : INFO : alphabet #32006\n",
      "2021-01-15 03:07:22,394 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.2229543022298046, 0.44985180262001717], [0.8675733357667923, 0.13242666], [1.0, 1.0], [4.392747410448784, 6.245180322479091, 6.442581473147159, 4.195346259780715, 2.049834062698375, 0.1974011506680684]]\n",
      "2021-01-15 03:07:22,399 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:07:22,401 : INFO : built Dictionary(393 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 1773 corpus positions)\n",
      "2021-01-15 03:07:22,661 : INFO : token count processed\n",
      "2021-01-15 03:07:22,721 : INFO : frequencies processed\n",
      "2021-01-15 03:07:32,150 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:07:32,151 : INFO : entropies processed\n",
      "2021-01-15 03:07:32,152 : INFO : extropies processed\n",
      "2021-01-15 03:07:32,162 : INFO : token count processed\n",
      "2021-01-15 03:07:32,167 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:07:32,171 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:07:32,172 : INFO : vocab #32006\n",
      "2021-01-15 03:07:32,178 : INFO : diff #set()\n",
      "2021-01-15 03:07:51,202 : INFO : alphabet #32006\n",
      "2021-01-15 03:08:00,645 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.1931840736389285, 0.45595808031780993], [0.8190713822841644, 0.18092862], [2.725480556997868, 1.3192201298976014], [4.392747410448784, 7.2691387000368, 7.477771561399621, 4.1841145490859635, 3.0850241509508365, 0.20863286136282078]]\n",
      "2021-01-15 03:08:00,650 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:08:00,652 : INFO : built Dictionary(316 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 1610 corpus positions)\n",
      "2021-01-15 03:08:00,846 : INFO : token count processed\n",
      "2021-01-15 03:08:00,879 : INFO : frequencies processed\n",
      "2021-01-15 03:08:10,311 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:08:10,312 : INFO : entropies processed\n",
      "2021-01-15 03:08:10,312 : INFO : extropies processed\n",
      "2021-01-15 03:08:10,320 : INFO : token count processed\n",
      "2021-01-15 03:08:10,326 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:08:10,331 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:08:10,332 : INFO : vocab #32006\n",
      "2021-01-15 03:08:10,339 : INFO : diff #set()\n",
      "2021-01-15 03:08:29,348 : INFO : alphabet #32006\n",
      "2021-01-15 03:08:38,932 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.1712172123408975, 0.4605711461368945], [0.7945047318935394, 0.20549527], [3.2516291673878226, 1.3589504783379556], [4.392747410448784, 7.08857858466988, 7.254978738278098, 4.226347256840565, 2.8622313278293134, 0.16640015360821803]]\n",
      "2021-01-15 03:08:38,935 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:08:38,937 : INFO : built Dictionary(148 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 533 corpus positions)\n",
      "2021-01-15 03:08:39,017 : INFO : token count processed\n",
      "2021-01-15 03:08:39,050 : INFO : frequencies processed\n",
      "2021-01-15 03:08:48,598 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:08:48,599 : INFO : entropies processed\n",
      "2021-01-15 03:08:48,600 : INFO : extropies processed\n",
      "2021-01-15 03:08:48,607 : INFO : token count processed\n",
      "2021-01-15 03:08:48,612 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:08:48,616 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:08:48,617 : INFO : vocab #32006\n",
      "2021-01-15 03:08:48,624 : INFO : diff #set()\n",
      "2021-01-15 03:09:07,533 : INFO : alphabet #32006\n",
      "2021-01-15 03:09:16,977 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.2250335458497907, 0.4494314262655655], [0.8784143403172493, 0.12158566], [1.0, 1.0], [4.392747410448784, 6.0479231618016716, 6.285648955085271, 4.155021617165184, 1.8929015446364872, 0.23772579328359988]]\n",
      "2021-01-15 03:09:16,981 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:09:16,983 : INFO : built Dictionary(150 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 652 corpus positions)\n",
      "2021-01-15 03:09:17,056 : INFO : token count processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 03:09:17,090 : INFO : frequencies processed\n",
      "2021-01-15 03:09:26,631 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:09:26,632 : INFO : entropies processed\n",
      "2021-01-15 03:09:26,633 : INFO : extropies processed\n",
      "2021-01-15 03:09:26,640 : INFO : token count processed\n",
      "2021-01-15 03:09:26,646 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:09:26,652 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:09:26,653 : INFO : vocab #32006\n",
      "2021-01-15 03:09:26,660 : INFO : diff #set()\n",
      "2021-01-15 03:09:45,585 : INFO : alphabet #32006\n",
      "2021-01-15 03:09:55,010 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.2236731726604266, 0.44970637425264665], [0.873454824090004, 0.12654518], [1.0, 1.0], [4.392747410448784, 6.036583168403119, 6.240505287916552, 4.188825290935352, 1.8477578774677674, 0.20392211951343242]]\n",
      "2021-01-15 03:09:55,024 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 03:09:55,025 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:09:55,028 : INFO : built Dictionary(561 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 7010 corpus positions)\n",
      "2021-01-15 03:09:55,468 : INFO : token count processed\n",
      "2021-01-15 03:09:55,515 : INFO : frequencies processed\n",
      "2021-01-15 03:10:05,052 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:10:05,053 : INFO : entropies processed\n",
      "2021-01-15 03:10:05,054 : INFO : extropies processed\n",
      "2021-01-15 03:10:05,063 : INFO : token count processed\n",
      "2021-01-15 03:10:05,068 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:10:05,074 : INFO : alphabet_target #32010\n",
      "2021-01-15 03:10:05,075 : INFO : vocab #32006\n",
      "2021-01-15 03:10:05,083 : INFO : diff #set()\n",
      "2021-01-15 03:10:24,389 : INFO : alphabet #32006\n",
      "2021-01-15 03:10:33,827 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.2111088504545382, 0.4522617689284857], [0.8573571145534515, 0.14264289], [2.5, 1.2968140217166515], [4.392747410448784, 7.29352035514053, 7.454149243040905, 4.232118522548409, 3.0614018325921206, 0.16062888790037455]]\n",
      "2021-01-15 03:10:33,834 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:10:33,836 : INFO : built Dictionary(369 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 3253 corpus positions)\n",
      "2021-01-15 03:10:34,072 : INFO : token count processed\n",
      "2021-01-15 03:10:34,101 : INFO : frequencies processed\n",
      "2021-01-15 03:10:43,438 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:10:43,439 : INFO : entropies processed\n",
      "2021-01-15 03:10:43,440 : INFO : extropies processed\n",
      "2021-01-15 03:10:43,447 : INFO : token count processed\n",
      "2021-01-15 03:10:43,454 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:10:43,460 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:10:43,461 : INFO : vocab #32006\n",
      "2021-01-15 03:10:43,468 : INFO : diff #set()\n",
      "2021-01-15 03:11:02,536 : INFO : alphabet #32006\n",
      "2021-01-15 03:11:12,095 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.2065508730603587, 0.45319598664546434], [0.8394015580415726, 0.16059844], [2.75, 1.3226647836567116], [4.392747410448784, 6.8153433747477745, 6.962105286151781, 4.245985499044778, 2.5693578757029965, 0.14676191140400618]]\n",
      "2021-01-15 03:11:12,099 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:11:12,100 : INFO : built Dictionary(125 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 274 corpus positions)\n",
      "2021-01-15 03:11:12,160 : INFO : token count processed\n",
      "2021-01-15 03:11:12,192 : INFO : frequencies processed\n",
      "2021-01-15 03:11:21,614 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:11:21,615 : INFO : entropies processed\n",
      "2021-01-15 03:11:21,616 : INFO : extropies processed\n",
      "2021-01-15 03:11:21,623 : INFO : token count processed\n",
      "2021-01-15 03:11:21,628 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:11:21,633 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:11:21,634 : INFO : vocab #32006\n",
      "2021-01-15 03:11:21,641 : INFO : diff #set()\n",
      "2021-01-15 03:11:40,660 : INFO : alphabet #32006\n",
      "2021-01-15 03:11:50,094 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.2157383317720811, 0.45131682999780487], [0.8738540858030319, 0.12614591], [1.0, 1.0], [4.392747410448784, 6.150121915859574, 6.409666205651181, 4.133203120657178, 2.016918795202397, 0.2595442897916067]]\n",
      "2021-01-15 03:11:50,099 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:11:50,100 : INFO : built Dictionary(284 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 985 corpus positions)\n",
      "2021-01-15 03:11:50,263 : INFO : token count processed\n",
      "2021-01-15 03:11:50,296 : INFO : frequencies processed\n",
      "2021-01-15 03:11:59,702 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:11:59,703 : INFO : entropies processed\n",
      "2021-01-15 03:11:59,704 : INFO : extropies processed\n",
      "2021-01-15 03:11:59,711 : INFO : token count processed\n",
      "2021-01-15 03:11:59,719 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:11:59,723 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:11:59,724 : INFO : vocab #32006\n",
      "2021-01-15 03:11:59,731 : INFO : diff #set()\n",
      "2021-01-15 03:12:18,746 : INFO : alphabet #32006\n",
      "2021-01-15 03:12:28,193 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.1643074332536334, 0.46204156795630746], [0.8003519177436829, 0.19964808], [2.94770277922009, 1.3393100707180505], [4.392747410448784, 7.0391145208191315, 7.282231991310099, 4.1496299399578165, 2.889484580861315, 0.24311747049096777]]\n",
      "2021-01-15 03:12:28,202 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 03:12:28,203 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:12:28,206 : INFO : built Dictionary(590 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 4360 corpus positions)\n",
      "2021-01-15 03:12:28,724 : INFO : token count processed\n",
      "2021-01-15 03:12:28,792 : INFO : frequencies processed\n",
      "2021-01-15 03:12:38,214 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:12:38,215 : INFO : entropies processed\n",
      "2021-01-15 03:12:38,216 : INFO : extropies processed\n",
      "2021-01-15 03:12:38,227 : INFO : token count processed\n",
      "2021-01-15 03:12:38,232 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:12:38,236 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:12:38,236 : INFO : vocab #32006\n",
      "2021-01-15 03:12:38,243 : INFO : diff #set()\n",
      "2021-01-15 03:12:57,264 : INFO : alphabet #32006\n",
      "2021-01-15 03:13:06,707 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.1884985611698085, 0.4569342734525146], [0.8060946762561798, 0.19390532], [2.75, 1.3226647836567116], [4.392747410448784, 7.482466367279176, 7.753691944120827, 4.121521833607134, 3.3609445336720425, 0.27122557684165116]]\n",
      "2021-01-15 03:13:06,711 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:13:06,712 : INFO : built Dictionary(174 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 582 corpus positions)\n",
      "2021-01-15 03:13:06,797 : INFO : token count processed\n",
      "2021-01-15 03:13:06,865 : INFO : frequencies processed\n",
      "2021-01-15 03:13:16,301 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:13:16,302 : INFO : entropies processed\n",
      "2021-01-15 03:13:16,303 : INFO : extropies processed\n",
      "2021-01-15 03:13:16,310 : INFO : token count processed\n",
      "2021-01-15 03:13:16,314 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:13:16,319 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:13:16,320 : INFO : vocab #32006\n",
      "2021-01-15 03:13:16,329 : INFO : diff #set()\n",
      "2021-01-15 03:13:35,512 : INFO : alphabet #32006\n",
      "2021-01-15 03:13:44,935 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.2248304345011374, 0.4494724561893298], [0.8698044121265411, 0.13019559], [1.0, 1.0], [4.392747410448784, 6.372162341197667, 6.604887371464297, 4.160022380182154, 2.212139961015513, 0.23272503026663038]]\n",
      "2021-01-15 03:13:44,940 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 03:13:44,942 : INFO : built Dictionary(319 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 2003 corpus positions)\n",
      "2021-01-15 03:13:45,128 : INFO : token count processed\n",
      "2021-01-15 03:13:45,161 : INFO : frequencies processed\n",
      "2021-01-15 03:13:54,578 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:13:54,579 : INFO : entropies processed\n",
      "2021-01-15 03:13:54,580 : INFO : extropies processed\n",
      "2021-01-15 03:13:54,592 : INFO : token count processed\n",
      "2021-01-15 03:13:54,596 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:13:54,601 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:13:54,602 : INFO : vocab #32006\n",
      "2021-01-15 03:13:54,608 : INFO : diff #set()\n",
      "2021-01-15 03:14:13,627 : INFO : alphabet #32006\n",
      "2021-01-15 03:14:23,066 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.2108011345682765, 0.4523247181141325], [0.8565149158239365, 0.14348508], [2.521640636343318, 1.2998438251349491], [4.392747410448784, 6.798155919669889, 6.992707678681957, 4.198195651436715, 2.599960268233173, 0.194551759012068]]\n",
      "2021-01-15 03:14:23,070 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:14:23,071 : INFO : built Dictionary(177 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 719 corpus positions)\n",
      "2021-01-15 03:14:23,162 : INFO : token count processed\n",
      "2021-01-15 03:14:23,194 : INFO : frequencies processed\n",
      "2021-01-15 03:14:32,729 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:14:32,730 : INFO : entropies processed\n",
      "2021-01-15 03:14:32,731 : INFO : extropies processed\n",
      "2021-01-15 03:14:32,738 : INFO : token count processed\n",
      "2021-01-15 03:14:32,744 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:14:32,748 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:14:32,749 : INFO : vocab #32006\n",
      "2021-01-15 03:14:32,756 : INFO : diff #set()\n",
      "2021-01-15 03:14:51,672 : INFO : alphabet #32006\n",
      "2021-01-15 03:15:00,993 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.210472824703671, 0.4523918995177228], [0.8643703609704971, 0.13562964], [2.0, 1.2451124978365313], [4.392747410448784, 6.271631856729336, 6.484392678224918, 4.179986588953202, 2.0916452677761335, 0.21276082149558206]]\n",
      "2021-01-15 03:15:01,000 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:15:01,003 : INFO : built Dictionary(350 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 3234 corpus positions)\n",
      "2021-01-15 03:15:01,237 : INFO : token count processed\n",
      "2021-01-15 03:15:01,318 : INFO : frequencies processed\n",
      "2021-01-15 03:15:10,966 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:15:10,967 : INFO : entropies processed\n",
      "2021-01-15 03:15:10,968 : INFO : extropies processed\n",
      "2021-01-15 03:15:10,976 : INFO : token count processed\n",
      "2021-01-15 03:15:10,980 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:15:10,985 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:15:10,985 : INFO : vocab #32006\n",
      "2021-01-15 03:15:10,992 : INFO : diff #set()\n",
      "2021-01-15 03:15:38,208 : INFO : alphabet #32006\n",
      "2021-01-15 03:15:51,001 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.2062599053888046, 0.4532557553883354], [0.8330559134483337, 0.16694409], [3.121928094887362, 1.3519647487142497], [4.392747410448784, 6.873598627629562, 7.030501462670756, 4.23584457540759, 2.637754052221972, 0.15690283504119407]]\n",
      "2021-01-15 03:15:51,004 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:15:51,006 : INFO : built Dictionary(105 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 182 corpus positions)\n",
      "2021-01-15 03:15:51,063 : INFO : token count processed\n",
      "2021-01-15 03:15:51,132 : INFO : frequencies processed\n",
      "2021-01-15 03:16:01,060 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:16:01,061 : INFO : entropies processed\n",
      "2021-01-15 03:16:01,062 : INFO : extropies processed\n",
      "2021-01-15 03:16:01,073 : INFO : token count processed\n",
      "2021-01-15 03:16:01,078 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:16:01,082 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:16:01,083 : INFO : vocab #32006\n",
      "2021-01-15 03:16:01,090 : INFO : diff #set()\n",
      "2021-01-15 03:16:20,941 : INFO : alphabet #32006\n",
      "2021-01-15 03:16:30,992 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.1876546734333506, 0.45711053583725775], [0.8470977544784546, 0.15290225], [1.584962500721156, 1.1699250014423124], [4.392747410448784, 6.049830202851529, 6.396345324371307, 4.046232288929007, 2.0035979139225226, 0.3465151215197775]]\n",
      "2021-01-15 03:16:30,996 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:16:30,998 : INFO : built Dictionary(251 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 1262 corpus positions)\n",
      "2021-01-15 03:16:31,155 : INFO : token count processed\n",
      "2021-01-15 03:16:31,243 : INFO : frequencies processed\n",
      "2021-01-15 03:16:41,183 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:16:41,184 : INFO : entropies processed\n",
      "2021-01-15 03:16:41,185 : INFO : extropies processed\n",
      "2021-01-15 03:16:41,193 : INFO : token count processed\n",
      "2021-01-15 03:16:41,197 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:16:41,201 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:16:41,202 : INFO : vocab #32006\n",
      "2021-01-15 03:16:41,209 : INFO : diff #set()\n",
      "2021-01-15 03:17:01,153 : INFO : alphabet #32006\n",
      "2021-01-15 03:17:11,232 : INFO : Computed distances or similarities ('264', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.1915018222499927, 0.4563080850981498], [0.8283142000436783, 0.1716858], [2.235926350629032, 1.2653331222512112], [4.392747410448784, 6.778844940588858, 6.980755603633005, 4.190836747404637, 2.588008193184221, 0.2019106630441474]]\n",
      "2021-01-15 03:17:11,235 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:17:11,237 : INFO : built Dictionary(151 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 295 corpus positions)\n",
      "2021-01-15 03:17:11,308 : INFO : token count processed\n",
      "2021-01-15 03:17:11,341 : INFO : frequencies processed\n",
      "2021-01-15 03:17:21,276 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:17:21,277 : INFO : entropies processed\n",
      "2021-01-15 03:17:21,278 : INFO : extropies processed\n",
      "2021-01-15 03:17:21,289 : INFO : token count processed\n",
      "2021-01-15 03:17:21,294 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:17:21,300 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:17:21,301 : INFO : vocab #32006\n",
      "2021-01-15 03:17:21,310 : INFO : diff #set()\n",
      "2021-01-15 03:17:41,332 : INFO : alphabet #32006\n",
      "2021-01-15 03:17:51,266 : INFO : Computed distances or similarities ('264', 'sacp-python-common/setup.py')[[1.182896432170888, 0.45810693776502315], [0.8405511975288391, 0.1594488], [2.0, 1.2451124978365313], [4.392747410448784, 6.469677430851302, 6.777010990624424, 4.085413850675662, 2.3842635801756398, 0.30733355977312193]]\n",
      "2021-01-15 03:17:51,271 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:17:51,272 : INFO : built Dictionary(219 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 1197 corpus positions)\n",
      "2021-01-15 03:17:51,394 : INFO : token count processed\n",
      "2021-01-15 03:17:51,461 : INFO : frequencies processed\n",
      "2021-01-15 03:18:01,389 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:18:01,390 : INFO : entropies processed\n",
      "2021-01-15 03:18:01,391 : INFO : extropies processed\n",
      "2021-01-15 03:18:01,402 : INFO : token count processed\n",
      "2021-01-15 03:18:01,407 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:18:01,411 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:18:01,412 : INFO : vocab #32006\n",
      "2021-01-15 03:18:01,419 : INFO : diff #set()\n",
      "2021-01-15 03:18:21,389 : INFO : alphabet #32006\n",
      "2021-01-15 03:18:31,291 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.1988509256200848, 0.4547829906740931], [0.8407324403524399, 0.15926756], [2.521640636343318, 1.2998438251349491], [4.392747410448784, 6.459180448028249, 6.6408255170904065, 4.211102341386627, 2.248078106641622, 0.18164506906215738]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 03:18:31,295 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:18:31,296 : INFO : built Dictionary(129 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 347 corpus positions)\n",
      "2021-01-15 03:18:31,365 : INFO : token count processed\n",
      "2021-01-15 03:18:31,402 : INFO : frequencies processed\n",
      "2021-01-15 03:18:41,536 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:18:41,537 : INFO : entropies processed\n",
      "2021-01-15 03:18:41,538 : INFO : extropies processed\n",
      "2021-01-15 03:18:41,549 : INFO : token count processed\n",
      "2021-01-15 03:18:41,554 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:18:41,558 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:18:41,559 : INFO : vocab #32006\n",
      "2021-01-15 03:18:41,566 : INFO : diff #set()\n",
      "2021-01-15 03:19:01,700 : INFO : alphabet #32006\n",
      "2021-01-15 03:19:11,623 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.2045394466046035, 0.45360948362261516], [0.8564743995666504, 0.1435256], [2.584962500721156, 1.315172029168969], [4.392747410448784, 6.097125733496388, 6.342379069625342, 4.147494074319829, 1.949631659176558, 0.24525333612895395]]\n",
      "2021-01-15 03:19:11,627 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:19:11,628 : INFO : built Dictionary(121 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 343 corpus positions)\n",
      "2021-01-15 03:19:11,682 : INFO : token count processed\n",
      "2021-01-15 03:19:11,714 : INFO : frequencies processed\n",
      "2021-01-15 03:19:21,637 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:19:21,638 : INFO : entropies processed\n",
      "2021-01-15 03:19:21,638 : INFO : extropies processed\n",
      "2021-01-15 03:19:21,646 : INFO : token count processed\n",
      "2021-01-15 03:19:21,650 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:19:21,655 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:19:21,656 : INFO : vocab #32006\n",
      "2021-01-15 03:19:21,662 : INFO : diff #set()\n",
      "2021-01-15 03:19:41,675 : INFO : alphabet #32006\n",
      "2021-01-15 03:19:51,590 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.205594063202746, 0.45339258782184916], [0.8546429723501205, 0.14535703], [2.584962500721156, 1.315172029168969], [4.392747410448784, 6.0695858597523715, 6.310037989299242, 4.152295280901915, 1.9172905788504577, 0.24045212954687045]]\n",
      "2021-01-15 03:19:51,594 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 03:19:51,595 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:19:51,596 : INFO : built Dictionary(123 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 439 corpus positions)\n",
      "2021-01-15 03:19:51,649 : INFO : token count processed\n",
      "2021-01-15 03:19:51,683 : INFO : frequencies processed\n",
      "2021-01-15 03:20:01,772 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:20:01,773 : INFO : entropies processed\n",
      "2021-01-15 03:20:01,774 : INFO : extropies processed\n",
      "2021-01-15 03:20:01,785 : INFO : token count processed\n",
      "2021-01-15 03:20:01,789 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:20:01,794 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:20:01,796 : INFO : vocab #32006\n",
      "2021-01-15 03:20:01,804 : INFO : diff #set()\n",
      "2021-01-15 03:20:21,566 : INFO : alphabet #32006\n",
      "2021-01-15 03:20:31,570 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.189500201806208, 0.45672523764787015], [0.8450931161642075, 0.15490688], [2.521640636343318, 1.2998438251349491], [4.392747410448784, 6.104787343210121, 6.294119045876212, 4.203415707782692, 1.9013716354274282, 0.18933170266609167]]\n",
      "2021-01-15 03:20:31,587 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 03:20:31,588 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:20:31,595 : INFO : built Dictionary(410 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 9124 corpus positions)\n",
      "2021-01-15 03:20:31,887 : INFO : token count processed\n",
      "2021-01-15 03:20:31,967 : INFO : frequencies processed\n",
      "2021-01-15 03:20:41,849 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:20:41,851 : INFO : entropies processed\n",
      "2021-01-15 03:20:41,851 : INFO : extropies processed\n",
      "2021-01-15 03:20:41,865 : INFO : token count processed\n",
      "2021-01-15 03:20:41,870 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:20:41,876 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:20:41,876 : INFO : vocab #32006\n",
      "2021-01-15 03:20:41,883 : INFO : diff #set()\n",
      "2021-01-15 03:21:01,948 : INFO : alphabet #32006\n",
      "2021-01-15 03:21:12,177 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.2082662338133014, 0.45284394820146734], [0.8485689163208008, 0.15143108], [2.725480556997868, 1.3192201298976014], [4.392747410448784, 6.89087415148015, 7.04954613257352, 4.234075429355416, 2.6567987221247353, 0.15867198109336922]]\n",
      "2021-01-15 03:21:12,183 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:21:12,185 : INFO : built Dictionary(268 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 2284 corpus positions)\n",
      "2021-01-15 03:21:12,339 : INFO : token count processed\n",
      "2021-01-15 03:21:12,376 : INFO : frequencies processed\n",
      "2021-01-15 03:21:22,269 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:21:22,270 : INFO : entropies processed\n",
      "2021-01-15 03:21:22,271 : INFO : extropies processed\n",
      "2021-01-15 03:21:22,283 : INFO : token count processed\n",
      "2021-01-15 03:21:22,287 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:21:22,292 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:21:22,293 : INFO : vocab #32006\n",
      "2021-01-15 03:21:22,300 : INFO : diff #set()\n",
      "2021-01-15 03:21:42,163 : INFO : alphabet #32006\n",
      "2021-01-15 03:21:52,196 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.1595242191309627, 0.46306496178237855], [0.7969705909490585, 0.20302941], [2.725480556997868, 1.3192201298976014], [4.392747410448784, 6.655493573668506, 6.796572820471013, 4.251668163646277, 2.403825410022229, 0.14107924680250683]]\n",
      "2021-01-15 03:21:52,201 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:21:52,202 : INFO : built Dictionary(245 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 1291 corpus positions)\n",
      "2021-01-15 03:21:52,334 : INFO : token count processed\n",
      "2021-01-15 03:21:52,368 : INFO : frequencies processed\n",
      "2021-01-15 03:22:02,321 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:22:02,322 : INFO : entropies processed\n",
      "2021-01-15 03:22:02,323 : INFO : extropies processed\n",
      "2021-01-15 03:22:02,334 : INFO : token count processed\n",
      "2021-01-15 03:22:02,338 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:22:02,343 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:22:02,343 : INFO : vocab #32006\n",
      "2021-01-15 03:22:02,350 : INFO : diff #set()\n",
      "2021-01-15 03:22:22,274 : INFO : alphabet #32006\n",
      "2021-01-15 03:22:32,350 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.1958023409267267, 0.45541439744433254], [0.84878970682621, 0.1512103], [2.75, 1.3226647836567116], [4.392747410448784, 6.6236746347295465, 6.75860093460278, 4.25782111057555, 2.365853524153996, 0.1349262998732339]]\n",
      "2021-01-15 03:22:32,354 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:22:32,356 : INFO : built Dictionary(246 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 1230 corpus positions)\n",
      "2021-01-15 03:22:32,506 : INFO : token count processed\n",
      "2021-01-15 03:22:32,542 : INFO : frequencies processed\n",
      "2021-01-15 03:22:42,474 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:22:42,475 : INFO : entropies processed\n",
      "2021-01-15 03:22:42,476 : INFO : extropies processed\n",
      "2021-01-15 03:22:42,488 : INFO : token count processed\n",
      "2021-01-15 03:22:42,492 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:22:42,497 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:22:42,498 : INFO : vocab #32006\n",
      "2021-01-15 03:22:42,504 : INFO : diff #set()\n",
      "2021-01-15 03:23:02,519 : INFO : alphabet #32006\n",
      "2021-01-15 03:23:12,495 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.1480182471978706, 0.46554539343626083], [0.7600791752338409, 0.23992082], [2.75, 1.3226647836567116], [4.392747410448784, 6.75472436518627, 6.880679774767092, 4.266792000867962, 2.4879323643183078, 0.12595540958082196]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 03:23:12,500 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:23:12,501 : INFO : built Dictionary(199 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 1025 corpus positions)\n",
      "2021-01-15 03:23:12,600 : INFO : token count processed\n",
      "2021-01-15 03:23:12,632 : INFO : frequencies processed\n",
      "2021-01-15 03:23:22,570 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:23:22,571 : INFO : entropies processed\n",
      "2021-01-15 03:23:22,572 : INFO : extropies processed\n",
      "2021-01-15 03:23:22,579 : INFO : token count processed\n",
      "2021-01-15 03:23:22,584 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:23:22,588 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:23:22,589 : INFO : vocab #32006\n",
      "2021-01-15 03:23:22,596 : INFO : diff #set()\n",
      "2021-01-15 03:23:42,821 : INFO : alphabet #32006\n",
      "2021-01-15 03:23:52,788 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.1572902987704359, 0.46354447547924255], [0.7800259739160538, 0.21997403], [2.584962500721156, 1.315172029168969], [4.392747410448784, 6.597313085495733, 6.724796785631831, 4.265263710312686, 2.332049375183047, 0.12748370013609822]]\n",
      "2021-01-15 03:23:52,793 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:23:52,794 : INFO : built Dictionary(221 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 991 corpus positions)\n",
      "2021-01-15 03:23:52,906 : INFO : token count processed\n",
      "2021-01-15 03:23:52,939 : INFO : frequencies processed\n",
      "2021-01-15 03:24:02,882 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:24:02,883 : INFO : entropies processed\n",
      "2021-01-15 03:24:02,884 : INFO : extropies processed\n",
      "2021-01-15 03:24:02,891 : INFO : token count processed\n",
      "2021-01-15 03:24:02,896 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:24:02,900 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:24:02,901 : INFO : vocab #32006\n",
      "2021-01-15 03:24:02,908 : INFO : diff #set()\n",
      "2021-01-15 03:24:23,109 : INFO : alphabet #32006\n",
      "2021-01-15 03:24:33,022 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.14188304298528, 0.46687890044931474], [0.7563623934984207, 0.2436376], [3.0849625007211556, 1.3480058660457088], [4.392747410448784, 6.659481538516613, 6.785568536620422, 4.266660412344974, 2.392821126171638, 0.1260869981038093]]\n",
      "2021-01-15 03:24:33,027 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:24:33,028 : INFO : built Dictionary(248 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 1153 corpus positions)\n",
      "2021-01-15 03:24:33,174 : INFO : token count processed\n",
      "2021-01-15 03:24:33,212 : INFO : frequencies processed\n",
      "2021-01-15 03:24:43,111 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:24:43,112 : INFO : entropies processed\n",
      "2021-01-15 03:24:43,113 : INFO : extropies processed\n",
      "2021-01-15 03:24:43,124 : INFO : token count processed\n",
      "2021-01-15 03:24:43,129 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:24:43,134 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:24:43,135 : INFO : vocab #32006\n",
      "2021-01-15 03:24:43,141 : INFO : diff #set()\n",
      "2021-01-15 03:25:03,102 : INFO : alphabet #32006\n",
      "2021-01-15 03:25:13,039 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.1905345873368138, 0.4565095688426312], [0.8281275629997253, 0.17187244], [2.807354922057604, 1.3343545280186873], [4.392747410448784, 6.774682571479102, 6.892540715774031, 4.2748892661538545, 2.4997933053252464, 0.11785814429492891]]\n",
      "2021-01-15 03:25:13,054 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:25:13,058 : INFO : built Dictionary(427 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 7890 corpus positions)\n",
      "2021-01-15 03:25:13,354 : INFO : token count processed\n",
      "2021-01-15 03:25:13,423 : INFO : frequencies processed\n",
      "2021-01-15 03:25:23,731 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:25:23,732 : INFO : entropies processed\n",
      "2021-01-15 03:25:23,733 : INFO : extropies processed\n",
      "2021-01-15 03:25:23,746 : INFO : token count processed\n",
      "2021-01-15 03:25:23,750 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:25:23,757 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:25:23,758 : INFO : vocab #32006\n",
      "2021-01-15 03:25:23,764 : INFO : diff #set()\n",
      "2021-01-15 03:25:43,405 : INFO : alphabet #32006\n",
      "2021-01-15 03:25:53,328 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.2111384692506884, 0.4522557107601138], [0.8536202311515808, 0.14637977], [2.521640636343318, 1.2998438251349491], [4.392747410448784, 6.839453716525233, 6.990393699461903, 4.241807427512114, 2.5976462890131184, 0.15093998293667]]\n",
      "2021-01-15 03:25:53,335 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:25:53,337 : INFO : built Dictionary(325 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 2332 corpus positions)\n",
      "2021-01-15 03:25:53,539 : INFO : token count processed\n",
      "2021-01-15 03:25:53,609 : INFO : frequencies processed\n",
      "2021-01-15 03:26:03,638 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:26:03,639 : INFO : entropies processed\n",
      "2021-01-15 03:26:03,640 : INFO : extropies processed\n",
      "2021-01-15 03:26:03,651 : INFO : token count processed\n",
      "2021-01-15 03:26:03,656 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:26:03,663 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:26:03,664 : INFO : vocab #32006\n",
      "2021-01-15 03:26:03,673 : INFO : diff #set()\n",
      "2021-01-15 03:26:23,310 : INFO : alphabet #32006\n",
      "2021-01-15 03:26:33,340 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.1664577864395087, 0.4615829610248082], [0.7761373668909073, 0.22386263], [2.9219280948873623, 1.3359016564230495], [4.392747410448784, 6.86432793886027, 6.978403603381204, 4.278671745927851, 2.5856561929324196, 0.11407566452093398]]\n",
      "2021-01-15 03:26:33,344 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:26:33,345 : INFO : built Dictionary(161 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 534 corpus positions)\n",
      "2021-01-15 03:26:33,436 : INFO : token count processed\n",
      "2021-01-15 03:26:33,473 : INFO : frequencies processed\n",
      "2021-01-15 03:26:43,376 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:26:43,378 : INFO : entropies processed\n",
      "2021-01-15 03:26:43,382 : INFO : extropies processed\n",
      "2021-01-15 03:26:43,390 : INFO : token count processed\n",
      "2021-01-15 03:26:43,395 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:26:43,399 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:26:43,400 : INFO : vocab #32006\n",
      "2021-01-15 03:26:43,406 : INFO : diff #set()\n",
      "2021-01-15 03:27:03,265 : INFO : alphabet #32006\n",
      "2021-01-15 03:27:13,329 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.1530601838692898, 0.4644552007844427], [0.7643410265445709, 0.23565897], [2.2516291673878226, 1.2667563532600834], [4.392747410448784, 6.431978396403875, 6.600001628072368, 4.22472417878029, 2.207254217623584, 0.1680232316684931]]\n",
      "2021-01-15 03:27:13,333 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:27:13,334 : INFO : built Dictionary(219 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 688 corpus positions)\n",
      "2021-01-15 03:27:13,464 : INFO : token count processed\n",
      "2021-01-15 03:27:13,500 : INFO : frequencies processed\n",
      "2021-01-15 03:27:23,411 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:27:23,412 : INFO : entropies processed\n",
      "2021-01-15 03:27:23,413 : INFO : extropies processed\n",
      "2021-01-15 03:27:23,424 : INFO : token count processed\n",
      "2021-01-15 03:27:23,429 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:27:23,433 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:27:23,434 : INFO : vocab #32006\n",
      "2021-01-15 03:27:23,441 : INFO : diff #set()\n",
      "2021-01-15 03:27:43,434 : INFO : alphabet #32006\n",
      "2021-01-15 03:27:53,340 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/test_auth_utility.py')[[1.1856531128958814, 0.4575291449954974], [0.7992273569107056, 0.20077264], [2.5, 1.2968140217166515], [4.392747410448784, 6.911818353685893, 7.051445705657001, 4.253120058477677, 2.6586982952082163, 0.13962735197110732]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 03:27:53,354 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:27:53,357 : INFO : built Dictionary(311 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 7218 corpus positions)\n",
      "2021-01-15 03:27:53,558 : INFO : token count processed\n",
      "2021-01-15 03:27:53,594 : INFO : frequencies processed\n",
      "2021-01-15 03:28:03,488 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:28:03,489 : INFO : entropies processed\n",
      "2021-01-15 03:28:03,490 : INFO : extropies processed\n",
      "2021-01-15 03:28:03,504 : INFO : token count processed\n",
      "2021-01-15 03:28:03,508 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:28:03,514 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:28:03,515 : INFO : vocab #32006\n",
      "2021-01-15 03:28:03,523 : INFO : diff #set()\n",
      "2021-01-15 03:28:23,529 : INFO : alphabet #32006\n",
      "2021-01-15 03:28:33,428 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.1882588365921778, 0.4569843307738317], [0.8187423497438431, 0.18125765], [2.251629167387823, 1.2667563532600834], [4.392747410448784, 6.363791471162389, 6.401212257156986, 4.355326624454189, 2.0084648467082014, 0.03742078599459653]]\n",
      "2021-01-15 03:28:33,433 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:28:33,435 : INFO : built Dictionary(210 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 1288 corpus positions)\n",
      "2021-01-15 03:28:33,568 : INFO : token count processed\n",
      "2021-01-15 03:28:33,605 : INFO : frequencies processed\n",
      "2021-01-15 03:28:43,494 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:28:43,496 : INFO : entropies processed\n",
      "2021-01-15 03:28:43,496 : INFO : extropies processed\n",
      "2021-01-15 03:28:43,508 : INFO : token count processed\n",
      "2021-01-15 03:28:43,513 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:28:43,517 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:28:43,518 : INFO : vocab #32006\n",
      "2021-01-15 03:28:43,525 : INFO : diff #set()\n",
      "2021-01-15 03:29:03,507 : INFO : alphabet #32006\n",
      "2021-01-15 03:29:13,408 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.2039325941833061, 0.45373438490779344], [0.8482832312583923, 0.15171677], [2.725480556997868, 1.3192201298976014], [4.392747410448784, 6.29000629755059, 6.5147076339868395, 4.168046074012535, 2.1219602235380552, 0.22470133643624912]]\n",
      "2021-01-15 03:29:13,412 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:29:13,414 : INFO : built Dictionary(219 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 1241 corpus positions)\n",
      "2021-01-15 03:29:13,546 : INFO : token count processed\n",
      "2021-01-15 03:29:13,581 : INFO : frequencies processed\n",
      "2021-01-15 03:29:23,517 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:29:23,519 : INFO : entropies processed\n",
      "2021-01-15 03:29:23,519 : INFO : extropies processed\n",
      "2021-01-15 03:29:23,529 : INFO : token count processed\n",
      "2021-01-15 03:29:23,534 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:29:23,538 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:29:23,539 : INFO : vocab #32006\n",
      "2021-01-15 03:29:23,546 : INFO : diff #set()\n",
      "2021-01-15 03:29:43,982 : INFO : alphabet #32006\n",
      "2021-01-15 03:29:53,891 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.1898629851001943, 0.45664957433592424], [0.8380824327468872, 0.16191757], [2.9219280948873623, 1.3359016564230495], [4.392747410448784, 6.361621244785958, 6.568981919637279, 4.185386735597463, 2.176234509188495, 0.20736067485132104]]\n",
      "2021-01-15 03:29:53,895 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:29:53,897 : INFO : built Dictionary(229 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 1264 corpus positions)\n",
      "2021-01-15 03:29:54,012 : INFO : token count processed\n",
      "2021-01-15 03:29:54,046 : INFO : frequencies processed\n",
      "2021-01-15 03:30:04,085 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:30:04,087 : INFO : entropies processed\n",
      "2021-01-15 03:30:04,088 : INFO : extropies processed\n",
      "2021-01-15 03:30:04,099 : INFO : token count processed\n",
      "2021-01-15 03:30:04,104 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:30:04,108 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:30:04,109 : INFO : vocab #32006\n",
      "2021-01-15 03:30:04,116 : INFO : diff #set()\n",
      "2021-01-15 03:30:23,981 : INFO : alphabet #32006\n",
      "2021-01-15 03:30:33,884 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.1458032082103766, 0.4660259599639665], [0.7697844952344894, 0.2302155], [2.521640636343318, 1.2998438251349493], [4.392747410448784, 6.620594433343389, 6.83082028911898, 4.182521554673192, 2.438072878670196, 0.21022585577559116]]\n",
      "2021-01-15 03:30:33,889 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:30:33,893 : INFO : built Dictionary(201 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 1357 corpus positions)\n",
      "2021-01-15 03:30:34,019 : INFO : token count processed\n",
      "2021-01-15 03:30:34,055 : INFO : frequencies processed\n",
      "2021-01-15 03:30:44,093 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:30:44,094 : INFO : entropies processed\n",
      "2021-01-15 03:30:44,095 : INFO : extropies processed\n",
      "2021-01-15 03:30:44,107 : INFO : token count processed\n",
      "2021-01-15 03:30:44,111 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:30:44,116 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:30:44,117 : INFO : vocab #32006\n",
      "2021-01-15 03:30:44,123 : INFO : diff #set()\n",
      "2021-01-15 03:31:03,602 : INFO : alphabet #32006\n",
      "2021-01-15 03:31:13,462 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.1450915145367708, 0.46618057701652343], [0.78220035135746, 0.21779965], [2.521640636343318, 1.2998438251349493], [4.392747410448784, 6.207411496248084, 6.340366793500458, 4.259792113196411, 1.9476193830516735, 0.13295529725237376]]\n",
      "2021-01-15 03:31:13,465 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:31:13,466 : INFO : built Dictionary(140 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 261 corpus positions)\n",
      "2021-01-15 03:31:13,537 : INFO : token count processed\n",
      "2021-01-15 03:31:13,572 : INFO : frequencies processed\n",
      "2021-01-15 03:31:23,599 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:31:23,600 : INFO : entropies processed\n",
      "2021-01-15 03:31:23,601 : INFO : extropies processed\n",
      "2021-01-15 03:31:23,612 : INFO : token count processed\n",
      "2021-01-15 03:31:23,616 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:31:23,621 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:31:23,622 : INFO : vocab #32006\n",
      "2021-01-15 03:31:23,629 : INFO : diff #set()\n",
      "2021-01-15 03:31:43,272 : INFO : alphabet #32006\n",
      "2021-01-15 03:31:53,299 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.1521176316950388, 0.46465861590120683], [0.7502975761890411, 0.24970242], [2.725480556997868, 1.3192201298976014], [4.392747410448784, 6.5805228788529595, 6.758174897685717, 4.215095391616027, 2.365427487236933, 0.17765201883275772]]\n",
      "2021-01-15 03:31:53,304 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 03:31:53,305 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:31:53,309 : INFO : built Dictionary(225 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 1222 corpus positions)\n",
      "2021-01-15 03:31:53,448 : INFO : token count processed\n",
      "2021-01-15 03:31:53,484 : INFO : frequencies processed\n",
      "2021-01-15 03:32:03,403 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:32:03,404 : INFO : entropies processed\n",
      "2021-01-15 03:32:03,405 : INFO : extropies processed\n",
      "2021-01-15 03:32:03,416 : INFO : token count processed\n",
      "2021-01-15 03:32:03,421 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:32:03,426 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:32:03,426 : INFO : vocab #32006\n",
      "2021-01-15 03:32:03,433 : INFO : diff #set()\n",
      "2021-01-15 03:32:23,430 : INFO : alphabet #32006\n",
      "2021-01-15 03:32:33,342 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.2034412258391458, 0.45383556787141704], [0.8378938138484955, 0.16210619], [2.251629167387823, 1.2667563532600834], [4.392747410448784, 6.422089779976135, 6.534530527182538, 4.280306663242381, 2.1417831167337535, 0.11244074720640285]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 03:32:33,347 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:32:33,349 : INFO : built Dictionary(236 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 1504 corpus positions)\n",
      "2021-01-15 03:32:33,472 : INFO : token count processed\n",
      "2021-01-15 03:32:33,504 : INFO : frequencies processed\n",
      "2021-01-15 03:32:43,409 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:32:43,410 : INFO : entropies processed\n",
      "2021-01-15 03:32:43,411 : INFO : extropies processed\n",
      "2021-01-15 03:32:43,418 : INFO : token count processed\n",
      "2021-01-15 03:32:43,423 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:32:43,427 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:32:43,428 : INFO : vocab #32006\n",
      "2021-01-15 03:32:43,435 : INFO : diff #set()\n",
      "2021-01-15 03:33:03,429 : INFO : alphabet #32006\n",
      "2021-01-15 03:33:13,353 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.1669665405769243, 0.461474591912141], [0.8044691383838654, 0.19553086], [2.521640636343318, 1.2998438251349493], [4.392747410448784, 6.485445644653597, 6.725229076357046, 4.152963978745335, 2.3324816659082614, 0.23978343170344907]]\n",
      "2021-01-15 03:33:13,357 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:33:13,359 : INFO : built Dictionary(211 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 1390 corpus positions)\n",
      "2021-01-15 03:33:13,468 : INFO : token count processed\n",
      "2021-01-15 03:33:13,500 : INFO : frequencies processed\n",
      "2021-01-15 03:33:23,404 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:33:23,405 : INFO : entropies processed\n",
      "2021-01-15 03:33:23,405 : INFO : extropies processed\n",
      "2021-01-15 03:33:23,413 : INFO : token count processed\n",
      "2021-01-15 03:33:23,417 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:33:23,422 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:33:23,423 : INFO : vocab #32006\n",
      "2021-01-15 03:33:23,429 : INFO : diff #set()\n",
      "2021-01-15 03:33:43,442 : INFO : alphabet #32006\n",
      "2021-01-15 03:33:53,777 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.147724455658825, 0.46560907632503773], [0.7868987023830414, 0.2131013], [2.521640636343318, 1.2998438251349493], [4.392747410448784, 6.2276600107346916, 6.357844963943415, 4.262562457240061, 1.9650975534946307, 0.13018495320872336]]\n",
      "2021-01-15 03:33:53,782 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:33:53,783 : INFO : built Dictionary(201 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 1588 corpus positions)\n",
      "2021-01-15 03:33:53,885 : INFO : token count processed\n",
      "2021-01-15 03:33:53,918 : INFO : frequencies processed\n",
      "2021-01-15 03:34:03,835 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:34:03,836 : INFO : entropies processed\n",
      "2021-01-15 03:34:03,837 : INFO : extropies processed\n",
      "2021-01-15 03:34:03,844 : INFO : token count processed\n",
      "2021-01-15 03:34:03,849 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:34:03,853 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:34:03,854 : INFO : vocab #32006\n",
      "2021-01-15 03:34:03,861 : INFO : diff #set()\n",
      "2021-01-15 03:34:23,860 : INFO : alphabet #32006\n",
      "2021-01-15 03:34:33,800 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.1968381207587184, 0.4551996756386546], [0.8446897119283676, 0.15531029], [2.521640636343318, 1.2998438251349493], [4.392747410448784, 6.253918170574241, 6.439674967234611, 4.206990613788415, 2.0469275567858265, 0.18575679666037015]]\n",
      "2021-01-15 03:34:33,804 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:34:33,806 : INFO : built Dictionary(173 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 685 corpus positions)\n",
      "2021-01-15 03:34:33,906 : INFO : token count processed\n",
      "2021-01-15 03:34:33,943 : INFO : frequencies processed\n",
      "2021-01-15 03:34:43,854 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:34:43,855 : INFO : entropies processed\n",
      "2021-01-15 03:34:43,856 : INFO : extropies processed\n",
      "2021-01-15 03:34:43,864 : INFO : token count processed\n",
      "2021-01-15 03:34:43,868 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:34:43,872 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:34:43,873 : INFO : vocab #32006\n",
      "2021-01-15 03:34:43,880 : INFO : diff #set()\n",
      "2021-01-15 03:35:03,897 : INFO : alphabet #32006\n",
      "2021-01-15 03:35:13,806 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.1209970425403402, 0.47147637641318424], [0.7440048158168793, 0.25599518], [2.521640636343318, 1.2998438251349493], [4.392747410448784, 6.374522245625576, 6.575772750473366, 4.191496905600994, 2.1830253400245816, 0.20125050484779017]]\n",
      "2021-01-15 03:35:13,811 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:35:13,814 : INFO : built Dictionary(295 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 1967 corpus positions)\n",
      "2021-01-15 03:35:13,989 : INFO : token count processed\n",
      "2021-01-15 03:35:14,029 : INFO : frequencies processed\n",
      "2021-01-15 03:35:23,976 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:35:23,977 : INFO : entropies processed\n",
      "2021-01-15 03:35:23,977 : INFO : extropies processed\n",
      "2021-01-15 03:35:23,985 : INFO : token count processed\n",
      "2021-01-15 03:35:23,990 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:35:23,994 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:35:23,995 : INFO : vocab #32006\n",
      "2021-01-15 03:35:24,001 : INFO : diff #set()\n",
      "2021-01-15 03:35:43,873 : INFO : alphabet #32006\n",
      "2021-01-15 03:35:53,772 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.1862446564989002, 0.4574053489518515], [0.831143394112587, 0.1688566], [2.521640636343318, 1.2998438251349493], [4.392747410448784, 6.731238669067808, 6.996786899648649, 4.127199179867942, 2.604039489199865, 0.2655482305808414]]\n",
      "2021-01-15 03:35:53,777 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:35:53,779 : INFO : built Dictionary(219 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 1359 corpus positions)\n",
      "2021-01-15 03:35:53,900 : INFO : token count processed\n",
      "2021-01-15 03:35:53,934 : INFO : frequencies processed\n",
      "2021-01-15 03:36:03,874 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:36:03,875 : INFO : entropies processed\n",
      "2021-01-15 03:36:03,875 : INFO : extropies processed\n",
      "2021-01-15 03:36:03,883 : INFO : token count processed\n",
      "2021-01-15 03:36:03,888 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:36:03,892 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:36:03,893 : INFO : vocab #32006\n",
      "2021-01-15 03:36:03,900 : INFO : diff #set()\n",
      "2021-01-15 03:36:23,761 : INFO : alphabet #32006\n",
      "2021-01-15 03:36:33,477 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.183028419983978, 0.4580792402177427], [0.8227094858884811, 0.17729051], [2.725480556997868, 1.3192201298976014], [4.392747410448784, 6.503741451859337, 6.691764868310747, 4.204723993997375, 2.2990174578619627, 0.18802341645140963]]\n",
      "2021-01-15 03:36:33,482 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:36:33,484 : INFO : built Dictionary(240 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 1681 corpus positions)\n",
      "2021-01-15 03:36:33,614 : INFO : token count processed\n",
      "2021-01-15 03:36:33,648 : INFO : frequencies processed\n",
      "2021-01-15 03:36:43,682 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:36:43,683 : INFO : entropies processed\n",
      "2021-01-15 03:36:43,684 : INFO : extropies processed\n",
      "2021-01-15 03:36:43,691 : INFO : token count processed\n",
      "2021-01-15 03:36:43,696 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:36:43,700 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:36:43,701 : INFO : vocab #32006\n",
      "2021-01-15 03:36:43,708 : INFO : diff #set()\n",
      "2021-01-15 03:37:03,444 : INFO : alphabet #32006\n",
      "2021-01-15 03:37:13,492 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.186626527130517, 0.4573254680634867], [0.832190752029419, 0.16780925], [2.9219280948873623, 1.3359016564230495], [4.392747410448784, 6.334729224484471, 6.465312093785892, 4.262164541147364, 2.0725646833371076, 0.1305828693014206]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 03:37:13,498 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:37:13,500 : INFO : built Dictionary(238 unique tokens: ['-', '-8', '.', '46', ':']...) from 2 documents (total 2030 corpus positions)\n",
      "2021-01-15 03:37:13,631 : INFO : token count processed\n",
      "2021-01-15 03:37:13,666 : INFO : frequencies processed\n",
      "2021-01-15 03:37:23,792 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:37:23,793 : INFO : entropies processed\n",
      "2021-01-15 03:37:23,794 : INFO : extropies processed\n",
      "2021-01-15 03:37:23,802 : INFO : token count processed\n",
      "2021-01-15 03:37:23,806 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:37:23,811 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:37:23,812 : INFO : vocab #32006\n",
      "2021-01-15 03:37:23,819 : INFO : diff #set()\n",
      "2021-01-15 03:37:43,718 : INFO : alphabet #32006\n",
      "2021-01-15 03:37:53,965 : INFO : Computed distances or similarities ('264', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.186386217677944, 0.45737573348868443], [0.841274619102478, 0.15872538], [2.75, 1.3226647836567116], [4.392747410448784, 6.21319712067992, 6.407052368905297, 4.198892162223408, 2.0143049584565125, 0.19385524822537636]]\n",
      "2021-01-15 03:37:53,970 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 03:37:53,971 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:37:53,973 : INFO : built Dictionary(337 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1514 corpus positions)\n",
      "2021-01-15 03:37:54,707 : INFO : token count processed\n",
      "2021-01-15 03:37:54,790 : INFO : frequencies processed\n",
      "2021-01-15 03:38:04,724 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:38:04,725 : INFO : entropies processed\n",
      "2021-01-15 03:38:04,726 : INFO : extropies processed\n",
      "2021-01-15 03:38:04,737 : INFO : token count processed\n",
      "2021-01-15 03:38:04,742 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:38:04,746 : INFO : alphabet_target #32010\n",
      "2021-01-15 03:38:04,747 : INFO : vocab #32006\n",
      "2021-01-15 03:38:04,754 : INFO : diff #set()\n",
      "2021-01-15 03:38:25,071 : INFO : alphabet #32006\n",
      "2021-01-15 03:38:35,019 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.1534756535167334, 0.4643655935310669], [0.8086307793855667, 0.19136922], [3.9705730958116847, 1.3904984042298727], [6.391502818259423, 6.905617163738059, 7.306207473147502, 5.990912508849981, 0.9147046548880793, 0.4005903094094432]]\n",
      "2021-01-15 03:38:35,025 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:38:35,028 : INFO : built Dictionary(424 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 2404 corpus positions)\n",
      "2021-01-15 03:38:36,089 : INFO : token count processed\n",
      "2021-01-15 03:38:36,129 : INFO : frequencies processed\n",
      "2021-01-15 03:38:46,224 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:38:46,225 : INFO : entropies processed\n",
      "2021-01-15 03:38:46,228 : INFO : extropies processed\n",
      "2021-01-15 03:38:46,236 : INFO : token count processed\n",
      "2021-01-15 03:38:46,245 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:38:46,251 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:38:46,252 : INFO : vocab #32006\n",
      "2021-01-15 03:38:46,261 : INFO : diff #set()\n",
      "2021-01-15 03:39:06,292 : INFO : alphabet #32006\n",
      "2021-01-15 03:39:16,247 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.1627937420566257, 0.4623649405648309], [0.8237410187721252, 0.17625898], [4.2359263506290326, 1.397233025487805], [6.391502818259423, 7.1219284286457345, 7.517246831570027, 5.996184415335129, 1.1257440133106043, 0.39531840292429266]]\n",
      "2021-01-15 03:39:16,253 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:39:16,255 : INFO : built Dictionary(343 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 2387 corpus positions)\n",
      "2021-01-15 03:39:16,977 : INFO : token count processed\n",
      "2021-01-15 03:39:17,026 : INFO : frequencies processed\n",
      "2021-01-15 03:39:26,937 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:39:26,938 : INFO : entropies processed\n",
      "2021-01-15 03:39:26,939 : INFO : extropies processed\n",
      "2021-01-15 03:39:26,951 : INFO : token count processed\n",
      "2021-01-15 03:39:26,956 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:39:26,960 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:39:26,961 : INFO : vocab #32006\n",
      "2021-01-15 03:39:26,968 : INFO : diff #set()\n",
      "2021-01-15 03:39:46,995 : INFO : alphabet #32006\n",
      "2021-01-15 03:39:56,952 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.1869762441380918, 0.4572523376421538], [0.8565228134393692, 0.14347719], [4.286790198827113, 1.4011749129952062], [6.391502818259423, 6.41099024988467, 6.7032603469844485, 6.099232721159645, 0.3117575287250256, 0.29227009709977825]]\n",
      "2021-01-15 03:39:56,957 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:39:56,958 : INFO : built Dictionary(230 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 786 corpus positions)\n",
      "2021-01-15 03:39:57,346 : INFO : token count processed\n",
      "2021-01-15 03:39:57,389 : INFO : frequencies processed\n",
      "2021-01-15 03:40:07,326 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:40:07,328 : INFO : entropies processed\n",
      "2021-01-15 03:40:07,328 : INFO : extropies processed\n",
      "2021-01-15 03:40:07,339 : INFO : token count processed\n",
      "2021-01-15 03:40:07,344 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:40:07,349 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:40:07,349 : INFO : vocab #32006\n",
      "2021-01-15 03:40:07,356 : INFO : diff #set()\n",
      "2021-01-15 03:40:27,703 : INFO : alphabet #32006\n",
      "2021-01-15 03:40:37,634 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.1725352345008901, 0.4602917292753303], [0.8585425913333893, 0.14145741], [3.506890595608519, 1.3728719392429896], [6.391502818259423, 6.077866832717642, 6.751052416006274, 5.71831723497079, 0.35954959774685147, 0.6731855832886326]]\n",
      "2021-01-15 03:40:37,637 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:40:37,639 : INFO : built Dictionary(205 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 551 corpus positions)\n",
      "2021-01-15 03:40:37,966 : INFO : token count processed\n",
      "2021-01-15 03:40:38,033 : INFO : frequencies processed\n",
      "2021-01-15 03:40:47,977 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:40:47,978 : INFO : entropies processed\n",
      "2021-01-15 03:40:47,979 : INFO : extropies processed\n",
      "2021-01-15 03:40:47,987 : INFO : token count processed\n",
      "2021-01-15 03:40:47,991 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:40:47,996 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:40:47,996 : INFO : vocab #32006\n",
      "2021-01-15 03:40:48,003 : INFO : diff #set()\n",
      "2021-01-15 03:41:08,216 : INFO : alphabet #32006\n",
      "2021-01-15 03:41:18,155 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.1695662996451632, 0.4609216137637978], [0.8557512164115906, 0.14424878], [3.6402239289418516, 1.3797477693995936], [6.391502818259423, 5.977547459003844, 6.7744223430916275, 5.594627934171639, 0.3829195248322046, 0.7968748840877833]]\n",
      "2021-01-15 03:41:18,161 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:41:18,163 : INFO : built Dictionary(303 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 2262 corpus positions)\n",
      "2021-01-15 03:41:18,753 : INFO : token count processed\n",
      "2021-01-15 03:41:18,796 : INFO : frequencies processed\n",
      "2021-01-15 03:41:29,036 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:41:29,037 : INFO : entropies processed\n",
      "2021-01-15 03:41:29,038 : INFO : extropies processed\n",
      "2021-01-15 03:41:29,049 : INFO : token count processed\n",
      "2021-01-15 03:41:29,054 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:41:29,058 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:41:29,059 : INFO : vocab #32006\n",
      "2021-01-15 03:41:29,066 : INFO : diff #set()\n",
      "2021-01-15 03:41:48,722 : INFO : alphabet #32006\n",
      "2021-01-15 03:41:58,796 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.169780834767569, 0.4608760405550922], [0.8400588184595108, 0.15994118], [4.213660689688186, 1.3989127715246337], [6.391502818259423, 6.4614394051846435, 6.7917495667842935, 6.061192656659774, 0.40024674852487063, 0.33031016159965]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 03:41:58,801 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 03:41:58,802 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:41:58,807 : INFO : built Dictionary(272 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1486 corpus positions)\n",
      "2021-01-15 03:41:59,286 : INFO : token count processed\n",
      "2021-01-15 03:41:59,340 : INFO : frequencies processed\n",
      "2021-01-15 03:42:09,062 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:42:09,063 : INFO : entropies processed\n",
      "2021-01-15 03:42:09,063 : INFO : extropies processed\n",
      "2021-01-15 03:42:09,073 : INFO : token count processed\n",
      "2021-01-15 03:42:09,078 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:42:09,082 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:42:09,083 : INFO : vocab #32006\n",
      "2021-01-15 03:42:09,090 : INFO : diff #set()\n",
      "2021-01-15 03:42:28,815 : INFO : alphabet #32006\n",
      "2021-01-15 03:42:38,823 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.1802141577962073, 0.4586705376736085], [0.8390401899814606, 0.16095981], [3.6644977792004623, 1.381962919072374], [6.391502818259423, 6.327195724598159, 6.768478075426071, 5.95022046743151, 0.3769752571666478, 0.4412823508279118]]\n",
      "2021-01-15 03:42:38,836 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 03:42:38,837 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:42:38,840 : INFO : built Dictionary(475 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 6378 corpus positions)\n",
      "2021-01-15 03:42:40,166 : INFO : token count processed\n",
      "2021-01-15 03:42:40,207 : INFO : frequencies processed\n",
      "2021-01-15 03:42:49,997 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:42:49,998 : INFO : entropies processed\n",
      "2021-01-15 03:42:49,999 : INFO : extropies processed\n",
      "2021-01-15 03:42:50,011 : INFO : token count processed\n",
      "2021-01-15 03:42:50,016 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:42:50,020 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:42:50,021 : INFO : vocab #32006\n",
      "2021-01-15 03:42:50,028 : INFO : diff #set()\n",
      "2021-01-15 03:43:09,912 : INFO : alphabet #32006\n",
      "2021-01-15 03:43:19,914 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.1741424110113385, 0.45995147095025546], [0.8418278992176056, 0.1581721], [4.511085408180428, 1.404840072262186], [6.391502818259423, 6.9079058562486315, 7.105538025690526, 6.193870648817528, 0.7140352074311034, 0.19763216944189477]]\n",
      "2021-01-15 03:43:19,921 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:43:19,924 : INFO : built Dictionary(385 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 2772 corpus positions)\n",
      "2021-01-15 03:43:20,758 : INFO : token count processed\n",
      "2021-01-15 03:43:20,801 : INFO : frequencies processed\n",
      "2021-01-15 03:43:30,736 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:43:30,737 : INFO : entropies processed\n",
      "2021-01-15 03:43:30,738 : INFO : extropies processed\n",
      "2021-01-15 03:43:30,748 : INFO : token count processed\n",
      "2021-01-15 03:43:30,753 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:43:30,758 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:43:30,758 : INFO : vocab #32006\n",
      "2021-01-15 03:43:30,765 : INFO : diff #set()\n",
      "2021-01-15 03:43:50,750 : INFO : alphabet #32006\n",
      "2021-01-15 03:44:00,655 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.1795398663988288, 0.45881243808229216], [0.8339076191186905, 0.16609238], [4.001822825622231, 1.3874928763412118], [6.391502818259423, 6.61034830706307, 6.957129638815767, 6.044721486506726, 0.5656268205563437, 0.34678133175269643]]\n",
      "2021-01-15 03:44:00,659 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:44:00,660 : INFO : built Dictionary(270 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 819 corpus positions)\n",
      "2021-01-15 03:44:01,154 : INFO : token count processed\n",
      "2021-01-15 03:44:01,186 : INFO : frequencies processed\n",
      "2021-01-15 03:44:11,310 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:44:11,311 : INFO : entropies processed\n",
      "2021-01-15 03:44:11,312 : INFO : extropies processed\n",
      "2021-01-15 03:44:11,322 : INFO : token count processed\n",
      "2021-01-15 03:44:11,326 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:44:11,331 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:44:11,332 : INFO : vocab #32006\n",
      "2021-01-15 03:44:11,338 : INFO : diff #set()\n",
      "2021-01-15 03:44:31,321 : INFO : alphabet #32006\n",
      "2021-01-15 03:44:41,216 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.14179451068532, 0.4668981991554481], [0.8337421715259552, 0.16625783], [4.105388542207534, 1.3936738798262605], [6.391502818259423, 6.616715366949855, 7.157105695150654, 5.851112490058623, 0.7656028768912311, 0.5403903282007994]]\n",
      "2021-01-15 03:44:41,223 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:44:41,226 : INFO : built Dictionary(480 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 2853 corpus positions)\n",
      "2021-01-15 03:44:42,676 : INFO : token count processed\n",
      "2021-01-15 03:44:42,759 : INFO : frequencies processed\n",
      "2021-01-15 03:44:52,740 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:44:52,741 : INFO : entropies processed\n",
      "2021-01-15 03:44:52,742 : INFO : extropies processed\n",
      "2021-01-15 03:44:52,750 : INFO : token count processed\n",
      "2021-01-15 03:44:52,754 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:44:52,759 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:44:52,760 : INFO : vocab #32006\n",
      "2021-01-15 03:44:52,767 : INFO : diff #set()\n",
      "2021-01-15 03:45:12,701 : INFO : alphabet #32006\n",
      "2021-01-15 03:45:22,346 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.1272151265603592, 0.47009819905566813], [0.7636820077896118, 0.23631799], [4.670017603477633, 1.4082478704781867], [6.391502818259423, 7.32185870753746, 7.657969858230514, 6.05539166756637, 1.266467039971091, 0.3361111506930534]]\n",
      "2021-01-15 03:45:22,350 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:45:22,351 : INFO : built Dictionary(137 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 218 corpus positions)\n",
      "2021-01-15 03:45:22,487 : INFO : token count processed\n",
      "2021-01-15 03:45:22,558 : INFO : frequencies processed\n",
      "2021-01-15 03:45:32,302 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:45:32,303 : INFO : entropies processed\n",
      "2021-01-15 03:45:32,304 : INFO : extropies processed\n",
      "2021-01-15 03:45:32,312 : INFO : token count processed\n",
      "2021-01-15 03:45:32,316 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:45:32,320 : INFO : alphabet_target #32008\n",
      "2021-01-15 03:45:32,321 : INFO : vocab #32006\n",
      "2021-01-15 03:45:32,328 : INFO : diff #set()\n",
      "2021-01-15 03:45:52,329 : INFO : alphabet #32006\n",
      "2021-01-15 03:46:01,997 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/fireException.py')[[1.2052023878022315, 0.45347311681293295], [0.8589182794094086, 0.14108172], [2.321928094887362, 1.2877123795494492], [6.391502818259423, 5.176618657501385, 6.82868434038533, 4.7394371353754785, 0.4371815221259068, 1.6520656828839444]]\n",
      "2021-01-15 03:46:02,001 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:46:02,002 : INFO : built Dictionary(225 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 611 corpus positions)\n",
      "2021-01-15 03:46:02,347 : INFO : token count processed\n",
      "2021-01-15 03:46:02,417 : INFO : frequencies processed\n",
      "2021-01-15 03:46:12,036 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:46:12,037 : INFO : entropies processed\n",
      "2021-01-15 03:46:12,038 : INFO : extropies processed\n",
      "2021-01-15 03:46:12,049 : INFO : token count processed\n",
      "2021-01-15 03:46:12,053 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:46:12,058 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:46:12,059 : INFO : vocab #32006\n",
      "2021-01-15 03:46:12,065 : INFO : diff #set()\n",
      "2021-01-15 03:46:31,923 : INFO : alphabet #32006\n",
      "2021-01-15 03:46:41,614 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.1755969444465848, 0.4596439623398966], [0.8454750329256058, 0.15452497], [3.6644977792004623, 1.381962919072374], [6.391502818259423, 6.468846789852156, 7.182410732457827, 5.677938875653753, 0.7909079141984039, 0.7135639426056706]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 03:46:41,621 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:46:41,623 : INFO : built Dictionary(431 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 2660 corpus positions)\n",
      "2021-01-15 03:46:42,825 : INFO : token count processed\n",
      "2021-01-15 03:46:42,894 : INFO : frequencies processed\n",
      "2021-01-15 03:46:52,672 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:46:52,673 : INFO : entropies processed\n",
      "2021-01-15 03:46:52,673 : INFO : extropies processed\n",
      "2021-01-15 03:46:52,683 : INFO : token count processed\n",
      "2021-01-15 03:46:52,688 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:46:52,692 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:46:52,693 : INFO : vocab #32006\n",
      "2021-01-15 03:46:52,699 : INFO : diff #set()\n",
      "2021-01-15 03:47:12,318 : INFO : alphabet #32006\n",
      "2021-01-15 03:47:22,331 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.1642947281296572, 0.4620442802927221], [0.8291044533252716, 0.17089555], [4.395998870534841, 1.4017337013706548], [6.391502818259423, 6.957796704012729, 7.266015916657141, 6.08328360561501, 0.8745130983977178, 0.30821921264441166]]\n",
      "2021-01-15 03:47:22,337 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:47:22,340 : INFO : built Dictionary(346 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 3157 corpus positions)\n",
      "2021-01-15 03:47:23,046 : INFO : token count processed\n",
      "2021-01-15 03:47:23,077 : INFO : frequencies processed\n",
      "2021-01-15 03:47:32,949 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:47:32,950 : INFO : entropies processed\n",
      "2021-01-15 03:47:32,951 : INFO : extropies processed\n",
      "2021-01-15 03:47:32,959 : INFO : token count processed\n",
      "2021-01-15 03:47:32,964 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:47:32,968 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:47:32,969 : INFO : vocab #32006\n",
      "2021-01-15 03:47:32,976 : INFO : diff #set()\n",
      "2021-01-15 03:47:52,945 : INFO : alphabet #32006\n",
      "2021-01-15 03:48:03,019 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.160441260315501, 0.46286840488038283], [0.8328211158514023, 0.16717888], [4.106603137064474, 1.3974772952154462], [6.391502818259423, 6.441859572014148, 6.768391818119547, 6.064970572154024, 0.3768889998601237, 0.32653224610539855]]\n",
      "2021-01-15 03:48:03,024 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:48:03,026 : INFO : built Dictionary(370 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1633 corpus positions)\n",
      "2021-01-15 03:48:03,843 : INFO : token count processed\n",
      "2021-01-15 03:48:03,878 : INFO : frequencies processed\n",
      "2021-01-15 03:48:13,828 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:48:13,829 : INFO : entropies processed\n",
      "2021-01-15 03:48:13,830 : INFO : extropies processed\n",
      "2021-01-15 03:48:13,838 : INFO : token count processed\n",
      "2021-01-15 03:48:13,842 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:48:13,847 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:48:13,848 : INFO : vocab #32006\n",
      "2021-01-15 03:48:13,854 : INFO : diff #set()\n",
      "2021-01-15 03:48:33,789 : INFO : alphabet #32006\n",
      "2021-01-15 03:48:43,861 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.1499349522325097, 0.4651303514841656], [0.8278198689222336, 0.17218013], [4.213660689688186, 1.3989127715246337], [6.391502818259423, 6.998955278238291, 7.401140993173912, 5.989317103323802, 1.009638174914489, 0.40218571493562116]]\n",
      "2021-01-15 03:48:43,866 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:48:43,867 : INFO : built Dictionary(287 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1452 corpus positions)\n",
      "2021-01-15 03:48:44,434 : INFO : token count processed\n",
      "2021-01-15 03:48:44,468 : INFO : frequencies processed\n",
      "2021-01-15 03:48:54,442 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:48:54,443 : INFO : entropies processed\n",
      "2021-01-15 03:48:54,444 : INFO : extropies processed\n",
      "2021-01-15 03:48:54,452 : INFO : token count processed\n",
      "2021-01-15 03:48:54,456 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:48:54,461 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:48:54,462 : INFO : vocab #32006\n",
      "2021-01-15 03:48:54,468 : INFO : diff #set()\n",
      "2021-01-15 03:49:14,963 : INFO : alphabet #32006\n",
      "2021-01-15 03:49:25,103 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.1756573494375258, 0.45963120077641395], [0.8435031771659851, 0.15649682], [4.106603137064474, 1.3974772952154462], [6.391502818259423, 6.492983191376071, 6.918754714196773, 5.965731295438722, 0.5272518959373498, 0.4257715228207015]]\n",
      "2021-01-15 03:49:25,111 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:49:25,113 : INFO : built Dictionary(503 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 3391 corpus positions)\n",
      "2021-01-15 03:49:26,578 : INFO : token count processed\n",
      "2021-01-15 03:49:26,612 : INFO : frequencies processed\n",
      "2021-01-15 03:49:36,559 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:49:36,561 : INFO : entropies processed\n",
      "2021-01-15 03:49:36,561 : INFO : extropies processed\n",
      "2021-01-15 03:49:36,569 : INFO : token count processed\n",
      "2021-01-15 03:49:36,574 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:49:36,578 : INFO : alphabet_target #32008\n",
      "2021-01-15 03:49:36,579 : INFO : vocab #32006\n",
      "2021-01-15 03:49:36,586 : INFO : diff #set()\n",
      "2021-01-15 03:49:57,092 : INFO : alphabet #32006\n",
      "2021-01-15 03:50:07,063 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.223990855916301, 0.4496421364952026], [0.8875741437077522, 0.11242586], [3.3248629576173565, 1.3574960179923317], [6.391502818259423, 6.560342487747443, 6.928887627816723, 6.022957678190144, 0.5373848095572997, 0.36854514006927985]]\n",
      "2021-01-15 03:50:07,071 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 03:50:07,072 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:50:07,074 : INFO : built Dictionary(509 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 3596 corpus positions)\n",
      "2021-01-15 03:50:08,558 : INFO : token count processed\n",
      "2021-01-15 03:50:08,592 : INFO : frequencies processed\n",
      "2021-01-15 03:50:18,554 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:50:18,555 : INFO : entropies processed\n",
      "2021-01-15 03:50:18,556 : INFO : extropies processed\n",
      "2021-01-15 03:50:18,564 : INFO : token count processed\n",
      "2021-01-15 03:50:18,569 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:50:18,574 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:50:18,574 : INFO : vocab #32006\n",
      "2021-01-15 03:50:18,581 : INFO : diff #set()\n",
      "2021-01-15 03:50:38,638 : INFO : alphabet #32006\n",
      "2021-01-15 03:50:48,591 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.1712019538374279, 0.4605743828816012], [0.8265636563301086, 0.17343634], [4.351823225551765, 1.4031855444927654], [6.391502818259423, 7.046173750105238, 7.372333036563979, 6.065343531800683, 0.9808302183045559, 0.3261592864587408]]\n",
      "2021-01-15 03:50:48,603 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 03:50:48,604 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:50:48,607 : INFO : built Dictionary(554 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 5705 corpus positions)\n",
      "2021-01-15 03:50:50,713 : INFO : token count processed\n",
      "2021-01-15 03:50:50,748 : INFO : frequencies processed\n",
      "2021-01-15 03:51:00,718 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:51:00,719 : INFO : entropies processed\n",
      "2021-01-15 03:51:00,719 : INFO : extropies processed\n",
      "2021-01-15 03:51:00,729 : INFO : token count processed\n",
      "2021-01-15 03:51:00,733 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:51:00,738 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:51:00,738 : INFO : vocab #32006\n",
      "2021-01-15 03:51:00,745 : INFO : diff #set()\n",
      "2021-01-15 03:51:20,972 : INFO : alphabet #32006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 03:51:30,935 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.1531634552625538, 0.46443292428909505], [0.8223168402910233, 0.17768316], [4.773269803731142, 1.4109683370918737], [6.391502818259423, 7.009229588004272, 7.207916776369163, 6.192815629894533, 0.8164139581097398, 0.1986871883648904]]\n",
      "2021-01-15 03:51:30,948 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:51:30,951 : INFO : built Dictionary(634 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 6646 corpus positions)\n",
      "2021-01-15 03:51:33,977 : INFO : token count processed\n",
      "2021-01-15 03:51:34,056 : INFO : frequencies processed\n",
      "2021-01-15 03:51:44,113 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:51:44,114 : INFO : entropies processed\n",
      "2021-01-15 03:51:44,115 : INFO : extropies processed\n",
      "2021-01-15 03:51:44,128 : INFO : token count processed\n",
      "2021-01-15 03:51:44,133 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:51:44,141 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:51:44,142 : INFO : vocab #32006\n",
      "2021-01-15 03:51:44,148 : INFO : diff #set()\n",
      "2021-01-15 03:52:04,084 : INFO : alphabet #32006\n",
      "2021-01-15 03:52:14,040 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.1528092969161343, 0.4645093280823733], [0.7965348809957504, 0.20346512], [4.71500235722449, 1.4099954901049734], [6.391502818259423, 7.376088004590871, 7.613448506451172, 6.154142316399123, 1.2219456881917488, 0.23736050186030067]]\n",
      "2021-01-15 03:52:14,044 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:52:14,046 : INFO : built Dictionary(202 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 432 corpus positions)\n",
      "2021-01-15 03:52:14,344 : INFO : token count processed\n",
      "2021-01-15 03:52:14,434 : INFO : frequencies processed\n",
      "2021-01-15 03:52:24,222 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:52:24,223 : INFO : entropies processed\n",
      "2021-01-15 03:52:24,224 : INFO : extropies processed\n",
      "2021-01-15 03:52:24,231 : INFO : token count processed\n",
      "2021-01-15 03:52:24,235 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:52:24,239 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:52:24,240 : INFO : vocab #32006\n",
      "2021-01-15 03:52:24,246 : INFO : diff #set()\n",
      "2021-01-15 03:52:43,639 : INFO : alphabet #32006\n",
      "2021-01-15 03:52:53,560 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.1615915520164777, 0.4626220892967189], [0.8427761644124985, 0.15722384], [3.5465935642949384, 1.3764678303056375], [6.391502818259423, 6.2993628166120885, 7.13143638922029, 5.559429245651221, 0.7399335709608668, 0.8320735726082011]]\n",
      "2021-01-15 03:52:53,563 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:52:53,564 : INFO : built Dictionary(110 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 148 corpus positions)\n",
      "2021-01-15 03:52:53,628 : INFO : token count processed\n",
      "2021-01-15 03:52:53,717 : INFO : frequencies processed\n",
      "2021-01-15 03:53:03,780 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:53:03,781 : INFO : entropies processed\n",
      "2021-01-15 03:53:03,782 : INFO : extropies processed\n",
      "2021-01-15 03:53:03,789 : INFO : token count processed\n",
      "2021-01-15 03:53:03,794 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:53:03,798 : INFO : alphabet_target #32008\n",
      "2021-01-15 03:53:03,799 : INFO : vocab #32006\n",
      "2021-01-15 03:53:03,806 : INFO : diff #set()\n",
      "2021-01-15 03:53:23,711 : INFO : alphabet #32006\n",
      "2021-01-15 03:53:33,762 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.2816623924328143, 0.43827693497360654], [0.9198506623506546, 0.08014934], [0.0, 0.0], [6.391502818259423, 3.8936606896881862, 6.572434384308664, 3.712729123638944, 0.18093156604924143, 2.678773694620478]]\n",
      "2021-01-15 03:53:33,786 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:53:33,791 : INFO : built Dictionary(780 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 12590 corpus positions)\n",
      "2021-01-15 03:53:40,613 : INFO : token count processed\n",
      "2021-01-15 03:53:40,675 : INFO : frequencies processed\n",
      "2021-01-15 03:53:50,583 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:53:50,585 : INFO : entropies processed\n",
      "2021-01-15 03:53:50,585 : INFO : extropies processed\n",
      "2021-01-15 03:53:50,601 : INFO : token count processed\n",
      "2021-01-15 03:53:50,606 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:53:50,612 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:53:50,613 : INFO : vocab #32006\n",
      "2021-01-15 03:53:50,622 : INFO : diff #set()\n",
      "2021-01-15 03:54:10,834 : INFO : alphabet #32006\n",
      "2021-01-15 03:54:20,762 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.1180813110199441, 0.472125406516362], [0.7797156721353531, 0.22028433], [5.259678854409918, 1.4206348945730183], [6.391502818259423, 7.434393313070278, 7.677778832693873, 6.148117298635828, 1.28627601443445, 0.24338551962359478]]\n",
      "2021-01-15 03:54:20,771 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:54:20,774 : INFO : built Dictionary(540 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 4217 corpus positions)\n",
      "2021-01-15 03:54:22,490 : INFO : token count processed\n",
      "2021-01-15 03:54:22,559 : INFO : frequencies processed\n",
      "2021-01-15 03:54:32,545 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:54:32,546 : INFO : entropies processed\n",
      "2021-01-15 03:54:32,547 : INFO : extropies processed\n",
      "2021-01-15 03:54:32,556 : INFO : token count processed\n",
      "2021-01-15 03:54:32,561 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:54:32,565 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:54:32,566 : INFO : vocab #32006\n",
      "2021-01-15 03:54:32,572 : INFO : diff #set()\n",
      "2021-01-15 03:54:53,002 : INFO : alphabet #32006\n",
      "2021-01-15 03:55:02,927 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.1417506339646448, 0.4669077642101016], [0.7927192747592926, 0.20728073], [4.898455706009981, 1.4135114001787032], [6.391502818259423, 7.2991514951718255, 7.589995461813077, 6.100658851618173, 1.1984926435536538, 0.29084396664125123]]\n",
      "2021-01-15 03:55:02,934 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:55:02,937 : INFO : built Dictionary(515 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 3634 corpus positions)\n",
      "2021-01-15 03:55:04,491 : INFO : token count processed\n",
      "2021-01-15 03:55:04,535 : INFO : frequencies processed\n",
      "2021-01-15 03:55:14,481 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:55:14,483 : INFO : entropies processed\n",
      "2021-01-15 03:55:14,483 : INFO : extropies processed\n",
      "2021-01-15 03:55:14,492 : INFO : token count processed\n",
      "2021-01-15 03:55:14,497 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:55:14,501 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:55:14,502 : INFO : vocab #32006\n",
      "2021-01-15 03:55:14,508 : INFO : diff #set()\n",
      "2021-01-15 03:55:34,532 : INFO : alphabet #32006\n",
      "2021-01-15 03:55:44,497 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.1702986507812674, 0.4607660791937637], [0.8229144811630249, 0.17708552], [4.432294243948857, 1.4021572132817344], [6.391502818259423, 7.170319527000998, 7.4815126806117505, 6.08030966464867, 1.0900098623523276, 0.31119315361075284]]\n",
      "2021-01-15 03:55:44,501 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:55:44,503 : INFO : built Dictionary(235 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 670 corpus positions)\n",
      "2021-01-15 03:55:44,878 : INFO : token count processed\n",
      "2021-01-15 03:55:44,922 : INFO : frequencies processed\n",
      "2021-01-15 03:55:54,878 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:55:54,880 : INFO : entropies processed\n",
      "2021-01-15 03:55:54,881 : INFO : extropies processed\n",
      "2021-01-15 03:55:54,891 : INFO : token count processed\n",
      "2021-01-15 03:55:54,896 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:55:54,900 : INFO : alphabet_target #32009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 03:55:54,901 : INFO : vocab #32006\n",
      "2021-01-15 03:55:54,908 : INFO : diff #set()\n",
      "2021-01-15 03:56:14,924 : INFO : alphabet #32006\n",
      "2021-01-15 03:56:24,872 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.1794367604924507, 0.45883414381523363], [0.8525241166353226, 0.14747588], [3.6644977792004623, 1.381962919072374], [6.391502818259423, 6.353654804387375, 7.062222203278997, 5.6829354193678, 0.6707193850195745, 0.7085673988916223]]\n",
      "2021-01-15 03:56:24,876 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:56:24,877 : INFO : built Dictionary(232 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 798 corpus positions)\n",
      "2021-01-15 03:56:25,269 : INFO : token count processed\n",
      "2021-01-15 03:56:25,352 : INFO : frequencies processed\n",
      "2021-01-15 03:56:35,392 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:56:35,394 : INFO : entropies processed\n",
      "2021-01-15 03:56:35,395 : INFO : extropies processed\n",
      "2021-01-15 03:56:35,402 : INFO : token count processed\n",
      "2021-01-15 03:56:35,407 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:56:35,411 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:56:35,412 : INFO : vocab #32006\n",
      "2021-01-15 03:56:35,419 : INFO : diff #set()\n",
      "2021-01-15 03:56:55,303 : INFO : alphabet #32006\n",
      "2021-01-15 03:57:05,227 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.1896925146417623, 0.4566851251092677], [0.8559735864400864, 0.14402641], [3.4182958340544896, 1.369895090630202], [6.391502818259423, 6.245180322479091, 6.897885444515545, 5.738797696222969, 0.506382626256122, 0.6527051220364539]]\n",
      "2021-01-15 03:57:05,233 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:57:05,235 : INFO : built Dictionary(440 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1870 corpus positions)\n",
      "2021-01-15 03:57:06,478 : INFO : token count processed\n",
      "2021-01-15 03:57:06,511 : INFO : frequencies processed\n",
      "2021-01-15 03:57:16,574 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:57:16,575 : INFO : entropies processed\n",
      "2021-01-15 03:57:16,576 : INFO : extropies processed\n",
      "2021-01-15 03:57:16,585 : INFO : token count processed\n",
      "2021-01-15 03:57:16,589 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:57:16,593 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:57:16,594 : INFO : vocab #32006\n",
      "2021-01-15 03:57:16,601 : INFO : diff #set()\n",
      "2021-01-15 03:57:36,699 : INFO : alphabet #32006\n",
      "2021-01-15 03:57:46,650 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.1280543704823613, 0.4699128057397012], [0.7858210951089859, 0.2141789], [4.850534387012964, 1.4135459214393753], [6.391502818259423, 7.2691387000368, 7.648554074796499, 6.012087443499725, 1.2570512565370757, 0.37941537475969866]]\n",
      "2021-01-15 03:57:46,656 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:57:46,658 : INFO : built Dictionary(374 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1707 corpus positions)\n",
      "2021-01-15 03:57:47,495 : INFO : token count processed\n",
      "2021-01-15 03:57:47,537 : INFO : frequencies processed\n",
      "2021-01-15 03:57:57,299 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:57:57,300 : INFO : entropies processed\n",
      "2021-01-15 03:57:57,301 : INFO : extropies processed\n",
      "2021-01-15 03:57:57,311 : INFO : token count processed\n",
      "2021-01-15 03:57:57,315 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:57:57,319 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:57:57,320 : INFO : vocab #32006\n",
      "2021-01-15 03:57:57,326 : INFO : diff #set()\n",
      "2021-01-15 03:58:17,227 : INFO : alphabet #32006\n",
      "2021-01-15 03:58:27,176 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.1126456313552109, 0.47334014997987345], [0.7476914823055267, 0.25230852], [4.35937791471612, 1.3990384606326727], [6.391502818259423, 7.08857858466988, 7.462010335180281, 6.0180710677490215, 1.0705075169208582, 0.37343175051040145]]\n",
      "2021-01-15 03:58:27,180 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:58:27,181 : INFO : built Dictionary(210 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 630 corpus positions)\n",
      "2021-01-15 03:58:27,491 : INFO : token count processed\n",
      "2021-01-15 03:58:27,534 : INFO : frequencies processed\n",
      "2021-01-15 03:58:37,581 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:58:37,582 : INFO : entropies processed\n",
      "2021-01-15 03:58:37,583 : INFO : extropies processed\n",
      "2021-01-15 03:58:37,594 : INFO : token count processed\n",
      "2021-01-15 03:58:37,599 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:58:37,604 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:58:37,604 : INFO : vocab #32006\n",
      "2021-01-15 03:58:37,611 : INFO : diff #set()\n",
      "2021-01-15 03:58:57,727 : INFO : alphabet #32006\n",
      "2021-01-15 03:59:07,768 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.1917532077834445, 0.4562557483428146], [0.8645188212394714, 0.13548118], [3.5465935642949384, 1.3764678303056375], [6.391502818259423, 6.0479231618016716, 6.828924517693462, 5.610501462367633, 0.4374216994340392, 0.7810013558917905]]\n",
      "2021-01-15 03:59:07,772 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:59:07,774 : INFO : built Dictionary(213 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 749 corpus positions)\n",
      "2021-01-15 03:59:08,086 : INFO : token count processed\n",
      "2021-01-15 03:59:08,130 : INFO : frequencies processed\n",
      "2021-01-15 03:59:18,091 : INFO : scalar_distribution processed\n",
      "2021-01-15 03:59:18,093 : INFO : entropies processed\n",
      "2021-01-15 03:59:18,094 : INFO : extropies processed\n",
      "2021-01-15 03:59:18,101 : INFO : token count processed\n",
      "2021-01-15 03:59:18,105 : INFO : alphabet_source #32006\n",
      "2021-01-15 03:59:18,110 : INFO : alphabet_target #32009\n",
      "2021-01-15 03:59:18,110 : INFO : vocab #32006\n",
      "2021-01-15 03:59:18,117 : INFO : diff #set()\n",
      "2021-01-15 03:59:38,184 : INFO : alphabet #32006\n",
      "2021-01-15 03:59:48,124 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.1960933947349366, 0.45535404022318354], [0.8626241534948349, 0.13737585], [3.4182958340544896, 1.369895090630202], [6.391502818259423, 6.036583168403119, 6.743099313385223, 5.684986673277319, 0.3515964951257997, 0.7065161449821034]]\n",
      "2021-01-15 03:59:48,138 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 03:59:48,139 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 03:59:48,143 : INFO : built Dictionary(604 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 7107 corpus positions)\n",
      "2021-01-15 03:59:51,563 : INFO : token count processed\n",
      "2021-01-15 03:59:51,599 : INFO : frequencies processed\n",
      "2021-01-15 04:00:01,519 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:00:01,520 : INFO : entropies processed\n",
      "2021-01-15 04:00:01,521 : INFO : extropies processed\n",
      "2021-01-15 04:00:01,534 : INFO : token count processed\n",
      "2021-01-15 04:00:01,539 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:00:01,543 : INFO : alphabet_target #32010\n",
      "2021-01-15 04:00:01,544 : INFO : vocab #32006\n",
      "2021-01-15 04:00:01,551 : INFO : diff #set()\n",
      "2021-01-15 04:00:21,633 : INFO : alphabet #32006\n",
      "2021-01-15 04:00:31,600 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.1457183999962712, 0.46604437935646065], [0.8186644911766052, 0.18133551], [5.01182768137281, 1.4178646467347809], [6.391502818259423, 7.29352035514053, 7.523030878244611, 6.1619922951553425, 1.1315280599851878, 0.22951052310408038]]\n",
      "2021-01-15 04:00:31,608 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:00:31,610 : INFO : built Dictionary(423 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 3350 corpus positions)\n",
      "2021-01-15 04:00:32,687 : INFO : token count processed\n",
      "2021-01-15 04:00:32,724 : INFO : frequencies processed\n",
      "2021-01-15 04:00:42,648 : INFO : scalar_distribution processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 04:00:42,650 : INFO : entropies processed\n",
      "2021-01-15 04:00:42,650 : INFO : extropies processed\n",
      "2021-01-15 04:00:42,659 : INFO : token count processed\n",
      "2021-01-15 04:00:42,664 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:00:42,668 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:00:42,668 : INFO : vocab #32006\n",
      "2021-01-15 04:00:42,675 : INFO : diff #set()\n",
      "2021-01-15 04:01:02,878 : INFO : alphabet #32006\n",
      "2021-01-15 04:01:12,804 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.171812599687884, 0.46044488375457077], [0.8313755840063095, 0.16862442], [4.502583407161069, 1.404989372901807], [6.391502818259423, 6.8153433747477745, 7.107269490089661, 6.099576702917537, 0.715766671830238, 0.2919261153418864]]\n",
      "2021-01-15 04:01:12,807 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:01:12,809 : INFO : built Dictionary(190 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 371 corpus positions)\n",
      "2021-01-15 04:01:13,058 : INFO : token count processed\n",
      "2021-01-15 04:01:13,104 : INFO : frequencies processed\n",
      "2021-01-15 04:01:23,034 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:01:23,035 : INFO : entropies processed\n",
      "2021-01-15 04:01:23,036 : INFO : extropies processed\n",
      "2021-01-15 04:01:23,045 : INFO : token count processed\n",
      "2021-01-15 04:01:23,050 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:01:23,054 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:01:23,055 : INFO : vocab #32006\n",
      "2021-01-15 04:01:23,062 : INFO : diff #set()\n",
      "2021-01-15 04:01:43,036 : INFO : alphabet #32006\n",
      "2021-01-15 04:01:52,967 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.1900106451727257, 0.45661878502929837], [0.8604120165109634, 0.13958798], [3.169925001442312, 1.3594000115384994], [6.391502818259423, 6.150121915859574, 7.088660882162621, 5.452963851956376, 0.6971580639031982, 0.9385389663030468]]\n",
      "2021-01-15 04:01:52,971 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:01:52,973 : INFO : built Dictionary(339 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1082 corpus positions)\n",
      "2021-01-15 04:01:53,693 : INFO : token count processed\n",
      "2021-01-15 04:01:53,726 : INFO : frequencies processed\n",
      "2021-01-15 04:02:03,632 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:02:03,633 : INFO : entropies processed\n",
      "2021-01-15 04:02:03,634 : INFO : extropies processed\n",
      "2021-01-15 04:02:03,645 : INFO : token count processed\n",
      "2021-01-15 04:02:03,650 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:02:03,654 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:02:03,655 : INFO : vocab #32006\n",
      "2021-01-15 04:02:03,662 : INFO : diff #set()\n",
      "2021-01-15 04:02:23,685 : INFO : alphabet #32006\n",
      "2021-01-15 04:02:33,595 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.1145754262741279, 0.4729081722859115], [0.7425454556941986, 0.25745454], [4.440404017720927, 1.401650704329017], [6.391502818259423, 7.0391145208191315, 7.5460892808917475, 5.884528058186808, 1.1545864626323246, 0.506974760072616]]\n",
      "2021-01-15 04:02:33,605 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 04:02:33,606 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:02:33,612 : INFO : built Dictionary(626 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 4457 corpus positions)\n",
      "2021-01-15 04:02:36,561 : INFO : token count processed\n",
      "2021-01-15 04:02:36,595 : INFO : frequencies processed\n",
      "2021-01-15 04:02:46,627 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:02:46,628 : INFO : entropies processed\n",
      "2021-01-15 04:02:46,629 : INFO : extropies processed\n",
      "2021-01-15 04:02:46,638 : INFO : token count processed\n",
      "2021-01-15 04:02:46,642 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:02:46,646 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:02:46,647 : INFO : vocab #32006\n",
      "2021-01-15 04:02:46,653 : INFO : diff #set()\n",
      "2021-01-15 04:03:06,498 : INFO : alphabet #32006\n",
      "2021-01-15 04:03:16,133 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.1044203154047971, 0.4751902425004125], [0.7384757995605469, 0.2615242], [5.245743495052968, 1.4200699962413812], [6.391502818259423, 7.482466367279176, 7.830845550730503, 6.0431236348080954, 1.4393427324710801, 0.34837918345132746]]\n",
      "2021-01-15 04:03:16,137 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:03:16,139 : INFO : built Dictionary(235 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 679 corpus positions)\n",
      "2021-01-15 04:03:16,512 : INFO : token count processed\n",
      "2021-01-15 04:03:16,580 : INFO : frequencies processed\n",
      "2021-01-15 04:03:26,634 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:03:26,636 : INFO : entropies processed\n",
      "2021-01-15 04:03:26,637 : INFO : extropies processed\n",
      "2021-01-15 04:03:26,648 : INFO : token count processed\n",
      "2021-01-15 04:03:26,653 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:03:26,659 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:03:26,660 : INFO : vocab #32006\n",
      "2021-01-15 04:03:26,666 : INFO : diff #set()\n",
      "2021-01-15 04:03:46,584 : INFO : alphabet #32006\n",
      "2021-01-15 04:03:56,665 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.1805292457153607, 0.45860425947735106], [0.8512385338544846, 0.14876147], [3.6644977792004623, 1.381962919072374], [6.391502818259423, 6.372162341197667, 7.070620731245773, 5.6930444282113175, 0.6791179129863503, 0.6984583900481063]]\n",
      "2021-01-15 04:03:56,671 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:03:56,673 : INFO : built Dictionary(379 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 2100 corpus positions)\n",
      "2021-01-15 04:03:57,536 : INFO : token count processed\n",
      "2021-01-15 04:03:57,608 : INFO : frequencies processed\n",
      "2021-01-15 04:04:07,539 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:04:07,540 : INFO : entropies processed\n",
      "2021-01-15 04:04:07,541 : INFO : extropies processed\n",
      "2021-01-15 04:04:07,553 : INFO : token count processed\n",
      "2021-01-15 04:04:07,557 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:04:07,562 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:04:07,563 : INFO : vocab #32006\n",
      "2021-01-15 04:04:07,569 : INFO : diff #set()\n",
      "2021-01-15 04:04:27,546 : INFO : alphabet #32006\n",
      "2021-01-15 04:04:37,598 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.172899810985986, 0.46021449997100183], [0.8400232642889023, 0.15997674], [4.004886164091842, 1.386889063317129], [6.391502818259423, 6.798155919669889, 7.2018804681181985, 5.9877782698111135, 0.8103776498587756, 0.4037245484483094]]\n",
      "2021-01-15 04:04:37,602 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:04:37,603 : INFO : built Dictionary(239 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 816 corpus positions)\n",
      "2021-01-15 04:04:37,991 : INFO : token count processed\n",
      "2021-01-15 04:04:38,045 : INFO : frequencies processed\n",
      "2021-01-15 04:04:48,010 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:04:48,011 : INFO : entropies processed\n",
      "2021-01-15 04:04:48,012 : INFO : extropies processed\n",
      "2021-01-15 04:04:48,023 : INFO : token count processed\n",
      "2021-01-15 04:04:48,027 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:04:48,033 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:04:48,033 : INFO : vocab #32006\n",
      "2021-01-15 04:04:48,041 : INFO : diff #set()\n",
      "2021-01-15 04:05:07,922 : INFO : alphabet #32006\n",
      "2021-01-15 04:05:17,965 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.177886382464037, 0.4591607753516557], [0.8525492697954178, 0.14745073], [3.7345216647797517, 1.383483006702452], [6.391502818259423, 6.271631856729336, 6.923381836061692, 5.739752838927066, 0.531879017802269, 0.6517499793323562]]\n",
      "2021-01-15 04:05:17,973 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:05:17,975 : INFO : built Dictionary(405 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 3331 corpus positions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 04:05:18,997 : INFO : token count processed\n",
      "2021-01-15 04:05:19,069 : INFO : frequencies processed\n",
      "2021-01-15 04:05:29,009 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:05:29,010 : INFO : entropies processed\n",
      "2021-01-15 04:05:29,011 : INFO : extropies processed\n",
      "2021-01-15 04:05:29,019 : INFO : token count processed\n",
      "2021-01-15 04:05:29,024 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:05:29,028 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:05:29,029 : INFO : vocab #32006\n",
      "2021-01-15 04:05:29,036 : INFO : diff #set()\n",
      "2021-01-15 04:05:49,047 : INFO : alphabet #32006\n",
      "2021-01-15 04:05:59,167 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.1408631583694775, 0.46710131663044696], [0.7996160238981247, 0.20038398], [4.486348298002912, 1.4032404569979096], [6.391502818259423, 6.873598627629562, 7.165094511434946, 6.100006934454038, 0.7735916931755229, 0.2914958838053838]]\n",
      "2021-01-15 04:05:59,170 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:05:59,171 : INFO : built Dictionary(171 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 279 corpus positions)\n",
      "2021-01-15 04:05:59,365 : INFO : token count processed\n",
      "2021-01-15 04:05:59,397 : INFO : frequencies processed\n",
      "2021-01-15 04:06:09,352 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:06:09,353 : INFO : entropies processed\n",
      "2021-01-15 04:06:09,354 : INFO : extropies processed\n",
      "2021-01-15 04:06:09,364 : INFO : token count processed\n",
      "2021-01-15 04:06:09,368 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:06:09,373 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:06:09,374 : INFO : vocab #32006\n",
      "2021-01-15 04:06:09,381 : INFO : diff #set()\n",
      "2021-01-15 04:06:29,880 : INFO : alphabet #32006\n",
      "2021-01-15 04:06:40,018 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.14711355902657, 0.46574155139394063], [0.8108259588479996, 0.18917404], [3.095795255000934, 1.3487605247277434], [6.391502818259423, 6.049830202851529, 7.109954517396574, 5.3313785037143795, 0.7184516991371508, 1.0601243145450443]]\n",
      "2021-01-15 04:06:40,023 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:06:40,025 : INFO : built Dictionary(304 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1359 corpus positions)\n",
      "2021-01-15 04:06:40,635 : INFO : token count processed\n",
      "2021-01-15 04:06:40,716 : INFO : frequencies processed\n",
      "2021-01-15 04:06:50,650 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:06:50,651 : INFO : entropies processed\n",
      "2021-01-15 04:06:50,652 : INFO : extropies processed\n",
      "2021-01-15 04:06:50,660 : INFO : token count processed\n",
      "2021-01-15 04:06:50,665 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:06:50,669 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:06:50,670 : INFO : vocab #32006\n",
      "2021-01-15 04:06:50,676 : INFO : diff #set()\n",
      "2021-01-15 04:07:10,929 : INFO : alphabet #32006\n",
      "2021-01-15 04:07:20,878 : INFO : Computed distances or similarities ('262', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.1142486615428322, 0.4729812619440276], [0.7767249345779419, 0.22327507], [4.381580488309163, 1.4003646830097933], [6.391502818259423, 6.778844940588858, 7.22998185015282, 5.94036590869546, 0.8384790318933968, 0.4511369095639619]]\n",
      "2021-01-15 04:07:20,881 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:07:20,883 : INFO : built Dictionary(215 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 392 corpus positions)\n",
      "2021-01-15 04:07:21,231 : INFO : token count processed\n",
      "2021-01-15 04:07:21,314 : INFO : frequencies processed\n",
      "2021-01-15 04:07:31,399 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:07:31,400 : INFO : entropies processed\n",
      "2021-01-15 04:07:31,401 : INFO : extropies processed\n",
      "2021-01-15 04:07:31,408 : INFO : token count processed\n",
      "2021-01-15 04:07:31,413 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:07:31,417 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:07:31,418 : INFO : vocab #32006\n",
      "2021-01-15 04:07:31,425 : INFO : diff #set()\n",
      "2021-01-15 04:07:51,378 : INFO : alphabet #32006\n",
      "2021-01-15 04:08:01,315 : INFO : Computed distances or similarities ('262', 'sacp-python-common/setup.py')[[1.1548868163522883, 0.46406149613591424], [0.8183157593011856, 0.18168424], [3.521640636343319, 1.3740281872300928], [6.391502818259423, 6.469677430851302, 7.333187771686351, 5.527992477424373, 0.941684953426928, 0.8635103408350489]]\n",
      "2021-01-15 04:08:01,320 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:08:01,321 : INFO : built Dictionary(281 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1294 corpus positions)\n",
      "2021-01-15 04:08:01,838 : INFO : token count processed\n",
      "2021-01-15 04:08:01,870 : INFO : frequencies processed\n",
      "2021-01-15 04:08:11,945 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:08:11,946 : INFO : entropies processed\n",
      "2021-01-15 04:08:11,946 : INFO : extropies processed\n",
      "2021-01-15 04:08:11,954 : INFO : token count processed\n",
      "2021-01-15 04:08:11,959 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:08:11,963 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:08:11,964 : INFO : vocab #32006\n",
      "2021-01-15 04:08:11,971 : INFO : diff #set()\n",
      "2021-01-15 04:08:31,792 : INFO : alphabet #32006\n",
      "2021-01-15 04:08:41,577 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.1626338408897832, 0.46239912697776203], [0.8309640288352966, 0.16903597], [3.8230679822736606, 1.3806412677662334], [6.391502818259423, 6.459180448028249, 6.956594871023318, 5.894088395264353, 0.5650920527638954, 0.4974144229950692]]\n",
      "2021-01-15 04:08:41,581 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:08:41,582 : INFO : built Dictionary(196 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 444 corpus positions)\n",
      "2021-01-15 04:08:41,844 : INFO : token count processed\n",
      "2021-01-15 04:08:41,876 : INFO : frequencies processed\n",
      "2021-01-15 04:08:51,850 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:08:51,851 : INFO : entropies processed\n",
      "2021-01-15 04:08:51,851 : INFO : extropies processed\n",
      "2021-01-15 04:08:51,860 : INFO : token count processed\n",
      "2021-01-15 04:08:51,865 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:08:51,869 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:08:51,870 : INFO : vocab #32006\n",
      "2021-01-15 04:08:51,877 : INFO : diff #set()\n",
      "2021-01-15 04:09:11,837 : INFO : alphabet #32006\n",
      "2021-01-15 04:09:22,064 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.172693757474709, 0.46025814570493623], [0.8351432383060455, 0.16485676], [3.4182958340544896, 1.369895090630202], [6.391502818259423, 6.097125733496388, 6.995501416211393, 5.493127135544418, 0.6039985979519704, 0.8983756827150051]]\n",
      "2021-01-15 04:09:22,068 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:09:22,069 : INFO : built Dictionary(188 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 440 corpus positions)\n",
      "2021-01-15 04:09:22,321 : INFO : token count processed\n",
      "2021-01-15 04:09:22,379 : INFO : frequencies processed\n",
      "2021-01-15 04:09:32,639 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:09:32,642 : INFO : entropies processed\n",
      "2021-01-15 04:09:32,643 : INFO : extropies processed\n",
      "2021-01-15 04:09:32,654 : INFO : token count processed\n",
      "2021-01-15 04:09:32,658 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:09:32,663 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:09:32,664 : INFO : vocab #32006\n",
      "2021-01-15 04:09:32,671 : INFO : diff #set()\n",
      "2021-01-15 04:09:52,573 : INFO : alphabet #32006\n",
      "2021-01-15 04:10:02,668 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.1766018934302582, 0.4594317422117236], [0.8394441306591034, 0.16055587], [3.4182958340544896, 1.369895090630202], [6.391502818259423, 6.0695858597523715, 6.972761138288985, 5.4883275397228095, 0.581258320029562, 0.9031752785366134]]\n",
      "2021-01-15 04:10:02,671 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 04:10:02,672 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 04:10:02,677 : INFO : built Dictionary(190 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 536 corpus positions)\n",
      "2021-01-15 04:10:02,924 : INFO : token count processed\n",
      "2021-01-15 04:10:02,957 : INFO : frequencies processed\n",
      "2021-01-15 04:10:12,899 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:10:12,900 : INFO : entropies processed\n",
      "2021-01-15 04:10:12,901 : INFO : extropies processed\n",
      "2021-01-15 04:10:12,908 : INFO : token count processed\n",
      "2021-01-15 04:10:12,913 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:10:12,917 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:10:12,918 : INFO : vocab #32006\n",
      "2021-01-15 04:10:12,925 : INFO : diff #set()\n",
      "2021-01-15 04:10:32,837 : INFO : alphabet #32006\n",
      "2021-01-15 04:10:42,891 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.1655324392675401, 0.4617801986555488], [0.8225201070308685, 0.1774799], [3.3927474104487847, 1.3672090515720436], [6.391502818259423, 6.104787343210121, 6.897335319494755, 5.5989548419747885, 0.5058325012353322, 0.7925479762846344]]\n",
      "2021-01-15 04:10:42,908 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 04:10:42,908 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:10:42,912 : INFO : built Dictionary(466 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 9221 corpus positions)\n",
      "2021-01-15 04:10:44,168 : INFO : token count processed\n",
      "2021-01-15 04:10:44,202 : INFO : frequencies processed\n",
      "2021-01-15 04:10:54,124 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:10:54,125 : INFO : entropies processed\n",
      "2021-01-15 04:10:54,126 : INFO : extropies processed\n",
      "2021-01-15 04:10:54,136 : INFO : token count processed\n",
      "2021-01-15 04:10:54,141 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:10:54,145 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:10:54,146 : INFO : vocab #32006\n",
      "2021-01-15 04:10:54,153 : INFO : diff #set()\n",
      "2021-01-15 04:11:14,171 : INFO : alphabet #32006\n",
      "2021-01-15 04:11:24,151 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.1538363672582141, 0.4642878239041798], [0.8227265030145645, 0.1772735], [4.373551149096554, 1.4018862281158964], [6.391502818259423, 6.89087415148015, 7.115584996985223, 6.16679197275435, 0.7240821787258005, 0.22471084550507303]]\n",
      "2021-01-15 04:11:24,157 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:11:24,159 : INFO : built Dictionary(328 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 2381 corpus positions)\n",
      "2021-01-15 04:11:24,836 : INFO : token count processed\n",
      "2021-01-15 04:11:24,870 : INFO : frequencies processed\n",
      "2021-01-15 04:11:34,832 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:11:34,833 : INFO : entropies processed\n",
      "2021-01-15 04:11:34,834 : INFO : extropies processed\n",
      "2021-01-15 04:11:34,845 : INFO : token count processed\n",
      "2021-01-15 04:11:34,850 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:11:34,855 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:11:34,856 : INFO : vocab #32006\n",
      "2021-01-15 04:11:34,862 : INFO : diff #set()\n",
      "2021-01-15 04:11:54,871 : INFO : alphabet #32006\n",
      "2021-01-15 04:12:04,795 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.1303266938647705, 0.4694115709482249], [0.7610501199960709, 0.23894988], [4.085055102756477, 1.3920086919086898], [6.391502818259423, 6.655493573668506, 6.991789320819167, 6.0552070711087636, 0.6002865025597437, 0.33629574715066024]]\n",
      "2021-01-15 04:12:04,800 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:12:04,802 : INFO : built Dictionary(307 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1388 corpus positions)\n",
      "2021-01-15 04:12:05,427 : INFO : token count processed\n",
      "2021-01-15 04:12:05,509 : INFO : frequencies processed\n",
      "2021-01-15 04:12:15,387 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:12:15,388 : INFO : entropies processed\n",
      "2021-01-15 04:12:15,389 : INFO : extropies processed\n",
      "2021-01-15 04:12:15,400 : INFO : token count processed\n",
      "2021-01-15 04:12:15,405 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:12:15,409 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:12:15,410 : INFO : vocab #32006\n",
      "2021-01-15 04:12:15,416 : INFO : diff #set()\n",
      "2021-01-15 04:12:35,417 : INFO : alphabet #32006\n",
      "2021-01-15 04:12:45,511 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.1631226211501853, 0.4622946430416763], [0.8249246329069138, 0.17507537], [4.021928094887363, 1.394616085503868], [6.391502818259423, 6.6236746347295465, 7.050074368874129, 5.96510308411484, 0.6585715506147061, 0.42639973414458243]]\n",
      "2021-01-15 04:12:45,516 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:12:45,518 : INFO : built Dictionary(304 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1327 corpus positions)\n",
      "2021-01-15 04:12:46,139 : INFO : token count processed\n",
      "2021-01-15 04:12:46,220 : INFO : frequencies processed\n",
      "2021-01-15 04:12:56,112 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:12:56,113 : INFO : entropies processed\n",
      "2021-01-15 04:12:56,114 : INFO : extropies processed\n",
      "2021-01-15 04:12:56,124 : INFO : token count processed\n",
      "2021-01-15 04:12:56,129 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:12:56,133 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:12:56,136 : INFO : vocab #32006\n",
      "2021-01-15 04:12:56,143 : INFO : diff #set()\n",
      "2021-01-15 04:13:16,124 : INFO : alphabet #32006\n",
      "2021-01-15 04:13:26,026 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.1070677520924412, 0.4745931871468972], [0.7267340123653412, 0.273266], [4.323856189774724, 1.403827785435418], [6.391502818259423, 6.75472436518627, 7.155092717522321, 5.991134465923371, 0.7635898992628984, 0.4003683523360513]]\n",
      "2021-01-15 04:13:26,031 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:13:26,033 : INFO : built Dictionary(260 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1122 corpus positions)\n",
      "2021-01-15 04:13:26,498 : INFO : token count processed\n",
      "2021-01-15 04:13:26,565 : INFO : frequencies processed\n",
      "2021-01-15 04:13:36,598 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:13:36,599 : INFO : entropies processed\n",
      "2021-01-15 04:13:36,600 : INFO : extropies processed\n",
      "2021-01-15 04:13:36,614 : INFO : token count processed\n",
      "2021-01-15 04:13:36,619 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:13:36,627 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:13:36,628 : INFO : vocab #32006\n",
      "2021-01-15 04:13:36,634 : INFO : diff #set()\n",
      "2021-01-15 04:13:56,089 : INFO : alphabet #32006\n",
      "2021-01-15 04:14:05,741 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.1364978122656892, 0.46805570979711475], [0.7560721039772034, 0.2439279], [4.037401197654112, 1.3956040576270576], [6.391502818259423, 6.597313085495733, 7.058833800342207, 5.92998210341295, 0.6673309820827837, 0.4615207148464737]]\n",
      "2021-01-15 04:14:05,745 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:14:05,747 : INFO : built Dictionary(285 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1088 corpus positions)\n",
      "2021-01-15 04:14:06,264 : INFO : token count processed\n",
      "2021-01-15 04:14:06,314 : INFO : frequencies processed\n",
      "2021-01-15 04:14:16,341 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:14:16,342 : INFO : entropies processed\n",
      "2021-01-15 04:14:16,343 : INFO : extropies processed\n",
      "2021-01-15 04:14:16,354 : INFO : token count processed\n",
      "2021-01-15 04:14:16,359 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:14:16,364 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:14:16,364 : INFO : vocab #32006\n",
      "2021-01-15 04:14:16,371 : INFO : diff #set()\n",
      "2021-01-15 04:14:36,222 : INFO : alphabet #32006\n",
      "2021-01-15 04:14:46,266 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.1411791780777825, 0.46703237647665613], [0.7962251305580139, 0.20377487], [3.9148663038831, 1.3846407177488558], [6.391502818259423, 6.659481538516613, 7.133016737072182, 5.9179676197038535, 0.7415139188127595, 0.47353519855556936]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 04:14:46,271 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:14:46,272 : INFO : built Dictionary(308 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1250 corpus positions)\n",
      "2021-01-15 04:14:46,891 : INFO : token count processed\n",
      "2021-01-15 04:14:46,923 : INFO : frequencies processed\n",
      "2021-01-15 04:14:56,760 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:14:56,761 : INFO : entropies processed\n",
      "2021-01-15 04:14:56,762 : INFO : extropies processed\n",
      "2021-01-15 04:14:56,770 : INFO : token count processed\n",
      "2021-01-15 04:14:56,775 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:14:56,779 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:14:56,779 : INFO : vocab #32006\n",
      "2021-01-15 04:14:56,786 : INFO : diff #set()\n",
      "2021-01-15 04:15:16,616 : INFO : alphabet #32006\n",
      "2021-01-15 04:15:26,623 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.1355712228449215, 0.46825879151332656], [0.798705443739891, 0.20129456], [4.221928094887363, 1.4022127267891302], [6.391502818259423, 6.774682571479102, 7.182299658445684, 5.98388573129284, 0.7907968401862613, 0.4076170869665825]]\n",
      "2021-01-15 04:15:26,638 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:15:26,642 : INFO : built Dictionary(479 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 7987 corpus positions)\n",
      "2021-01-15 04:15:28,021 : INFO : token count processed\n",
      "2021-01-15 04:15:28,054 : INFO : frequencies processed\n",
      "2021-01-15 04:15:37,911 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:15:37,912 : INFO : entropies processed\n",
      "2021-01-15 04:15:37,913 : INFO : extropies processed\n",
      "2021-01-15 04:15:37,923 : INFO : token count processed\n",
      "2021-01-15 04:15:37,928 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:15:37,932 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:15:37,933 : INFO : vocab #32006\n",
      "2021-01-15 04:15:37,940 : INFO : diff #set()\n",
      "2021-01-15 04:15:57,895 : INFO : alphabet #32006\n",
      "2021-01-15 04:16:07,802 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.1500285490638924, 0.465110103042768], [0.8281114548444748, 0.17188855], [4.536286231168867, 1.4059370971230252], [6.391502818259423, 6.839453716525233, 7.0627342177298305, 6.168222317054826, 0.6712313994704076, 0.22328050120459775]]\n",
      "2021-01-15 04:16:07,808 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:16:07,810 : INFO : built Dictionary(382 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 2429 corpus positions)\n",
      "2021-01-15 04:16:08,679 : INFO : token count processed\n",
      "2021-01-15 04:16:08,746 : INFO : frequencies processed\n",
      "2021-01-15 04:16:18,641 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:16:18,642 : INFO : entropies processed\n",
      "2021-01-15 04:16:18,643 : INFO : extropies processed\n",
      "2021-01-15 04:16:18,655 : INFO : token count processed\n",
      "2021-01-15 04:16:18,659 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:16:18,666 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:16:18,667 : INFO : vocab #32006\n",
      "2021-01-15 04:16:18,676 : INFO : diff #set()\n",
      "2021-01-15 04:16:38,627 : INFO : alphabet #32006\n",
      "2021-01-15 04:16:48,574 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.1072667386177377, 0.47454837191420307], [0.7402919232845306, 0.25970808], [4.300554385304355, 1.3977524393134486], [6.391502818259423, 6.86432793886027, 7.151226615381684, 6.104604141738009, 0.7597237971222608, 0.2868986765214139]]\n",
      "2021-01-15 04:16:48,578 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:16:48,580 : INFO : built Dictionary(221 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 631 corpus positions)\n",
      "2021-01-15 04:16:48,916 : INFO : token count processed\n",
      "2021-01-15 04:16:48,973 : INFO : frequencies processed\n",
      "2021-01-15 04:16:58,862 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:16:58,863 : INFO : entropies processed\n",
      "2021-01-15 04:16:58,864 : INFO : extropies processed\n",
      "2021-01-15 04:16:58,872 : INFO : token count processed\n",
      "2021-01-15 04:16:58,876 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:16:58,880 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:16:58,881 : INFO : vocab #32006\n",
      "2021-01-15 04:16:58,887 : INFO : diff #set()\n",
      "2021-01-15 04:17:18,888 : INFO : alphabet #32006\n",
      "2021-01-15 04:17:28,796 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.1056438837354523, 0.474914114264175], [0.7128590941429138, 0.2871409], [4.037401197654112, 1.3956040576270576], [6.391502818259423, 6.431978396403875, 7.063703535651174, 5.759777679012124, 0.6722007173917515, 0.6317251392472993]]\n",
      "2021-01-15 04:17:28,801 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:17:28,802 : INFO : built Dictionary(271 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 785 corpus positions)\n",
      "2021-01-15 04:17:29,321 : INFO : token count processed\n",
      "2021-01-15 04:17:29,367 : INFO : frequencies processed\n",
      "2021-01-15 04:17:39,257 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:17:39,258 : INFO : entropies processed\n",
      "2021-01-15 04:17:39,259 : INFO : extropies processed\n",
      "2021-01-15 04:17:39,267 : INFO : token count processed\n",
      "2021-01-15 04:17:39,271 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:17:39,275 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:17:39,276 : INFO : vocab #32006\n",
      "2021-01-15 04:17:39,283 : INFO : diff #set()\n",
      "2021-01-15 04:17:59,441 : INFO : alphabet #32006\n",
      "2021-01-15 04:18:09,341 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/test_auth_utility.py')[[1.1137035726401703, 0.4731032359239129], [0.7728779017925262, 0.2271221], [4.631615665225585, 1.411413581385923], [6.391502818259423, 6.911818353685893, 7.382161359274538, 5.921159812670779, 0.9906585410151152, 0.47034300558864484]]\n",
      "2021-01-15 04:18:09,355 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:18:09,358 : INFO : built Dictionary(369 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 7315 corpus positions)\n",
      "2021-01-15 04:18:10,302 : INFO : token count processed\n",
      "2021-01-15 04:18:10,336 : INFO : frequencies processed\n",
      "2021-01-15 04:18:20,367 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:18:20,369 : INFO : entropies processed\n",
      "2021-01-15 04:18:20,370 : INFO : extropies processed\n",
      "2021-01-15 04:18:20,382 : INFO : token count processed\n",
      "2021-01-15 04:18:20,387 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:18:20,392 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:18:20,393 : INFO : vocab #32006\n",
      "2021-01-15 04:18:20,399 : INFO : diff #set()\n",
      "2021-01-15 04:18:40,555 : INFO : alphabet #32006\n",
      "2021-01-15 04:18:50,463 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.1491589711719141, 0.46529829268735307], [0.827179566025734, 0.17282043], [4.152391277629867, 1.3967461997807682], [6.391502818259423, 6.363791471162389, 6.491386263252851, 6.263908026168961, 0.09988344499342805, 0.12759479209046187]]\n",
      "2021-01-15 04:18:50,468 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:18:50,469 : INFO : built Dictionary(269 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1385 corpus positions)\n",
      "2021-01-15 04:18:50,978 : INFO : token count processed\n",
      "2021-01-15 04:18:51,025 : INFO : frequencies processed\n",
      "2021-01-15 04:19:01,011 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:19:01,014 : INFO : entropies processed\n",
      "2021-01-15 04:19:01,016 : INFO : extropies processed\n",
      "2021-01-15 04:19:01,026 : INFO : token count processed\n",
      "2021-01-15 04:19:01,032 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:19:01,038 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:19:01,041 : INFO : vocab #32006\n",
      "2021-01-15 04:19:01,049 : INFO : diff #set()\n",
      "2021-01-15 04:19:20,723 : INFO : alphabet #32006\n",
      "2021-01-15 04:19:30,503 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.1676581153746428, 0.4613273619613982], [0.8360356241464615, 0.16396438], [4.168295834054489, 1.393424708894837], [6.391502818259423, 6.29000629755059, 6.826758102869578, 5.854751012940435, 0.4352552846101547, 0.5367518053189873]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 04:19:30,507 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:19:30,509 : INFO : built Dictionary(282 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1338 corpus positions)\n",
      "2021-01-15 04:19:31,027 : INFO : token count processed\n",
      "2021-01-15 04:19:31,068 : INFO : frequencies processed\n",
      "2021-01-15 04:19:41,002 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:19:41,003 : INFO : entropies processed\n",
      "2021-01-15 04:19:41,004 : INFO : extropies processed\n",
      "2021-01-15 04:19:41,015 : INFO : token count processed\n",
      "2021-01-15 04:19:41,022 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:19:41,027 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:19:41,028 : INFO : vocab #32006\n",
      "2021-01-15 04:19:41,035 : INFO : diff #set()\n",
      "2021-01-15 04:20:00,910 : INFO : alphabet #32006\n",
      "2021-01-15 04:20:10,940 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.1453748296408461, 0.4661190138822541], [0.8223048895597458, 0.17769511], [3.916126946588284, 1.3829873586081372], [6.391502818259423, 6.361621244785958, 6.888551559077497, 5.864572503967883, 0.49704874081807393, 0.5269303142915387]]\n",
      "2021-01-15 04:20:10,945 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:20:10,947 : INFO : built Dictionary(291 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1361 corpus positions)\n",
      "2021-01-15 04:20:11,490 : INFO : token count processed\n",
      "2021-01-15 04:20:11,557 : INFO : frequencies processed\n",
      "2021-01-15 04:20:21,478 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:20:21,479 : INFO : entropies processed\n",
      "2021-01-15 04:20:21,480 : INFO : extropies processed\n",
      "2021-01-15 04:20:21,491 : INFO : token count processed\n",
      "2021-01-15 04:20:21,496 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:20:21,501 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:20:21,502 : INFO : vocab #32006\n",
      "2021-01-15 04:20:21,508 : INFO : diff #set()\n",
      "2021-01-15 04:20:41,481 : INFO : alphabet #32006\n",
      "2021-01-15 04:20:51,400 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.1359235660499853, 0.4681815472682498], [0.7373580634593964, 0.26264194], [3.8208888513501886, 1.379550937737299], [6.391502818259423, 6.620594433343389, 7.124667419818271, 5.887429831784541, 0.7331646015588484, 0.5040729864748821]]\n",
      "2021-01-15 04:20:51,405 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:20:51,407 : INFO : built Dictionary(264 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1454 corpus positions)\n",
      "2021-01-15 04:20:51,873 : INFO : token count processed\n",
      "2021-01-15 04:20:51,922 : INFO : frequencies processed\n",
      "2021-01-15 04:21:01,826 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:21:01,827 : INFO : entropies processed\n",
      "2021-01-15 04:21:01,828 : INFO : extropies processed\n",
      "2021-01-15 04:21:01,836 : INFO : token count processed\n",
      "2021-01-15 04:21:01,840 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:21:01,845 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:21:01,845 : INFO : vocab #32006\n",
      "2021-01-15 04:21:01,852 : INFO : diff #set()\n",
      "2021-01-15 04:21:21,816 : INFO : alphabet #32006\n",
      "2021-01-15 04:21:31,739 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.1518284293358665, 0.46472106528894447], [0.7565940469503403, 0.24340595], [3.7216117239699, 1.3734758422688993], [6.391502818259423, 6.207411496248084, 6.6591897100546955, 5.939724604452811, 0.2676868917952726, 0.45177821380661154]]\n",
      "2021-01-15 04:21:31,749 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:21:31,750 : INFO : built Dictionary(205 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 358 corpus positions)\n",
      "2021-01-15 04:21:32,045 : INFO : token count processed\n",
      "2021-01-15 04:21:32,080 : INFO : frequencies processed\n",
      "2021-01-15 04:21:41,982 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:21:41,984 : INFO : entropies processed\n",
      "2021-01-15 04:21:41,984 : INFO : extropies processed\n",
      "2021-01-15 04:21:41,995 : INFO : token count processed\n",
      "2021-01-15 04:21:42,000 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:21:42,004 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:21:42,005 : INFO : vocab #32006\n",
      "2021-01-15 04:21:42,012 : INFO : diff #set()\n",
      "2021-01-15 04:22:02,027 : INFO : alphabet #32006\n",
      "2021-01-15 04:22:11,973 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.1259018468920536, 0.4703886030589524], [0.7126229107379913, 0.2873771], [3.773557262275185, 1.3866235995561977], [6.391502818259423, 6.5805228788529595, 7.34120410463686, 5.630821592475522, 0.9497012863774374, 0.7606812257839008]]\n",
      "2021-01-15 04:22:11,978 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 04:22:11,979 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:22:11,983 : INFO : built Dictionary(287 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1319 corpus positions)\n",
      "2021-01-15 04:22:12,571 : INFO : token count processed\n",
      "2021-01-15 04:22:12,654 : INFO : frequencies processed\n",
      "2021-01-15 04:22:22,565 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:22:22,566 : INFO : entropies processed\n",
      "2021-01-15 04:22:22,567 : INFO : extropies processed\n",
      "2021-01-15 04:22:22,578 : INFO : token count processed\n",
      "2021-01-15 04:22:22,583 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:22:22,588 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:22:22,588 : INFO : vocab #32006\n",
      "2021-01-15 04:22:22,595 : INFO : diff #set()\n",
      "2021-01-15 04:22:42,603 : INFO : alphabet #32006\n",
      "2021-01-15 04:22:52,515 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.1639186712827627, 0.46212457670934737], [0.8499040603637695, 0.15009594], [3.794653473544342, 1.3826918618338855], [6.391502818259423, 6.422089779976135, 6.853153537023321, 5.960439061212238, 0.46165071876389785, 0.4310637570471858]]\n",
      "2021-01-15 04:22:52,520 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:22:52,522 : INFO : built Dictionary(298 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1601 corpus positions)\n",
      "2021-01-15 04:22:53,086 : INFO : token count processed\n",
      "2021-01-15 04:22:53,158 : INFO : frequencies processed\n",
      "2021-01-15 04:23:03,104 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:23:03,106 : INFO : entropies processed\n",
      "2021-01-15 04:23:03,106 : INFO : extropies processed\n",
      "2021-01-15 04:23:03,117 : INFO : token count processed\n",
      "2021-01-15 04:23:03,122 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:23:03,127 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:23:03,128 : INFO : vocab #32006\n",
      "2021-01-15 04:23:03,134 : INFO : diff #set()\n",
      "2021-01-15 04:23:23,301 : INFO : alphabet #32006\n",
      "2021-01-15 04:23:33,311 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.1529027948579988, 0.46448915500895055], [0.7774437069892883, 0.2225563], [3.8219280948873626, 1.378552993212668], [6.391502818259423, 6.485445644653597, 7.002599525926437, 5.874348936986584, 0.6110967076670137, 0.51715388127284]]\n",
      "2021-01-15 04:23:33,316 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:23:33,318 : INFO : built Dictionary(275 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1487 corpus positions)\n",
      "2021-01-15 04:23:33,839 : INFO : token count processed\n",
      "2021-01-15 04:23:33,874 : INFO : frequencies processed\n",
      "2021-01-15 04:23:43,921 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:23:43,922 : INFO : entropies processed\n",
      "2021-01-15 04:23:43,923 : INFO : extropies processed\n",
      "2021-01-15 04:23:43,931 : INFO : token count processed\n",
      "2021-01-15 04:23:43,935 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:23:43,940 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:23:43,941 : INFO : vocab #32006\n",
      "2021-01-15 04:23:43,947 : INFO : diff #set()\n",
      "2021-01-15 04:24:03,873 : INFO : alphabet #32006\n",
      "2021-01-15 04:24:13,807 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/third_party/test_cve_bd.py')[[1.153390607869457, 0.46438393310788606], [0.7592335343360901, 0.24076647], [3.614369445886757, 1.3676145452014115], [6.391502818259423, 6.2276600107346916, 6.671730672946703, 5.947432156047411, 0.28022785468728006, 0.4440706622120114]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 04:24:13,812 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:24:13,817 : INFO : built Dictionary(262 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1685 corpus positions)\n",
      "2021-01-15 04:24:14,268 : INFO : token count processed\n",
      "2021-01-15 04:24:14,303 : INFO : frequencies processed\n",
      "2021-01-15 04:24:24,184 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:24:24,185 : INFO : entropies processed\n",
      "2021-01-15 04:24:24,186 : INFO : extropies processed\n",
      "2021-01-15 04:24:24,197 : INFO : token count processed\n",
      "2021-01-15 04:24:24,201 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:24:24,205 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:24:24,206 : INFO : vocab #32006\n",
      "2021-01-15 04:24:24,212 : INFO : diff #set()\n",
      "2021-01-15 04:24:43,975 : INFO : alphabet #32006\n",
      "2021-01-15 04:24:53,776 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/third_party/test_ipCentralScan.py')[[1.156016939764061, 0.4638182481578428], [0.8204553574323654, 0.17954464], [3.916126946588284, 1.3829873586081372], [6.391502818259423, 6.253918170574241, 6.714824347341677, 5.930596641491986, 0.3233215290822544, 0.4609061767674367]]\n",
      "2021-01-15 04:24:53,780 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:24:53,782 : INFO : built Dictionary(237 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 782 corpus positions)\n",
      "2021-01-15 04:24:54,194 : INFO : token count processed\n",
      "2021-01-15 04:24:54,269 : INFO : frequencies processed\n",
      "2021-01-15 04:25:04,328 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:25:04,329 : INFO : entropies processed\n",
      "2021-01-15 04:25:04,330 : INFO : extropies processed\n",
      "2021-01-15 04:25:04,337 : INFO : token count processed\n",
      "2021-01-15 04:25:04,342 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:25:04,346 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:25:04,347 : INFO : vocab #32006\n",
      "2021-01-15 04:25:04,354 : INFO : diff #set()\n",
      "2021-01-15 04:25:24,240 : INFO : alphabet #32006\n",
      "2021-01-15 04:25:34,290 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/third_party/test_ipcReport.py')[[1.118842084368655, 0.4719558891987776], [0.7132961452007294, 0.28670385], [3.5841837197791886, 1.366946801689373], [6.391502818259423, 6.374522245625576, 7.014910017500387, 5.751115046384612, 0.6234071992409644, 0.6403877718748117]]\n",
      "2021-01-15 04:25:34,295 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:25:34,297 : INFO : built Dictionary(355 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 2064 corpus positions)\n",
      "2021-01-15 04:25:35,111 : INFO : token count processed\n",
      "2021-01-15 04:25:35,143 : INFO : frequencies processed\n",
      "2021-01-15 04:25:45,011 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:25:45,012 : INFO : entropies processed\n",
      "2021-01-15 04:25:45,013 : INFO : extropies processed\n",
      "2021-01-15 04:25:45,020 : INFO : token count processed\n",
      "2021-01-15 04:25:45,025 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:25:45,030 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:25:45,030 : INFO : vocab #32006\n",
      "2021-01-15 04:25:45,037 : INFO : diff #set()\n",
      "2021-01-15 04:26:05,152 : INFO : alphabet #32006\n",
      "2021-01-15 04:26:15,216 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/third_party/test_triage.py')[[1.147983067777506, 0.4655530180853281], [0.7975567430257797, 0.20244326], [3.970175521464345, 1.3854625861850216], [6.391502818259423, 6.731238669067808, 7.2083105854931615, 5.914430901834069, 0.8168077672337386, 0.47707191642535385]]\n",
      "2021-01-15 04:26:15,220 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:26:15,222 : INFO : built Dictionary(278 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1456 corpus positions)\n",
      "2021-01-15 04:26:15,735 : INFO : token count processed\n",
      "2021-01-15 04:26:15,767 : INFO : frequencies processed\n",
      "2021-01-15 04:26:25,648 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:26:25,649 : INFO : entropies processed\n",
      "2021-01-15 04:26:25,650 : INFO : extropies processed\n",
      "2021-01-15 04:26:25,661 : INFO : token count processed\n",
      "2021-01-15 04:26:25,665 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:26:25,670 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:26:25,671 : INFO : vocab #32006\n",
      "2021-01-15 04:26:25,678 : INFO : diff #set()\n",
      "2021-01-15 04:26:45,693 : INFO : alphabet #32006\n",
      "2021-01-15 04:26:55,645 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/third_party/test_UploadBom.py')[[1.1315215251409454, 0.469148440775833], [0.8028544634580612, 0.19714554], [4.183465189601646, 1.396402945576406], [6.391502818259423, 6.503741451859337, 6.975866718800859, 5.9193775513179006, 0.5843639005414358, 0.47212526694152146]]\n",
      "2021-01-15 04:26:55,650 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:26:55,652 : INFO : built Dictionary(301 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 1778 corpus positions)\n",
      "2021-01-15 04:26:56,269 : INFO : token count processed\n",
      "2021-01-15 04:26:56,300 : INFO : frequencies processed\n",
      "2021-01-15 04:27:06,201 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:27:06,202 : INFO : entropies processed\n",
      "2021-01-15 04:27:06,203 : INFO : extropies processed\n",
      "2021-01-15 04:27:06,211 : INFO : token count processed\n",
      "2021-01-15 04:27:06,216 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:27:06,220 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:27:06,221 : INFO : vocab #32006\n",
      "2021-01-15 04:27:06,227 : INFO : diff #set()\n",
      "2021-01-15 04:27:26,424 : INFO : alphabet #32006\n",
      "2021-01-15 04:27:36,350 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/third_party/unused/test_bd_image.py')[[1.1355504519109398, 0.4682633459233786], [0.8106783628463745, 0.18932164], [4.084962500721156, 1.3908103068654332], [6.391502818259423, 6.334729224484471, 6.7274378824648355, 5.9987941602790595, 0.33593506420541264, 0.39270865798036425]]\n",
      "2021-01-15 04:27:36,356 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:27:36,358 : INFO : built Dictionary(300 unique tokens: [\"'\", ')', ',', '.', '/']...) from 2 documents (total 2127 corpus positions)\n",
      "2021-01-15 04:27:36,896 : INFO : token count processed\n",
      "2021-01-15 04:27:36,928 : INFO : frequencies processed\n",
      "2021-01-15 04:27:46,879 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:27:46,880 : INFO : entropies processed\n",
      "2021-01-15 04:27:46,881 : INFO : extropies processed\n",
      "2021-01-15 04:27:46,892 : INFO : token count processed\n",
      "2021-01-15 04:27:46,896 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:27:46,901 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:27:46,902 : INFO : vocab #32006\n",
      "2021-01-15 04:27:46,908 : INFO : diff #set()\n",
      "2021-01-15 04:28:06,957 : INFO : alphabet #32006\n",
      "2021-01-15 04:28:16,922 : INFO : Computed distances or similarities ('262', 'sacp-python-common/test/python/third_party/unused/test_ipc.py')[[1.1519037313157359, 0.4647048032156026], [0.8080240339040756, 0.19197597], [3.8820451081368623, 1.3816526416158188], [6.391502818259423, 6.21319712067992, 6.646515129628318, 5.958184809311026, 0.2550123113688949, 0.43331800894839745]]\n",
      "2021-01-15 04:28:16,927 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 04:28:16,928 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:28:16,933 : INFO : built Dictionary(268 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 1399 corpus positions)\n",
      "2021-01-15 04:28:17,005 : INFO : token count processed\n",
      "2021-01-15 04:28:17,062 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:28:17,069 : INFO : frequencies processed\n",
      "2021-01-15 04:28:17,070 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:28:17,080 : INFO : token count processed\n",
      "2021-01-15 04:28:17,085 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:28:17,090 : INFO : alphabet_target #32010\n",
      "2021-01-15 04:28:17,091 : INFO : vocab #32006\n",
      "2021-01-15 04:28:17,097 : INFO : diff #set()\n",
      "2021-01-15 04:28:37,102 : INFO : alphabet #32006\n",
      "2021-01-15 04:28:47,040 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/auth_utility.py')[[1.3109235880638461, 0.4327274190999222], [0.9857005346566439, 0.014299465], [nan, nan], [3.0, 6.905617163738059, 7.015930750814052, 2.889686412924007, 4.015930750814052, 0.11031358707599281]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 04:28:47,046 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:28:47,048 : INFO : built Dictionary(358 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 2289 corpus positions)\n",
      "2021-01-15 04:28:47,157 : INFO : token count processed\n",
      "2021-01-15 04:28:47,229 : INFO : frequencies processed\n",
      "2021-01-15 04:28:57,448 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:28:57,450 : INFO : entropies processed\n",
      "2021-01-15 04:28:57,450 : INFO : extropies processed\n",
      "2021-01-15 04:28:57,459 : INFO : token count processed\n",
      "2021-01-15 04:28:57,463 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:28:57,467 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:28:57,468 : INFO : vocab #32006\n",
      "2021-01-15 04:28:57,474 : INFO : diff #set()\n",
      "2021-01-15 04:29:17,809 : INFO : alphabet #32006\n",
      "2021-01-15 04:29:27,739 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/bandit/bandit.py')[[1.3092433607561778, 0.4330422756623378], [0.9874938316643238, 0.012506168], [0.0, 0.0], [3.0, 7.1219284286457345, 7.320689425763435, 2.8012390028822995, 4.320689425763435, 0.19876099711770046]]\n",
      "2021-01-15 04:29:27,745 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:29:27,747 : INFO : built Dictionary(278 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 2272 corpus positions)\n",
      "2021-01-15 04:29:27,819 : INFO : token count processed\n",
      "2021-01-15 04:29:27,847 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:29:27,848 : INFO : frequencies processed\n",
      "2021-01-15 04:29:27,849 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:29:27,855 : INFO : token count processed\n",
      "2021-01-15 04:29:27,860 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:29:27,865 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:29:27,866 : INFO : vocab #32006\n",
      "2021-01-15 04:29:27,872 : INFO : diff #set()\n",
      "2021-01-15 04:29:47,670 : INFO : alphabet #32006\n",
      "2021-01-15 04:29:57,612 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/bandit/banditReport.py')[[1.3138275172638485, 0.43218433203807766], [0.9929377613589168, 0.0070622386], [nan, nan], [3.0, 6.41099024988467, 6.461072655249922, 2.949917594634748, 3.4610726552499216, 0.050082405365251326]]\n",
      "2021-01-15 04:29:57,616 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:29:57,617 : INFO : built Dictionary(155 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 671 corpus positions)\n",
      "2021-01-15 04:29:57,654 : INFO : token count processed\n",
      "2021-01-15 04:29:57,690 : INFO : frequencies processed\n",
      "2021-01-15 04:30:07,352 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:30:07,354 : INFO : entropies processed\n",
      "2021-01-15 04:30:07,354 : INFO : extropies processed\n",
      "2021-01-15 04:30:07,366 : INFO : token count processed\n",
      "2021-01-15 04:30:07,370 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:30:07,375 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:30:07,376 : INFO : vocab #32006\n",
      "2021-01-15 04:30:07,383 : INFO : diff #set()\n",
      "2021-01-15 04:30:27,404 : INFO : alphabet #32006\n",
      "2021-01-15 04:30:37,330 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/cave/caveCa.py')[[1.297152912117902, 0.43532147761031365], [0.9684516675770283, 0.031548332], [0.0, 0.0], [3.0, 6.077866832717642, 6.164466248816318, 2.913400583901325, 3.1644662488163178, 0.08659941609867605]]\n",
      "2021-01-15 04:30:37,333 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:30:37,335 : INFO : built Dictionary(131 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 436 corpus positions)\n",
      "2021-01-15 04:30:37,365 : INFO : token count processed\n",
      "2021-01-15 04:30:37,405 : INFO : frequencies processed\n",
      "2021-01-15 04:30:47,324 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:30:47,325 : INFO : entropies processed\n",
      "2021-01-15 04:30:47,326 : INFO : extropies processed\n",
      "2021-01-15 04:30:47,336 : INFO : token count processed\n",
      "2021-01-15 04:30:47,341 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:30:47,346 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:30:47,347 : INFO : vocab #32006\n",
      "2021-01-15 04:30:47,353 : INFO : diff #set()\n",
      "2021-01-15 04:31:07,394 : INFO : alphabet #32006\n",
      "2021-01-15 04:31:17,336 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/cave/caveSsl.py')[[1.3089306622469459, 0.4331009225832904], [0.9869166240096092, 0.013083376], [0.0, 0.0], [3.0, 5.977547459003844, 6.041597533990332, 2.935949925013512, 3.0415975339903323, 0.06405007498648807]]\n",
      "2021-01-15 04:31:17,342 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:31:17,344 : INFO : built Dictionary(237 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 2147 corpus positions)\n",
      "2021-01-15 04:31:17,415 : INFO : token count processed\n",
      "2021-01-15 04:31:17,447 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:31:17,449 : INFO : frequencies processed\n",
      "2021-01-15 04:31:17,452 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:31:17,460 : INFO : token count processed\n",
      "2021-01-15 04:31:17,468 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:31:17,474 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:31:17,476 : INFO : vocab #32006\n",
      "2021-01-15 04:31:17,485 : INFO : diff #set()\n",
      "2021-01-15 04:31:37,471 : INFO : alphabet #32006\n",
      "2021-01-15 04:31:47,381 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/cave/caveZap.py')[[1.3114833034586395, 0.4326226360812186], [0.9921405017375946, 0.007859498], [nan, nan], [3.0, 6.4614394051846435, 6.542646537224702, 2.918792867959942, 3.5426465372247016, 0.08120713204005803]]\n",
      "2021-01-15 04:31:47,385 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 04:31:47,387 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:31:47,391 : INFO : built Dictionary(199 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 1371 corpus positions)\n",
      "2021-01-15 04:31:47,455 : INFO : token count processed\n",
      "2021-01-15 04:31:47,486 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:31:47,487 : INFO : frequencies processed\n",
      "2021-01-15 04:31:47,487 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:31:47,494 : INFO : token count processed\n",
      "2021-01-15 04:31:47,498 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:31:47,503 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:31:47,504 : INFO : vocab #32006\n",
      "2021-01-15 04:31:47,511 : INFO : diff #set()\n",
      "2021-01-15 04:32:07,511 : INFO : alphabet #32006\n",
      "2021-01-15 04:32:17,446 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/csbcicd_report/aggregator.py')[[1.318257304872162, 0.4313585027418447], [0.9940828303806484, 0.0059171696], [nan, nan], [3.0, 6.327195724598159, 6.409100195367461, 2.9180955292306976, 3.4091001953674613, 0.0819044707693024]]\n",
      "2021-01-15 04:32:17,459 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 04:32:17,460 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:32:17,466 : INFO : built Dictionary(414 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 6263 corpus positions)\n",
      "2021-01-15 04:32:17,593 : INFO : token count processed\n",
      "2021-01-15 04:32:17,629 : INFO : frequencies processed\n",
      "2021-01-15 04:32:27,554 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:32:27,555 : INFO : entropies processed\n",
      "2021-01-15 04:32:27,556 : INFO : extropies processed\n",
      "2021-01-15 04:32:27,568 : INFO : token count processed\n",
      "2021-01-15 04:32:27,573 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:32:27,577 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:32:27,578 : INFO : vocab #32006\n",
      "2021-01-15 04:32:27,585 : INFO : diff #set()\n",
      "2021-01-15 04:32:47,581 : INFO : alphabet #32006\n",
      "2021-01-15 04:32:57,519 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicd_func.py')[[1.3115254278097148, 0.4326147521325559], [0.9900118913501501, 0.009988109], [0.0, 0.0], [3.0, 6.9079058562486315, 7.006339516839505, 2.9015663394091273, 4.006339516839505, 0.09843366059087355]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 04:32:57,526 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:32:57,528 : INFO : built Dictionary(315 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 2657 corpus positions)\n",
      "2021-01-15 04:32:57,609 : INFO : token count processed\n",
      "2021-01-15 04:32:57,645 : INFO : frequencies processed\n",
      "2021-01-15 04:33:07,988 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:33:07,989 : INFO : entropies processed\n",
      "2021-01-15 04:33:07,990 : INFO : extropies processed\n",
      "2021-01-15 04:33:08,002 : INFO : token count processed\n",
      "2021-01-15 04:33:08,007 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:33:08,011 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:33:08,012 : INFO : vocab #32006\n",
      "2021-01-15 04:33:08,019 : INFO : diff #set()\n",
      "2021-01-15 04:33:28,029 : INFO : alphabet #32006\n",
      "2021-01-15 04:33:37,981 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/csbcicd_report/csbcicdReport.py')[[1.3106200103502843, 0.43278427241197587], [0.9850259246304631, 0.014974075], [1.0, 1.0], [3.0, 6.61034830706307, 6.7467000346619965, 2.8636482724010737, 3.7467000346619965, 0.13635172759892633]]\n",
      "2021-01-15 04:33:37,985 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:33:37,987 : INFO : built Dictionary(202 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 704 corpus positions)\n",
      "2021-01-15 04:33:38,035 : INFO : token count processed\n",
      "2021-01-15 04:33:38,078 : INFO : frequencies processed\n",
      "2021-01-15 04:33:48,187 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:33:48,188 : INFO : entropies processed\n",
      "2021-01-15 04:33:48,189 : INFO : extropies processed\n",
      "2021-01-15 04:33:48,200 : INFO : token count processed\n",
      "2021-01-15 04:33:48,205 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:33:48,212 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:33:48,213 : INFO : vocab #32006\n",
      "2021-01-15 04:33:48,221 : INFO : diff #set()\n",
      "2021-01-15 04:34:08,214 : INFO : alphabet #32006\n",
      "2021-01-15 04:34:18,168 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/ctsm2csdl.py')[[1.31798476191875, 0.43140922081482264], [0.9859347678720951, 0.014065232], [0.0, 0.0], [3.0, 6.616715366949855, 6.710785909979132, 2.905929456970723, 3.7107859099791316, 0.09407054302927698]]\n",
      "2021-01-15 04:34:18,175 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:34:18,178 : INFO : built Dictionary(422 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 2738 corpus positions)\n",
      "2021-01-15 04:34:18,300 : INFO : token count processed\n",
      "2021-01-15 04:34:18,376 : INFO : frequencies processed\n",
      "2021-01-15 04:34:28,288 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:34:28,290 : INFO : entropies processed\n",
      "2021-01-15 04:34:28,291 : INFO : extropies processed\n",
      "2021-01-15 04:34:28,302 : INFO : token count processed\n",
      "2021-01-15 04:34:28,307 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:34:28,311 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:34:28,312 : INFO : vocab #32006\n",
      "2021-01-15 04:34:28,319 : INFO : diff #set()\n",
      "2021-01-15 04:34:48,375 : INFO : alphabet #32006\n",
      "2021-01-15 04:34:58,305 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/custom_scan/customScan.py')[[1.3010017582961806, 0.43459332284059987], [0.971679987385869, 0.028320013], [0.0, 0.0], [3.0, 7.32185870753746, 7.506910095331916, 2.8149486122055443, 4.506910095331916, 0.18505138779445574]]\n",
      "2021-01-15 04:34:58,308 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:34:58,309 : INFO : built Dictionary(56 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 103 corpus positions)\n",
      "2021-01-15 04:34:58,327 : INFO : token count processed\n",
      "2021-01-15 04:34:58,354 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:34:58,356 : INFO : frequencies processed\n",
      "2021-01-15 04:34:58,358 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:34:58,366 : INFO : token count processed\n",
      "2021-01-15 04:34:58,370 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:34:58,374 : INFO : alphabet_target #32008\n",
      "2021-01-15 04:34:58,375 : INFO : vocab #32006\n",
      "2021-01-15 04:34:58,381 : INFO : diff #set()\n",
      "2021-01-15 04:35:18,297 : INFO : alphabet #32006\n",
      "2021-01-15 04:35:27,816 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/fireException.py')[[1.3147584405487347, 0.4320105210472592], [0.9899022895842791, 0.01009771], [nan, nan], [3.0, 5.176618657501385, 5.471489557519598, 2.7051290999817885, 2.4714895575195976, 0.29487090001821237]]\n",
      "2021-01-15 04:35:27,820 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:35:27,821 : INFO : built Dictionary(152 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 496 corpus positions)\n",
      "2021-01-15 04:35:27,860 : INFO : token count processed\n",
      "2021-01-15 04:35:27,889 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:35:27,890 : INFO : frequencies processed\n",
      "2021-01-15 04:35:27,891 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:35:27,897 : INFO : token count processed\n",
      "2021-01-15 04:35:27,902 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:35:27,906 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:35:27,907 : INFO : vocab #32006\n",
      "2021-01-15 04:35:27,914 : INFO : diff #set()\n",
      "2021-01-15 04:35:47,764 : INFO : alphabet #32006\n",
      "2021-01-15 04:35:57,691 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/gosec/gosec_display.py')[[1.3143200171860998, 0.43209236085503194], [0.9901472041383386, 0.009852796], [nan, nan], [3.0, 6.468846789852156, 6.600380644986368, 2.8684661448657884, 3.600380644986368, 0.13153385513421156]]\n",
      "2021-01-15 04:35:57,697 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:35:57,699 : INFO : built Dictionary(368 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 2545 corpus positions)\n",
      "2021-01-15 04:35:57,833 : INFO : token count processed\n",
      "2021-01-15 04:35:57,923 : INFO : frequencies processed\n",
      "2021-01-15 04:36:07,827 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:36:07,828 : INFO : entropies processed\n",
      "2021-01-15 04:36:07,829 : INFO : extropies processed\n",
      "2021-01-15 04:36:07,838 : INFO : token count processed\n",
      "2021-01-15 04:36:07,842 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:36:07,846 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:36:07,847 : INFO : vocab #32006\n",
      "2021-01-15 04:36:07,854 : INFO : diff #set()\n",
      "2021-01-15 04:36:27,914 : INFO : alphabet #32006\n",
      "2021-01-15 04:36:37,850 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/gosec/gosec_report.py')[[1.3125623896591516, 0.43242076601764234], [0.9877581708133221, 0.012241829], [0.0, 0.0], [3.0, 6.957796704012729, 7.079534085351, 2.8782626186617293, 4.079534085351, 0.12173738133827072]]\n",
      "2021-01-15 04:36:37,858 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:36:37,860 : INFO : built Dictionary(278 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 3042 corpus positions)\n",
      "2021-01-15 04:36:37,936 : INFO : token count processed\n",
      "2021-01-15 04:36:37,995 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:36:37,998 : INFO : frequencies processed\n",
      "2021-01-15 04:36:38,000 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:36:38,008 : INFO : token count processed\n",
      "2021-01-15 04:36:38,014 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:36:38,020 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:36:38,023 : INFO : vocab #32006\n",
      "2021-01-15 04:36:38,031 : INFO : diff #set()\n",
      "2021-01-15 04:36:58,269 : INFO : alphabet #32006\n",
      "2021-01-15 04:37:08,394 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/harden_check/harden_func.py')[[1.3125499778566831, 0.4324230868847296], [0.9933150094002485, 0.0066849906], [nan, nan], [3.0, 6.441859572014148, 6.570569689022154, 2.871289882991995, 3.570569689022154, 0.12871011700800583]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 04:37:08,399 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:37:08,401 : INFO : built Dictionary(303 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 1518 corpus positions)\n",
      "2021-01-15 04:37:08,473 : INFO : token count processed\n",
      "2021-01-15 04:37:08,505 : INFO : frequencies processed\n",
      "2021-01-15 04:37:18,461 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:37:18,462 : INFO : entropies processed\n",
      "2021-01-15 04:37:18,463 : INFO : extropies processed\n",
      "2021-01-15 04:37:18,471 : INFO : token count processed\n",
      "2021-01-15 04:37:18,475 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:37:18,480 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:37:18,481 : INFO : vocab #32006\n",
      "2021-01-15 04:37:18,487 : INFO : diff #set()\n",
      "2021-01-15 04:37:38,549 : INFO : alphabet #32006\n",
      "2021-01-15 04:37:48,489 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/harden_check/hardenPostCheck.py')[[1.314599386947769, 0.4320402077521875], [0.989480939693749, 0.01051906], [0.0, 0.0], [3.0, 6.998955278238291, 7.137011881221668, 2.8619433970166233, 4.137011881221668, 0.13805660298337763]]\n",
      "2021-01-15 04:37:48,493 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:37:48,495 : INFO : built Dictionary(219 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 1337 corpus positions)\n",
      "2021-01-15 04:37:48,552 : INFO : token count processed\n",
      "2021-01-15 04:37:48,580 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:37:48,582 : INFO : frequencies processed\n",
      "2021-01-15 04:37:48,583 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:37:48,589 : INFO : token count processed\n",
      "2021-01-15 04:37:48,594 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:37:48,598 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:37:48,598 : INFO : vocab #32006\n",
      "2021-01-15 04:37:48,606 : INFO : diff #set()\n",
      "2021-01-15 04:38:08,631 : INFO : alphabet #32006\n",
      "2021-01-15 04:38:18,559 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/harden_check/hardenReport.py')[[1.3101482313704258, 0.4328726557112658], [0.99130570422858, 0.008694296], [nan, nan], [3.0, 6.492983191376071, 6.577889701844996, 2.915093489531075, 3.577889701844996, 0.08490651046892506]]\n",
      "2021-01-15 04:38:18,566 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:38:18,569 : INFO : built Dictionary(427 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 3276 corpus positions)\n",
      "2021-01-15 04:38:18,687 : INFO : token count processed\n",
      "2021-01-15 04:38:18,723 : INFO : frequencies processed\n",
      "2021-01-15 04:38:28,669 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:38:28,670 : INFO : entropies processed\n",
      "2021-01-15 04:38:28,671 : INFO : extropies processed\n",
      "2021-01-15 04:38:28,683 : INFO : token count processed\n",
      "2021-01-15 04:38:28,688 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:38:28,692 : INFO : alphabet_target #32008\n",
      "2021-01-15 04:38:28,693 : INFO : vocab #32006\n",
      "2021-01-15 04:38:28,699 : INFO : diff #set()\n",
      "2021-01-15 04:38:48,896 : INFO : alphabet #32006\n",
      "2021-01-15 04:38:58,991 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/psb_mapping.py')[[1.3183106784048482, 0.4313485717488333], [0.9969331389293075, 0.003066861], [0.0, 0.0], [3.0, 6.560342487747443, 6.731765421701259, 2.8285770660461838, 3.731765421701259, 0.17142293395381625]]\n",
      "2021-01-15 04:38:58,999 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 04:38:59,000 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:38:59,005 : INFO : built Dictionary(443 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 3481 corpus positions)\n",
      "2021-01-15 04:38:59,132 : INFO : token count processed\n",
      "2021-01-15 04:38:59,167 : INFO : frequencies processed\n",
      "2021-01-15 04:39:09,080 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:39:09,081 : INFO : entropies processed\n",
      "2021-01-15 04:39:09,082 : INFO : extropies processed\n",
      "2021-01-15 04:39:09,094 : INFO : token count processed\n",
      "2021-01-15 04:39:09,099 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:39:09,103 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:39:09,104 : INFO : vocab #32006\n",
      "2021-01-15 04:39:09,111 : INFO : diff #set()\n",
      "2021-01-15 04:39:29,354 : INFO : alphabet #32006\n",
      "2021-01-15 04:39:39,293 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push.py')[[1.3109972465282924, 0.4327136267696793], [0.985255366191268, 0.014744634], [1.0, 1.0], [3.0, 7.046173750105238, 7.218870426931401, 2.8273033231738367, 4.218870426931401, 0.1726966768261633]]\n",
      "2021-01-15 04:39:39,304 : INFO : Removed 0 and 5 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 04:39:39,305 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:39:39,311 : INFO : built Dictionary(498 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 5590 corpus positions)\n",
      "2021-01-15 04:39:39,495 : INFO : token count processed\n",
      "2021-01-15 04:39:39,530 : INFO : frequencies processed\n",
      "2021-01-15 04:39:49,566 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:39:49,567 : INFO : entropies processed\n",
      "2021-01-15 04:39:49,568 : INFO : extropies processed\n",
      "2021-01-15 04:39:49,581 : INFO : token count processed\n",
      "2021-01-15 04:39:49,586 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:39:49,594 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:39:49,595 : INFO : vocab #32006\n",
      "2021-01-15 04:39:49,601 : INFO : diff #set()\n",
      "2021-01-15 04:40:09,479 : INFO : alphabet #32006\n",
      "2021-01-15 04:40:19,185 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/security_results_push/security_results_push_func.py')[[1.3076544033107218, 0.43334045105078567], [0.9844849389046431, 0.015515061], [0.0, 0.0], [3.0, 7.009229588004272, 7.106991804116543, 2.9022377838877293, 4.106991804116543, 0.09776221611227065]]\n",
      "2021-01-15 04:40:19,198 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:40:19,201 : INFO : built Dictionary(577 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 6531 corpus positions)\n",
      "2021-01-15 04:40:19,420 : INFO : token count processed\n",
      "2021-01-15 04:40:19,489 : INFO : frequencies processed\n",
      "2021-01-15 04:40:29,546 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:40:29,547 : INFO : entropies processed\n",
      "2021-01-15 04:40:29,548 : INFO : extropies processed\n",
      "2021-01-15 04:40:29,562 : INFO : token count processed\n",
      "2021-01-15 04:40:29,567 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:40:29,571 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:40:29,572 : INFO : vocab #32006\n",
      "2021-01-15 04:40:29,578 : INFO : diff #set()\n",
      "2021-01-15 04:40:49,922 : INFO : alphabet #32006\n",
      "2021-01-15 04:40:59,596 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/spotbugs/spotbugs.py')[[1.3074618009691732, 0.4333766216974786], [0.982019055634737, 0.017980944], [0.0, 0.0], [3.0, 7.376088004590871, 7.53259543860755, 2.843492565983321, 4.53259543860755, 0.156507434016679]]\n",
      "2021-01-15 04:40:59,600 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:40:59,601 : INFO : built Dictionary(128 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 317 corpus positions)\n",
      "2021-01-15 04:40:59,637 : INFO : token count processed\n",
      "2021-01-15 04:40:59,694 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:40:59,701 : INFO : frequencies processed\n",
      "2021-01-15 04:40:59,701 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:40:59,707 : INFO : token count processed\n",
      "2021-01-15 04:40:59,717 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:40:59,723 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:40:59,724 : INFO : vocab #32006\n",
      "2021-01-15 04:40:59,731 : INFO : diff #set()\n",
      "2021-01-15 04:41:19,769 : INFO : alphabet #32006\n",
      "2021-01-15 04:41:29,700 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/spotbugs/spotbugsdisplay.py')[[1.3181753520532646, 0.43137375225488245], [0.9933630591258407, 0.006636941], [nan, nan], [3.0, 6.2993628166120885, 6.416253931193674, 2.883108885418414, 3.4162539311936744, 0.11689111458158585]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 04:41:29,703 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:41:29,704 : INFO : built Dictionary(25 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 33 corpus positions)\n",
      "2021-01-15 04:41:29,718 : INFO : token count processed\n",
      "2021-01-15 04:41:29,747 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:41:29,749 : INFO : frequencies processed\n",
      "2021-01-15 04:41:29,750 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:41:29,756 : INFO : token count processed\n",
      "2021-01-15 04:41:29,761 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:41:29,768 : INFO : alphabet_target #32008\n",
      "2021-01-15 04:41:29,768 : INFO : vocab #32006\n",
      "2021-01-15 04:41:29,776 : INFO : diff #set()\n",
      "2021-01-15 04:41:49,817 : INFO : alphabet #32006\n",
      "2021-01-15 04:41:59,748 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/template/__init__.py')[[1.3271468268037683, 0.4297107464308365], [1.0, 0.0], [nan, nan], [3.0, 3.8936606896881862, 4.349199939349345, 2.5444607503388417, 1.3491999393493446, 0.45553924966115833]]\n",
      "2021-01-15 04:41:59,771 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:41:59,776 : INFO : built Dictionary(735 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 12475 corpus positions)\n",
      "2021-01-15 04:42:00,144 : INFO : token count processed\n",
      "2021-01-15 04:42:00,212 : INFO : frequencies processed\n",
      "2021-01-15 04:42:10,249 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:42:10,250 : INFO : entropies processed\n",
      "2021-01-15 04:42:10,251 : INFO : extropies processed\n",
      "2021-01-15 04:42:10,266 : INFO : token count processed\n",
      "2021-01-15 04:42:10,270 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:42:10,274 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:42:10,275 : INFO : vocab #32006\n",
      "2021-01-15 04:42:10,282 : INFO : diff #set()\n",
      "2021-01-15 04:42:30,202 : INFO : alphabet #32006\n",
      "2021-01-15 04:42:40,163 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/third_party/binary_scan_func.py')[[1.3050297296864342, 0.43383388384150495], [0.9797718692570925, 0.02022813], [1.0, 1.0], [3.0, 7.434393313070278, 7.634847796045199, 2.799545517025078, 4.634847796045199, 0.20045448297492108]]\n",
      "2021-01-15 04:42:40,172 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:42:40,174 : INFO : built Dictionary(486 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 4102 corpus positions)\n",
      "2021-01-15 04:42:40,335 : INFO : token count processed\n",
      "2021-01-15 04:42:40,400 : INFO : frequencies processed\n",
      "2021-01-15 04:42:50,437 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:42:50,439 : INFO : entropies processed\n",
      "2021-01-15 04:42:50,439 : INFO : extropies processed\n",
      "2021-01-15 04:42:50,452 : INFO : token count processed\n",
      "2021-01-15 04:42:50,456 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:42:50,463 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:42:50,463 : INFO : vocab #32006\n",
      "2021-01-15 04:42:50,472 : INFO : diff #set()\n",
      "2021-01-15 04:43:10,380 : INFO : alphabet #32006\n",
      "2021-01-15 04:43:20,310 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/third_party/binaryScan.py')[[1.3071417297297656, 0.43343674431181545], [0.9781968984752893, 0.021803102], [1.0, 1.0], [3.0, 7.2991514951718255, 7.476971122111966, 2.82218037305986, 4.476971122111966, 0.1778196269401402]]\n",
      "2021-01-15 04:43:20,318 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:43:20,321 : INFO : built Dictionary(452 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 3519 corpus positions)\n",
      "2021-01-15 04:43:20,463 : INFO : token count processed\n",
      "2021-01-15 04:43:20,534 : INFO : frequencies processed\n",
      "2021-01-15 04:43:30,553 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:43:30,554 : INFO : entropies processed\n",
      "2021-01-15 04:43:30,555 : INFO : extropies processed\n",
      "2021-01-15 04:43:30,567 : INFO : token count processed\n",
      "2021-01-15 04:43:30,572 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:43:30,576 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:43:30,577 : INFO : vocab #32006\n",
      "2021-01-15 04:43:30,584 : INFO : diff #set()\n",
      "2021-01-15 04:43:50,536 : INFO : alphabet #32006\n",
      "2021-01-15 04:44:00,554 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/third_party/blackduck.py')[[1.3115030888631873, 0.43261893303019844], [0.9872647058218718, 0.012735294], [1.0, 1.0], [3.0, 7.170319527000998, 7.340507171865925, 2.829812355135074, 4.340507171865925, 0.1701876448649271]]\n",
      "2021-01-15 04:44:00,558 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:44:00,560 : INFO : built Dictionary(162 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 555 corpus positions)\n",
      "2021-01-15 04:44:00,614 : INFO : token count processed\n",
      "2021-01-15 04:44:00,665 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:44:00,674 : INFO : frequencies processed\n",
      "2021-01-15 04:44:00,675 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:44:00,679 : INFO : token count processed\n",
      "2021-01-15 04:44:00,688 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:44:00,695 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:44:00,697 : INFO : vocab #32006\n",
      "2021-01-15 04:44:00,704 : INFO : diff #set()\n",
      "2021-01-15 04:44:20,614 : INFO : alphabet #32006\n",
      "2021-01-15 04:44:30,784 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/third_party/bom.py')[[1.3179836037754926, 0.43140943636150697], [0.9929757378995419, 0.007024262], [nan, nan], [3.0, 6.353654804387375, 6.487873988478401, 2.865780815908974, 3.487873988478401, 0.13421918409102584]]\n",
      "2021-01-15 04:44:30,787 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:44:30,789 : INFO : built Dictionary(157 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 683 corpus positions)\n",
      "2021-01-15 04:44:30,842 : INFO : token count processed\n",
      "2021-01-15 04:44:30,896 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:44:30,904 : INFO : frequencies processed\n",
      "2021-01-15 04:44:30,904 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:44:30,910 : INFO : token count processed\n",
      "2021-01-15 04:44:30,917 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:44:30,925 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:44:30,925 : INFO : vocab #32006\n",
      "2021-01-15 04:44:30,933 : INFO : diff #set()\n",
      "2021-01-15 04:44:50,862 : INFO : alphabet #32006\n",
      "2021-01-15 04:45:00,727 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/third_party/bom_bd.py')[[1.319394504307522, 0.4311470076103159], [0.9920737333595753, 0.007926267], [nan, nan], [3.0, 6.245180322479091, 6.348911325581525, 2.8962689968975655, 3.3489113255815246, 0.10373100310243366]]\n",
      "2021-01-15 04:45:00,732 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:45:00,738 : INFO : built Dictionary(384 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 1755 corpus positions)\n",
      "2021-01-15 04:45:00,859 : INFO : token count processed\n",
      "2021-01-15 04:45:00,894 : INFO : frequencies processed\n",
      "2021-01-15 04:45:10,950 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:45:10,951 : INFO : entropies processed\n",
      "2021-01-15 04:45:10,952 : INFO : extropies processed\n",
      "2021-01-15 04:45:10,962 : INFO : token count processed\n",
      "2021-01-15 04:45:10,967 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:45:10,971 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:45:10,972 : INFO : vocab #32006\n",
      "2021-01-15 04:45:10,978 : INFO : diff #set()\n",
      "2021-01-15 04:45:30,955 : INFO : alphabet #32006\n",
      "2021-01-15 04:45:40,706 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite.py')[[1.301612365303512, 0.4344780272624798], [0.9691033065319061, 0.030896693], [1.0, 1.0], [3.0, 7.2691387000368, 7.449583371723266, 2.819555328313534, 4.449583371723266, 0.18044467168646605]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 04:45:40,711 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:45:40,713 : INFO : built Dictionary(310 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 1592 corpus positions)\n",
      "2021-01-15 04:45:40,802 : INFO : token count processed\n",
      "2021-01-15 04:45:40,842 : INFO : frequencies processed\n",
      "2021-01-15 04:45:50,928 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:45:50,929 : INFO : entropies processed\n",
      "2021-01-15 04:45:50,930 : INFO : extropies processed\n",
      "2021-01-15 04:45:50,938 : INFO : token count processed\n",
      "2021-01-15 04:45:50,943 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:45:50,947 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:45:50,947 : INFO : vocab #32006\n",
      "2021-01-15 04:45:50,954 : INFO : diff #set()\n",
      "2021-01-15 04:46:10,864 : INFO : alphabet #32006\n",
      "2021-01-15 04:46:20,598 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/third_party/corona_lite/corona_lite_func.py')[[1.2942217481134084, 0.4358776569101584], [0.9625064432621002, 0.037493557], [1.0, 1.0], [3.0, 7.08857858466988, 7.229052182366789, 2.85952640230309, 4.229052182366789, 0.14047359769690893]]\n",
      "2021-01-15 04:46:20,602 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:46:20,603 : INFO : built Dictionary(136 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 515 corpus positions)\n",
      "2021-01-15 04:46:20,635 : INFO : token count processed\n",
      "2021-01-15 04:46:20,664 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:46:20,666 : INFO : frequencies processed\n",
      "2021-01-15 04:46:20,666 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:46:20,672 : INFO : token count processed\n",
      "2021-01-15 04:46:20,677 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:46:20,682 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:46:20,683 : INFO : vocab #32006\n",
      "2021-01-15 04:46:20,689 : INFO : diff #set()\n",
      "2021-01-15 04:46:40,711 : INFO : alphabet #32006\n",
      "2021-01-15 04:46:50,647 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/third_party/cve.py')[[1.3203864593876327, 0.43096269414703764], [0.9950508773326874, 0.0049491227], [nan, nan], [3.0, 6.0479231618016716, 6.165834952764462, 2.8820882090372093, 3.1658349527644623, 0.1179117909627907]]\n",
      "2021-01-15 04:46:50,651 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:46:50,652 : INFO : built Dictionary(138 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 634 corpus positions)\n",
      "2021-01-15 04:46:50,686 : INFO : token count processed\n",
      "2021-01-15 04:46:50,718 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:46:50,719 : INFO : frequencies processed\n",
      "2021-01-15 04:46:50,720 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:46:50,726 : INFO : token count processed\n",
      "2021-01-15 04:46:50,730 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:46:50,735 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:46:50,736 : INFO : vocab #32006\n",
      "2021-01-15 04:46:50,743 : INFO : diff #set()\n",
      "2021-01-15 04:47:10,764 : INFO : alphabet #32006\n",
      "2021-01-15 04:47:20,695 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/third_party/cve_bd.py')[[1.3211491653695644, 0.43082108419377857], [0.9938813517801464, 0.006118648], [nan, nan], [3.0, 6.036583168403119, 6.135989450922483, 2.9005937174806364, 3.135989450922483, 0.09940628251936356]]\n",
      "2021-01-15 04:47:20,709 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 04:47:20,710 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:47:20,716 : INFO : built Dictionary(553 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 6992 corpus positions)\n",
      "2021-01-15 04:47:20,920 : INFO : token count processed\n",
      "2021-01-15 04:47:20,950 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:47:20,950 : INFO : frequencies processed\n",
      "2021-01-15 04:47:20,951 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:47:20,959 : INFO : token count processed\n",
      "2021-01-15 04:47:20,964 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:47:20,969 : INFO : alphabet_target #32010\n",
      "2021-01-15 04:47:20,970 : INFO : vocab #32006\n",
      "2021-01-15 04:47:20,977 : INFO : diff #set()\n",
      "2021-01-15 04:47:40,986 : INFO : alphabet #32006\n",
      "2021-01-15 04:47:51,085 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/third_party/HubRestApi.py')[[1.3088637070147162, 0.43311348216953294], [0.9900255212560296, 0.009974479], [nan, nan], [3.0, 7.29352035514053, 7.443017978037626, 2.850502377102903, 4.443017978037626, 0.1494976228970959]]\n",
      "2021-01-15 04:47:51,092 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:47:51,094 : INFO : built Dictionary(361 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 3235 corpus positions)\n",
      "2021-01-15 04:47:51,188 : INFO : token count processed\n",
      "2021-01-15 04:47:51,223 : INFO : frequencies processed\n",
      "2021-01-15 04:48:01,149 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:48:01,150 : INFO : entropies processed\n",
      "2021-01-15 04:48:01,151 : INFO : extropies processed\n",
      "2021-01-15 04:48:01,163 : INFO : token count processed\n",
      "2021-01-15 04:48:01,168 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:48:01,175 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:48:01,176 : INFO : vocab #32006\n",
      "2021-01-15 04:48:01,183 : INFO : diff #set()\n",
      "2021-01-15 04:48:21,198 : INFO : alphabet #32006\n",
      "2021-01-15 04:48:31,123 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/third_party/ipCentralScan.py')[[1.3121017963302046, 0.4325069084705578], [0.9888515993952751, 0.011148401], [0.0, 0.0], [3.0, 6.8153433747477745, 6.939857609194301, 2.8754857655534733, 3.939857609194301, 0.12451423444652665]]\n",
      "2021-01-15 04:48:31,126 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:48:31,127 : INFO : built Dictionary(113 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 256 corpus positions)\n",
      "2021-01-15 04:48:31,166 : INFO : token count processed\n",
      "2021-01-15 04:48:31,197 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:48:31,199 : INFO : frequencies processed\n",
      "2021-01-15 04:48:31,201 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:48:31,209 : INFO : token count processed\n",
      "2021-01-15 04:48:31,215 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:48:31,221 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:48:31,224 : INFO : vocab #32006\n",
      "2021-01-15 04:48:31,232 : INFO : diff #set()\n",
      "2021-01-15 04:48:51,279 : INFO : alphabet #32006\n",
      "2021-01-15 04:49:01,208 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/third_party/ipcReport.py')[[1.3183072667176419, 0.4313492065337149], [0.9931368101388216, 0.00686319], [nan, nan], [3.0, 6.150121915859574, 6.25239128927831, 2.8977306265812652, 3.25239128927831, 0.10226937341873565]]\n",
      "2021-01-15 04:49:01,212 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:49:01,213 : INFO : built Dictionary(278 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 967 corpus positions)\n",
      "2021-01-15 04:49:01,294 : INFO : token count processed\n",
      "2021-01-15 04:49:01,324 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:49:01,325 : INFO : frequencies processed\n",
      "2021-01-15 04:49:01,326 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:49:01,332 : INFO : token count processed\n",
      "2021-01-15 04:49:01,337 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:49:01,341 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:49:01,344 : INFO : vocab #32006\n",
      "2021-01-15 04:49:01,352 : INFO : diff #set()\n",
      "2021-01-15 04:49:21,354 : INFO : alphabet #32006\n",
      "2021-01-15 04:49:31,277 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/third_party/rest_request.py')[[1.2981193030191154, 0.4351384189177067], [0.9667727500200272, 0.03322725], [nan, nan], [3.0, 7.0391145208191315, 7.24838767274227, 2.7907268480768614, 4.24838767274227, 0.20927315192313856]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 04:49:31,287 : INFO : Removed 0 and 1 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 04:49:31,288 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:49:31,290 : INFO : built Dictionary(580 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 4342 corpus positions)\n",
      "2021-01-15 04:49:31,517 : INFO : token count processed\n",
      "2021-01-15 04:49:31,551 : INFO : frequencies processed\n",
      "2021-01-15 04:49:41,475 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:49:41,476 : INFO : entropies processed\n",
      "2021-01-15 04:49:41,477 : INFO : extropies processed\n",
      "2021-01-15 04:49:41,489 : INFO : token count processed\n",
      "2021-01-15 04:49:41,493 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:49:41,498 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:49:41,499 : INFO : vocab #32006\n",
      "2021-01-15 04:49:41,505 : INFO : diff #set()\n",
      "2021-01-15 04:50:01,633 : INFO : alphabet #32006\n",
      "2021-01-15 04:50:11,580 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/third_party/run_ipcentral_automation.py')[[1.296900380942661, 0.43536933873884176], [0.9651344567537308, 0.034865543], [1.584962500721156, 1.1699250014423124], [3.0, 7.482466367279176, 7.739681443389704, 2.742784923889473, 4.739681443389704, 0.2572150761105281]]\n",
      "2021-01-15 04:50:11,583 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:50:11,585 : INFO : built Dictionary(162 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 564 corpus positions)\n",
      "2021-01-15 04:50:11,620 : INFO : token count processed\n",
      "2021-01-15 04:50:11,647 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:50:11,648 : INFO : frequencies processed\n",
      "2021-01-15 04:50:11,649 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:50:11,654 : INFO : token count processed\n",
      "2021-01-15 04:50:11,658 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:50:11,663 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:50:11,664 : INFO : vocab #32006\n",
      "2021-01-15 04:50:11,670 : INFO : diff #set()\n",
      "2021-01-15 04:50:31,713 : INFO : alphabet #32006\n",
      "2021-01-15 04:50:41,635 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/third_party/tpsd_triage.py')[[1.3176552780170434, 0.4314705510715931], [0.9928213348612189, 0.007178665], [nan, nan], [3.0, 6.372162341197667, 6.503484562879181, 2.868677778318486, 3.503484562879181, 0.13132222168151397]]\n",
      "2021-01-15 04:50:41,640 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:50:41,642 : INFO : built Dictionary(310 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 1985 corpus positions)\n",
      "2021-01-15 04:50:41,716 : INFO : token count processed\n",
      "2021-01-15 04:50:41,749 : INFO : frequencies processed\n",
      "2021-01-15 04:50:51,489 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:50:51,490 : INFO : entropies processed\n",
      "2021-01-15 04:50:51,490 : INFO : extropies processed\n",
      "2021-01-15 04:50:51,498 : INFO : token count processed\n",
      "2021-01-15 04:50:51,503 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:50:51,507 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:50:51,508 : INFO : vocab #32006\n",
      "2021-01-15 04:50:51,515 : INFO : diff #set()\n",
      "2021-01-15 04:51:11,583 : INFO : alphabet #32006\n",
      "2021-01-15 04:51:21,523 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/third_party/unused/bd_image.py')[[1.3144303697247275, 0.4320717585981805], [0.9918540343642235, 0.008145966], [0.0, 0.0], [3.0, 6.798155919669889, 6.959746200358184, 2.838409719311704, 3.9597462003581843, 0.16159028068829517]]\n",
      "2021-01-15 04:51:21,527 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:51:21,529 : INFO : built Dictionary(167 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 701 corpus positions)\n",
      "2021-01-15 04:51:21,571 : INFO : token count processed\n",
      "2021-01-15 04:51:21,600 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:51:21,601 : INFO : frequencies processed\n",
      "2021-01-15 04:51:21,602 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:51:21,608 : INFO : token count processed\n",
      "2021-01-15 04:51:21,612 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:51:21,617 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:51:21,618 : INFO : vocab #32006\n",
      "2021-01-15 04:51:21,624 : INFO : diff #set()\n",
      "2021-01-15 04:51:41,656 : INFO : alphabet #32006\n",
      "2021-01-15 04:51:51,631 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/third_party/unused/ipc.py')[[1.3170353370310564, 0.4315859944032423], [0.9942885003983974, 0.0057114996], [nan, nan], [3.0, 6.271631856729336, 6.400548059984814, 2.8710837967445215, 3.400548059984814, 0.12891620325547848]]\n",
      "2021-01-15 04:51:51,639 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:51:51,641 : INFO : built Dictionary(343 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 3216 corpus positions)\n",
      "2021-01-15 04:51:51,740 : INFO : token count processed\n",
      "2021-01-15 04:51:51,775 : INFO : frequencies processed\n",
      "2021-01-15 04:52:01,718 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:52:01,719 : INFO : entropies processed\n",
      "2021-01-15 04:52:01,720 : INFO : extropies processed\n",
      "2021-01-15 04:52:01,728 : INFO : token count processed\n",
      "2021-01-15 04:52:01,733 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:52:01,738 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:52:01,739 : INFO : vocab #32006\n",
      "2021-01-15 04:52:01,746 : INFO : diff #set()\n",
      "2021-01-15 04:52:21,978 : INFO : alphabet #32006\n",
      "2021-01-15 04:52:32,134 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/third_party/UploadBom.py')[[1.302319639264022, 0.43434455535447203], [0.9718140047043562, 0.028185995], [1.0, 1.0], [3.0, 6.873598627629562, 7.008210751930242, 2.8653878756993203, 4.008210751930242, 0.1346121243006797]]\n",
      "2021-01-15 04:52:32,138 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:52:32,139 : INFO : built Dictionary(94 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 164 corpus positions)\n",
      "2021-01-15 04:52:32,160 : INFO : token count processed\n",
      "2021-01-15 04:52:32,189 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:52:32,191 : INFO : frequencies processed\n",
      "2021-01-15 04:52:32,191 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:52:32,197 : INFO : token count processed\n",
      "2021-01-15 04:52:32,202 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:52:32,206 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:52:32,208 : INFO : vocab #32006\n",
      "2021-01-15 04:52:32,214 : INFO : diff #set()\n",
      "2021-01-15 04:52:52,373 : INFO : alphabet #32006\n",
      "2021-01-15 04:53:02,455 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/webex_send_func.py')[[1.3039716788839946, 0.4340331129783606], [0.9788133893162012, 0.02118661], [nan, nan], [3.0, 6.049830202851529, 6.230620793902422, 2.8192094089491073, 3.230620793902422, 0.18079059105089268]]\n",
      "2021-01-15 04:53:02,460 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:53:02,462 : INFO : built Dictionary(241 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 1244 corpus positions)\n",
      "2021-01-15 04:53:02,524 : INFO : token count processed\n",
      "2021-01-15 04:53:02,558 : INFO : frequencies processed\n",
      "2021-01-15 04:53:12,506 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:53:12,507 : INFO : entropies processed\n",
      "2021-01-15 04:53:12,508 : INFO : extropies processed\n",
      "2021-01-15 04:53:12,515 : INFO : token count processed\n",
      "2021-01-15 04:53:12,520 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:53:12,525 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:53:12,526 : INFO : vocab #32006\n",
      "2021-01-15 04:53:12,532 : INFO : diff #set()\n",
      "2021-01-15 04:53:33,186 : INFO : alphabet #32006\n",
      "2021-01-15 04:53:43,134 : INFO : Computed distances or similarities ('263', 'sacp-python-common/sacp_python_common/webexSend.py')[[1.3037639184533152, 0.43407225540339783], [0.9754859171807766, 0.024514083], [0.0, 0.0], [3.0, 6.778844940588858, 6.938675043092317, 2.8401698974965406, 3.938675043092317, 0.15983010250345941]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 04:53:43,137 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:53:43,139 : INFO : built Dictionary(141 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 277 corpus positions)\n",
      "2021-01-15 04:53:43,171 : INFO : token count processed\n",
      "2021-01-15 04:53:43,200 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:53:43,201 : INFO : frequencies processed\n",
      "2021-01-15 04:53:43,202 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:53:43,208 : INFO : token count processed\n",
      "2021-01-15 04:53:43,212 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:53:43,217 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:53:43,218 : INFO : vocab #32006\n",
      "2021-01-15 04:53:43,225 : INFO : diff #set()\n",
      "2021-01-15 04:54:03,354 : INFO : alphabet #32006\n",
      "2021-01-15 04:54:13,431 : INFO : Computed distances or similarities ('263', 'sacp-python-common/setup.py')[[1.3108024787239558, 0.4327500983780354], [0.9813045989722013, 0.018695401], [nan, nan], [3.0, 6.469677430851302, 6.67745094811306, 2.7922264827382417, 3.6774509481130604, 0.2077735172617583]]\n",
      "2021-01-15 04:54:13,436 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:54:13,437 : INFO : built Dictionary(211 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 1179 corpus positions)\n",
      "2021-01-15 04:54:13,489 : INFO : token count processed\n",
      "2021-01-15 04:54:13,518 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:54:13,519 : INFO : frequencies processed\n",
      "2021-01-15 04:54:13,520 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:54:13,526 : INFO : token count processed\n",
      "2021-01-15 04:54:13,530 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:54:13,535 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:54:13,536 : INFO : vocab #32006\n",
      "2021-01-15 04:54:13,543 : INFO : diff #set()\n",
      "2021-01-15 04:54:33,666 : INFO : alphabet #32006\n",
      "2021-01-15 04:54:43,727 : INFO : Computed distances or similarities ('263', 'sacp-python-common/test/python/bandit/test_bandit.py')[[1.313255320092525, 0.4322912353488081], [0.9895045273005962, 0.010495473], [nan, nan], [3.0, 6.459180448028249, 6.593314470225353, 2.865865977802896, 3.593314470225353, 0.13413402219710413]]\n",
      "2021-01-15 04:54:43,730 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:54:43,732 : INFO : built Dictionary(120 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 329 corpus positions)\n",
      "2021-01-15 04:54:43,759 : INFO : token count processed\n",
      "2021-01-15 04:54:43,794 : INFO : frequencies processed\n",
      "2021-01-15 04:54:53,758 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:54:53,759 : INFO : entropies processed\n",
      "2021-01-15 04:54:53,760 : INFO : extropies processed\n",
      "2021-01-15 04:54:53,767 : INFO : token count processed\n",
      "2021-01-15 04:54:53,772 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:54:53,777 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:54:53,778 : INFO : vocab #32006\n",
      "2021-01-15 04:54:53,785 : INFO : diff #set()\n",
      "2021-01-15 04:55:13,830 : INFO : alphabet #32006\n",
      "2021-01-15 04:55:23,765 : INFO : Computed distances or similarities ('263', 'sacp-python-common/test/python/cave/test_cave_ca.py')[[1.2924791737393175, 0.4362089791066133], [0.9592414610087872, 0.04075854], [0.0, 0.0], [3.0, 6.097125733496388, 6.210430329779807, 2.8866954037165806, 3.2104303297798067, 0.11330459628341849]]\n",
      "2021-01-15 04:55:23,768 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:55:23,770 : INFO : built Dictionary(113 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 325 corpus positions)\n",
      "2021-01-15 04:55:23,794 : INFO : token count processed\n",
      "2021-01-15 04:55:23,823 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:55:23,824 : INFO : frequencies processed\n",
      "2021-01-15 04:55:23,825 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:55:23,831 : INFO : token count processed\n",
      "2021-01-15 04:55:23,836 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:55:23,840 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:55:23,841 : INFO : vocab #32006\n",
      "2021-01-15 04:55:23,848 : INFO : diff #set()\n",
      "2021-01-15 04:55:43,762 : INFO : alphabet #32006\n",
      "2021-01-15 04:55:53,826 : INFO : Computed distances or similarities ('263', 'sacp-python-common/test/python/cave/test_cave_ssl.py')[[1.3136842710687247, 0.4322110896911986], [0.9894572142511606, 0.010542786], [nan, nan], [3.0, 6.0695858597523715, 6.188948284286823, 2.880637575465549, 3.1889482842868233, 0.11936242453445178]]\n",
      "2021-01-15 04:55:53,829 : INFO : Removed 0 and 3 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 04:55:53,830 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:55:53,831 : INFO : built Dictionary(115 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 421 corpus positions)\n",
      "2021-01-15 04:55:53,857 : INFO : token count processed\n",
      "2021-01-15 04:55:53,886 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:55:53,888 : INFO : frequencies processed\n",
      "2021-01-15 04:55:53,888 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:55:53,894 : INFO : token count processed\n",
      "2021-01-15 04:55:53,899 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:55:53,903 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:55:53,904 : INFO : vocab #32006\n",
      "2021-01-15 04:55:53,911 : INFO : diff #set()\n",
      "2021-01-15 04:56:13,684 : INFO : alphabet #32006\n",
      "2021-01-15 04:56:23,753 : INFO : Computed distances or similarities ('263', 'sacp-python-common/test/python/cave/test_cave_zap.py')[[1.3107907951740267, 0.4327522863984273], [0.9860165026038885, 0.013983497], [nan, nan], [3.0, 6.104787343210121, 6.196101290591262, 2.908686052618858, 3.196101290591262, 0.09131394738114107]]\n",
      "2021-01-15 04:56:23,770 : INFO : Removed 0 and 2 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 04:56:23,771 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:56:23,777 : INFO : built Dictionary(402 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 9106 corpus positions)\n",
      "2021-01-15 04:56:23,891 : INFO : token count processed\n",
      "2021-01-15 04:56:23,924 : INFO : frequencies processed\n",
      "2021-01-15 04:56:33,870 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:56:33,871 : INFO : entropies processed\n",
      "2021-01-15 04:56:33,872 : INFO : extropies processed\n",
      "2021-01-15 04:56:33,887 : INFO : token count processed\n",
      "2021-01-15 04:56:33,891 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:56:33,896 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:56:33,897 : INFO : vocab #32006\n",
      "2021-01-15 04:56:33,903 : INFO : diff #set()\n",
      "2021-01-15 04:56:53,953 : INFO : alphabet #32006\n",
      "2021-01-15 04:57:03,807 : INFO : Computed distances or similarities ('263', 'sacp-python-common/test/python/csbcicdReport/test_csbcicd_func.py')[[1.3111068530531693, 0.4326931048985964], [0.9865971114486456, 0.013402889], [0.0, 0.0], [3.0, 6.89087415148015, 7.040436998886816, 2.8504371525933356, 4.040436998886816, 0.14956284740666526]]\n",
      "2021-01-15 04:57:03,813 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:57:03,815 : INFO : built Dictionary(261 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 2266 corpus positions)\n",
      "2021-01-15 04:57:03,877 : INFO : token count processed\n",
      "2021-01-15 04:57:03,905 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:57:03,905 : INFO : frequencies processed\n",
      "2021-01-15 04:57:03,910 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:57:03,918 : INFO : token count processed\n",
      "2021-01-15 04:57:03,924 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:57:03,934 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:57:03,940 : INFO : vocab #32006\n",
      "2021-01-15 04:57:03,949 : INFO : diff #set()\n",
      "2021-01-15 04:57:23,743 : INFO : alphabet #32006\n",
      "2021-01-15 04:57:33,641 : INFO : Computed distances or similarities ('263', 'sacp-python-common/test/python/csbcicdReport/test_csbcicdReport.py')[[1.3064151516149405, 0.4335732876623729], [0.9768260344862938, 0.023173966], [nan, nan], [3.0, 6.655493573668506, 6.77315160118259, 2.882341972485916, 3.7731516011825903, 0.11765802751408394]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 04:57:33,646 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:57:33,648 : INFO : built Dictionary(238 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 1273 corpus positions)\n",
      "2021-01-15 04:57:33,704 : INFO : token count processed\n",
      "2021-01-15 04:57:33,734 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:57:33,734 : INFO : frequencies processed\n",
      "2021-01-15 04:57:33,735 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:57:33,741 : INFO : token count processed\n",
      "2021-01-15 04:57:33,746 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:57:33,750 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:57:33,751 : INFO : vocab #32006\n",
      "2021-01-15 04:57:33,758 : INFO : diff #set()\n",
      "2021-01-15 04:57:53,718 : INFO : alphabet #32006\n",
      "2021-01-15 04:58:03,640 : INFO : Computed distances or similarities ('263', 'sacp-python-common/test/python/custom_scan/test_customScan.py')[[1.3114191448878587, 0.4326346444831044], [0.9838486704975367, 0.01615133], [nan, nan], [3.0, 6.6236746347295465, 6.716168446829697, 2.907506187899849, 3.7161684468296974, 0.09249381210015084]]\n",
      "2021-01-15 04:58:03,645 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:58:03,646 : INFO : built Dictionary(238 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 1212 corpus positions)\n",
      "2021-01-15 04:58:03,704 : INFO : token count processed\n",
      "2021-01-15 04:58:03,738 : INFO : frequencies processed\n",
      "2021-01-15 04:58:13,627 : INFO : scalar_distribution processed\n",
      "2021-01-15 04:58:13,628 : INFO : entropies processed\n",
      "2021-01-15 04:58:13,629 : INFO : extropies processed\n",
      "2021-01-15 04:58:13,640 : INFO : token count processed\n",
      "2021-01-15 04:58:13,645 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:58:13,651 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:58:13,652 : INFO : vocab #32006\n",
      "2021-01-15 04:58:13,659 : INFO : diff #set()\n",
      "2021-01-15 04:58:33,642 : INFO : alphabet #32006\n",
      "2021-01-15 04:58:43,580 : INFO : Computed distances or similarities ('263', 'sacp-python-common/test/python/gosec/test_gosec.py')[[1.304767315344739, 0.4338832789506231], [0.9681832380592823, 0.031816762], [0.0, 0.0], [3.0, 6.75472436518627, 6.842204173686447, 2.912520191499823, 3.842204173686447, 0.08747980850017711]]\n",
      "2021-01-15 04:58:43,585 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:58:43,588 : INFO : built Dictionary(191 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 1007 corpus positions)\n",
      "2021-01-15 04:58:43,662 : INFO : token count processed\n",
      "2021-01-15 04:58:43,736 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:58:43,737 : INFO : frequencies processed\n",
      "2021-01-15 04:58:43,740 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:58:43,746 : INFO : token count processed\n",
      "2021-01-15 04:58:43,751 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:58:43,756 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:58:43,756 : INFO : vocab #32006\n",
      "2021-01-15 04:58:43,765 : INFO : diff #set()\n",
      "2021-01-15 04:59:03,757 : INFO : alphabet #32006\n",
      "2021-01-15 04:59:13,645 : INFO : Computed distances or similarities ('263', 'sacp-python-common/test/python/gosec/test_gosec_display.py')[[1.3070446175588295, 0.43345498929194426], [0.9710346907377243, 0.02896531], [nan, nan], [3.0, 6.597313085495733, 6.675285550650409, 2.9220275348453235, 3.6752855506504094, 0.07797246515467648]]\n",
      "2021-01-15 04:59:13,649 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:59:13,651 : INFO : built Dictionary(216 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 973 corpus positions)\n",
      "2021-01-15 04:59:13,717 : INFO : token count processed\n",
      "2021-01-15 04:59:13,748 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:59:13,750 : INFO : frequencies processed\n",
      "2021-01-15 04:59:13,752 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:59:13,760 : INFO : token count processed\n",
      "2021-01-15 04:59:13,766 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:59:13,772 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:59:13,775 : INFO : vocab #32006\n",
      "2021-01-15 04:59:13,783 : INFO : diff #set()\n",
      "2021-01-15 04:59:33,940 : INFO : alphabet #32006\n",
      "2021-01-15 04:59:43,847 : INFO : Computed distances or similarities ('263', 'sacp-python-common/test/python/harden_check/test_HardenPostCheck.py')[[1.310012138575191, 0.43289815810959215], [0.9842385072261095, 0.015761493], [nan, nan], [3.0, 6.659481538516613, 6.756671516028945, 2.9028100224876674, 3.756671516028945, 0.09718997751233172]]\n",
      "2021-01-15 04:59:43,852 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 04:59:43,853 : INFO : built Dictionary(241 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 1135 corpus positions)\n",
      "2021-01-15 04:59:43,932 : INFO : token count processed\n",
      "2021-01-15 04:59:43,963 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 04:59:43,964 : INFO : frequencies processed\n",
      "2021-01-15 04:59:43,965 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 04:59:43,971 : INFO : token count processed\n",
      "2021-01-15 04:59:43,976 : INFO : alphabet_source #32006\n",
      "2021-01-15 04:59:43,982 : INFO : alphabet_target #32009\n",
      "2021-01-15 04:59:43,984 : INFO : vocab #32006\n",
      "2021-01-15 04:59:43,993 : INFO : diff #set()\n",
      "2021-01-15 05:00:04,187 : INFO : alphabet #32006\n",
      "2021-01-15 05:00:14,091 : INFO : Computed distances or similarities ('263', 'sacp-python-common/test/python/security_results_push/test_security_results_push.py')[[1.3147606336373783, 0.43201011174473614], [0.9849391607567668, 0.015060839], [nan, nan], [3.0, 6.774682571479102, 6.848349719886709, 2.9263328515923916, 3.848349719886709, 0.07366714840760746]]\n",
      "2021-01-15 05:00:14,106 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 05:00:14,109 : INFO : built Dictionary(418 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 7872 corpus positions)\n",
      "2021-01-15 05:00:14,259 : INFO : token count processed\n",
      "2021-01-15 05:00:14,295 : INFO : frequencies processed\n",
      "2021-01-15 05:00:24,205 : INFO : scalar_distribution processed\n",
      "2021-01-15 05:00:24,206 : INFO : entropies processed\n",
      "2021-01-15 05:00:24,207 : INFO : extropies processed\n",
      "2021-01-15 05:00:24,221 : INFO : token count processed\n",
      "2021-01-15 05:00:24,225 : INFO : alphabet_source #32006\n",
      "2021-01-15 05:00:24,229 : INFO : alphabet_target #32009\n",
      "2021-01-15 05:00:24,230 : INFO : vocab #32006\n",
      "2021-01-15 05:00:24,237 : INFO : diff #set()\n",
      "2021-01-15 05:00:44,227 : INFO : alphabet #32006\n",
      "2021-01-15 05:00:54,152 : INFO : Computed distances or similarities ('263', 'sacp-python-common/test/python/security_results_push/test_security_results_push_func.py')[[1.311336244383357, 0.4326501617538519], [0.9880009535700083, 0.011999046], [0.0, 0.0], [3.0, 6.839453716525233, 6.979399726109058, 2.8600539904161746, 3.979399726109058, 0.13994600958382541]]\n",
      "2021-01-15 05:00:54,158 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 05:00:54,160 : INFO : built Dictionary(319 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 2314 corpus positions)\n",
      "2021-01-15 05:00:54,243 : INFO : token count processed\n",
      "2021-01-15 05:00:54,277 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 05:00:54,278 : INFO : frequencies processed\n",
      "2021-01-15 05:00:54,278 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 05:00:54,285 : INFO : token count processed\n",
      "2021-01-15 05:00:54,289 : INFO : alphabet_source #32006\n",
      "2021-01-15 05:00:54,294 : INFO : alphabet_target #32009\n",
      "2021-01-15 05:00:54,295 : INFO : vocab #32006\n",
      "2021-01-15 05:00:54,302 : INFO : diff #set()\n",
      "2021-01-15 05:01:14,298 : INFO : alphabet #32006\n",
      "2021-01-15 05:01:24,023 : INFO : Computed distances or similarities ('263', 'sacp-python-common/test/python/spotbugs/test_spotbugs.py')[[1.3049340870388528, 0.4338518856670211], [0.97480583563447, 0.025194164], [nan, nan], [3.0, 6.86432793886027, 6.958783043577704, 2.905544895282567, 3.9587830435777036, 0.09445510471743379]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 05:01:24,027 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 05:01:24,028 : INFO : built Dictionary(152 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 516 corpus positions)\n",
      "2021-01-15 05:01:24,061 : INFO : token count processed\n",
      "2021-01-15 05:01:24,091 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 05:01:24,092 : INFO : frequencies processed\n",
      "2021-01-15 05:01:24,092 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 05:01:24,098 : INFO : token count processed\n",
      "2021-01-15 05:01:24,103 : INFO : alphabet_source #32006\n",
      "2021-01-15 05:01:24,107 : INFO : alphabet_target #32009\n",
      "2021-01-15 05:01:24,108 : INFO : vocab #32006\n",
      "2021-01-15 05:01:24,115 : INFO : diff #set()\n",
      "2021-01-15 05:01:43,971 : INFO : alphabet #32006\n",
      "2021-01-15 05:01:54,451 : INFO : Computed distances or similarities ('263', 'sacp-python-common/test/python/spotbugs/test_spotbugsdisplay.py')[[1.3031868290775184, 0.4341810170912292], [0.9628388434648514, 0.037161157], [nan, nan], [3.0, 6.431978396403875, 6.5216212507215765, 2.9103571456822976, 3.5216212507215765, 0.08964285431770147]]\n",
      "2021-01-15 05:01:54,455 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 05:01:54,456 : INFO : built Dictionary(211 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 670 corpus positions)\n",
      "2021-01-15 05:01:54,512 : INFO : token count processed\n",
      "2021-01-15 05:01:54,551 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 05:01:54,551 : INFO : frequencies processed\n",
      "2021-01-15 05:01:54,552 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 05:01:54,558 : INFO : token count processed\n",
      "2021-01-15 05:01:54,563 : INFO : alphabet_source #32006\n",
      "2021-01-15 05:01:54,568 : INFO : alphabet_target #32009\n",
      "2021-01-15 05:01:54,569 : INFO : vocab #32006\n",
      "2021-01-15 05:01:54,575 : INFO : diff #set()\n",
      "2021-01-15 05:02:14,445 : INFO : alphabet #32006\n",
      "2021-01-15 05:02:24,491 : INFO : Computed distances or similarities ('263', 'sacp-python-common/test/python/test_auth_utility.py')[[1.3140395133775107, 0.43214473833267725], [0.9768274296075106, 0.02317257], [nan, nan], [3.0, 6.911818353685893, 6.999711470391392, 2.912106883294501, 3.999711470391392, 0.08789311670549882]]\n",
      "2021-01-15 05:02:24,505 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 05:02:24,508 : INFO : built Dictionary(302 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 7200 corpus positions)\n",
      "2021-01-15 05:02:24,595 : INFO : token count processed\n",
      "2021-01-15 05:02:24,623 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 05:02:24,624 : INFO : frequencies processed\n",
      "2021-01-15 05:02:24,625 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 05:02:24,633 : INFO : token count processed\n",
      "2021-01-15 05:02:24,637 : INFO : alphabet_source #32006\n",
      "2021-01-15 05:02:24,641 : INFO : alphabet_target #32009\n",
      "2021-01-15 05:02:24,642 : INFO : vocab #32006\n",
      "2021-01-15 05:02:24,649 : INFO : diff #set()\n",
      "2021-01-15 05:02:44,307 : INFO : alphabet #32006\n",
      "2021-01-15 05:02:54,230 : INFO : Computed distances or similarities ('263', 'sacp-python-common/test/python/third_party/test_binary_scan_func.py')[[1.3194533645592783, 0.43113606648867075], [0.9916758323088288, 0.008324168], [nan, nan], [3.0, 6.363791471162389, 6.388278673306464, 2.9755127978559264, 3.3882786733064636, 0.024487202144074516]]\n",
      "2021-01-15 05:02:54,235 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 05:02:54,236 : INFO : built Dictionary(203 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 1270 corpus positions)\n",
      "2021-01-15 05:02:54,286 : INFO : token count processed\n",
      "2021-01-15 05:02:54,316 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 05:02:54,317 : INFO : frequencies processed\n",
      "2021-01-15 05:02:54,317 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 05:02:54,324 : INFO : token count processed\n",
      "2021-01-15 05:02:54,328 : INFO : alphabet_source #32006\n",
      "2021-01-15 05:02:54,333 : INFO : alphabet_target #32009\n",
      "2021-01-15 05:02:54,334 : INFO : vocab #32006\n",
      "2021-01-15 05:02:54,340 : INFO : diff #set()\n",
      "2021-01-15 05:03:14,382 : INFO : alphabet #32006\n",
      "2021-01-15 05:03:24,305 : INFO : Computed distances or similarities ('263', 'sacp-python-common/test/python/third_party/test_BinaryScan.py')[[1.3137601664041545, 0.43219691241988717], [0.9880117019638419, 0.011988298], [nan, nan], [3.0, 6.29000629755059, 6.469451618833765, 2.8205546787168254, 3.469451618833765, 0.17944532128317459]]\n",
      "2021-01-15 05:03:24,310 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 05:03:24,311 : INFO : built Dictionary(213 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 1223 corpus positions)\n",
      "2021-01-15 05:03:24,364 : INFO : token count processed\n",
      "2021-01-15 05:03:24,393 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 05:03:24,394 : INFO : frequencies processed\n",
      "2021-01-15 05:03:24,394 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 05:03:24,401 : INFO : token count processed\n",
      "2021-01-15 05:03:24,405 : INFO : alphabet_source #32006\n",
      "2021-01-15 05:03:24,410 : INFO : alphabet_target #32009\n",
      "2021-01-15 05:03:24,411 : INFO : vocab #32006\n",
      "2021-01-15 05:03:24,418 : INFO : diff #set()\n",
      "2021-01-15 05:03:44,430 : INFO : alphabet #32006\n",
      "2021-01-15 05:03:54,361 : INFO : Computed distances or similarities ('263', 'sacp-python-common/test/python/third_party/test_blackduck.py')[[1.30917760224203, 0.43305460741914287], [0.9872367680072784, 0.012763232], [nan, nan], [3.0, 6.361621244785958, 6.526556475043176, 2.8350647697427824, 3.5265564750431757, 0.16493523025721757]]\n",
      "2021-01-15 05:03:54,366 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 05:03:54,368 : INFO : built Dictionary(221 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 1246 corpus positions)\n",
      "2021-01-15 05:03:54,418 : INFO : token count processed\n",
      "2021-01-15 05:03:54,447 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 05:03:54,448 : INFO : frequencies processed\n",
      "2021-01-15 05:03:54,449 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 05:03:54,455 : INFO : token count processed\n",
      "2021-01-15 05:03:54,460 : INFO : alphabet_source #32006\n",
      "2021-01-15 05:03:54,465 : INFO : alphabet_target #32009\n",
      "2021-01-15 05:03:54,466 : INFO : vocab #32006\n",
      "2021-01-15 05:03:54,472 : INFO : diff #set()\n",
      "2021-01-15 05:04:14,501 : INFO : alphabet #32006\n",
      "2021-01-15 05:04:24,415 : INFO : Computed distances or similarities ('263', 'sacp-python-common/test/python/third_party/test_bom.py')[[1.298778462592079, 0.43501364584406726], [0.9672170914709568, 0.03278291], [nan, nan], [3.0, 6.620594433343389, 6.793972210246901, 2.8266222230964875, 3.793972210246901, 0.17337777690351164]]\n",
      "2021-01-15 05:04:24,420 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 05:04:24,422 : INFO : built Dictionary(193 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 1339 corpus positions)\n",
      "2021-01-15 05:04:24,469 : INFO : token count processed\n",
      "2021-01-15 05:04:24,498 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 05:04:24,499 : INFO : frequencies processed\n",
      "2021-01-15 05:04:24,500 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 05:04:24,506 : INFO : token count processed\n",
      "2021-01-15 05:04:24,511 : INFO : alphabet_source #32006\n",
      "2021-01-15 05:04:24,515 : INFO : alphabet_target #32009\n",
      "2021-01-15 05:04:24,516 : INFO : vocab #32006\n",
      "2021-01-15 05:04:24,523 : INFO : diff #set()\n",
      "2021-01-15 05:04:44,400 : INFO : alphabet #32006\n",
      "2021-01-15 05:04:54,455 : INFO : Computed distances or similarities ('263', 'sacp-python-common/test/python/third_party/test_bom_bd.py')[[1.3010020179601973, 0.4345932737975104], [0.9706193953752518, 0.029380605], [nan, nan], [3.0, 6.207411496248084, 6.298014978778129, 2.909396517469956, 3.298014978778129, 0.09060348253004502]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-15 05:04:54,459 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 05:04:54,460 : INFO : built Dictionary(133 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 243 corpus positions)\n",
      "2021-01-15 05:04:54,490 : INFO : token count processed\n",
      "2021-01-15 05:04:54,519 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 05:04:54,520 : INFO : frequencies processed\n",
      "2021-01-15 05:04:54,521 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 05:04:54,527 : INFO : token count processed\n",
      "2021-01-15 05:04:54,532 : INFO : alphabet_source #32006\n",
      "2021-01-15 05:04:54,536 : INFO : alphabet_target #32009\n",
      "2021-01-15 05:04:54,537 : INFO : vocab #32006\n",
      "2021-01-15 05:04:54,544 : INFO : diff #set()\n",
      "2021-01-15 05:05:14,489 : INFO : alphabet #32006\n",
      "2021-01-15 05:05:24,542 : INFO : Computed distances or similarities ('263', 'sacp-python-common/test/python/third_party/test_corona_lite.py')[[1.300974177062332, 0.43459853220808686], [0.9614968001842499, 0.0385032], [nan, nan], [3.0, 6.5805228788529595, 6.695829831645147, 2.884693047207813, 3.6958298316451472, 0.11530695279218772]]\n",
      "2021-01-15 05:05:24,546 : INFO : Removed 0 and 4 OOV words from document 1 and 2 (respectively).\n",
      "2021-01-15 05:05:24,547 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 05:05:24,552 : INFO : built Dictionary(214 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 1204 corpus positions)\n",
      "2021-01-15 05:05:24,631 : INFO : token count processed\n",
      "2021-01-15 05:05:24,711 : INFO : frequencies processed\n",
      "2021-01-15 05:05:34,642 : INFO : scalar_distribution processed\n",
      "2021-01-15 05:05:34,643 : INFO : entropies processed\n",
      "2021-01-15 05:05:34,644 : INFO : extropies processed\n",
      "2021-01-15 05:05:34,655 : INFO : token count processed\n",
      "2021-01-15 05:05:34,660 : INFO : alphabet_source #32006\n",
      "2021-01-15 05:05:34,664 : INFO : alphabet_target #32009\n",
      "2021-01-15 05:05:34,665 : INFO : vocab #32006\n",
      "2021-01-15 05:05:34,672 : INFO : diff #set()\n",
      "2021-01-15 05:05:55,082 : INFO : alphabet #32006\n",
      "2021-01-15 05:06:05,020 : INFO : Computed distances or similarities ('263', 'sacp-python-common/test/python/third_party/test_corona_lite_func.py')[[1.311967367041911, 0.4325320565745997], [0.9832157846540213, 0.016784215], [1.0, 1.0], [3.0, 6.422089779976135, 6.4788900884516725, 2.9431996915244625, 3.4788900884516725, 0.05680030847553752]]\n",
      "2021-01-15 05:06:05,025 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 05:06:05,027 : INFO : built Dictionary(228 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 1486 corpus positions)\n",
      "2021-01-15 05:06:05,085 : INFO : token count processed\n",
      "2021-01-15 05:06:05,120 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 05:06:05,121 : INFO : frequencies processed\n",
      "2021-01-15 05:06:05,122 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 05:06:05,128 : INFO : token count processed\n",
      "2021-01-15 05:06:05,133 : INFO : alphabet_source #32006\n",
      "2021-01-15 05:06:05,138 : INFO : alphabet_target #32009\n",
      "2021-01-15 05:06:05,139 : INFO : vocab #32006\n",
      "2021-01-15 05:06:05,145 : INFO : diff #set()\n",
      "2021-01-15 05:06:25,262 : INFO : alphabet #32006\n",
      "2021-01-15 05:06:35,282 : INFO : Computed distances or similarities ('263', 'sacp-python-common/test/python/third_party/test_cve.py')[[1.304501959217262, 0.4339332392408362], [0.9754047207534313, 0.02459528], [nan, nan], [3.0, 6.485445644653597, 6.6893613228791935, 2.796084321774403, 3.6893613228791935, 0.20391567822559686]]\n",
      "2021-01-15 05:06:35,286 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-01-15 05:06:35,288 : INFO : built Dictionary(203 unique tokens: ['ation', '▁divided', '▁integ', '▁into', '▁jobs']...) from 2 documents (total 1372 corpus positions)\n",
      "2021-01-15 05:06:35,337 : INFO : token count processed\n",
      "2021-01-15 05:06:35,364 : INFO : ---------------> NO SHARED INFORMATION <-------------------------\n",
      "2021-01-15 05:06:35,365 : INFO : frequencies processed\n",
      "2021-01-15 05:06:35,366 : INFO : FREQUENCIES NOT COMPUTED!!!<--------------\n",
      "2021-01-15 05:06:35,371 : INFO : token count processed\n",
      "2021-01-15 05:06:35,376 : INFO : alphabet_source #32006\n",
      "2021-01-15 05:06:35,380 : INFO : alphabet_target #32009\n",
      "2021-01-15 05:06:35,381 : INFO : vocab #32006\n",
      "2021-01-15 05:06:35,387 : INFO : diff #set()\n",
      "2021-01-15 05:06:55,205 : INFO : alphabet #32006\n"
     ]
    }
   ],
   "source": [
    "#[step 2]NonGroundTruth Computation\n",
    "metric_list = [DistanceMetric.WMD,DistanceMetric.SCM,EntropyMetric.MSI_I,EntropyMetric.MI]\n",
    "#metric_list = [EntropyMetric.MSI_I,EntropyMetric.MI]\n",
    "word2vec.ComputeDistanceArtifacts( sampling=False, samples = 100, metric_list = metric_list )\n",
    "word2vec.df_nonground_link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_nonground_link.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_nonground_link['Target'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 3]Saving Non-GroundTruth Links\n",
    "word2vec.SaveLinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_nonglinks = ds.mining.ir.LoadLinks(timestamp=1610135018.858762, params=parameters, logging=logging)\n",
    "df_nonglinks.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_nonground_link = df_nonglinks # Only to load links from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 4]GroundTruthMatching Testing\n",
    "#TODO change the path for a param\n",
    "path_to_ground_truth =  parameters['path_mappings']\n",
    "word2vec.MatchWithGroundTruth(path_to_ground_truth, semeru_format=True)\n",
    "word2vec.df_ground_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.df_ground_link[word2vec.df_ground_link ['Linked?']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Only SACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 4.1]GroundTruthMatching Testing For CISCO Mappings\n",
    "word2vec.MatchWithGroundTruth(from_mappings=True)\n",
    "word2vec.df_ground_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[optional]GroundTruth Direct Processing\n",
    "ground_links = word2vec.ground_truth_processing(path_to_ground_truth)\n",
    "ground_links # A tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[step 5]Saving GroundTruth Links\n",
    "word2vec.SaveLinks(grtruth = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Non-GroundTruth Links (change the timestamp with the assigned in the previous step)\n",
    "df_glinks = ds.mining.ir.LoadLinks(timestamp=1610080791.954672, params=parameters,grtruth = True, logging=logging)\n",
    "df_glinks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_build_docs #<-------- [Activate when stable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
